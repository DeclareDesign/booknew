<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 20 Research Design Lifecycle | Research Design: Declare, Diagnose, Redesign</title>
<meta name="author" content="Graeme Blair, Jasper Cooper, Alexander Coppock, and Macartan Humphreys">
<!-- CSS --><!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.1.9000/tabs.js"></script><script src="libs/bs3compat-0.2.1.9000/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://hypothes.is/embed.js" async></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Research Design: Declare, Diagnose, Redesign</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li class="book-part">Introduction</li>
<li><a class="" href="preamble.html"><span class="header-section-number">1</span> Preamble</a></li>
<li><a class="" href="improving-research-designs.html"><span class="header-section-number">2</span> Improving research designs</a></li>
<li><a class="" href="software-primer.html"><span class="header-section-number">3</span> Software primer</a></li>
<li><a class="" href="part-i-exercises.html"><span class="header-section-number">4</span> Part I Exercises</a></li>
<li class="book-part">Declaration, Diagnosis, Redesign</li>
<li><a class="" href="formalizing-mida.html"><span class="header-section-number">5</span> Formalizing MIDA</a></li>
<li><a class="" href="specifying-the-model.html"><span class="header-section-number">6</span> Specifying the model</a></li>
<li><a class="" href="defining-the-inquiry.html"><span class="header-section-number">7</span> Defining the inquiry</a></li>
<li><a class="" href="crafting-a-data-strategy.html"><span class="header-section-number">8</span> Crafting a data strategy</a></li>
<li><a class="" href="choosing-an-answer-strategy.html"><span class="header-section-number">9</span> Choosing an answer strategy</a></li>
<li><a class="" href="p2diagnosis.html"><span class="header-section-number">10</span> Diagnosis</a></li>
<li><a class="" href="redesign-1.html"><span class="header-section-number">11</span> Redesign</a></li>
<li><a class="" href="part-ii-exercises.html"><span class="header-section-number">12</span> Part II Exercises</a></li>
<li class="book-part">Design Library</li>
<li><a class="" href="design-library.html"><span class="header-section-number">13</span> Design Library</a></li>
<li><a class="" href="observational-designs-for-descriptive-inference.html"><span class="header-section-number">14</span> Observational designs for descriptive inference</a></li>
<li><a class="" href="experimental-designs-for-descriptive-inference.html"><span class="header-section-number">15</span> Experimental designs for descriptive inference</a></li>
<li><a class="" href="observational-designs-for-causal-inference.html"><span class="header-section-number">16</span> Observational designs for causal inference</a></li>
<li><a class="" href="experimental-designs-for-causal-inference.html"><span class="header-section-number">17</span> Experimental designs for causal inference</a></li>
<li><a class="" href="multi-study-designs.html"><span class="header-section-number">18</span> Multi-study designs</a></li>
<li><a class="" href="part-iii-exercises.html"><span class="header-section-number">19</span> Part III Exercises</a></li>
<li class="book-part">Research Design Lifecycle</li>
<li><a class="active" href="research-design-lifecycle.html"><span class="header-section-number">20</span> Research Design Lifecycle</a></li>
<li><a class="" href="part-iv-exercises.html"><span class="header-section-number">21</span> Part IV Exercises</a></li>
<li class="book-part">Epilogue</li>
<li><a class="" href="epilogue.html"><span class="header-section-number">22</span> Epilogue</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="glossary.html"><span class="header-section-number">23</span> Glossary</a></li>
<li><a class="" href="references-4.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="research-design-lifecycle" class="section level1">
<h1>
<span class="header-section-number">20</span> Research Design Lifecycle<a class="anchor" aria-label="anchor" href="#research-design-lifecycle"><i class="fas fa-link"></i></a>
</h1>
<!-- make sure to rename the section title below -->
<p>Empirical results are what we think we now know as a result of conducting a study. Research design is why we think we know it. The set of reasons why a study means what we think it means – the research design itself – is of central importance throughout the research design lifecycle. The process begins with a research idea and continues through the many phases of implementation, to the writing and publishing of the piece, and beyond, to the integration of the acquired knowledge into our collective scientific understanding of the world. At each stage of this process, your research design – your specification of <span class="math inline">\(M\)</span>, <span class="math inline">\(I\)</span>, <span class="math inline">\(D\)</span>, and <span class="math inline">\(A\)</span> – shapes your own choices as well as how others will learn from your work.</p>
<p>This part of the book works through the discrete stages of the lifecycle. While it is presented in a linear fashion, the stages are all deeply entwined by their common connection to MIDA. For example, we show how many disputes among scholars about the proper interpretation come down to differing understandings of some part of M, I, D, or A. If your preanalysis plan is sufficiently precise about your beliefs about these features of your design, then the disputes can be specified more precisely, the better to resolve them.</p>
<p>Not every research project will explicitly feature all of these stages. For example, prospective research designs like experiments and surveys often included pilot studies to learn about important unknown features of <span class="math inline">\(M\)</span> before implementing the full studies. Retrospective studies, like textual analyses of speeches delivered to Parliament, might not.</p>

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
<!-- start post here, do not edit above -->
<div id="p4planning" class="section level2">
<h2>
<span class="header-section-number">20.1</span> Planning<a class="anchor" aria-label="anchor" href="#p4planning"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<p>In many research communities, it is becoming standard practice to publicly register a pre-analysis plan (PAP) prior to the implementation of some or all of the Data strategy. PAPs serve many functions, but most importantly, they clarify which design choices were made before data collection and which were made afterward. Sometimes – perhaps every time! – we conduct a research study, aspects of <span class="math inline">\(M\)</span>, <span class="math inline">\(I\)</span>, <span class="math inline">\(D\)</span>, and <span class="math inline">\(A\)</span> shift along the way. A concern is that they shift in ways that invalidate the apparent conclusions of the study. For example, “<span class="math inline">\(p\)</span>-hacking” is the shady practice of trying out many regression specifications until the <span class="math inline">\(p\)</span>-value associated with an important test attains statistical significance. PAPs help researchers to credibly communicate to skeptics <em>when</em> design decisions were made: if the regression specification was detailed in a PAP posted before any data were collected, the test cannot be the result of a <span class="math inline">\(p\)</span>-hack.</p>
<p>PAPs are sometimes misinterpreted as a binding commitment to report all pre-registered analyses and nothing but. This view is unrealistic and unnecessarily rigid. While we think that researchers should report all pre-registered analyses <em>somewhere</em> (see Section <a href="research-design-lifecycle.html#p4populatedpap">20.10</a> on “populated PAPs”), study write-ups inevitably deviate in some way from the PAP – and that’s a good thing. Researchers learn more by conducting research; this learning can and should be reflected in the finalized Answer strategy. Even if not binding, the main point of publicly posting pre-analysis plan is to communicate at what stage in the research process choices were made.</p>
<p>Our hunch is that the main consequence of actually writing a PAP is improvement to the research design itself. Just like research design declaration forces us to think through the details of our model, inquiry, data strategy, and answer strategy, describing those choices in a publicly-posted document surely causes deeper reflection about the design. In this way, the main audiencce for a PAP is the study authors themselves.</p>
<p>What belongs in a PAP? Thus far, recommendations for the set of decisions that should be specified in a PAP remain remarkably unclear and inconsistent across research communities. PAP templates and checklists are proliferating, and the number of items they suggest range from nine to sixty. PAPs themselves are becoming longer and more detailed, with some in the American Economic Association (AEA) and Evidence in Governance and Politics (EGAP) study registries reaching hundreds of pages as researchers seek to be ever more comprehensive. Some registries emphasize the registration of the hypotheses to be tested, while others emphasize the registration of the tests that will used. We read many PAPs – it is often hard to assess whether these detailed plans actually contain the key analytically-relevant details.</p>
<p>Our view is, minimally, a PAP should include a design declaration. A good deal of the discussion of what goes in a PAP centers on the answer strategy <span class="math inline">\(A\)</span> – what estimator to use, what covariates to condition on, what subsets of the data to include. But of course we also need to know the details of <span class="math inline">\(D\)</span> – how units were sampled, how treatments were assigned, how the outcomes will be measured. We need to know about <span class="math inline">\(I\)</span> because we need to know what the target of inference is<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;A major concern in medical trials is “outcome switching,” wherein the eventual report focuses on different health outcomes that originally intended. When we switch outcomes, we switch inquiries!&lt;/p&gt;"><sup>23</sup></a>. We need enough of <span class="math inline">\(M\)</span> to describe <span class="math inline">\(I\)</span> in sufficient detail. In short, a design declaration is what belongs in a PAP, because a design declaration specifies all of the analytically-relevent design decisions.</p>
<p>In addition to a design declaration, a PAP should include mock analyses conducted on simulated data. If the design declaration is done formally in code, creating simulated data that resemble the eventual realized data is quite straightforward. We think researchers should run their Answer strategy on the mock data, creating mock figures and tables that will eventually be made with real data. In our experience, <em>this</em> is the step that really causes researchers to think hard about all aspects of their design.</p>
<p>Strictly speaking, preanalysis plans should include design declaration, but they do not <em>require</em> design diagnosis. But since the design that is finally settled on as the design to be implemented is usually chosen as the result of a diagnosis, it can be informative to describe, in a preanalysis plan, the reasons why the particular design was chosen. For this reason, a PAP might include estimates of diagnosands like power, rmse, or bias. If a researcher writes in a PAP that the power to detect a very small effect is large, then if the study comes back null, the eventual writeup can much more credibly rule out “low power” as an explanation for the null. Moreover, ex ante design diagnosis communicates the assumptions under which they thought the design was a good one before they ran the study. These reasons are often the basis on which we convince skeptics of the value of the design, so writing them down <em>before</em> the results are known increases the faith we put in them.</p>
<div id="example-21" class="section level3">
<h3>
<span class="header-section-number">20.1.1</span> Example<a class="anchor" aria-label="anchor" href="#example-21"><i class="fas fa-link"></i></a>
</h3>
<p>In this section, we provide an example of how to supplement a PAP with design declaration. We follow the actual PAP for <span class="citation">Bonilla and Tillery (<a href="references-4.html#ref-bonilla_tillery_2020">2020</a>)</span>, which was posted to the As Predicted registry here: <a href="https://aspredicted.org/q56qq.pdf" class="uri">https://aspredicted.org/q56qq.pdf</a>. The goal of the study is to estimate the causal effects of alternative framings of the Black Lives Matter (BLM) movement on support for the movement among Black Americans overall as well as among subsets of the Black community. These study authors are models of research transparency: they prominently link to the PAP in the published article, they conduct no non-preregistered analyses except those requested during the review process, and their replication archive includes all materials required to confirm their analyses, all of which we were able to reproduce exactly with minimal effort. Our goal with this section is to show how design declaration can supplement and complement existing planning practices.</p>
<div id="model-1" class="section level4">
<h4>
<span class="header-section-number">20.1.1.1</span> Model<a class="anchor" aria-label="anchor" href="#model-1"><i class="fas fa-link"></i></a>
</h4>
<p>The authors write in their PAP:</p>
<blockquote>
<p>We hypothesize that: H1: Black Nationalist frames of the BLM movement will increase perceived effectiveness of BLM among African American test subjects. H2: Feminist frames of the BLM movement will increase perceived effectiveness of BLM among African American women, but decrease perceived effectiveness in male subjects. H3: LGBTQ and Intersectional frames of the BLM movement will have no effect (or a demobilizing effect) on the perceived effectiveness of BLM African American subjects.</p>
</blockquote>
<p>These hypotheses reflect a model of coalition politics that emphasizes the tensions induced by overlapping identities. Framing the BLM movement as feminist or pro-LGBTQ may increase support among Black women or Black LGBTQ identifiers, but that increase may come at the expense of support among Black men or Blacks who do not identify as LGBTQ. Similarly, this model predicts that subjects with stronger attachment to their Black identity will have a larger response to a Black nationalist framing of BLM than those with weaker attachements.</p>
<p>The model also includes beliefs about the distributions of gender, LGBTQ status, and Black identity strength. In the Data strategy, Black identity will be measured with the standard linked fate measure. Other background characteristics that may be correlated with support for BLM include age, religiosity, income, eductation, and familiarity with the movement, so these are included in the model as well.</p>
<p>The focus of the study will be on the causal effects of the nationalism, feminism, and intersectional frames relative to a general description of the Black Lives Matter movement. Model beliefs about treatment effect heterogeneity are embedded in the <code>declare_potential_outcomes</code> call. The effect of the nationalism treatment is hypothesized to be stronger, the greater subjects’ sense of linked fate; the effect of the feminism treatment should be negative for men but postive for women; the effect of the intersectionality treatment should be positive for LGBTQ identifies, but negative for non-identifiers.</p>
<div class="sourceCode" id="cb160"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">model</span> <span class="op">&lt;-</span> 
  <span class="fu">declare_population</span><span class="op">(</span>
    N <span class="op">=</span> <span class="fl">800</span>,
    female <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">N</span>, <span class="fl">1</span>, prob <span class="op">=</span> <span class="fl">0.51</span><span class="op">)</span>,
    lgbtq <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">N</span>, <span class="fl">1</span>, prob <span class="op">=</span> <span class="fl">0.05</span><span class="op">)</span>,
    linked_fate <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, <span class="va">N</span>, replace <span class="op">=</span> <span class="cn">TRUE</span>, prob <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.05</span>, <span class="fl">0.05</span>, <span class="fl">0.15</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span><span class="op">)</span><span class="op">)</span>,
    age <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fl">18</span><span class="op">:</span><span class="fl">80</span>, <span class="va">N</span>, replace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,
    religiosity <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">6</span>, <span class="va">N</span>, replace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,
    income <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">12</span>, <span class="va">N</span>, replace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,
    college <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">N</span>, <span class="fl">1</span>, prob <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span>,
    blm_familiarity <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">4</span>, <span class="va">N</span>, replace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,
    U <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span>,
    blm_support_latent <span class="op">=</span> <span class="fu">rescale</span><span class="op">(</span>
      <span class="va">U</span> <span class="op">+</span> <span class="fl">0.1</span> <span class="op">*</span> <span class="va">blm_familiarity</span> <span class="op">+</span> <span class="fl">0.45</span> <span class="op">*</span> <span class="va">linked_fate</span> <span class="op">+</span> <span class="fl">0.001</span> <span class="op">*</span> <span class="va">age</span> <span class="op">+</span> 
        <span class="fl">0.25</span> <span class="op">*</span> <span class="va">lgbtq</span> <span class="op">+</span> <span class="fl">0.01</span> <span class="op">*</span> <span class="va">income</span> <span class="op">+</span> <span class="fl">0.1</span> <span class="op">*</span> <span class="va">college</span> <span class="op">+</span> <span class="op">-</span><span class="fl">0.1</span> <span class="op">*</span> <span class="va">religiosity</span><span class="op">)</span>
  <span class="op">)</span> <span class="op">+</span> 
  <span class="fu">declare_potential_outcomes</span><span class="op">(</span>
    blm_support_Z_general <span class="op">=</span> 
      <span class="fu">likert_cut</span><span class="op">(</span><span class="va">blm_support_latent</span><span class="op">)</span>,
    blm_support_Z_nationalism <span class="op">=</span> 
      <span class="fu">likert_cut</span><span class="op">(</span><span class="va">blm_support_latent</span> <span class="op">+</span> <span class="fl">0.01</span> <span class="op">+</span> <span class="fl">0.01</span> <span class="op">*</span> <span class="va">linked_fate</span> <span class="op">+</span> <span class="fl">0.01</span> <span class="op">*</span> <span class="va">blm_familiarity</span><span class="op">)</span>,
    blm_support_Z_feminism <span class="op">=</span> 
      <span class="fu">likert_cut</span><span class="op">(</span><span class="va">blm_support_latent</span> <span class="op">-</span> <span class="fl">0.02</span> <span class="op">+</span> <span class="fl">0.07</span> <span class="op">*</span> <span class="va">female</span> <span class="op">+</span> <span class="fl">0.01</span> <span class="op">*</span> <span class="va">blm_familiarity</span><span class="op">)</span>,
    blm_support_Z_intersectional <span class="op">=</span> 
      <span class="fu">likert_cut</span><span class="op">(</span><span class="va">blm_support_latent</span>  <span class="op">-</span> <span class="fl">0.05</span> <span class="op">+</span> <span class="fl">0.15</span> <span class="op">*</span> <span class="va">lgbtq</span> <span class="op">+</span> <span class="fl">0.01</span> <span class="op">*</span> <span class="va">blm_familiarity</span><span class="op">)</span>
  <span class="op">)</span></code></pre></div>
</div>
<div id="inquiry-1" class="section level4">
<h4>
<span class="header-section-number">20.1.1.2</span> Inquiry<a class="anchor" aria-label="anchor" href="#inquiry-1"><i class="fas fa-link"></i></a>
</h4>
<p>The inquiries for this study naturally include the average affects of all three treatments relative to the “general” framing, as well as the differences in average effects for subgroups. When describing their planned analyses, the authors write:</p>
<blockquote>
<p>We will also look at differences in responses between those indicating a pre-treatment familiarity BLM (4-Extensive knowledge to 1-Never heard of BLM), gender (particularly on the Feminist treatment), linked fate (particularly on the Nationalist treatment), and LGBT+ affiliation (particularly on the LGBT+ treatment), though we are not necessarily expecting these moderations to have a strong effect because samples may lack adequate representation.</p>
</blockquote>
<p>In the code below, we specify how each treatment effect changes with its corresponding covariate <span class="math inline">\(X\)</span> with <span class="math inline">\(\frac{cov(\tau_i, X)}{X}\)</span>, which is identical to the difference-in-difference for the binary covariates (<code>female</code> and <code>lgbtq</code>) and is the slope of the best linear predictor of how the effect changes over the range of <code>linked_fate</code>, and <code>blm_familiarity</code> which we are treating as quasi-continuous here.</p>
<div class="sourceCode" id="cb161"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">inquiry</span> <span class="op">&lt;-</span>  
  <span class="fu">declare_estimands</span><span class="op">(</span>
    
    <span class="co"># Average effects</span>
    ATE_nationalism <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">blm_support_Z_nationalism</span> <span class="op">-</span> <span class="va">blm_support_Z_general</span><span class="op">)</span>,
    ATE_feminism <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">blm_support_Z_feminism</span> <span class="op">-</span> <span class="va">blm_support_Z_general</span><span class="op">)</span>,
    ATE_intersectional <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">blm_support_Z_intersectional</span> <span class="op">-</span> <span class="va">blm_support_Z_general</span><span class="op">)</span>,
    
    <span class="co"># Overall heterogeneity w.r.t. blm_familiarity</span>
    DID_nationalism_familiarity <span class="op">=</span> 
      <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cov</a></span><span class="op">(</span><span class="va">blm_support_Z_nationalism</span> <span class="op">-</span> <span class="va">blm_support_Z_general</span>, <span class="va">blm_familiarity</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">blm_familiarity</span><span class="op">)</span>,
    DID_feminism_familiarity <span class="op">=</span> 
      <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cov</a></span><span class="op">(</span><span class="va">blm_support_Z_feminism</span> <span class="op">-</span> <span class="va">blm_support_Z_general</span>, <span class="va">blm_familiarity</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">blm_familiarity</span><span class="op">)</span>,
    DID_intersectional_familiarity <span class="op">=</span> 
      <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cov</a></span><span class="op">(</span><span class="va">blm_support_Z_intersectional</span> <span class="op">-</span> <span class="va">blm_support_Z_general</span>, <span class="va">blm_familiarity</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">blm_familiarity</span><span class="op">)</span>,
    
    <span class="co"># Treatment-specific heterogeneity</span>
    DID_nationalism_linked_fate <span class="op">=</span> 
      <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cov</a></span><span class="op">(</span><span class="va">blm_support_Z_nationalism</span> <span class="op">-</span> <span class="va">blm_support_Z_general</span>, <span class="va">linked_fate</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">linked_fate</span><span class="op">)</span>,
    DID_feminism_gender <span class="op">=</span> 
      <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cov</a></span><span class="op">(</span><span class="va">blm_support_Z_feminism</span> <span class="op">-</span> <span class="va">blm_support_Z_general</span>, <span class="va">female</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">female</span><span class="op">)</span>,
    DID_intersectional_lgbtq <span class="op">=</span> 
      <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cov</a></span><span class="op">(</span><span class="va">blm_support_Z_intersectional</span> <span class="op">-</span> <span class="va">blm_support_Z_general</span>, <span class="va">lgbtq</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">lgbtq</span><span class="op">)</span>
  <span class="op">)</span></code></pre></div>
</div>
<div id="data-strategy-1" class="section level4">
<h4>
<span class="header-section-number">20.1.1.3</span> Data strategy<a class="anchor" aria-label="anchor" href="#data-strategy-1"><i class="fas fa-link"></i></a>
</h4>
<p>The subjects for this study are 800 Black Americans recruited by the survey firm Qualtrics using a quota sampling procedure. We elide this sampling step in our declaration – the 800 subjects are described by the <code>declare_population</code> call. After subjects’ background charcteristics are measured, they were assigned to one of four treatment conditions. Since the survey was conducted on Qualtrics, we assume that the authors used the built-in randomization tools, which typically use simple (Bernoulli) random assignment.</p>
<div class="sourceCode" id="cb162"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">data_strategy</span> <span class="op">&lt;-</span> 
  <span class="fu">declare_assignment</span><span class="op">(</span>conditions <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"general"</span>, <span class="st">"nationalism"</span>, <span class="st">"feminism"</span>, <span class="st">"intersectional"</span><span class="op">)</span>, simple <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu">declare_reveal</span><span class="op">(</span><span class="va">blm_support</span>, <span class="va">Z</span><span class="op">)</span> </code></pre></div>
</div>
<div id="answer-strategy-1" class="section level4">
<h4>
<span class="header-section-number">20.1.1.4</span> Answer strategy<a class="anchor" aria-label="anchor" href="#answer-strategy-1"><i class="fas fa-link"></i></a>
</h4>
<p>The authors write:</p>
<blockquote>
<p>We will run an OLS regression predicting the support for, effectiveness of, and trust in BLM on each treatment condition. […] We will also look at differences in responses between those indicating a pre-treatment familiarity BLM (4-Extensive knowledge to 1-Never heard of BLM), gender (particularly on the Feminist treatment), linked fate (particularly on the Nationalist treatment), and LGBT+ affiliation (particularly on the LGBT+ treatment), though we are not necessarily expecting these moderations to have a strong effect because samples may lack adequate representation. We plan to conduct analyses without controls. As we will check for between group balance, we may also run OLS analyses with demographic controls (age, linked fate, gender, sexual orientation, religiosity, income, education, and ethnic or multi-racial backgrounds), and will report differences in OLS results.</p>
</blockquote>
<p>In DeclareDesign, this corresponds to five estimators, with two shooting at the ATEs and three shooting at the differences-in-CATEs. We use OLS for all five – the majority of the code is bookkeeping to ensure we match the right regression coefficient with the appropriate estimand.</p>
<div class="sourceCode" id="cb163"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">answer_strategy</span> <span class="op">&lt;-</span>
  <span class="fu">declare_estimator</span><span class="op">(</span>
    <span class="va">blm_support</span> <span class="op">~</span> <span class="va">Z</span>,
    term <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Znationalism"</span>, <span class="st">"Zfeminism"</span>, <span class="st">"Zintersectional"</span><span class="op">)</span>,
    model <span class="op">=</span> <span class="va">lm_robust</span>,
    estimand <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"ATE_nationalism"</span>, <span class="st">"ATE_feminism"</span>, <span class="st">"ATE_intersectional"</span><span class="op">)</span>,
    label <span class="op">=</span> <span class="st">"OLS"</span>
  <span class="op">)</span> <span class="op">+</span>
  <span class="fu">declare_estimator</span><span class="op">(</span>
    <span class="va">blm_support</span> <span class="op">~</span> <span class="va">Z</span> <span class="op">+</span> <span class="va">age</span> <span class="op">+</span> <span class="va">female</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="va">linked_fate</span><span class="op">)</span> <span class="op">+</span> <span class="va">lgbtq</span>,
    term <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Znationalism"</span>, <span class="st">"Zfeminism"</span>, <span class="st">"Zintersectional"</span><span class="op">)</span>,
    estimand <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"ATE_nationalism"</span>, <span class="st">"ATE_feminism"</span>, <span class="st">"ATE_intersectional"</span><span class="op">)</span>,
    model <span class="op">=</span> <span class="va">lm_robust</span>,
    label <span class="op">=</span> <span class="st">"OLS with controls"</span>
  <span class="op">)</span> <span class="op">+</span>
    <span class="fu">declare_estimator</span><span class="op">(</span>
    <span class="va">blm_support</span> <span class="op">~</span> <span class="va">Z</span><span class="op">*</span><span class="va">blm_familiarity</span>,
    term <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Znationalism:blm_familiarity"</span>, <span class="st">"Zfeminism:blm_familiarity"</span>, <span class="st">"Zintersectional:blm_familiarity"</span><span class="op">)</span>,
    model <span class="op">=</span> <span class="va">lm_robust</span>,
    estimand <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"DID_nationalism_familiarity"</span>, <span class="st">"DID_feminism_familiarity"</span>, <span class="st">"DID_intersectional_familiarity"</span><span class="op">)</span>,
    label <span class="op">=</span> <span class="st">"DID_familiarity"</span>
  <span class="op">)</span> <span class="op">+</span>
  <span class="fu">declare_estimator</span><span class="op">(</span>
    <span class="va">blm_support</span> <span class="op">~</span> <span class="va">Z</span> <span class="op">*</span> <span class="va">linked_fate</span>,
    term <span class="op">=</span> <span class="st">"Zfeminism:linked_fate"</span>,
    model <span class="op">=</span> <span class="va">lm_robust</span>,
    estimand <span class="op">=</span> <span class="st">"DID_nationalism_linked_fate"</span>,
    label <span class="op">=</span> <span class="st">"DID_nationalism_linked_fate"</span>
  <span class="op">)</span> <span class="op">+</span>
  <span class="fu">declare_estimator</span><span class="op">(</span>
    <span class="va">blm_support</span> <span class="op">~</span> <span class="va">Z</span> <span class="op">*</span> <span class="va">female</span>,
    term <span class="op">=</span> <span class="st">"Zfeminism:female"</span>,
    model <span class="op">=</span> <span class="va">lm_robust</span>,
    estimand <span class="op">=</span> <span class="st">"DID_feminism_gender"</span>,
    label <span class="op">=</span> <span class="st">"DID_feminism_gender"</span>
  <span class="op">)</span> <span class="op">+</span>
  <span class="fu">declare_estimator</span><span class="op">(</span>
    <span class="va">blm_support</span> <span class="op">~</span> <span class="va">Z</span> <span class="op">*</span> <span class="va">lgbtq</span>,
    term <span class="op">=</span> <span class="st">"Zintersectional:lgbtq"</span>,
    model <span class="op">=</span> <span class="va">lm_robust</span>,
    estimand <span class="op">=</span> <span class="st">"DID_intersectional_lgbtq"</span>,
    label <span class="op">=</span> <span class="st">"DID_intersectional_lgbtq"</span>
  <span class="op">)</span></code></pre></div>
</div>
<div id="mock-analysis" class="section level4">
<h4>
<span class="header-section-number">20.1.1.5</span> Mock analysis<a class="anchor" aria-label="anchor" href="#mock-analysis"><i class="fas fa-link"></i></a>
</h4>
<p>Putting it all together, we can declare the complete design and draw mock data from it.</p>
<div class="sourceCode" id="cb164"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">design</span> <span class="op">&lt;-</span> <span class="va">model</span> <span class="op">+</span> <span class="va">inquiry</span> <span class="op">+</span> <span class="va">data_strategy</span> <span class="op">+</span> <span class="va">answer_strategy</span>
<span class="va">mock_data</span> <span class="op">&lt;-</span> <span class="fu">draw_data</span><span class="op">(</span><span class="va">design</span><span class="op">)</span>
<span class="va">mock_data</span> <span class="op">%&gt;%</span> <span class="va">head</span> <span class="op">%&gt;%</span> <span class="fu">kable</span><span class="op">(</span>digits <span class="op">=</span> <span class="fl">3</span>, caption <span class="op">=</span> <span class="st">"Mock analysis from Bonilla and Tillery design."</span><span class="op">)</span></code></pre></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:unnamed-chunk-428">Table 20.1: </span>Mock analysis from Bonilla and Tillery design.
</caption>
<thead><tr>
<th style="text-align:left;">
ID
</th>
<th style="text-align:right;">
female
</th>
<th style="text-align:right;">
lgbtq
</th>
<th style="text-align:right;">
linked_fate
</th>
<th style="text-align:right;">
age
</th>
<th style="text-align:right;">
religiosity
</th>
<th style="text-align:right;">
income
</th>
<th style="text-align:right;">
college
</th>
<th style="text-align:right;">
blm_familiarity
</th>
<th style="text-align:right;">
U
</th>
<th style="text-align:right;">
blm_support_latent
</th>
<th style="text-align:right;">
blm_support_Z_general
</th>
<th style="text-align:right;">
blm_support_Z_nationalism
</th>
<th style="text-align:right;">
blm_support_Z_feminism
</th>
<th style="text-align:right;">
blm_support_Z_intersectional
</th>
<th style="text-align:left;">
Z
</th>
<th style="text-align:right;">
Z_cond_prob
</th>
<th style="text-align:right;">
blm_support
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
001
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
27
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0.976
</td>
<td style="text-align:right;">
0.758
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
nationalism
</td>
<td style="text-align:right;">
0.25
</td>
<td style="text-align:right;">
5
</td>
</tr>
<tr>
<td style="text-align:left;">
002
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
44
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.274
</td>
<td style="text-align:right;">
0.429
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
feminism
</td>
<td style="text-align:right;">
0.25
</td>
<td style="text-align:right;">
3
</td>
</tr>
<tr>
<td style="text-align:left;">
003
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
78
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
0.349
</td>
<td style="text-align:right;">
0.491
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
feminism
</td>
<td style="text-align:right;">
0.25
</td>
<td style="text-align:right;">
3
</td>
</tr>
<tr>
<td style="text-align:left;">
004
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
23
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.929
</td>
<td style="text-align:right;">
0.841
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:left;">
feminism
</td>
<td style="text-align:right;">
0.25
</td>
<td style="text-align:right;">
5
</td>
</tr>
<tr>
<td style="text-align:left;">
005
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
69
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0.351
</td>
<td style="text-align:right;">
0.540
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
feminism
</td>
<td style="text-align:right;">
0.25
</td>
<td style="text-align:right;">
3
</td>
</tr>
<tr>
<td style="text-align:left;">
006
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
44
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0.150
</td>
<td style="text-align:right;">
0.514
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
feminism
</td>
<td style="text-align:right;">
0.25
</td>
<td style="text-align:right;">
3
</td>
</tr>
</tbody>
</table></div>
<p>Table <a href="#tab:bonillatilleryregtable"><strong>??</strong></a> shows a mock analysis average effects (estimated with and without covariate adjustment) as well as the heterogeneous effects analyses with respect to the quasicontinuous moderators.</p>
<div class="inline-table"><table class="texreg" style="margin: 10px auto;border-collapse: collapse;border-spacing: 0px;caption-side: bottom;color: #000000;border-top: 2px solid #000000;">
<caption>
Mock regression table from Bonilla and Tillery design.
</caption>
<thead><tr>
<th style="padding-left: 5px;padding-right: 5px;">
 
</th>
<th style="padding-left: 5px;padding-right: 5px;">
Model 1
</th>
<th style="padding-left: 5px;padding-right: 5px;">
Model 2
</th>
<th style="padding-left: 5px;padding-right: 5px;">
Model 3
</th>
<th style="padding-left: 5px;padding-right: 5px;">
Model 4
</th>
</tr></thead>
<tbody>
<tr style="border-top: 1px solid #000000;">
<td style="padding-left: 5px;padding-right: 5px;">
(Intercept)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
3.667<sup>***</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
1.248<sup>***</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
1.578<sup>***</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
3.530<sup>***</sup>
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.055)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.100)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.127)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.142)
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
Znationalism
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.430<sup>***</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.360<sup>***</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.019
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.057
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.087)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.045)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.192)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.209)
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
Zfeminism
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.174<sup>*</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.161<sup>***</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.059
</td>
<td style="padding-left: 5px;padding-right: 5px;">
-0.190
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.082)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.046)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.207)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.200)
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
Zintersectional
</td>
<td style="padding-left: 5px;padding-right: 5px;">
-0.144
</td>
<td style="padding-left: 5px;padding-right: 5px;">
-0.163<sup>***</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
-0.326
</td>
<td style="padding-left: 5px;padding-right: 5px;">
-0.373
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.088)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.049)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.200)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.209)
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
female
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.104<sup>**</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.034)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
lgbtq
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.374<sup>***</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.100)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
age
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.000
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.001)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
religiosity
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
-0.141<sup>***</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.010)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
income
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.013<sup>**</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.005)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
college
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.158<sup>***</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.034)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
linked_fate
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.564<sup>***</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.523<sup>***</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.015)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.032)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
blm_familiarity
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.170<sup>***</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.055
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.015)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.054)
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
Znationalism:linked_fate
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.087
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.046)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
Zfeminism:linked_fate
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.032
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.051)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
Zintersectional:linked_fate
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.045
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.050)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
Znationalism:blm_familiarity
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.147
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.076)
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
Zfeminism:blm_familiarity
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.153<sup>*</sup>
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.075)
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
Zintersectional:blm_familiarity
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.089
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.080)
</td>
</tr>
<tr style="border-top: 1px solid #000000;">
<td style="padding-left: 5px;padding-right: 5px;">
R<sup>2</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.052
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.729
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.598
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.092
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
Adj. R<sup>2</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.049
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.726
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.594
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.084
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
Num. obs.
</td>
<td style="padding-left: 5px;padding-right: 5px;">
800
</td>
<td style="padding-left: 5px;padding-right: 5px;">
800
</td>
<td style="padding-left: 5px;padding-right: 5px;">
800
</td>
<td style="padding-left: 5px;padding-right: 5px;">
800
</td>
</tr>
<tr style="border-bottom: 2px solid #000000;">
<td style="padding-left: 5px;padding-right: 5px;">
RMSE
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.884
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.475
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.577
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.868
</td>
</tr>
</tbody>
<tfoot><tr>
<td style="font-size: 0.8em;" colspan="5">
<sup><em><strong></strong></em></sup>p &lt; 0.001; <sup></sup>p &lt; 0.01; <sup></sup>p &lt; 0.05
</td>
</tr></tfoot>
</table></div>
<div class="figure">
<span id="fig:bonillatilleryresultsfig"></span>
<img src="book_files/figure-html/bonillatilleryresultsfig-1.png" alt="Mock coefficient plot from Bonilla and Tillery design." width="100%"><p class="caption">
Figure 20.1: Mock coefficient plot from Bonilla and Tillery design.
</p>
</div>
</div>
<div id="design-diagnosis" class="section level4">
<h4>
<span class="header-section-number">20.1.1.6</span> Design diagnosis<a class="anchor" aria-label="anchor" href="#design-diagnosis"><i class="fas fa-link"></i></a>
</h4>
<p>Finally, while a design diagnosis is not a necessary component of a preanalysis plan, it can be useful to show readers why a particular design was chosen over others. This diagnosis shows that the design produces unbiased estimates of all estimands, but that we are better powered from some inquires than others (under the above assumptions about effect size, which were our own and not the original authors’.) We are well-powered for the average effects, and the power increases when we include covariate controls. The design is probably too small for most of the heterogeneous effect analyses, which is a point directly conceded in the authors’ original PAP.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:bonillatillerydiagnosis">Table 20.2: </span>Design diagnosis for Bonilla and Tillery design.
</caption>
<thead><tr>
<th style="text-align:left;">
Estimand
</th>
<th style="text-align:left;">
Estimator
</th>
<th style="text-align:right;">
Bias
</th>
<th style="text-align:right;">
Power
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
ATE feminism
</td>
<td style="text-align:left;">
OLS
</td>
<td style="text-align:right;">
-0.015
</td>
<td style="text-align:right;">
0.45
</td>
</tr>
<tr>
<td style="text-align:left;">
ATE feminism
</td>
<td style="text-align:left;">
OLS with controls
</td>
<td style="text-align:right;">
-0.001
</td>
<td style="text-align:right;">
0.81
</td>
</tr>
<tr>
<td style="text-align:left;">
ATE intersectional
</td>
<td style="text-align:left;">
OLS
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.13
</td>
</tr>
<tr>
<td style="text-align:left;">
ATE intersectional
</td>
<td style="text-align:left;">
OLS with controls
</td>
<td style="text-align:right;">
-0.002
</td>
<td style="text-align:right;">
0.33
</td>
</tr>
<tr>
<td style="text-align:left;">
ATE nationalism
</td>
<td style="text-align:left;">
OLS
</td>
<td style="text-align:right;">
-0.006
</td>
<td style="text-align:right;">
0.97
</td>
</tr>
<tr>
<td style="text-align:left;">
ATE nationalism
</td>
<td style="text-align:left;">
OLS with controls
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
1.00
</td>
</tr>
<tr>
<td style="text-align:left;">
DID feminism familiarity
</td>
<td style="text-align:left;">
DID familiarity
</td>
<td style="text-align:right;">
-0.002
</td>
<td style="text-align:right;">
0.05
</td>
</tr>
<tr>
<td style="text-align:left;">
DID feminism gender
</td>
<td style="text-align:left;">
DID feminism gender
</td>
<td style="text-align:right;">
0.006
</td>
<td style="text-align:right;">
0.43
</td>
</tr>
<tr>
<td style="text-align:left;">
DID intersectional familiarity
</td>
<td style="text-align:left;">
DID familiarity
</td>
<td style="text-align:right;">
-0.008
</td>
<td style="text-align:right;">
0.04
</td>
</tr>
<tr>
<td style="text-align:left;">
DID intersectional lgbtq
</td>
<td style="text-align:left;">
DID intersectional lgbtq
</td>
<td style="text-align:right;">
0.027
</td>
<td style="text-align:right;">
0.34
</td>
</tr>
<tr>
<td style="text-align:left;">
DID nationalism familiarity
</td>
<td style="text-align:left;">
DID familiarity
</td>
<td style="text-align:right;">
-0.019
</td>
<td style="text-align:right;">
0.08
</td>
</tr>
<tr>
<td style="text-align:left;">
DID nationalism linked fate
</td>
<td style="text-align:left;">
DID nationalism linked fate
</td>
<td style="text-align:right;">
-0.053
</td>
<td style="text-align:right;">
0.04
</td>
</tr>
</tbody>
</table></div>
<!-- ## Debates over the value of PAPs --><!-- PAPs are often described as a tool for tying researchers' hand and reducing "researcher degrees of freedom" to seek out congenial analyses. PAPs are sometimes criticized because (A) they don't *actually* constrain what researchers do and (B) they *shouldn't*.   --><!-- PAPs might not actually constrain researchers because most journals and pressess do not check against PAPs. As reviewers, we have sometimes requested that authors send in the PAPs referenced in their papers. In nearly every case, some analyses promised in the PAP were not included, even in an appendix. Some analyses that appear in the paper were not included in the PAP. As we discuss in greater detail in section XXX on reconciliation, current practice is clearly too causal when distinguishing which analyses were pre-specified and which were not. --><!-- Some critics of PAPs charge that we shouldn't pre-register our analysis plans because we should be open to discovery. We learn once we arrive in the field what the interesting questions are, so we shouldn't be constrained by what we thought would happen when sitting in our offices. Furthermore, sometimes people pre-register analyses that are biased, misspecified, or are otherwise inappropriate, so they shouldn't be required to present them. We agree with all the above points -- researchers should create post-implementation reports (CITE JENS in PA) that follow the PAP. Analyses over and above the PAP should simply be labeled as such. --><!-- Our take is that PAPs are helpful tools for researchers to plan research better and they are useful for clarifying what the researcher was thinking at each stage.  --><!-- ## Other benefits of PAPS --><!-- - involving partners: agreeing on the analysis procedure ex ante reduces ex post conflict --><!-- - frontloading research design decisions (get more specific, identify problems) --><!-- - Anticipating what might go wrong: attrition, noncompliance, study failure, missing covariate data, etc. --><!-- - Faithful reanalysis. Reanalysts lose a degree of freedom in determining what an author might have intended by a given analysis.  --><!-- - Easier design comparison. If a design is declared at the preanalysis plan phase, then it enables direct comparison with the design as implemented in the final write-up. Side-by-side comparison of the code neatly clarifies which design choices were made ex-ante and ex-post. Side-by-side comparison of the performance of a planned and implemented designs clarify under what conditions deviations from plans are defensible improvements. --><!-- - ethics (summarize and cite Jay's ideas here: http://www.jasonlyall.com/wp-content/uploads/2020/08/PreregisterYourEthics.pdf) --><!-- - ethical outcomes are potential outcomes, so we need to think about them ex ante not just on the basis of revealed outcomes --><!-- ### countering objections: --><!-- - Time-consuming. Yes but in our experience we only pay this cost for failed studies. Fur successful studies, nearly all work that goes in to the pap pays off in terms of higher qualtity design, literal words we already wrote, and written-in-advance analysis code. --><!-- - Won't stop determined cheaters. Yes -- remember that the goal is not to prevent fraud, it's to help *researchers* improve their designs. Science depends on trust. --><!-- - Replication is better (@Coffman2015). These are complements, not substitutes. --><!-- - I can't preregister what I will do because I don't know what I will find. That's fine too, just write down how you will go about "finding" things so we (and YOU) can understand your own process. --><!-- ### Other thoughts --><!-- - when is the right moment to write a pap? --><!-- - relationship to registered reports? --><!-- - SOPs --><!-- ### citations on paps --><p><strong>Further reading</strong>.</p>
<ul>
<li>
<span class="citation">Casey, Glennerster, and Miguel (<a href="references-4.html#ref-Casey2012">2012</a>)</span>: early entry</li>
<li>
<span class="citation">Olken (<a href="references-4.html#ref-Olken2015">2015</a>)</span>: halfway skeptical</li>
<li>
<span class="citation">Green and Lin (<a href="references-4.html#ref-Green2015">2016</a>)</span>: Standard operating procedures</li>
<li>
<span class="citation">Christensen and Miguel (<a href="references-4.html#ref-Christensen2018">2018</a>)</span>: review</li>
<li>
<span class="citation">Coffman and Niederle (<a href="references-4.html#ref-Coffman2015">2015</a>)</span>: a skeptical take (prefer replications).</li>
<li>
<span class="citation">Humphreys, Sierra, and Windt (<a href="references-4.html#ref-Humphreys2013a">2013</a>)</span>: nonbinding</li>
<li>
<span class="citation">Miguel et al. (<a href="references-4.html#ref-Miguel2014">2014</a>)</span>: distinguishes between disclosure and PAP</li>
<li>
<span class="citation">Ofosu and Posner (<a href="references-4.html#ref-Ofosu2020">2020</a>)</span>: apparently paps hinder publication?</li>
<li><span class="citation">Rennie (<a href="references-4.html#ref-Rennie2004">2004</a>)</span></li>
<li><span class="citation">Zarin and Tse (<a href="references-4.html#ref-Zarin2008">2008</a>)</span></li>
<li><span class="citation">Nosek et al. (<a href="references-4.html#ref-Nosek2015a">2015</a>)</span></li>
<li>
<span class="citation">Findley et al. (<a href="references-4.html#ref-Findley:2016">2016</a>)</span> on results-blind review</li>
</ul>
<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book--><!-- start post here, do not edit above -->
</div>
</div>
</div>
<div id="ethical-review" class="section level2">
<h2>
<span class="header-section-number">20.2</span> Ethical Review<a class="anchor" aria-label="anchor" href="#ethical-review"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<p>Ethical review by institutional review boards (IRBs) is required in many countries, and in some additional review is required by entities governing health and other specific types of research. Social scientists are also increasingly focused on meeting ethical standards that go beyond the requirements of these national laws. Ethical appendices are required by some journals describing protections for human subjects and many funding agencies now require defenses of the ethics of a research study before funds are disbursed.</p>
<p>Common ethical principles include respect for persons, beneficence (researchers must have the welfare of participants in mind in designing the research), informed consent by participants, and minimizing harm to participants and others.
<!-- Several parts of a research design are implicated in these principles: the data strategy, which affects who is selected to participate (thus who is affected directly and also indirectly by the research), what treatments are given to them and to whom, and what measures are collected from them; the answer strategy, which determines what information about participants is shared with the broader public; and even the inquiry, which affects the value of the research to participants and society. --></p>
<p>Ethical considerations extend well beyond the elements that are captured by a design declaration. For instance a declaration of analytic relevant components of a design may tell you little about the level of care and respect accorded to subjects. Nevertheless we think it useful to identify when design relevant features can be informative for ethical judgements.</p>
</div>
<div id="ethical-principle-as-diagnosands" class="section level2">
<h2>
<span class="header-section-number">20.3</span> Ethical principle as diagnosands<a class="anchor" aria-label="anchor" href="#ethical-principle-as-diagnosands"><i class="fas fa-link"></i></a>
</h2>
<p>Researchers can use the declare-diagnose-redesign process to help inform some ethical judgements. In particular, a diagnostic-statistic could be defined for each relevant ethical criteria. For example, a design can be scored based on the total cost to participants, how many participants were harmed (i.e., how many were retraumatized by being asked out past experience with violence), the average level of informed consent measured by a survey about comprehension of the study goals, or the risks of adverse events.</p>
<p>Consider two examples.</p>
<p><strong>Costs.</strong> A common concern is that measurement imposes cost on subjects, if only by wasting their time. Subjects’ time is a valuable resource – they often donate willingly to the scientific enterprise by taking a survey or similar. Sometimes their generosity is repaid with financial compensation for their time. Sometimes subjects are unknowing participants in a research study because obtaining informed consent would so distort their behavior as to hinder the ability of the researchers to study it.</p>
<p><strong>Risks not just realizations.</strong> More subtly, different realizations of the data from the same data strategy may also differ in their ethical status. As a result, the ethics of the study cannot only be determined by looking at what in fact happened during the study — how many participants were there and how many were treated, how many were harmed, and how many raised complaints. Instead, the ethical status of the project depends on judgments about <em>potential</em> harms and <em>potential</em> participants: not only what did happen, but what could have happened.</p>
<!-- tara's paper relevant here -->
<p>When the design is diagnosed, then diagnosands can be constructed that summarize the level of ethical encumberance across possible realizations of the design. The first way these ethical diagnosands can be used is to determine whether the study design meets a set of ethical thresholds. Is the probability of harm minimal enough? Is the average level of informed consent sufficient? Given that these characteristics vary across designs and across realizations of the same design, writing down concretely both what the measure of the ethical characteristic is and what the threshold is for the design to be ethical can help structure thinking. (These diagnoses and threshold determinations can also be shared in ethical writeups of the design.)</p>
<p>Among ethical designs, researchers must select a single design to implement. Often, once an ethical threshold is met, we select among feasible designs based on research design criteria such as statistical power and bias. Instead, we should continue to assess ethical considerations alongside the quality of the research design. Among ethical designs, there are still often tradeoffs between how much time is asked of subjects and the risk of harm. We should select the designs that weight these considerations (perhaps highly!) against the power of our designs. To do so, we can simply continue to include the diagnosands related to ethical criteria in our diagnoses and the redesign of studies.</p>
<p>A difficult challenge in this process is that in order to weigh ethical criteria against research design criteria such as power and cost, we must put be able to measure the two on a common scale. We must be able to think about the value to society of the research to weigh against the risks to participants and others. Similarly, we must be able to weigh more precise estimates of the same question against ethical considerations that also change based on the number of units and the proportion treated among other design features. By moving forward with research we must implicitly weigh these considerations. In IRB applications, we are often more directly asked to weigh the costs to subjects against the benefits of the research <em>to subjects</em> as well as to society as a whole.</p>
</div>
<div id="illustration-estimating-expected-costs-and-expected-learning" class="section level2">
<h2>
<span class="header-section-number">20.4</span> Illustration: Estimating expected costs and expected learning<a class="anchor" aria-label="anchor" href="#illustration-estimating-expected-costs-and-expected-learning"><i class="fas fa-link"></i></a>
</h2>
<!-- I would prefer to have an example where there is consent but people are willing to tkae part because they value the outcome -->
<p>We illustrate how researchers can weigh the tradeoffs between the value of research and its ethical costs with an audit study of discrimination in government hiring. The characteristics of applicants to a municipal government job are randomized. The rate of callbacks for a job interview are compared across applicant characteristics. We consider three different inquiries that could be studied with the design: the hiring rate for job applications from a Black applicant and a White applicant; the hiring rate between someone from the local area vs. someone equally qualified who lives far away; and the rates between someone who went to East High School and someone who went to West High School in town. We judge the questions to rank in importance between high (the question of racial discrimination), medium (local favoratism), and low (personal interest). The value of the research is a function of the importance of the inquiry, but also how much we learn about it. We proxy for the learning from the experiment by sample size: the higher the <span class="math inline">\(N\)</span>, the more we learn, but with decreasing marginal returns (it’s a lot better to have a sample of 100 compared to 10; it matters less if it 1010 or 1100). Figure <a href="research-design-lifecycle.html#fig:ethicsplot">20.2</a> shows the three research value curves labeled by the importance of the inquiry.</p>
<p>Because the job applicants are fictitious but appear real, a primary ethical concern in audit experiments is how much time hiring managers (the participants in the research) spend reviewing the fictitious applications. In the case of government hiring, it is public money spent on their review. The time cost to participants is linear in the number of applications: each application takes about ten minutes to review, regardless of how many are sent. We represent the cost to participants as the purple line in Figure <a href="#ethicsplot"><strong>??</strong></a>.</p>
<p>We have placed the costs to participants on the same scale as the value of the research, by placing a value to society of the research and the value to society of the time of the hiring managers. When benefit exceeds cost (upper blue region), we decide to move forward with the research; if costs exceed benefits (lower gray region), we do not conduct the study.</p>
<p>The conclusion of the graph is that for high-importance inquiries, it is almost always worth doing the study. We get a lot of value from the research, despite the costs to participants. However, there is a region at low sample sizes where the cost to participants exceed the benefits from the research, because of the very imprecise answers we get from the research. We don’t learn enough about the inquiry, despite its importance, to make it worth wasting the hiring managers time. By contrast, for low importance inquiries, it is never worth conducting the study. The costs to participants always exceed the (low) value of the research. Medium importance questions are in the middle: there is a region of the benefits curve (highlighted in blue) where it is worth doing the study, but two regions (highlighted in gray) where it is not worth it. The left region is where the sample is too small so the value of the research is low both because of its medium importance and we do not learn enough about it. The second gray region at right in the medium importance curve is where though we learn a lot about the inquiry, the cost is too high from the many hours of hiring manager time to justify what we learn because the inquiry is not important enough.</p>
<p>In short, ethical determinations require diagnosis both of how much we learn and how much it costs to participants (along with other ethical costs), and we must place a value on both ethical costs and research benefits in order to compare them on the same scale.</p>
<div class="figure">
<span id="fig:ethicsplot"></span>
<img src="book_files/figure-html/ethicsplot-1.png" alt="Tradeoffs between ethical costs and scientific benefits. A design might have too *many* subjects but also too *few* subjects." width="100%"><p class="caption">
Figure 20.2: Tradeoffs between ethical costs and scientific benefits. A design might have too <em>many</em> subjects but also too <em>few</em> subjects.
</p>
</div>
</div>
<div id="illustration-assessing-risks-of-adverse-events" class="section level2">
<h2>
<span class="header-section-number">20.5</span> Illustration: Assessing risks of adverse events<a class="anchor" aria-label="anchor" href="#illustration-assessing-risks-of-adverse-events"><i class="fas fa-link"></i></a>
</h2>
<!-- Tara like exampe of swinging an election -->
<hr>
<p>Scholars are increasingly calling for reporting of ethical considerations in study design beyond IRB approval. By declaring your expectations about the ethical outcomes of an experiment in terms of diagnosands such as the time participants devote to the study and the probability of harm to individuals, a declared research design can be an input to ethical reporting. Readers can review how you considered ethical outcomes in your design and judge the mitigation efforts you undertook in relation to those expectations. Other scholars have proposed including ethical assessments in preanalysis plans. Declarations of ethical diagnosands are a natural complement to these preregistered assessments.</p>
<!-- - Is it the measurement they object to? Is it the random assignment?  -->
<!-- papers to cite here: the A/B illusion (Michelle Mayer); Cite APSA guidelines; preregistering ethics guidelines (Lyall paper); report on ethics in paper (Lauren Young's paper); Mac's paper; Belmont report -->
<p><strong>Further readings</strong>.
– history: belmont report. tuskegee experiment. stanford prison experiment.
– laws and IRBs: revised common rule, paper on IRBs
– guidelines from econ, poli sci, soc, psych.
– ethics of experiments: macartan’s paper, dawn teele’s chapter; A/B illusion;
– ethics of outcome data collection: <a href="selection%20effects%20in%20archival%20research">https://link.springer.com/epdf/10.1007/s11133-020-09458-9?sharing_token=ZXNJXewvrZN0ouaL02wFDPe4RwlQNchNByi7wbcMAY4-EQXsmvNo8IbQcOueStzgc77hZU2onyyln_mVDwcUpdOR-pwRaPn3QvTGa3Im4FtNdYCpNVCfjs7pObv1rfyp-y7KtLBchLUMAHPMkKVeCSMwqZH6G5ldDK9hxy4NRoQ%3D</a>
– new ideas: jay lyall paper; lauren young paper</p>

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
<!-- start post here, do not edit above -->
</div>
<div id="partners" class="section level2">
<h2>
<span class="header-section-number">20.6</span> Partners<a class="anchor" aria-label="anchor" href="#partners"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<p>Partnering with third-party organizations in research — cooperating to intervene in the world or to measure outcomes — is increasingly common in the social sciences. Researchers seek to to produce (and publish) scientific knowledge; they work with political parties, government agencies, non-profit organizations, and businesses to learn more than they could if they worked independently. These groups consent to working with researchers in order to learn about how to achieve their own organizational goals. For example, a government may want to learn how to expand access to healthcare or a corporation may want to learn how to improve its ad targeting.</p>
<p>In the best case scenario, the goals of the researchers and partner organizations are aligned. When the scientific question to be answered (the inquiry) is the same as the practical question the organization cares about, the gains from cooperation are clear. The research team gains access to the financial and logistical capacity of the organization to act in the world and the partner organziation gains access to the scientific expertise of the researchers. Finding a good research partner almost always amounts to finding an organization with common – or at least not conflicting – goal. Understanding the private goals of the partner and of the researcher is essential to selecting a research design amenable to both parties. Research design declaration and diagnosis can help with this problem by formalizing tradeoffs between the two sets of goals.</p>
<p>One common divergence between partner and researcher goals is that partner organizations often do want to learn, but they care most about their primary mission. In business settings, this dynamic is sometimes referred to as the “learn versus earn” or “exploration-exploitation” tradeoff. An aid organization (and their donors) cares about delivering their programme to as many people as possible; learning whether the programme has the intended effects on the outcomes of interest is obviously also important, but resources spent on evaluation are resources <em>not</em> spent on programme delivery.</p>
<p>Research design diagnosis can help navigate the learning-doing tradeoff. One instance of the tradeoff is that the proportion of units who receive a treatment (e.g., a medicine) represents the rate of “doing” and also affects the amount of learning in that in the extreme if all units are treated there can (typically) be no learning about the <em>effect</em> of the treatment. The tradeoff here is represented in a graph of the power of the study vs. the proportion treated (top facet), and the utility of the partner (bottom facet) which is increasing in the proportion treated. The researchers have a power cut-off at the standard 80% threshold. The partner also has a strict cut-off: they need to treat at least 2/3 of the sample in order to fulfill a donor requirement.</p>
<p>In the absence of partners, researchers might simply ignore the proportion treated axis and select the design with the highest power. With a partner organization, the researcher might use this graph in conversation with the partner to jointly select the design that has the highest power that has a sufficiently high proportion treated to meet the partner’s needs. This is represented in the “zone of agreement” in gray: in this region, the design has at least 80% power and at least 2/3 of the sample are treated. Deciding within this region involves a trade-off between power (which is decreasing in the proportion treated here) and the utility of the partner (which is increasing in proportion treated). The diagnosis surfaces the zone of the agreement and clarifies the choice between designs in the zone.</p>
<div class="figure">
<span id="fig:partnersplot"></span>
<img src="book_files/figure-html/partnersplot-1.png" alt="Navigating research partnerships." width="100%"><p class="caption">
Figure 20.3: Navigating research partnerships.
</p>
</div>
<p>Choosing the proportion treated is one example of integrating partner constraints into research designs to generate feasible designs. A second common problem is that there are a set of units that <em>must</em> be treated, for ethical or political reasons (e.g., the home district of a government partner must receive the treatment), or that must not be treated. If these constraints are discovered after treatment assignment, they lead to noncompliance, which may substantially complicate analysis of the experiment and even prevent providing an answer to the original inquiry. Considerable thought has been given to avoiding this type of noncompliance. Gerber and Green recommend, before randomizing treatment, exploring possible treatment assignments with the partner organization and using this exercise to elicit the set of units that must or cannot be treated. King et al. (cite) describe a ``politically-robust’’ design, which uses pair-matched block randomization and when any unit is dropped due to political constraints the pair is dropped from the study<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Note the inquiry that can be answered from such a design is complex, reflecting effects for units in pairs that would not be dropped in either possible random assignment within the pair.&lt;/p&gt;"><sup>24</sup></a></p>
<p>Design diagnosis can help in these circumstances by providing a mechanism to specify possible patterns of noncompliance and diagnosing alternative designs to mitigate the negative consequences. In addition, they can be used to communicate with partners about the consequences of noncompliance for the research to help make better decisions together about how to avoid noncompliance once treatment is randomized and the research is in progress.</p>
<p>Our best advice for working with partners is to involve them in the design declaration and diagnosis process. How can we develop intuitions about the means, variances, and covariances of the important variables to be measured? Ask your partner for their best guesses, which may be far more educated than your own. For experimental studies, solicit your partner’s beliefs about the magnitude of the treatment effect on each outcome variable, subgroup by subgroup if possible. Be specific – ask what they think the average will be in the control group and what the average will be in the treatment group. Sorting out these beliefs very quickly sharpens the discussion of key design details. Share your design diagnoses and mock analyses  the study is launched in order to build consensus around the goals of the study.</p>
<p>Sometimes a partnership simply will not work out. Indeed most partnerships are never even initiated because researcher and organizational goals are too far apart. If you find yourself in a setting where partnership is doing too much violence to the research design, find a way to walk away from the project.</p>
<!-- ### Scattered thoughts -->
<!-- - Partnerships and ethics should be in here -->
<!-- - Research for Impact (@levine_2020) -->
<!-- - When to walk away -->
<!-- - Advice: there's usually a boss that gives you the green light, then staff people who actually help you. Our advice is to ensure that at least some meetings are with both the boss and the staff people so that the staff people know that the boss cares about the project and are therefore motivated to help you. A crucial person at the partner organization is the person who knows where the data are and how you can access them. Many people at partner organizations do not have this power, but you must be frequent touch.  -->
<!-- - In some cases, some member of the research staff can actually embed wiht the partner organization to ensure all the good things. -->
<!-- - Involve partners in PAP writing. -->
<!-- - when should partners be coauthors on the paper? -->

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
<!-- start post here, do not edit above -->
</div>
<div id="funding" class="section level2">
<h2>
<span class="header-section-number">20.7</span> Funding<a class="anchor" aria-label="anchor" href="#funding"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<!-- point 1 -->
<p>In any design, there will be a tradeoffs between diagnosands about the quality of the research as well as about its costs. Costs are a function both of the data strategy (some are more expensive than others!) and how the data are realized each time. Collecting original data is more expensive than analyzing existing data, but collecting new data may be more and less expensive depending on how easy it is to reach specific individuals to interview them. As a result, diagnosing research designs including cost diagnosands is important, and those diagnosands may usefully include both average cost and maximum cost. Researchers may make different decisions about cost: in some cases, the researcher will select the “best” design in terms of research design quality subject to a budget constraint, others will choose the cheapest among similar quality designs in order to save money for future research. Diagnosis can help identify each set and decide among them.</p>
<p>To relax the budget constraint, researchers apply for funding. Funding applicants wish to obtain as large a grant as possible for their design, but have difficulty credibly communicating the quality of their design given the subjectivity of the exercise. Funders, on the flip side, wish to get the most value for money in the set of proposals they decide to fund, and have difficulty assessing the quality of proposed research. MIDA and design declaration provide a tool for speaking in a common language that can be more easily verified by both sides about what design is being proposed and what its value is to knowledge under a set of assumptions that can be interrogated by funders.</p>
<p>Funding applications often aim to communicate what research design is being proposed; why learning answers from the design would be useful, important, or interesting to scholars, the public, policymakers, or another audience; how the research design provides credible answers to the question; that the researcher is capable of executing the design; and that there is value-for-money in the design and the answers it provides.</p>
<p>A new section of funding applications that would aid in communicating about each of these questions is declaring the MIDA of the design and presenting a diagnosis of the design. In addition to common diagnosands such as bias and efficiency, two special diagnosands may be valuable: cost and, related, value-for-money. Cost can be included for each design variant as a function of design features such as sample size, the number of treated units, and the duration of survey interviews. The cost may vary by these parameters and may vary across possible designs when, for example, the number of treated units is a random number. Simulating the design across possible realizations of the design, thus, provides a distribution of costs as a function of choices the researcher makes. Value for money is a diagnosand that is a function of cost and also the amount that is learned from the design. RMSE might be one value criterion, another would be the average difference between priors and posteriors under a Bayesian answer strategy (a direct measure of learning).</p>
<p>In some cases, funders request applicants to provide multiple options and multiple price points or to make clear how a design could be altered such that it could be funded at a lower (or higher) level. Redesigning a design with differing sample sizes would communicate how the researcher conceptualizes these options, but also provide the funder with an understanding of tradeoffs between the amount of learning and cost in these design variants. Applicants could use redesign to justify the high cost of their request and to ask for additional funding.</p>
<p>Ex ante power analyses, required by an increasing number of funders, illustrate the crux of the misaligned incentives between applicants and funders. A power analysis can demonstrate almost any design is “sufficiently powered” by changing expected effect sizes and noise. By clarifying the assumptions of the power analysis in code, researchers can more easily defend these choices. Funders can more easily interrogate these assumptions. Power analyses using standard power calculators online have difficult-to-interrogate assumptions built in and cannot accommodate the specifics of many common designs. As a result, many return incorrect estimates of power for these designs <span class="citation">(Blair et al. <a href="references-4.html#ref-bccmapsr">2020</a>)</span>.</p>
<p>Funders who request design declarations can compare funding applications on common scales: root mean-squared error, bias, and power. Of course, they also want to weigh considerations like the importance of the question and the fit with their funding program. But moving design considerations onto a common scale takes guesswork out of the process and reduces reliance on researcher claims about properties.</p>
<!-- -- funders often request power analysis, but these are typically described in words and thus the assumptions behind them cannot be interrogated fully. (a) not in code, so not specific; (b) user power calculators that are wrong (cite paper); (c) do not provide the details funders need to verify whether they agree with the assumptions.  -->
<!-- -- for funders, providing MIDA declared in code allows them to change the parameters of the design and test how the properties of the design change with beliefs about the world in M or data strategy parameters in D such as sample size, rather than having to rely on claims by applicants -->
<!-- -- often funders require regular reporting on changes in plan -- MIDA and design declaration provides a way to communicate (a) what those changes are and (b) how they change the values of diagnosands.  -->
<!-- -- value for money as a diagnosand (cost of each design as a function of design parameters) -->
<!-- -- allows funders to compare on a common scale (the same set of diagnosands) funding proposals -- often trying to evaluate "quality" but hard to do that with narrative proposals -->
<!-- ### References -->

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
<!-- start post here, do not edit above -->
</div>
<div id="p4piloting" class="section level2">
<h2>
<span class="header-section-number">20.8</span> Piloting<a class="anchor" aria-label="anchor" href="#p4piloting"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<!-- -- can't learn causal effect -->
<!-- -- what can you learn? about Y0, about M, about se -->
<!-- -- bring in blog post -->
<p>The designs and results of past studies are important guides for selecting M, I, D, and A. Our understanding of the nodes and edges in the causal graph of M, expected effect sizes, the distribution of outcomes, feasible randomization schemes, and many other features are directly selected from past research or chosen based on a literature review of the distribution over past studies. However, researchers face a problem in being guided by past research: the research context and our inquiries often differ in at least subtle ways from any past study. Even when we are replicating a past study, we are collecting data in a different time period and if effects vary over time then aspects of M may differ from the original study. To deal with this, we often run pilot studies. These take many forms: focus groups to learn about features of M or to learn how to ask survey questions; small-scale tests of measurement tools to verify our data collection technology works; up to mini studies with the planned design but on a smaller scale.</p>
<p>Pilot studies are constrained by our time and by money. If we were not constrained, we would run the full study and learn what is wrong with our design and then run a corrected design for the main study. Since we cannot due to our constraints, we run either smaller mini studies or test out only a subset of the elements of our planned design. This places us in a bind: we are running a design smaller or less complete than the study we imagine conducting, and so the properties of the pilot design will not measure up.</p>
<p>MIDA provides a framework for thinking about two aspects of piloting: what can be learned from a pilot? To answer this question, pilot studies should be diagnosed before they are run. Just like for a full study, we can define inquiries about the decisions we would make and the parameter estimates we would draw on in designing the full study.</p>
<p>In Figure XX, we display the results of a diagnosis of a 50-unit pilot study that we are conducting to prepare for a larger main study. We consider two strategies: (1) determining the sample size from a power analysis of the main study, selecting the minimum <span class="math inline">\(N\)</span> such that the study is 80% powered to detect the pilot study’s effect size); (2) setting a fixed <span class="math inline">\(N\)</span> determined by our budget constraint, in this case to 500, and using the standard deviation of units in the treated and control group from the pilot to determine the minimum detectable effect size of our 500-unit main study.</p>
<p>In the left panel is the sampling distribution of effect size estimates, i.e., a histogram of the effect estimates from the pilot. In the design, the standard deviation of the outcome is set to one, so effect estimates are in standard deviation units. The true effect size is set to 0.2. We can see that the sampling distribution has a huge range, from nearly -0.5 to nearly 0.75. The first problem with the sampling distribution is that many estimates, in fact nearly a quarter of them, are negative (the wrong sign!). This might lead us not only to choose the wrong sample size but to choose one-sided tests in the wrong direction. The second is that we have a high likelihood of guessing the effect size is <em>much</em> higher than it really is. If we obtain one of the estimates over 0.75 or even over 0.5, we would choose an <span class="math inline">\(N\)</span> too small to detect the true effect size of 0.2. In short, our estimates of the effect size from our 50-person pilot study are simple too variable to be useful in designing our main study.</p>
<p>However, there is good news: we can learn a lot about the power of our main study from the pilot study, just not from the effect estimates. In the right panel of Figure <a href="research-design-lifecycle.html#fig:pilotingfig">20.4</a>, we estimate the minimum detectable effect size of a 500-unit main study, relying on the estimated standard deviation in the control group and the estimated standard deviation in the treatment group to calculate the estimated standard error of the effect estimate in the main study. We then calculate the minimum detectable effect size using the approximation from <span class="citation">(Gelman and Hill <a href="references-4.html#ref-gelman2006data">2006</a>, pg. 441)</span>, 2.8 times the estimated standard error. We find that our estimates of the MDE for the full study are much more precise, tightly centered around 0.25. Since we don’t know if that is larger or smaller than the true effect size, we then must make an argument based on past studies’ effect sizes to justify whether that minimum size is sufficiently large or whether we should increase the sample size in order to detect even smaller effects. The reason the MDE is more precisely estimated is that the standard deviation of the control group is a much less variable estimate of the true standard deviation of the control potential outcome than the effect size estimate is of the true effect size.</p>
<div class="figure">
<span id="fig:pilotingfig"></span>
<img src="book_files/figure-html/pilotingfig-1.png" alt="Learning from pilot studies." width="100%"><p class="caption">
Figure 20.4: Learning from pilot studies.
</p>
</div>
<p>By diagnosing our pilot studies in this way, we can learn what decisions can be made with confidence from pilot data and what should be shaped instead by expectations from past studies and qualitative knowledge. Diagnosis can also help us to decide how large a pilot study we need in order to estimate quantities like the MDE of the full study with precision.</p>
<p>Beyond estimating the MDE of studies, other facts that can often be usefully learned from pilot studies take the form of existence proofs. We often wish to study how variation in <span class="math inline">\(D\)</span> (a treatment) affects variation in <span class="math inline">\(Y\)</span> (an outcome), but in the absence of past data from these two variables we may not know even if there is variation in <span class="math inline">\(Y\)</span> to explain. In experimental studies, we can learn whether a treatment <em>can</em> be implemented, and in an observational study we can learn whether there is variation in the treatment variable.</p>
<p>Baseline measurement may often be used instead of a pilot study to learn about some empirical features.</p>
<!-- Diagnosing the pilot study on its own provides stark insights, which amount to: we cannot provide answers to the inquiry in the main study, and should not try to do so. There are also aspects of the logistics of research that within time and financial constraints we simply cannot learn until we run the main study. Science is imperfect, and also iterative, but these mistakes or suboptimal design choices also often lead to discoveries. -->
<!-- -- how does it help to diagnose the design together? the properties of the main study *change* when we do a pilot. This is because if we run the pilot study, we are doing so to make decisions about how to run the main study, and so our *design* of the main study and thus its results may depend on the *results* (and design) of the pilot study.  -->
<!-- In this section, we illustrate several general principles that flow from diagnosing pilot studies.  -->
<!-- Purposes of pilot studies: -->
<!-- Existence proofs: -->
<!-- -- is there variation in Y -->
<!-- -- is there variation in X -->
<!-- -- what are nodes in M -->
<!-- -- what are feasible D's, what are feasible treatments / can you implement the treatment (existence proof) -->
<!-- Harder questions requiring bigger sample sizes: -->
<!-- -- what is the distribution of X (helps select stratification proportions etc.) -->
<!-- -- what is the standard deviation of Y0 -->
<!-- #### Assessing a pilot design -->
<!-- declare pilot itself and diagnose just as if it were the main study -->
<!-- if you can't learn the answer, don't make any decisions based on it -->
<!-- #### Assessing a sequenced design -->
<!-- if you are making decisions about MIDA for main study based on pilot, diagnose the procedure of two studies, think about POs of pilot -->
<!-- #### Pilots and baselines -->
<!-- Designs can be reassessed after baselines and before treatment assignment -- so some of the questions you might do a pilot for can just be answered in a baseline -->
<!-- #### BLOG material -->
<!-- Data collection is expensive, and we often only get one bite at the apple. In response, we often conduct an inexpensive (and small) pilot test to help better design the study. Pilot studies have many virtues, including practicing the logistics of data collection and improving measurement tools. But using pilots to get noisy estimates in order to determine sample sizes for scale up comes with risks. -->
<!-- Pilot studies are often used to get a guess of the average effect size, which is then plugged into power calculators when designing the full study. -->
<!-- The procedure is: -->
<!-- 1. Conduct a small pilot study (say, N = 50) -->
<!-- 2. Obtain an estimate of the effect size (this is noisy, but better than nothing!) -->
<!-- 3. Conduct a power analysis for a larger study (say, N = 500) on the basis of the estimated effect size in the pilot -->
<!-- We show in this post that this procedure turns out to be dangerous: at common true effect sizes found in the social sciences, you are at risk of selecting an underpowered design based on the noisy effect estimate in your pilot study. -->
<!-- A different procedure has better properties: -->
<!-- 1. Conduct a small pilot study (say, N = 50) -->
<!-- 2. Obtain an estimate of the **standard deviation of the outcome variable** (again, this is a noisy estimate but better than nothing!) -->
<!-- 3. Estimate the minimum detectable effect (MDE) for a larger study (say, N = 500), using the estimated standard deviation -->
<!-- We show what happens in each procedure, using DeclareDesign. In each case, we'll think about a decision the researcher wants to make based on the pilot: should I move forward with my planned study, or should I go back to the drawing board? We'll rely on power to make that decision in the first procedure and the MDE in the second procedure. -->
<!-- [omitting code] -->
<!-- For each true effect size, the simulations will give us a distribution of estimated effects that a researcher might use as a basis for power analysis. For example, for a true effect size of 0 the researcher might still estimate an effect of 0.10, and so conduct their power analysis assuming that the true effect is 0.10. For each true effect, we can thus construct a distribution of *power estimates* a researcher might obtain from *estimated* effects. Since we know the true power for the true underlying effect, we can compare the distribution of post-hoc power estimates to the true power one would estimate if one knew the true effect size. -->
<!-- What did we find? In the plot, we show our guesses for the power of the main study based on our pilot effect size estimates.  -->
<!-- At high true effect sizes (top row), we do pretty well. Most of our guesses are above 80\% power, leading us to the correct decision that the study is powered. Indeed we often *underestimate* our power in these cases meaning that we run larger studies than we need to. -->
<!-- However, at low true effect sizes (bottom row) we show we are equally likely to find that the design is in fact powered as underpowered. We are equally likely to guess the power of the design is 90% as 10%. There is a good chance that we will falsely infer that our design is well powered just because we happened to get a high estimate from a noisy pilot. -->
<!-- ### How about estimating the standard deviation of the outcome? -->
<!-- Now, let's look at the second approach. Here, instead of using our pilot study to estimate the effect size for a power calculation, we estimate the **standard deviation of the outcome** and use this to calculate the main study's minimum detectable effect. The decision we want to make is: is this MDE small enough to be able to rule out substantively important effects? -->
<!-- We calculate the minimum detectable effect size using the approximation from [@gelman2006data, pg. 441], 2.8 times the estimated standard error. We estimate the standard error using Equation 3.6 from @gerber2012field.  -->
<!-- In summary, pilot studies can be valuable in planning research for many reasons, but power calculations based on noisy effect size estimates can be misleading. A better approach is to use the pilot to learn about the distribution of outcome variables. The variability of the outcome variable can then be plugged into MDE formulas or even power calculations with, say, the smallest effect size of political, economic, or social importance. -->
<!-- In the same spirit, pilot studies could also be used to learn the strength of the correlation between pre-treatment covariates and the outcome variable. With this knowledge in hand, researchers can develop their expectations about how much precision there is to be gained from covariate control or blocking. -->

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
<!-- start post here, do not edit above -->
</div>
<div id="implementation" class="section level2">
<h2>
<span class="header-section-number">20.9</span> Implementation<a class="anchor" aria-label="anchor" href="#implementation"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<p>Your design declaration is a road map for implementing your study. The data strategy tells you the procedure to use to sample units; how to assign treatments; and which variables to measure. Your answer strategy is the function that translates the realized data into a set of answers to your inquiries and statistics that communicate your confidence in those answers. If you have specified the data and answer strategies in sufficient detail in code, you can directly run the functions you declared to sample units and assign them to treatment and to analyze your data.</p>
<p>The road map is useful as a tool to learn where to go when things go right, but also to identify when you take a wrong turn and need to make decisions about how to get to an answer. In this sense, a design declaration should be a living document, updated to reflect the set of decisions you make along the way through the twists and turns of the research road. With the model of the world and an inquiry about it declared, when you are unable to collect a variable, treat a subset of units with treatment, or to reach some units for followup surveys, you can compare alternative ways of handling this deviation from your original plan on the same terms as you originally designed the experiment. These are all changes in your data strategy <span class="math inline">\(D\)</span>. You can assess options and also guide your decisionmaking about whether to continue your study at all or use the money for another better purpose. You can also use the comparison of diagnosands under alternative options as a tool to communciate with your research partners about why a change to their practices is needed. You can also use it to defend your intermediate data strategy choices when you are finished to reviewers and readers.</p>
<p>As you make changes to <span class="math inline">\(D\)</span>, changes in <span class="math inline">\(A\)</span> may also be required in order to follow the principle of analyzing as you sample, assign treatment, and measure. When you switch from an individual randomization to a cluster randomization because it is not logistically possible to individually assign units to treatment, you will typically want to adjust your answer strategy to account for clustering in the calculation of standard errors. By keeping both your data strategy and your answer strategy up to date as you implement your study, you may also identify new data that must be collected or new steps to take in order to still be able to provide credible answers.</p>
<p>You also learn more about your design as you go along, not because anything goes wrong but as a natural progression of the research. For example, you may not know how many units there are per cluster or per block, key details in assigning treatments and analyzing data from experiments. When you learn these details, change your data and answer strategies to reflect these new details — and diagnose the new design to be sure you still agree with your original choices. Beyond the data and answer strategies, you may also learn about new nodes or edges in the model during the course of research. When you learn about new confounders or mediators, update your model, but also consider whether changes to your data and answer strategy are necessitated to ensure you can answer your original inquiry.</p>
<p>In short, your design declaration is a living document that you can keep updated and use as a tool to guide you along the research path, not just as a document to write at the beginning of the study and revisit when you are writing up. This advice is in apparent tension with the idea of preanalysis plans, in which you precommit to your analysis choices before data is collected. It need not be. It is useful to keep your original design declaration and to preregister it, but it is also useful to keep the declaration updated as you make the changes along the way that inevitably happen. You will be in a better position to make good choices when things go awry, and also to communicate when and why you made changes to your design.</p>
<!-- outline: -->
<!-- -- use your MIDA to figure out how to implement -->
<!-- -- redesign as you go when you haven't gotten specific enough -->
<!-- -- consider the whole design -- ex ante declared, then changed -- and what *could have happened*, which may be known to you ex ante (there could or could not be attrition) or unknown (you learn during implementation that there are some kinds of units that won't comply, so you need to think not just about which ones did or did not comply but which ones *could* have). the whole process is a function of your interventions in the world (treatment or measurement), so write down the whole process and potential outcomes to understand what you can and cannot learn. -->
<!-- -- use your MIDA to help you *make* logistical choices, help it prevent you from making bad decisions and use as tool to communicate with partners and implementers about why you do or do not wnat to make different changes (or BETTER evaluate tradeoffs in those decisions) -->
<!-- idea bin: -->
<!-- -- MIDA is a roadmap for how to implement the study -->
<!-- -- when there is a part not specified, redesign to specify and diagnose again -->
<!-- -- lots of choices you make after you start, because it was not clear what decisions would have to be made ex ante: how to make these choices? (redesign and diagnose!).  -->
<!-- -- often randomization procedure will have to take into account specifities of the number of units (odd numbers), blocks with differing numbers of units, cluster of varying sizes, etc. that require revising D -->
<!-- -- unexpected constraints come in that affect D, and may require changes to D but also A. redesign. -->
<!-- -- how to make decisions about unexpected changes when things going wrong? (even if you don't have a PAP). often want to create multiple variants of the design incorporating what went wrong. common example: unexpected noncompliance or unexpectedly high rates of attrition. may want to change analysis plan, and register it, so can use redesign to develop that new plan. but also may discover diagnosands are not good enough, so may want to change data strategy midstream to mitigate these problems. -->
<!-- -- what to do when you run out of money or feasibility of sample size or other aspects of D becomes clear. redesign mountain subject to new cost constraints. -->
<!-- -- make go-no go decisions about whether to continue -->
<!-- -- how to know if your inquiry is no longer answerable -->
<!-- -- updating your PAP -->
<!-- -- assessing what you can learn based on the implementation challenges: these are potential outcomes, i.e. could be affected by treatment, so are often informative about what we can learn about the original research question -->
<!-- -- often need to convince partners to not change plans -- MIDA can be a tool for helping assess tradeoffs in learning and doing -->
<!-- -- Projects that succeed have direct researcher invovlement. Outsourcing too much of the design can lead to big troubles. -->
<p><strong>Related readings</strong>.</p>
<ul>
<li>Failure (<span class="citation">Karlan and Appel (<a href="references-4.html#ref-karlan2018failing">2018</a>)</span>)</li>
</ul>
<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book--><!-- start post here, do not edit above -->
</div>
<div id="p4populatedpap" class="section level2">
<h2>
<span class="header-section-number">20.10</span> Populated Preanalysis Plan<a class="anchor" aria-label="anchor" href="#p4populatedpap"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<p>Inevitably, authors of pre-analysis plans fail to anticipate how, eventually, the data generated by the study will be analyzed. Many of the reasons for the discrepancy were discussed in the previous section on implementation, but other reasons intervene as well. A common reason is that PAPs promise too many analyses – in the process of writing a paper, some analyses are dropped, others are combined, and still others are added during the writing and revision process. In the next section, we’ll describe how to reconcile analyses-as-planned with analyses-as-implemented, but this present section is about what to do with your analysis plan immediately after getting the data back.</p>
<p>We echo proposals made in <span class="citation">Banerjee et al. (<a href="references-4.html#ref-Banerjee2020">2020</a>)</span> and <span class="citation">Alrababa’h et al. (<a href="references-4.html#ref-alrababag_2020">2020</a>)</span> that researchers should produce short reports that fulfill the promises made in their PAPs. <span class="citation">Banerjee et al. (<a href="references-4.html#ref-Banerjee2020">2020</a>)</span> emphasize that writing PAPs is difficult and usually time constrained, so it is natural that the final paper will reflect further thinking about the full set of empirical approaches. A “populated PAP” serves simply to communicate the results of the promised analyses. <span class="citation">Alrababa’h et al. (<a href="references-4.html#ref-alrababag_2020">2020</a>)</span> cite the tendency of researchers to abandon publication of studies that return null results. In order to address the resulting publication bias, they recommend “null results reports” that share the results of the pre-registered analyses.</p>
<p>We recommend that authors include mock analyses in their PAPs using mock data. Doing so has the major benefit of being quite specific about the details of the answer strategy. A further benefit comes when it is time to produce a populated PAP, since the realized data can quite straightforwardly be swapped in for the mock data. Given the time invested in producing mock analyses for the PAP, writing up a populated PAP takes only as much effort as is needed to clean the data, which will need to be done in any case.</p>
<div id="example-22" class="section level3">
<h3>
<span class="header-section-number">20.10.1</span> Example<a class="anchor" aria-label="anchor" href="#example-22"><i class="fas fa-link"></i></a>
</h3>
<p>In Section <a href="research-design-lifecycle.html#p4planning">20.1</a>, <span class="citation">Bonilla and Tillery (<a href="references-4.html#ref-bonilla_tillery_2020">2020</a>)</span></p>
<div class="inline-table"><table class="texreg" style="margin: 10px auto;border-collapse: collapse;border-spacing: 0px;caption-side: bottom;color: #000000;border-top: 2px solid #000000;">
<caption>
Statistical models
</caption>
<thead><tr>
<th style="padding-left: 5px;padding-right: 5px;">
 
</th>
<th style="padding-left: 5px;padding-right: 5px;">
Model 1
</th>
<th style="padding-left: 5px;padding-right: 5px;">
Model 2
</th>
<th style="padding-left: 5px;padding-right: 5px;">
Model 3
</th>
<th style="padding-left: 5px;padding-right: 5px;">
Model 4
</th>
</tr></thead>
<tbody>
<tr style="border-top: 1px solid #000000;">
<td style="padding-left: 5px;padding-right: 5px;">
(Intercept)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.84<sup>***</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.41<sup>***</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.61<sup>***</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.54<sup>***</sup>
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.02)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.04)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.06)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.07)
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
Znationalism
</td>
<td style="padding-left: 5px;padding-right: 5px;">
-0.01
</td>
<td style="padding-left: 5px;padding-right: 5px;">
-0.00
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.02
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.09
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.02)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.02)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.08)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.09)
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
Zfeminism
</td>
<td style="padding-left: 5px;padding-right: 5px;">
-0.04
</td>
<td style="padding-left: 5px;padding-right: 5px;">
-0.01
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.01
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.02
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.02)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.02)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.08)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.09)
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
Zintersectional
</td>
<td style="padding-left: 5px;padding-right: 5px;">
-0.04
</td>
<td style="padding-left: 5px;padding-right: 5px;">
-0.03
</td>
<td style="padding-left: 5px;padding-right: 5px;">
-0.08
</td>
<td style="padding-left: 5px;padding-right: 5px;">
-0.01
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.02)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.02)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.08)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.10)
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
female
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.03<sup>*</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.01)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
lgbtq
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.02
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.02)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
age
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
-0.00
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.03)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
religiosity
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
-0.01
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.02)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
income
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
-0.00
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.03)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
college
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
-0.02
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.02)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
linked_fate
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.27<sup>***</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.30<sup>***</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.03)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.07)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
blm_familiarity
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.07<sup>***</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.10<sup>***</sup>
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.01)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.02)
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
Znationalism:linked_fate
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
-0.04
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.09)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
Zfeminism:linked_fate
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
-0.05
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.09)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
Zintersectional:linked_fate
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.05
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.09)
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
Znationalism:blm_familiarity
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
-0.03
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.03)
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
Zfeminism:blm_familiarity
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
-0.01
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.03)
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
Zintersectional:blm_familiarity
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
-0.01
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
 
</td>
<td style="padding-left: 5px;padding-right: 5px;">
(0.03)
</td>
</tr>
<tr style="border-top: 1px solid #000000;">
<td style="padding-left: 5px;padding-right: 5px;">
R<sup>2</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.00
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.20
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.14
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.09
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
Adj. R<sup>2</sup>
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.00
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.19
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.13
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.09
</td>
</tr>
<tr>
<td style="padding-left: 5px;padding-right: 5px;">
Num. obs.
</td>
<td style="padding-left: 5px;padding-right: 5px;">
849
</td>
<td style="padding-left: 5px;padding-right: 5px;">
849
</td>
<td style="padding-left: 5px;padding-right: 5px;">
849
</td>
<td style="padding-left: 5px;padding-right: 5px;">
849
</td>
</tr>
<tr style="border-bottom: 2px solid #000000;">
<td style="padding-left: 5px;padding-right: 5px;">
RMSE
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.23
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.20
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.21
</td>
<td style="padding-left: 5px;padding-right: 5px;">
0.22
</td>
</tr>
</tbody>
<tfoot><tr>
<td style="font-size: 0.8em;" colspan="5">
<sup><em><strong></strong></em></sup>p &lt; 0.001; <sup></sup>p &lt; 0.01; <sup></sup>p &lt; 0.05
</td>
</tr></tfoot>
</table></div>
<div class="figure">
<span id="fig:bonilatillerypopulatedpapcoefplot"></span>
<img src="book_files/figure-html/bonilatillerypopulatedpapcoefplot-1.png" alt="Coefficient plot from Bonilla and Tillery design based on the study's realized data." width="100%"><p class="caption">
Figure 20.5: Coefficient plot from Bonilla and Tillery design based on the study’s realized data.
</p>
</div>

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
<!-- start post here, do not edit above -->
</div>
</div>
<div id="reconciliation" class="section level2">
<h2>
<span class="header-section-number">20.11</span> Reconciliation<a class="anchor" aria-label="anchor" href="#reconciliation"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<p>Inevitably, the research design as implemented will differ in some way from the research design as planned. Treatments cannot be implemented as conceived, some people cannot be found to interview, and sometimes what we learn from baseline measures informs how we measure later. An understanding of how your research design changed from conception to implementation is crucial to understanding what was learned from the design.</p>
<p>Suppose the original design described a three-arm trial: one control and two treatments, but the design as implemented drops all subjects assigned to the second treatment. Sometimes this is an entirely appropriate and reasonable design modification: perhaps it turns out that due to an implementation failure, the second treatment was simply not delivered. Other times, these modification is less benign – perhaps the estimate of the effect of the second treatment does not achieve statistical significance, so the author simply omits it from the analysis.</p>
<p>For this reason, explicitly <strong>reconciling</strong> the design as planned with the design as implemented should be the first step to writing up a paper. Having a publicly-posted preanalysis plan can make the reconciliation process especially credible – we know for sure what the planned design was because the preanalysis plan describes it pre-implementation. However, a preanalysis plan is not a prerequisite for engaging in reconciliation. The scientific enterprise is built in large measure on trust: we are ready to believe researchers who say, here is the design I though I would implement but due to unanticipated developements, here is the design I ended up implementing.</p>
<p>In some cases, reconciliation will lead to additional learning beyond what can be inferred from the final design itself. When some units could refused to be included in the study sample or some units refused measurement, we learn that important features about those units. Understanding sample exclusions, noncompliance, and attrition not only may inform future research design planning choices, but contribute substantively to our understanding of the social setting. A policy implemented in the same way the study would likely also not be able to work in the units who refused to participate, and future research could examine why or how to convince them of the policy’s benefits.</p>
<p>What belongs in a reconciliation? At a minimum, we need a full description of the planned design, a full description of the implemented design, and a list of the differences. This can be made explicit through the declaration of both design in computer code, then comparing the two design objects line-by-line.</p>
<p>In <code>DeclareDesign</code> we take the first steps of comparing designs for you:</p>
<div class="sourceCode" id="cb165"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">design1</span> <span class="op">&lt;-</span> <span class="fu">declare_population</span><span class="op">(</span>N <span class="op">=</span> <span class="fl">100</span>, u <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">declare_potential_outcomes</span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">Z</span> <span class="op">+</span> <span class="va">u</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">declare_estimand</span><span class="op">(</span>ATE <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">Y_Z_1</span> <span class="op">-</span> <span class="va">Y_Z_0</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">declare_sampling</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">75</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">declare_assignment</span><span class="op">(</span>m <span class="op">=</span> <span class="fl">50</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">declare_reveal</span><span class="op">(</span><span class="va">Y</span>, <span class="va">Z</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">declare_estimator</span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">Z</span>, estimand <span class="op">=</span> <span class="st">"ATE"</span><span class="op">)</span>

<span class="va">design2</span> <span class="op">&lt;-</span> <span class="fu">declare_population</span><span class="op">(</span>N <span class="op">=</span> <span class="fl">200</span>, u <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">declare_potential_outcomes</span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="fl">0.5</span><span class="op">*</span><span class="va">Z</span> <span class="op">+</span> <span class="va">u</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">declare_estimand</span><span class="op">(</span>ATE <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">Y_Z_1</span> <span class="op">-</span> <span class="va">Y_Z_0</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">declare_sampling</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">declare_assignment</span><span class="op">(</span>m <span class="op">=</span> <span class="fl">25</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">declare_reveal</span><span class="op">(</span><span class="va">Y</span>, <span class="va">Z</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">declare_estimator</span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">Z</span>, model <span class="op">=</span> <span class="va">lm_robust</span>, estimand <span class="op">=</span> <span class="st">"ATE"</span><span class="op">)</span>

<span class="fu">compare_designs</span><span class="op">(</span><span class="va">design1</span>, <span class="va">design2</span><span class="op">)</span>
<span class="fu">compare_design_code</span><span class="op">(</span><span class="va">design1</span>, <span class="va">design2</span><span class="op">)</span>
<span class="fu">compare_design_summaries</span><span class="op">(</span><span class="va">design1</span>, <span class="va">design2</span><span class="op">)</span>
<span class="fu">compare_design_data</span><span class="op">(</span><span class="va">design1</span>, <span class="va">design2</span><span class="op">)</span>
<span class="fu">compare_design_estimates</span><span class="op">(</span><span class="va">design1</span>, <span class="va">design2</span><span class="op">)</span>
<span class="fu">compare_design_estimands</span><span class="op">(</span><span class="va">design1</span>, <span class="va">design2</span><span class="op">)</span></code></pre></div>
<div id="example-23" class="section level3">
<h3>
<span class="header-section-number">20.11.1</span> Example<a class="anchor" aria-label="anchor" href="#example-23"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:reconciliation">Table 20.3: </span> Reconciliation of Bonilla and Tillery preanalysis plan.</caption>
<colgroup>
<col width="18%">
<col width="20%">
<col width="20%">
<col width="40%">
</colgroup>
<thead><tr class="header">
<th>Covariate</th>
<th>In the preanalysis plan</th>
<th>In the paper</th>
<th>In the appendix (at the request of reviewers)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Familiarity with BLM</td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Gender</td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>LGBT status</td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>Linked fate</td>
<td><span class="math inline">\(\checkmark\)</span></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Religiosity</td>
<td></td>
<td></td>
<td><span class="math inline">\(\checkmark\)</span></td>
</tr>
<tr class="even">
<td>Region</td>
<td></td>
<td></td>
<td><span class="math inline">\(\checkmark\)</span></td>
</tr>
<tr class="odd">
<td>Age</td>
<td></td>
<td></td>
<td><span class="math inline">\(\checkmark\)</span></td>
</tr>
<tr class="even">
<td>Education</td>
<td></td>
<td></td>
<td><span class="math inline">\(\checkmark\)</span></td>
</tr>
</tbody>
</table></div>
<!-- ### Scattered thoughts --><!-- - @ofosu2019pre --><!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book--><!-- start post here, do not edit above -->
</div>
</div>
<div id="writing" class="section level2">
<h2>
<span class="header-section-number">20.12</span> Writing<a class="anchor" aria-label="anchor" href="#writing"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<p>When writing up an empirical paper, the authors must convince reviewers and readers that the question is important and that the research design they selected provides useful answers to the question. This is where MIDA comes in. Elements of MIDA will appear in every section of a paper, and every element of MIDA should be described somewhere in the paper.</p>
<p>A common model for social science empirical papers has five sections: an introduction, theory and hypotheses, research design, results, and discussion. We outline how elements of MIDA can usefully fit into each section here, before providing an example paper highlighting where each element was described.</p>
<p>The introduction section should have a capitulation of each aspect of MIDA in brief. The reader is brought quickly up to speed on the whole research design, as well as expectations and actual findings.</p>
<p>The next section, theory and hypotheses, contains information about the causal model of the world (<span class="math inline">\(M\)</span>) and the research question about the world (<span class="math inline">\(I\)</span>) as well as guesses about <span class="math inline">\(a^M\)</span> (i.e., hypotheses!). The section should lay out all of the features of the model necessary to describe <span class="math inline">\(I\)</span>.</p>
<p>To motivate hypotheses, the theory section should outline our prior beliefs about <span class="math inline">\(a^W\)</span>, the true answer to the inquiry, based on past literature. A meta-analysis or systematic review of past evidence could provide a systematic summary of past answers to <span class="math inline">\(a^W\)</span>, or an informal literature review could be offered. These priors are used along with the study results to construct posterior beliefs reported in the discussion section, i.e., what we know after conducting the study. In summarizing the literature, it is important to consider the research designs of past studies (see Synthesis section). Meta-analyses often formally account for the quality of the research designs of past studies by weighting by the inverse of their precision (upweighting informative studies and down-weighting uninformative ones). Literature reviews may do so informally. Moreover, the summary of past literature is itself a research design, and so we should try to prevent common research design issues such as selection on the dependent variable by ensuring we discuss all literature and not only present views consonant with our hypotheses.</p>
<p>Though research questions are nearly ubiquitously included, details of the model are often left out. Expected effect sizes, how effects vary by subgroups, expected proportions of subgroups, how variables are expected to be correlated, and the amount of variability in the outcome are some of the features of the model that will have important effects on how reviewers and readers judge the quality of the research design. Without specifying these portions of <span class="math inline">\(M\)</span>, a diagnosis cannot be conducted. Moreover, we describe in Reanalysis, when you define <span class="math inline">\(M\)</span> yourself in your paper, you can clarify the terms of the debate about alternative analysis strategies. When <span class="math inline">\(M\)</span> is undefined, the author of the reanalysis must infer your model or make up their own, and you may not agree.</p>
<p>The research design or methods section should have a description of <span class="math inline">\(D\)</span> and <span class="math inline">\(A\)</span> and a description of a diagnosis of the design. In this section, we defend our choice of <span class="math inline">\(D\)</span> and <span class="math inline">\(A\)</span> under <span class="math inline">\(M\)</span> and <span class="math inline">\(I\)</span>. Papers commonly have more than one <span class="math inline">\(D\)</span> and associated <span class="math inline">\(A\)</span> answering the same or related questions; both <span class="math inline">\(D\)</span> and <span class="math inline">\(A\)</span> for each should be described in this section.</p>
<p>The results section describes <span class="math inline">\(a^D\)</span>. Increasingly, results are presented visually as well as in text, in order to effectively communicate the results to the reader. Visualizing the modeled results as well as the raw data can both improve how the results are communicated and simultaneously help readers connect the research design, the data, and the results.</p>
<p>The discussion is where we update our understanding of the model and form our posteriors about <span class="math inline">\(a^W\)</span> given our prior expectations and the results <span class="math inline">\(a^D\)</span>. We may learn from the study about new variables and new edges that matter in the model — new outcomes that are affected by a treatment. We also often learn more about the functional form of causal relationships between two variables, for example from new moderators or more precision on the size of the moderation. The discussion section is also a chance to point to new MIDAs that could be implemented to learn more about <span class="math inline">\(M\)</span>. These might relate to new nodes we discovered, or parts of the model that we did not yet learn enough about.</p>
<div class="figure">
<img src="figures/mousa-highlighted-sheet.png" alt="Paper with MIDA elements highlighted (Mousa 2020)"><p class="caption">Paper with MIDA elements highlighted (Mousa 2020)</p>
</div>
<p>In Figure XX, we illustrate where elements of MIDA are incorporated into <span class="citation">Mousa (<a href="references-4.html#ref-mousa2020building">2020</a>)</span>. The study reports on the outcome of a randomized experiment in which Iraqi Christians were assigned either to an all-Christian soccer team or a team in which they would play alongside Muslims. The experiment tested whether being on a mixed team affected intergroup attitudes and behaviors, both among teammates and back at home after the games were over. We highlight in color areas discussing the model <span class="math inline">\(M\)</span> in yellow, the inquiry <span class="math inline">\(I\)</span> in green, the data strategy <span class="math inline">\(D\)</span> in blue, and the answer strategy <span class="math inline">\(A\)</span> in pink.</p>
<p>The model and the inquiry largely appear in the abstract and introductory portion of the paper, though aspects of the model are discussed later on. Much of the first three pages are devoted to the data strategy, while the answer strategy only appears briefly. This division makes sense: in this paper, the action is all in the experimental design whereas the answer strategy follows straightforwardly from it using the principles of analyze as you randomize. The paper mostly describes <span class="math inline">\(M\)</span> and <span class="math inline">\(D\)</span>, with only a small portion devoted to <span class="math inline">\(I\)</span> and <span class="math inline">\(A\)</span>. Finally, it is notable that the data strategy is interspersed with aspects of the model. The reason is that the author is justifying choices about randomization and measurement using features of the model. This highlights the deep connection between the two discussed in Part II.</p>
<p>Papers often report on the results from multiple data strategies and others on the answers to multiple related inquiries. Typically, a single <span class="math inline">\(M\)</span> should be described in the theory section, which describes how the nodes and edges used in each of the <span class="math inline">\(I\)</span>’s fit together. Each data and strategy should be described, and which inquiries each answer strategy is targeting. Each estimate should be linked to one or more inquiries. In some cases, the reason why multiple inquiries are studied in a single paper is that the aim of the paper is to falsify a model of the world. In this case, the model should be described and the diagnosis of the design should include an assessment of how good the <em>overall</em> design, with data and answer strategies to generate answers to each inquiry, is at falsifying the model. Psychology three-study papers often take this form: study 1 asks about the correlation between X and Y, in study 2 X is randomized, and in study 3 an analysis of the mechanisms that lead between X and Y are examined.</p>
<!-- Big ideas: -->
<!-- - make sure to have all of MIDA in your paper! -->
<!-- - please keep kosher.  Theory section material belongs in the theory section, not in the design or results section, etc. -->
<!-- 8. Reconciliation  -->
<!-- 9 Appendices and robustness -->

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
<!-- start post here, do not edit above -->
</div>
<div id="publication" class="section level2">
<h2>
<span class="header-section-number">20.13</span> Publication<a class="anchor" aria-label="anchor" href="#publication"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<!-- Publication in peer-reviewed journal is a major goal of many (but not all) research projects. The peer review process is demanding, slow, overly-critical, and, in the minds of many academics, broken. The reasons reviewers and editors provide when rejecting your paper never seem fair and moreover, never seem like the *real* reasons. It's a frustrating process. -->
<!-- The advice we gave in the previous section on writing papers was to build the case for your findings by grounding your conclusions in the specifics of the research design. By detailing $M$, $I$, $D$, and $A$ in ways that leave little room for confusion or ambiguity, you greatly improve the chances that reviewers and, later, readers will understand your paper. -->
<!-- Eventually, you *will* receive that coveted invitation revise and resubmit your paper, but the reviwers will have made suggestions that are mostly not optional. You will receive comments about all four aspects of the research design. Comments about framing, additional literature to cite, and theoretical distinctions are all about the model. Comments about the research question itself (is the answer already known, is it the right question to be asking) are about the Inquiry. Criticism of data strategy will include comments about the sample quality, the measurement properties of the survey instrument, or requests for replication studies. Many reviewer comments are about specifics of the answer strategy -- you ran OLS, but they want logit; you ran logit so they want to see OLS just to be sure.  -->
<!-- The goal is to respond to reviewer comments in a way that does not compromise the essential strength of your design: do not let the review process make your paper worse. -->
<div id="reviewing-papers" class="section level3">
<h3>
<span class="header-section-number">20.13.1</span> Reviewing papers<a class="anchor" aria-label="anchor" href="#reviewing-papers"><i class="fas fa-link"></i></a>
</h3>
<p>Reviewers and editors must decide whether to devote scarce space and editing bandwidth to publishing a paper. Criteria may include the topic fit with the journal, the importance of the question, and how much we learned from the research. The problem of the publication filter — publishing only studies with statistically-significant or splashy results — has long been recognized as a cause both of “false” findings making their way into the literature and bias in the literature simply from the missing null findings. One remedy for this problem is results-blind review of studies: selecting studies without knowing the nature of the results.</p>
<p>Results-blind review can take place in multiple ways. Journals have instituted alternative submission paths for “registered reports,” in which authors lay out how they will analyze their data and often include mock tables and figures based on simulated data. These reports can be constructed before or after data is collected. They are closely related to preanalysis plans, in fact often PAPs consist of a registered report.</p>
<p>But results-blind review does not require institutional setup: reviewers can review papers without regard for their results themselves. Reviewers can print out papers and hide results or simply aim to write comments only on the basis of the importance of the question and the quality of the research design.</p>
<p>In both cases, what is needed from authors is a clear statement about the research design and how they will report on and interpret results. Results-blind review need not ignore features of the research design such as how data are displayed and whether authors are overclaiming based on their results. Authors can write plans for how they will visualize and what claims they will make based on different patterns of findings. Journals who institute results-blind review can ensure reviewers are able to judge the quality of the research design by requesting details about <span class="math inline">\(M\)</span>, <span class="math inline">\(I\)</span>, <span class="math inline">\(D\)</span>, and <span class="math inline">\(A\)</span> from authors.</p>
</div>
<div id="responding-to-reviewers" class="section level3">
<h3>
<span class="header-section-number">20.13.2</span> Responding to reviewers<a class="anchor" aria-label="anchor" href="#responding-to-reviewers"><i class="fas fa-link"></i></a>
</h3>
<p>The research design of a study is not set in stone until the final version is post on a journal Web site or published in print. Until then, journal editors and reviewers may ask for changes to answer strategies or even data strategies or inquiries that would, in their view, improve the paper.</p>
<p>The reviewer may make three kinds of requests: an alternative analysis strategy to replace or augment the original; additional data collection; the addition of a new inquiry, based on the same realized data or new data collection; a change to the model in the form of adding or dropping a node or an edge, and with it related changes to other parts of the design; or a change to the inquiry that the reviewer feels can be better answered with the data and answer strategy the author selected.</p>
<p>You can understand how making the changes requested by reviewers alters the design. If the change improves the design, then adopting the suggestions is easy. Some changes are irrelevant to the design – like reporting the reviewer’s preferred descriptive statistics – in which case, following the advice is also easy The trouble comes when reviewers propose changes that actively undermine the design? If so, diagnosing the reviewer’s alternative design can be an effective way to demonstrate that the proposed changes would harm the research design.</p>
<!-- For example, reviewers sometimes ask authors to explore treatment effect heterogeneity by a number of additional covariates (as in the Bonilla) example. -->
<!-- Three big ideas: -->
<!-- - understand whether a reviewer suggestion improves or diminishes the quality of your design through diagnosis -->
<!-- - use diagnosis and your original declared design (ideally preregistered, but need not be) to defend against reviewer criticisms that *diminish* the quality -->
<!-- - Notes: could also do mediation analysis? What's a good example here? -->
<!-- ### Publication bias -->
<!-- change this to a section on how to review a paper (results blind!) and idea of registered reports -->
<!--  Here we look at risks from publication bias and illustrate two distinct types of upwards bias that arise from  a "[significance filter](https://andrewgelman.com/2011/09/10/the-statistical-significance-filter/)." A journal for publishing null results might help, but the results in there are *also* likely to be biased, *downwards*. -->
<!-- Two distinct problems arise if only significant results are published: -->
<!-- * The results of published studies will be *biased* towards larger magnitudes.  -->
<!-- * The published studies will be *unrepresentative* of the distribution of true effects in the relevant population of studies.  -->
<!-- These two problems are quite distinct. The first problem is more familiar: conditional on any true effect size, larger estimates have an easier time passing the statistical significance filter, so the distribution of published results will be biased upwards because it will be missing all of the smaller estimates.  The second problem is more subtle. If different studies seek to measure effects that are of different size, conditioning on statistical significance means that we are more likely to learn from places that have large effects than from places that have small effects. The significance filter means that our answers to any particular question will be biased *and* it means that the set of questions we see answers to will be biased as well. The *Journal of Significant Results* is a poor guide to the true distribution of causal effects. -->
<!-- What about a *Journal of Null Results*? Such a journal would condition acceptance on *failing* to achieve statistical significance. The set of articles published in such a journal would *also* be biased. -->
<!-- Looking first at the *Journal of Significant Results*, we see the familiar problem: the average estimate is biased away from the true value of the estimand. This problem is greatly helped by increasing the sample size. But we can also see the second problem -- the distribution of estimands (the true effects under study) is also biased towards larger effects, this problem is also allayed, though less dramatically, by larger sample sizes. -->
<!-- The *Journal of Null Results* suffers from a parallel problem, only in reverse. Now estimands are smaller than is typical in the population and, on average, estimates are biased down relative to these estimands. Strikingly, the bias in estimand selection is *worse* at the larger sample size (though downwards bias within the set of published studies is smaller). -->
<!-- Now, we agree that proactively publishing null results may help when considering entire research literatures as a whole, and for this reason alone a *Journal of Null Results* is probably a good thing.  -->
<!-- But, better would be to not do any conditioning at all. The *Journal of Interesting Designs* would condition only on the question being interesting and the design being appropriate to answering the question. We see that the distribution of estimates and estimands are both centered on the correct average value.  -->
<!-- Idea of results-blind review  -->
<!-- Idea of registered reports -->

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
<!-- start post here, do not edit above -->
</div>
</div>
<div id="archiving" class="section level2">
<h2>
<span class="header-section-number">20.14</span> Archiving<a class="anchor" aria-label="anchor" href="#archiving"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<p>One of the biggest successes in the push for greater research transparency has been changing norms surrounding the sharing of data and analysis code after studies have been published. It has been become de rigeur at many journals to post these materials at publicly-available repositories like the OSF or Dataverse. This development is undoubtedly a good thing. In older manuscripts, sometimes data or analyses are described as being “available upon request” but of course such requests are sometimes ignored. Furthermore, a century from now, study authors will no longer be with us even if they wanted to respond to such requests. Public repositories have a much better chance of preserving study information for the future.</p>
<!-- That's the promise of publicly-posted replication archives, but the mundane reality of replication archives often falls short. We see many archives that are disorganized, poorly documented, and contain dozens of bugs and inconsistencies.  -->
<p>What belongs in a replication archive? First, the data <span class="math inline">\(d\)</span> itself. Sometimes this is the raw data, sometimes it is only the “cleaned” data that is actually called by analysis scripts. Where ethically possible, we think it is preferable to post as much of the raw data as possible, for example after removing information like IP address or geographic location that could be used to identify a subject. We usually consider data processing scripts that clean and prepare data for analysis as part of the data strategy <span class="math inline">\(D\)</span> in the sense that they complete the measurement procedures laid out in <span class="math inline">\(D\)</span>. Cleaning scripts might also be considered part of the answer strategy in the sense that they apply an interpretation to the data provided by the world. We do not take a hard stance on whether data cleaning procedures rightly belong in <span class="math inline">\(D\)</span> or <span class="math inline">\(A\)</span> since the choice likely varies from study to study. In either case, the output of these scripts – the cleaned data – should be included in the replication archive as well.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;We emphasize here that cleaning should be accomplished &lt;strong&gt;separately&lt;/strong&gt; from analysis. If you find yourself recoding a variable midway through an analysis script, do yourself and others a favor: stop, move the code over to the cleaning file, rerun it, and get back to writing the analysis.&lt;/p&gt;"><sup>25</sup></a></p>
<p>Replication archives also include <span class="math inline">\(A\)</span>, or the set of functions applied to <span class="math inline">\(d\)</span> that produce <span class="math inline">\(a^D\)</span>. It is vitally important that the <em>actual</em> analysis code is archived because the natural-language descriptions of <span class="math inline">\(A\)</span> that are typically given in papers are imprecise. As a small example, many articles describe their answer strategies as “ordinary least squares” but do not fully describe the set of covariates used or what flavor of standard errors were estimated. The actual analysis code makes <span class="math inline">\(A\)</span> explicit.</p>
<p>While typical replication archives include <span class="math inline">\(d\)</span> and <span class="math inline">\(A\)</span>, we think that future replication archives should also include a design declaration that fully describes <span class="math inline">\(M\)</span>, <span class="math inline">\(I\)</span>, <span class="math inline">\(D\)</span>, and <span class="math inline">\(A\)</span> – that is, we should archive designs, not just data and analysis code. This should be done in code and words. In addition, a diagnosis should be included, demonstrating the properties as understood by the author and also indicating the diagnosands that the author considered in judging the quality of the design.</p>
<p>Figure <a href="#fig:filestructure"><strong>??</strong></a> shows the file structure for an example replication. Our view on replication archives shares much in common with the TIER protocal, which can be found here: <a href="https://www.projecttier.org/" class="uri">https://www.projecttier.org/</a>. It includes raw data in a platform-independent format (.csv) and cleaned data in a language-specifc format (.rds), so that data features like labels, attributes, and factor levels are preserved when imported by the analysis scripts. The analysis scripts are labeled by the outputs they create, such as figures and tables. A master script is included that runs the cleaning and analysis scripts in the correct order. The documents folder includes the paper, the supplemental appendix, the pre-analysis plan, the populated analysis plan, and codebooks that describe the data. A README file explains each part of the replication archive. We also suggest that authors include a script that includes a design declaration and diagnosis.</p>
<div class="figure">
<img src="figures/file_structure.png" alt="File structure for archiving"><p class="caption">File structure for archiving</p>
</div>
<!-- Example is archive at OSF: https://osf.io/4vuqh -->

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
<!-- start post here, do not edit above -->
</div>
<div id="reanalysis" class="section level2">
<h2>
<span class="header-section-number">20.15</span> Reanalysis<a class="anchor" aria-label="anchor" href="#reanalysis"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<p>A reanalysis of an existing study is a followup study in which <span class="math inline">\(d\)</span>, the original realized data, is fixed and changes to <span class="math inline">\(A\)</span> and sometimes <span class="math inline">\(M\)</span> or <span class="math inline">\(I\)</span> are proposed. Given <span class="math inline">\(d\)</span> is fixed, so too is the data strategy <span class="math inline">\(D\)</span>. The results given the new MIDA, which may differ from the original study’s results, are reported.</p>
<p>We can learn from reanalyses in at least five ways. We can confirm that there were not errors in the analysis strategy. Many reanalyses correct simple mathematical errors, typos in data transcription, or failures to analyze following the data strategy faithfully. These reanalyses show whether results do or do not depend on these corrections. We can reassess what is known about the same <span class="math inline">\(I\)</span>, using new information about the world that was learned after the original study was published. Here, we may learn about new confounders or alternative causal channels that undermine the credibility of the original answer strategy. When reanalyzed, demonstrating the results do (or do not) change improves our understanding of <span class="math inline">\(a^W\)</span>. Many reanalyses show that original findings are not “robust” to alternative answer strategies. These are better conceptualized as claims about robustness to alternative models: one model may imply one answer strategy and a different model, with another confounder, implies another. If both models are plausible, a good answer strategy should be robust to both and even help to distinguish between them and a reanalysis could uncover robustness to these alternative models or lack thereof. Reanalyses may also aim to answer new questions that were not considered by the original study, but for which the realized data can provide useful answers. For example, authors may analyze outcomes not originally analyzed.</p>
<p>Reanalyses are themselves research designs. Whether a reanalysis is a good design, and how much it can contribute to our knowledge about the original inquiry, depend on possible realizations of the data. Because <span class="math inline">\(d\)</span> is fixed in a reanalysis, analysts are often instead tempted to judge the reanalysis based on whether it overturns or confirms the results of the original study. A successful reanalysis in this way of thinking demonstrates, by showing that the original results are changed under an alternative <span class="math inline">\(A\)</span>, that the results are not robust to other plausible models. This way of thinking can lead to incorrect assessments of reanalyses. We need to consider what answers would obtain under the original answer strategy <span class="math inline">\(A\)</span> and the reanalysis strategy <span class="math inline">\(A^{\prime}\)</span> under many <em>possible</em> realizations of the data. A good reanalysis strategy reveals with high probability the set of models of the world under which we can make credible claims about <span class="math inline">\(I\)</span>. Whether or not the results from the fixed <span class="math inline">\(d\)</span> that was realized change under <span class="math inline">\(A\)</span> and <span class="math inline">\(A^{\prime}\)</span> tells us little about this probability. It is only one draw.</p>
<p>To diagnose a reanalysis, we need to define two answer strategies — <span class="math inline">\(A\)</span> and <span class="math inline">\(A^{\prime}\)</span> — but also a new diagnostic-statistic. We need to decide how we summarize the answers from the two answer strategies. If one returns TRUE and one FALSE, what do we conclude about the inquiry? The function we define to summarize the two results depends on the inquiry and the goals of the reanalysis. But our diagnosis of the reanalysis should assess the properties of this summary of the two studies under possible realizations of the data. If the goal of the reanalysis is instead to learn about a new question, then we should simply construct a new MIDA altogether, but holding constant <span class="math inline">\(D\)</span> from the original study, which we cannot change because we already collected <span class="math inline">\(d\)</span> using it.</p>
<!-- how do we update from the reanalysis research design (the original design plus the reanalysis of d)? -->
<!-- -- the design is the research design from before with two sets of estimates from two different A's -->
<!-- -- need an aggregation function (decisionmaking function) that converts the two sets of results into a decision or posterior -->
<!-- what can be learned from reanalysis? -->
<!-- (1) confirm there were not errors (consider changing A only) -->
<!-- (2) reassess what is known about the same inquiry, using new information about the world (change M, change A to suit new M) -->
<!-- (3) learn something new from the data about another node or edge or a different summary about the same ones (change I and possibly A to match it; possibly M if a node was missing; possibly add data) -->
<!-- (4) assess "robustness" of findings - point to discussion of this in answer strategy (or move it here) (change A) -->
<!-- (5) update M based on new research and assess what d can tell us from this study (change M and possibly I, possibly A to fit changed M and I) -->
<!-- how can we assess the properties of a *reanalysis*? diagnose changed MIDA. important to not condition on d, the design includes the actual D, and we need to consider what results d' we would get from the reanalysis under different realizations of D. -->
<!-- there are now two A's, so need to specify a decision function about how to integrate the two findings. this could be throw away the old a, or combine them in some way. if it's a "robustness" to alternative A, then you may want to combine not throw out for example. it's crucial to specify how you do that, that's part of the answer strategy. -->
<!-- ## Example -->
<!-- Knox, Lowe, and Mummolo (2020) (https://www.cambridge.org/core/journals/american-political-science-review/article/administrative-records-mask-racially-biased-policing/66BC0F9998543868BB20F241796B79B8) study the statistical biases that accompany estimates of racial bias in police use of force when presence in the dataset (being stopped by police) is conditioned on an outcome that is a downstream consequence of race. They show the estimate is not identified unless additional modelling assumptions are brought to bear. -->
<!-- Gaebler et al. (2020) (https://5harad.com/papers/post-treatment-bias.pdf) study the same question and make such modeling assumptions (subset ignorability, definition 2). -->
<!-- In a twitter thread (https://twitter.com/jonmummolo/status/1275790509647241222?s=20), Mummolo shows the three DAGs that are compatible with subset ignorability. We agree with Mummolo that these DAGs assume away causal paths that are very plausible. -->
<!-- ![DAG](figures/mummolo_dag.png) -->
<!-- This document provides a design declaration for this setting and shows how estimates of the controlled direct effect (effect of race on force among the stopped) are biased unless those paths are set to zero by assumption. -->
<!-- Design Declaration -->
<!-- There are four variables: (D: minority, M: stop, U: suspicion (unobserved), Y: force) and five paths: -->
<!-- ```{r} -->
<!-- D_M = 1 # effect of minority on stop -->
<!-- U_M = 1 # effect of suspicion on stop -->
<!-- D_Y = 1 # effect of minority on force -->
<!-- U_Y = 1 # effect of suspicion on force -->
<!-- M_Y = 1 # effect of stop on force -->
<!-- ``` -->
<!-- This basic design allows all five paths. -->
<!-- ```{r} -->
<!-- design_1 <- -->
<!--   declare_population(N = 1000, -->
<!--                      D = rbinom(N, size = 1, prob = 0.5), -->
<!--                      U = rnorm(N)) + -->
<!--   declare_potential_outcomes(M ~ rbinom(N, size = 1, prob = pnorm(D_M * -->
<!--                                                                     D + U_M * U)), -->
<!--                              assignment_variable = "D") + -->
<!--   declare_reveal(M, D) + -->
<!--   declare_potential_outcomes(Y ~ rnorm(N, D_Y * D + M_Y * M + U_Y * U), -->
<!--                              conditions = list(D = c(0, 1), M = c(0, 1))) + -->
<!--   declare_reveal(outcome_variables = "Y", -->
<!--                  assignment_variables = c("D", "M")) + -->
<!--   declare_estimand(CDE = mean(Y_D_1_M_1 - Y_D_0_M_1)) + -->
<!--   declare_estimator(Y ~ D, subset = M == 1, estimand = "CDE") -->
<!-- ``` -->
<!-- We redesign the design 3 times, removing one path at a time, then simulate all four designs. -->
<!-- ```{r, message=FALSE} -->
<!-- # no effect of D on M -->
<!-- design_2 <- redesign(design_1, D_M = 0) -->
<!-- # no effect of U on M -->
<!-- design_3 <- redesign(design_1, U_M = 0) -->
<!-- # no effect of U on Y -->
<!-- design_4 <- redesign(design_1, U_Y = 0) -->
<!-- ``` -->
<!-- This chunk is set to `echo = TRUE` and `eval = do_diagnosis` -->
<!-- ```{r, eval = do_diagnosis & !exists("do_bookdown")} -->
<!-- simulations <- simulate_designs(design_1, design_2, design_3, design_4, sims = sims) -->
<!-- ``` -->
<!-- Right after you do simulations, you want to save the simulations rds. -->
<!-- ```{r, echo = FALSE, purl = FALSE} -->
<!-- # figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file -->
<!-- rds_file_path <- paste0(get_dropbox_path("policing"), "/simulations_policing.RDS") -->
<!-- if (do_diagnosis & !exists("do_bookdown")) { -->
<!--   write_rds(simulations, path = rds_file_path) -->
<!-- } -->
<!-- simulations <- read_rds(rds_file_path) -->
<!-- ``` -->
<!-- ```{r, echo=FALSE, message = FALSE} -->
<!-- simulations <- -->
<!--   simulations %>% -->
<!--   mutate(`Assumed DAG` = factor( -->
<!--     design_label, -->
<!--     levels = c("design_1", "design_2", "design_3", "design_4"), -->
<!--     labels = c( -->
<!--       "All paths possible", -->
<!--       "no effect of D on M", -->
<!--       "no effect of U on M", -->
<!--       "no effect of U on Y" -->
<!--     ) -->
<!--   )) -->
<!-- summary_df <- -->
<!--   simulations %>% -->
<!--   group_by(`Assumed DAG`) %>% -->
<!--   summarise( -->
<!--     mean_estimand = mean(estimand), -->
<!--     mean_estimate = mean(estimate), -->
<!--     bias = mean(estimate - estimand) -->
<!--   ) %>% -->
<!--   pivot_longer(cols = c("mean_estimand", "mean_estimate")) -->
<!-- ``` -->
<!-- This plot confirms that unless one of those implausible assumptions hold, estimates of the CDE are biased. -->
<!-- ```{r, echo=FALSE} -->
<!-- ggplot(simulations, aes(estimate)) + -->
<!--   geom_histogram(bins = 50) + -->
<!--   geom_vline(data = summary_df, aes(xintercept = value, color = name)) + -->
<!--   facet_wrap(~`Assumed DAG`) + -->
<!--   xlab("Simulated CDE estimates") + -->
<!--   theme_bw() + -->
<!--   theme(legend.position = "bottom", -->
<!--         strip.background = element_blank(), -->
<!--         axis.title.y = element_blank(), -->
<!--         legend.title = element_blank()) -->
<!-- ``` -->
<!-- ### Grab bag -->
<!-- - @Clemens2017 on taxonomy of these kinds of efforts -->
<!-- - if you're going to use d to learn about a different M for a different I, you need to understand their D -->

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
<!-- start post here, do not edit above -->
</div>
<div id="replication" class="section level2">
<h2>
<span class="header-section-number">20.16</span> Replication<a class="anchor" aria-label="anchor" href="#replication"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<p>After your study is completed, it may one day be replicated. Replication differs from reanalysis in that a replication study involves the specification of a new MIDA and collection of new data to study the same inquiry. As discussed in the previous, a reanalysis may re-specify parts of the research design, but always re-uses the original data <span class="math inline">\(d\)</span> in some way.</p>
<p>So-called “exact” replications hold key features of I, D, and A fixed, but draw a new dataset <span class="math inline">\(d_{\rm new}\)</span> from <span class="math inline">\(D()\)</span> and apply the same <span class="math inline">\(A\)</span> to the new <span class="math inline">\(d\)</span> in order to produce a fresh answer <span class="math inline">\(a_{\rm new}^D\)</span>. Replications are said to “succeed” when <span class="math inline">\(a_{\rm old}^D\)</span> and <span class="math inline">\(a_{\rm new}^D\)</span> are similar and to “fail” when they are not. Dichotomizing replication attempts into successes and failures is usually not that helpful, and it would be better to simply characterize how similar <span class="math inline">\(a_{\rm old}^D\)</span> and <span class="math inline">\(a_{\rm new}^D\)</span> are.</p>
<p>Of course, exact replication is impossible: at least some elements of M have changed between the first study and the replication. Specifying how they might have changed, e.g., how outcomes vary with time, will help judge differences observed between <span class="math inline">\(a_{\rm old}^D\)</span> and <span class="math inline">\(a_{\rm new}^D\)</span>. Statistical noise will also play a role.</p>
<p>Replication studies benefit enormously from the knowledge gains produced by the original studies. For example, we learn a large amount about <span class="math inline">\(M\)</span> and the likely value of <span class="math inline">\(a^M\)</span> from the original study. The <span class="math inline">\(M\)</span> of the replication study can and should incorporate this new information. For example, if we learn from the original study that <span class="math inline">\(a^M\)</span> is positive but it might be small, the replication study could respond by changing <span class="math inline">\(D\)</span> in order to increase the sample size. Design diagnosis can help you learn about how to change the design of the replication study in light of the original study.</p>
<p>When designing <strong>original</strong> studies, you should anticipate that someday your work will be replicated. This improves your <em>ex ante</em> incentives. To the extent that you want future replication studies to arrive a similar answers to the original study you produce (i.e., you want their <span class="math inline">\(a_{\rm new}^D\)</span> to match your <span class="math inline">\(a_{\rm old}^D\)</span> as closely as possible), you will want to choose designs that bring <span class="math inline">\(a_{\rm old}^D\)</span> as close to <span class="math inline">\(a^M\)</span> as possible, under the presupposition that faithful replicators will also design their studies in such a way that <span class="math inline">\(a_{\rm new}^D\)</span> will also be close to <span class="math inline">\(a^M\)</span>.</p>
<p>Replication studies necessarily differ from original studies – it is literally impossible to reproduce the exact conditions of the original study in the same way it’s impossible to step in the same river twice. Another way of putting that same statement is that <span class="math inline">\(D_{\rm new}\)</span> is necessarily different from <span class="math inline">\(D_{\rm old}\)</span>. Theory (i.e., beliefs about <span class="math inline">\(M\)</span>) is the tool we use to say that <span class="math inline">\(D_{\rm old}\)</span> is similar enough to <span class="math inline">\(D_{\rm new}\)</span> to consititute a close enough replication study. As a concrete example, many survey experimental replications involve using the exact same experimental stimuli but changing the study sample, e.g., from a nationally representative sample to a convenience sample.</p>
<p>So-called “conceptual” replications alter both <span class="math inline">\(M\)</span> and <span class="math inline">\(D\)</span>, but keep <span class="math inline">\(I\)</span> and <span class="math inline">\(A\)</span> as similar as possible. That is, a conceptual replication tries to ascertain whether a relationship in one context (<span class="math inline">\(I(M_{\rm old})\)</span>) also holds in a new context (<span class="math inline">\(I(M_{\rm new}\)</span>). The trouble and promise of conceptual replications lies in the success of the designer at holding <span class="math inline">\(I\)</span> constant. Too often, a conceptual replication fails because in changing <span class="math inline">\(M\)</span>, too much changes about <span class="math inline">\(I\)</span> such that too much changes about the “concept” under replication.</p>
<p>There should be a summary function for how to interpret the difference between <span class="math inline">\(a_{\rm old}^D\)</span> and <span class="math inline">\(a_{\rm new}^D\)</span>. This may be take the new one and throw out the old if MIDA was poor in the first. It may be taking the average. It may be a precision-weighted average. Specifying this function ex ante may be useful, to avoid the choice of summary depending on the results of the replication. This summary function will be reflected in A and in the discussion section of the replication paper.</p>
<p><strong>Further reading</strong>.</p>
<ul>
<li>
<span class="citation">(<span class="citeproc-not-found" data-reference-id="Clemens2015"><strong>???</strong></span>)</span> on distinctions between replication and reanalysis</li>
</ul>
<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book--><!-- start post here, do not edit above -->
</div>
<div id="resolving-disputes" class="section level2">
<h2>
<span class="header-section-number">20.17</span> Resolving Disputes<a class="anchor" aria-label="anchor" href="#resolving-disputes"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<p>The problem: Acrimonious debates arise; Hard to interpret contribution of replication and reanalysis; First main task (accumulation of knowledge) damaged.</p>
<p>The solution: Some of this comes down to basic disagreements we can’t resolve. But some of it comes down to a lack of principles guiding how design decisions are made and how the results they produce should be interpreted. And a lack of procedures for understanding the consequences of decisions.</p>
<ul>
<li>Principles for making design choices, tailored to the distinct challenges posed to reanalysis and to replication</li>
<li>Some changes are justifiable / encouraged, some things are not justifiable / discouraged, conditions for justification are clarified</li>
<li>Procedures for putting principles into practice</li>
<li>Declaration and diagnosis through DD</li>
</ul>
<p>Current practice for replication is to exactly replicate data strategy and analysis strategy in new or same context. This is not needed! Standard should be: best answer to same inquiry in new or same context! But how can we justify changes to D and A that give a “better” answer to same inquiry?</p>
<p>Replacing them with alternative practices justified by design simulation
1. M always changes! (you have more information on tau or sd(tau))
2. Home ground dominance: Change A or D-and-A if A’ &gt; A under M
3. Robustness to alternative models: Change A or D-and-A if A’ ≥ A under M AND A’ &gt; A under M’ E.g. change from simple to complete RA
4. Model plausibility:If A’ &lt; A under M AND A’ &gt; A under M’, then change to A’ or D-and-A IFF M’ is more plausible than M E.g. switching to balanced design if you believe variances equal across treatment groups
5. Undefined inquiries. Change I to I’ if I is undefined under M If I is defined under M: You can’t change to I’, You can’t change D to D’ if that means I unidentifiable.</p>
<p>Disorganized thoughts:
- Changes to D include both interventions (sampling and randomization), as well as the inclusion of different / new datasets on the same model (this is common in econ reanalyses at state-level). The collection of “different” data through a change in question wording also fits into this. Need to think about a good typology of data strategies.
- There’s often a broader research question that’s being answered, and when I changes sometimes both are answering the same broader question. But focuses debate on whether that claim is true that I and I’ answer the same broader I.
- In replication can you use data from study 1 to assess the plausibility of M?
- When does changing outcomes change the inquiry?
Example: you used z-scores in your original analysis in order to measure an effect on five different measures of some latent construct. I show that taking a simple average has better properties (e.g., statistical power), and use this instead of z-score. Have I changed estimand? If so, are there any instances of “recoding” or even “rewording” of outcome measures that we would be OK with, insofar as they get better answers to the inquiry without changing the inquiry?
One way of looking at this: inquiry is in reference to summary of a latent variable, which stays constant, but D changes which is different measurement of the latent variable
Point to keep in mind from this: D change might be in sampling/treat assignment or measurement
Key thing we are saying here: there are two dimensions of change with measurement. (1) are you changing estimands because the latent construct is changing implicitly; (2) are you changing to a better/worse measurement of the same latent construct.</p>

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
<!-- start post here, do not edit above -->
</div>
<div id="synthesis" class="section level2">
<h2>
<span class="header-section-number">20.18</span> Synthesis<a class="anchor" aria-label="anchor" href="#synthesis"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<p>One of the last, if not the last, stage of the lifecyle of a research design is its eventual incorportation in to our common scientific understanding of the world. Research findings about specific Is – specific <span class="math inline">\(a^D\)</span>s need to be synthesized into our broader scientific understanding.</p>
<p>Research synthesis takes two basic forms. The first is meta-analysis, in which a series of <span class="math inline">\(a^D\)</span>s are analyzed together in order to better understand features of the distribution of answers obtained in the literature. Traditional meta-analysis typically focuses on the average of k answers: <span class="math inline">\(a_1^D\)</span>,<span class="math inline">\(a_2^D\)</span>,…<span class="math inline">\(a_k^D\)</span>. Studies can be averaged together in many ways that are better and worse. Sometimes the answers are averaged together according to their precision – a precision weighted average of estimates from many studies is equivalent to fixed-effects meta analysis. Sometimes studies are “averaged” by counting up how many of the estimates are positive and significant, how many are negative and significant, and how many are null. This is the typical averaging approach taken in a literature review. Regardless of the averaging approach, the goal of this kind of synthesis is to learn as much as possible about a particular <span class="math inline">\(I\)</span> by drawing on evidence from many studies.</p>
<p>A second kind of synthesis is an attempt to bring together many <span class="math inline">\(a^D\)</span>, each of which targets a different Inquiry about a common Model. This is the kind of synthesis that takes place across an entire research literature. Different scholars focus on different nodes and edges of the common model, so a synthesis needs to incorporate the diverse sources of evidence.</p>
<p>How can you best anticipate how your research findings will be synthesized? For the first kind of synthesis – meta analysis – you must be cognizant of keeping a commonly understood <span class="math inline">\(I\)</span> in mind. You want to select inquiries not for their novelty, but because of their commonly-understood importance. We want <em>many</em> studies on the effects of having women versus men elected officials on public goods because we want to understand this particular <span class="math inline">\(I\)</span> in great detail and specificity. While the specifics of the models <span class="math inline">\(M\)</span> might differ from study to study, the fact that the <span class="math inline">\(I\)</span>s are all similar enough to be synthesized allows for a specific kind of knowledge accumulation.</p>
<p>For the second kind of synthesis – literature-wide progress on a full causal model – even greater care is required. Specific studies cannot make up bespoke models <span class="math inline">\(M\)</span> but instead must understand how the specific <span class="math inline">\(M\)</span> adopted in the study is a specical case of some master <span class="math inline">\(M\)</span> that is in principle agreed to by a wider research community. The nonstop, neverending proliferation of study-specific theories is a threat to this kind of knowledge accumulation. (Cite cyrus on causal empiricism, that psych paper on crazy proliferation of theories).</p>
<p>A research synthesis is a “meta MIDA”</p>
<p>M: A master Model that subsumes portions of the sub Ms?
I: This is a summary of all of the Is across the stueis.
D: This is the inclusion / exclusion criteria. Transformations of the study data. standardization. (sampling, measurement.)
A: things like Random effects, fixed effects, Quimpo.</p>
<p><span class="math inline">\(I_1 \approx I_2 \approx I_3\)</span></p>
<p>Not</p>
<p><span class="math inline">\(a^M_1 \approx a^M_2 \approx a^M_3\)</span></p>
<p>Meta-analysis can be used not just to guess about effects out-of-sample but also to re-evaluate effects in sample: <a href="https://declaredesign.org/blog/2018-12-11-meta-analysis.html" class="uri">https://declaredesign.org/blog/2018-12-11-meta-analysis.html</a></p>
<ul>
<li>don’t select on DV</li>
<li>select on high quality MIDAs (drop those with bias)</li>
<li>precision weighting (accounting for the quality of the design indirectly!)</li>
</ul>
<!-- ## grab bag --><!-- -- systematic reviews are sign and significance, meta-analysis are point estimates --><!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book--><!-- start post here, do not edit above -->
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="part-iii-exercises.html"><span class="header-section-number">19</span> Part III Exercises</a></div>
<div class="next"><a href="part-iv-exercises.html"><span class="header-section-number">21</span> Part IV Exercises</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#research-design-lifecycle"><span class="header-section-number">20</span> Research Design Lifecycle</a></li>
<li>
<a class="nav-link" href="#p4planning"><span class="header-section-number">20.1</span> Planning</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#example-21"><span class="header-section-number">20.1.1</span> Example</a></li></ul>
</li>
<li><a class="nav-link" href="#ethical-review"><span class="header-section-number">20.2</span> Ethical Review</a></li>
<li><a class="nav-link" href="#ethical-principle-as-diagnosands"><span class="header-section-number">20.3</span> Ethical principle as diagnosands</a></li>
<li><a class="nav-link" href="#illustration-estimating-expected-costs-and-expected-learning"><span class="header-section-number">20.4</span> Illustration: Estimating expected costs and expected learning</a></li>
<li><a class="nav-link" href="#illustration-assessing-risks-of-adverse-events"><span class="header-section-number">20.5</span> Illustration: Assessing risks of adverse events</a></li>
<li><a class="nav-link" href="#partners"><span class="header-section-number">20.6</span> Partners</a></li>
<li><a class="nav-link" href="#funding"><span class="header-section-number">20.7</span> Funding</a></li>
<li><a class="nav-link" href="#p4piloting"><span class="header-section-number">20.8</span> Piloting</a></li>
<li><a class="nav-link" href="#implementation"><span class="header-section-number">20.9</span> Implementation</a></li>
<li>
<a class="nav-link" href="#p4populatedpap"><span class="header-section-number">20.10</span> Populated Preanalysis Plan</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#example-22"><span class="header-section-number">20.10.1</span> Example</a></li></ul>
</li>
<li>
<a class="nav-link" href="#reconciliation"><span class="header-section-number">20.11</span> Reconciliation</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#example-23"><span class="header-section-number">20.11.1</span> Example</a></li></ul>
</li>
<li><a class="nav-link" href="#writing"><span class="header-section-number">20.12</span> Writing</a></li>
<li>
<a class="nav-link" href="#publication"><span class="header-section-number">20.13</span> Publication</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#reviewing-papers"><span class="header-section-number">20.13.1</span> Reviewing papers</a></li>
<li><a class="nav-link" href="#responding-to-reviewers"><span class="header-section-number">20.13.2</span> Responding to reviewers</a></li>
</ul>
</li>
<li><a class="nav-link" href="#archiving"><span class="header-section-number">20.14</span> Archiving</a></li>
<li><a class="nav-link" href="#reanalysis"><span class="header-section-number">20.15</span> Reanalysis</a></li>
<li><a class="nav-link" href="#replication"><span class="header-section-number">20.16</span> Replication</a></li>
<li><a class="nav-link" href="#resolving-disputes"><span class="header-section-number">20.17</span> Resolving Disputes</a></li>
<li><a class="nav-link" href="#synthesis"><span class="header-section-number">20.18</span> Synthesis</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Research Design: Declare, Diagnose, Redesign</strong>" was written by Graeme Blair, Jasper Cooper, Alexander Coppock, and Macartan Humphreys. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
