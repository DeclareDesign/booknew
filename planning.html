<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 22 Planning | Research Design: Declare, Diagnose, Redesign</title>
<meta name="author" content="Graeme Blair, Alexander Coppock, and Macartan Humphreys">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.3.9000/tabs.js"></script><script src="libs/bs3compat-0.2.3.9000/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<link href="libs/bs4_book-1.0.0/dd_imgpopup.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/bs4_book-1.0.0/dd_imgpopup.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://hypothes.is/embed.js" async></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Research Design: Declare, Diagnose, Redesign</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li class="book-part">Introduction</li>
<li><a class="" href="preamble.html"><span class="header-section-number">1</span> Preamble</a></li>
<li><a class="" href="improving-research-designs.html"><span class="header-section-number">2</span> Improving research designs</a></li>
<li><a class="" href="primer.html"><span class="header-section-number">3</span> Software primer</a></li>
<li><a class="" href="part-i-exercises.html"><span class="header-section-number">4</span> Part I Exercises</a></li>
<li class="book-part">Declaration, Diagnosis, Redesign</li>
<li><a class="" href="declaration.html"><span class="header-section-number">5</span> Declaration</a></li>
<li><a class="" href="specifying-the-model.html"><span class="header-section-number">6</span> Specifying the model</a></li>
<li><a class="" href="defining-the-inquiry.html"><span class="header-section-number">7</span> Defining the inquiry</a></li>
<li><a class="" href="crafting-a-data-strategy.html"><span class="header-section-number">8</span> Crafting a data strategy</a></li>
<li><a class="" href="choosing-an-answer-strategy.html"><span class="header-section-number">9</span> Choosing an answer strategy</a></li>
<li><a class="" href="p2diagnosis.html"><span class="header-section-number">10</span> Diagnosis</a></li>
<li><a class="" href="redesign-2.html"><span class="header-section-number">11</span> Redesign</a></li>
<li><a class="" href="part-ii-exercises.html"><span class="header-section-number">12</span> Part II Exercises</a></li>
<li class="book-part">Research Design Library</li>
<li><a class="" href="research-design-library.html"><span class="header-section-number">13</span> Research Design Library</a></li>
<li><a class="" href="observational-designs-for-descriptive-inference.html"><span class="header-section-number">14</span> Observational designs for descriptive inference</a></li>
<li><a class="" href="observational-designs-for-causal-inference.html"><span class="header-section-number">15</span> Observational designs for causal inference</a></li>
<li><a class="" href="experimental-designs-for-causal-inference.html"><span class="header-section-number">16</span> Experimental designs for causal inference</a></li>
<li><a class="" href="experimental-designs-for-descriptive-inference.html"><span class="header-section-number">17</span> Experimental designs for descriptive inference</a></li>
<li><a class="" href="complex-designs-1.html"><span class="header-section-number">18</span> Complex designs</a></li>
<li><a class="" href="part-iii-exercises.html"><span class="header-section-number">19</span> Part III Exercises</a></li>
<li class="book-part">Research Design Lifecycle</li>
<li><a class="" href="research-design-lifecycle.html"><span class="header-section-number">20</span> Research Design Lifecycle</a></li>
<li><a class="" href="brainstorming.html"><span class="header-section-number">21</span> Brainstorming</a></li>
<li><a class="active" href="planning.html"><span class="header-section-number">22</span> Planning</a></li>
<li><a class="" href="during.html"><span class="header-section-number">23</span> During</a></li>
<li><a class="" href="planning-1.html"><span class="header-section-number">24</span> Planning</a></li>
<li><a class="" href="realization.html"><span class="header-section-number">25</span> Realization</a></li>
<li><a class="" href="integration.html"><span class="header-section-number">26</span> Integration</a></li>
<li><a class="" href="part-iv-exercises.html"><span class="header-section-number">27</span> Part IV Exercises</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="planning" class="section level1">
<h1>
<span class="header-section-number">22</span> Planning<a class="anchor" aria-label="anchor" href="#planning"><i class="fas fa-link"></i></a>
</h1>
<div id="ethics" class="section level2">
<h2>
<span class="header-section-number">22.1</span> Ethics<a class="anchor" aria-label="anchor" href="#ethics"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<p>Social scientists are increasingly focused on meeting ethical standards that go beyond the requirements of national laws. Journals are also starting to require ethical appendices describing human subjects protections and funders defenses of study ethics before funds are disbursed.</p>
<p>Common ethical principles include respect for persons, beneficence (researchers must have the welfare of participants in mind in designing the research), informed consent by participants, and minimizing harm to participants and others.</p>
<p>Ethical considerations extend well beyond the elements that are captured by a design declaration. For instance, a declaration of analytic relevant components of a design may tell you little about the level of care and respect accorded to subjects. Nevertheless, we think it useful to identify when design relevant features can be informative for ethical judgments.</p>
<div id="ethical-principles-as-diagnosands" class="section level3">
<h3>
<span class="header-section-number">22.1.1</span> Ethical principles as diagnosands<a class="anchor" aria-label="anchor" href="#ethical-principles-as-diagnosands"><i class="fas fa-link"></i></a>
</h3>
<p>Researchers can use the declare-diagnose-redesign process to help inform some ethical judgments. In particular, a diagnostic-statistic could be defined for each relevant ethical criterion. For example, a design can be scored based on the total cost to participants, how many participants were harmed (i.e., how many were retraumatized by being asked out past experience with violence), the average level of informed consent measured by a survey about comprehension of the study goals, or the risks of adverse events.</p>
<p>Consider two examples.</p>
<p><strong>Costs.</strong> A common concern is that measurement imposes a cost on subjects, if only by wasting their time. Subjects’ time is a valuable resource – they often donate willingly to the scientific enterprise by taking a survey or similar. Sometimes their generosity is repaid with financial compensation for their time. Sometimes subjects are unknowing participants in a research study because obtaining informed consent would so distort their behavior as to hinder the ability of the researchers to study it.</p>
<p><strong>Risks not just realizations.</strong> More subtly, different realizations of the data from the same data strategy may also differ in their ethical status. As a result, the ethics of the study cannot only be determined by looking at what in fact happened during the study — how many participants were there and how many were treated, how many were harmed, and how many raised complaints. Instead, the ethical status of the project depends on judgments about <em>potential</em> harms and <em>potential</em> participants: not only what did happen, but what could have happened.</p>
<!-- tara's paper relevant here -->
<p>When the design is diagnosed, then diagnosands can be constructed that summarize the level of ethical encumbrance across possible realizations of the design. The first way these ethical diagnosands can be used is to determine whether the study design meets a set of ethical thresholds. Is the probability of harm minimal enough? Is the average level of informed consent sufficient? Given that these characteristics vary across designs and across realizations of the same design, writing down concretely both what the measure of the ethical characteristic is and what the threshold is for the design to be ethical can help structure thinking. (These diagnoses and threshold determinations can also be shared in ethical writeups of the design.)</p>
<p>Among ethical designs, researchers must select a single design to implement. Often, once an ethical threshold is met, we select among feasible designs based on research design criteria such as statistical power and bias. Instead, we should continue to assess ethical considerations alongside the quality of the research design. Among ethical designs, there are still often tradeoffs between how much time is asked of subjects and the risk of harm. We should select the designs that weight these considerations (perhaps highly!) against the power of our designs. To do so, we can simply continue to include the diagnosands related to ethical criteria in our diagnoses and the redesign of studies.</p>
<p>A difficult challenge in this process is that in order to weigh ethical criteria against research design criteria such as power and cost, we must put be able to measure the two on a common scale. We must be able to think about the value to society of the research to weigh against the risks to participants and others. Similarly, we must be able to weigh more precise estimates of the same question against ethical considerations that also change based on the number of units and the proportion treated among other design features. By moving forward with research we must implicitly weigh these considerations. In IRB applications, we are often more directly asked to weigh the costs to subjects against the benefits of the research <em>to subjects</em> as well as to society as a whole.</p>
<p><strong>Further readings</strong>.</p>
<ul>
<li><span class="citation">Humphreys (<a href="references.html#ref-humphreys2015reflections" role="doc-biblioref">2015</a>)</span></li>
<li><span class="citation">Teele (<a href="references.html#ref-teele2020" role="doc-biblioref">2021</a>)</span></li>
<li><span class="citation">Meyer (<a href="references.html#ref-meyer2015two" role="doc-biblioref">2015</a>)</span></li>
<li><span class="citation">Luft (<a href="references.html#ref-luft2020you" role="doc-biblioref">2020</a>)</span></li>
<li><span class="citation">Baron and Young (<a href="references.html#ref-young2020" role="doc-biblioref">2020</a>)</span></li>
<li><span class="citation">Lyall (<a href="references.html#ref-lyall2020" role="doc-biblioref">2020</a>)</span></li>
</ul>
<!-- By declaring your expectations about the ethical outcomes of an experiment in terms of diagnosands such as the time participants devote to the study and the probability of harm to individuals, a declared research design can be an input to ethical reporting. Readers can review how you considered ethical outcomes in your design and judge the mitigation efforts you undertook in relation to those expectations. Other scholars have proposed including ethical assessments in preanalysis plans. Declarations of ethical diagnosands are a natural complement to these preregistered assessments. --><!-- ### Illustration: Estimating expected costs and expected learning  --><!-- <!-- I would prefer to have an example where there is consent but people are willing to tkae part because they value the outcome --><!-- We illustrate how researchers can weigh the tradeoffs between the value of research and its ethical costs with an audit study of discrimination in government hiring. The characteristics of applicants to a municipal government job are randomized. The rate of callbacks for a job interview are compared across applicant characteristics. We consider three different inquiries that could be studied with the design: the hiring rate for job applications from a Black applicant and a White applicant; the hiring rate between someone from the local area vs. someone equally qualified who lives far away; and the rates between someone who went to East High School and someone who went to West High School in town. We judge the questions to rank in importance between high (the question of racial discrimination), medium (local favoratism), and low (personal interest). The value of the research is a function of the importance of the inquiry, but also how much we learn about it. We proxy for the learning from the experiment by sample size: the higher the $N$, the more we learn, but with decreasing marginal returns (it's a lot better to have a sample of 100 compared to 10; it matters less if it 1010 or 1100). Figure \@ref(fig:ethicsplot) shows the three research value curves labeled by the importance of the inquiry. --><!-- Because the job applicants are fictitious but appear real, a primary ethical concern in audit experiments is how much time hiring managers (the participants in the research) spend reviewing the fictitious applications. In the case of government hiring, it is public money spent on their review. The time cost to participants is linear in the number of applications: each application takes about ten minutes to review, regardless of how many are sent. We represent the cost to participants as the purple line in Figure \@ref(ethicsplot).  --><!-- We have placed the costs to participants on the same scale as the value of the research, by placing a value to society of the research and the value to society of the time of the hiring managers. When benefit exceeds cost (upper blue region), we decide to move forward with the research; if costs exceed benefits (lower gray region), we do not conduct the study. --><!-- The conclusion of the graph is that for high-importance inquiries, it is almost always worth doing the study. We get a lot of value from the research, despite the costs to participants. However, there is a region at low sample sizes where the cost to participants exceed the benefits from the research, because of the very imprecise answers we get from the research. We don't learn enough about the inquiry, despite its importance, to make it worth wasting the hiring managers time. By contrast, for low importance inquiries, it is never worth conducting the study. The costs to participants always exceed the (low) value of the research. Medium importance questions are in the middle: there is a region of the benefits curve (highlighted in blue) where it is worth doing the study, but two regions (highlighted in gray) where it is not worth it. The left region is where the sample is too small so the value of the research is low both because of its medium importance and we do not learn enough about it. The second gray region at right in the medium importance curve is where though we learn a lot about the inquiry, the cost is too high from the many hours of hiring manager time to justify what we learn because the inquiry is not important enough.  --><!-- In short, ethical determinations require diagnosis both of how much we learn and how much it costs to participants (along with other ethical costs), and we must place a value on both ethical costs and research benefits in order to compare them on the same scale.  --><!-- ```{r ethicsplot, echo=FALSE, fig.cap = "Tradeoffs between ethical costs and scientific benefits. A design might have too *many* subjects but also too *few* subjects.", fig.height=5, fig.width=5} --><!-- dat <-  --><!--   tibble( --><!--     X = seq(1, 10, 0.001), --><!--     cost =  4* X + 6 , --><!--     high = log(X, base = 1.05), --><!--     med = log(X, base = 1.06), --><!--     low = log(X, base = 1.10) --><!--   )  --><!-- dat_long <- --><!--   dat %>%  --><!--   pivot_longer(c(high, med, low)) %>% --><!--   mutate(cost_benefit = if_else(value > cost, "A", "B")) --><!-- label_df <- --><!--   tibble( --><!--     X = c(9.9, 9.9, 9.9), --><!--     value = c(22, 36, 48), --><!--     label = c("Low", --><!--               "Medium", --><!--               "Inquiry importance: High"), --><!--     cost_benefit = c("B", "B", "A") --><!--   ) --><!-- ggplot(dat_long, aes(X)) + --><!--   geom_ribbon(data = dat, aes(ymax = cost, x = X, ymin = 0), fill = gray(0.8, alpha = 0.3)) +  --><!--   geom_ribbon(data = dat, aes(ymin = cost, x = X, ymax = 50), fill = "#72B4F344") +  --><!--   geom_line(aes(y = value, color = cost_benefit, group = name)) + --><!--   geom_line(data = dat, aes(y = cost), color = dd_pink) + --><!--   geom_text(data = label_df, aes(y = value, label = label, color = cost_benefit), hjust = 1) + --><!--   scale_color_manual("", values = c(dd_dark_blue, gray(0.5))) +  --><!--   annotate("text", x = 1.5, y = 42, label = "Scientific benefits exceed ethical costs", hjust = 0, color = dd_dark_blue) +  --><!--   annotate("text", x = 4, y = 7, label = "Ethical costs exceed scientific benefits", hjust = 0, color = dd_gray) +  --><!--   labs(x = "Sample size", y = "Costs and benefits") +  --><!--   dd_theme() + --><!--     theme(legend.position = "none", --><!--           axis.text = element_blank(), --><!--           panel.grid.major = element_blank()) --><!-- ``` --><!-- ### Illustration: Assessing risks of adverse events  --><!-- <!-- Tara like exampe of swinging an election --><!-- * * * --><!-- - Is it the measurement they object to? Is it the random assignment?  --><!-- papers to cite here: the A/B illusion (Michelle Mayer); Cite APSA guidelines; preregistering ethics guidelines (Lyall paper); report on ethics in paper (Lauren Young's paper); Mac's paper; Belmont report --><!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book--><!-- start post here, do not edit above -->
</div>
</div>
<div id="approvals" class="section level2">
<h2>
<span class="header-section-number">22.2</span> Approvals<a class="anchor" aria-label="anchor" href="#approvals"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<p>When researchers sit at universities in the U.S., research must be approved by the university’s institutional review board (IRB) under the federal regulation known as the “Common Rule.” Similar research review bodies exist at universities around the world and at many independent research organizations and think tanks. Though these boards are commonly thought to judge the ethics of a piece of research, in fact they exist to protect their institution from liability for research gone awry (King and Sands 2015). This means that a researcher’s obligation to consider the ethics of their study is neither constrained nor checked by IRBs. Instead, a set of idiosyncratic rules and practices specific to the institution are checked. The researcher, as a result, remains responsible for their own ethical decision about whether or not to move forward with the research. The IRB process is not necessarily without benefit: in some cases, useful discussions can be had with IRB board members about study decisions. The approval itself may protect the researcher from some kinds of liability.</p>
<p>Laws and regulations at the country, state or province, and municipality level may also govern research on human subjects besides the IRB. Many countries require human subjects approval for research, especially for health research, in addition to the approvals researchers must seek from their home institutions. These approvals serve a similar purpose to the home institution IRB, but by virtue of their authority coming from the context in which the research is conducted rather than from far away bureaucrats.</p>
<p>Though the goals of these bodies differ from the broader ethical aims social scientists hold, design diagnosis may also be useful here. Many IRBs ask researchers to describe tradeoffs between the costs and benefits to research subjects. In some cases, researchers are asked to defend research design choices that provide benefits to science, but the only direct effects on participants are costs (e.g., the time to participate) without direct benefits. Defining the costs and benefits to participants in terms of their time and money as well as the compensation provide by researchers, if any, can both simplify communication with IRBs and provide tools for researchers to more easily clarify these tradeoffs for themselves. The expected benefit and expected cost can be diagnosands across possible realizations of the design. The design diagnosis can highlight tradeoffs, then, between the value to participants and the scientific value in the form of standard diagnosands such as root mean-squared error or more ambitiously the likelihood of shifting priors about the research question. Rather than argue in the abstract about these quantities, they can be simulated and described formally through declaration and diagnosis.</p>
<!-- Local research approval -->
<!-- Partner approval  -->
<!-- - IRB approval (purpose is to protect the university from liability) - Gary and Melissa's paper -->
<!-- - IRB approval at your institution and with others -->
<!-- - IRB in context where you are working - country IRBs, what to do if there is not a country IRB -->
<!-- - research clearances -->
<!-- - history: belmont report. tuskegee experiment. stanford prison experiment.  -->
<!-- - laws and IRBs: revised common rule, paper on IRBs -->
<!-- - guidelines from econ, poli sci, soc, psych. -->

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
<!-- start post here, do not edit above -->
</div>
<div id="partners" class="section level2">
<h2>
<span class="header-section-number">22.3</span> Partners<a class="anchor" aria-label="anchor" href="#partners"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<p>Partnering with third-party organizations in research — cooperating to intervene in the world or to measure outcomes — is increasingly common in the social sciences. Researchers seek to produce (and publish) scientific knowledge; they work with political parties, government agencies, non-profit organizations, and businesses to learn more than they could if they worked independently. These groups consent to working with researchers in order to learn about how to achieve their own organizational goals. For example, a government may want to learn how to expand access to healthcare or a corporation may want to learn how to improve its ad targeting.</p>
<p>In the best case scenario, the goals of the researchers and partner organizations are aligned. When the scientific question to be answered (the inquiry) is the same as the practical question the organization cares about, the gains from cooperation are clear. The research team gains access to the financial and logistical capacity of the organization to act in the world and the partner organization gains access to the scientific expertise of the researchers. Finding a good research partner almost always amounts to finding an organization with a common – or at least not conflicting – goal. Understanding the private goals of the partner and of the researcher is essential to selecting a research design amenable to both parties. Research design declaration and diagnosis can help with this problem by formalizing tradeoffs between the two sets of goals.</p>
<p>One common divergence between partner and researcher goals is that partner organizations often do want to learn, but they care most about their primary mission. In business settings, this dynamic is sometimes referred to as the “learn versus earn” or “exploration-exploitation” tradeoff. An aid organization (and their donors) cares about delivering their program to as many people as possible; learning whether the program has the intended effects on the outcomes of interest is obviously also important, but resources spent on evaluation are resources <em>not</em> spent on program delivery.</p>
<p>Research design diagnosis can help navigate the learning-doing tradeoff. One instance of the tradeoff is that the proportion of units who receive a treatment (e.g., a medicine) represents the rate of “doing” and also affects the amount of learning in that in the extreme if all units are treated there can (typically) be no learning about the <em>effect</em> of the treatment. The tradeoff here is represented in a graph of the power of the study vs. the proportion treated (top facet), and the utility of the partner (bottom facet) which is increasing in the proportion treated. The researchers have a power cut-off at the standard 80% threshold. The partner also has a strict cut-off: they need to treat at least 2/3 of the sample in order to fulfill a donor requirement.</p>
<p>In the absence of partners, researchers might simply ignore the proportion treated axis and select the design with the highest power. With a partner organization, the researcher might use this graph in conversation with the partner to jointly select the design that has the highest power that has a sufficiently high proportion treated to meet the partner’s needs. This is represented in the “zone of agreement” in gray: in this region, the design has at least 80% power and at least 2/3 of the sample are treated. Deciding within this region involves a trade-off between power (which is decreasing in the proportion treated here) and the utility of the partner (which is increasing in proportion treated). The diagnosis surfaces the zone of the agreement and clarifies the choice between designs in the zone.</p>
<div class="figure">
<span id="fig:partnersplot"></span>
<img src="book_files/figure-html/partnersplot-1.svg" alt="Navigating research partnerships." width="100%"><p class="caption">
Figure 22.1: Navigating research partnerships.
</p>
</div>
<p>Choosing the proportion treated is one example of integrating partner constraints into research designs to generate feasible designs. A second common problem is that there are a set of units that <em>must</em> be treated, for ethical or political reasons (e.g., the home district of a government partner must receive the treatment), or that must not be treated. If these constraints are discovered after treatment assignment, they lead to noncompliance, which may substantially complicate analysis of the experiment and even prevent providing an answer to the original inquiry. Considerable thought has been given to avoiding this type of noncompliance. <span class="citation">Alan S. Gerber and Green (<a href="references.html#ref-Gerber2012" role="doc-biblioref">2012</a>)</span> recommend, before randomizing treatment, exploring possible treatment assignments with the partner organization and using this exercise to elicit the set of units that must or cannot be treated. <span class="citation">King et al. (<a href="references.html#ref-king2007politically" role="doc-biblioref">2007</a>)</span> describe a “politically-robust” design, which uses pair-matched block randomization and when any unit is dropped due to political constraints the pair is dropped from the study.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Note the inquiry that can be answered from such a design is complex, reflecting effects for units in pairs that would not be dropped in either possible random assignment within the pair.&lt;/p&gt;"><sup>27</sup></a></p>
<p>Design diagnosis can help in these circumstances by providing a mechanism to specify possible patterns of noncompliance and diagnosing alternative designs to mitigate the negative consequences. In addition, they can be used to communicate with partners about the consequences of noncompliance for the research to help make better decisions together about how to avoid noncompliance once treatment is randomized and the research is in progress.</p>
<p>Our best advice for working with partners is to involve them in the design declaration and diagnosis process. How can we develop intuitions about the means, variances, and covariances of the important variables to be measured? Ask your partner for their best guesses, which may be far more educated than your own. For experimental studies, solicit your partner’s beliefs about the magnitude of the treatment effect on each outcome variable, subgroup by subgroup if possible. Be specific – ask what they think the average will be in the control group and what the average will be in the treatment group. Sorting out these beliefs very quickly sharpens the discussion of key design details. Share your design diagnoses and mock analyses <em>before</em> the study is launched in order to build a consensus around the goals of the study.</p>
<p>Sometimes a partnership simply will not work out. Indeed most partnerships are never even initiated because researcher and organizational goals are too far apart. If you find yourself in a setting where partnership is doing too much violence to the research design, find a way to walk away from the project.</p>
<!-- ### Scattered thoughts -->
<!-- - Partnerships and ethics should be in here -->
<!-- - Research for Impact (@levine_2020) -->
<!-- - When to walk away -->
<!-- - Advice: there's usually a boss that gives you the green light, then staff people who actually help you. Our advice is to ensure that at least some meetings are with both the boss and the staff people so that the staff people know that the boss cares about the project and are therefore motivated to help you. A crucial person at the partner organization is the person who knows where the data are and how you can access them. Many people at partner organizations do not have this power, but you must be frequent touch.  -->
<!-- - In some cases, some member of the research staff can actually embed wiht the partner organization to ensure all the good things. -->
<!-- - Involve partners in PAP writing. -->
<!-- - when should partners be coauthors on the paper? -->

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
<!-- start post here, do not edit above -->
</div>
<div id="funding" class="section level2">
<h2>
<span class="header-section-number">22.4</span> Funding<a class="anchor" aria-label="anchor" href="#funding"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<!-- point 1 -->
<p>In any design, there will be tradeoffs between diagnosands about the quality of the research as well as about its costs. Costs are a function both of the data strategy (some are more expensive than others!) and how the data are realized each time. Collecting original data is more expensive than analyzing existing data, but collecting new data may be more and less expensive depending on how easy it is to reach specific individuals to interview them. As a result, diagnosing research designs including cost diagnosands is important, and those diagnosands may usefully include both average cost and maximum cost. Researchers may make different decisions about cost: in some cases, the researcher will select the “best” design in terms of research design quality subject to a budget constraint, others will choose the cheapest among similar quality designs in order to save money for future research. Diagnosis can help identify each set and decide among them.</p>
<p>To relax the budget constraint, researchers apply for funding. Funding applicants wish to obtain as large a grant as possible for their design, but have difficulty credibly communicating the quality of their design given the subjectivity of the exercise. Funders, on the flip side, wish to get the most value for money in the set of proposals they decide to fund, and have difficulty assessing the quality of proposed research. MIDA and design declaration provide a tool for speaking in a common language that can be more easily verified by both sides about what design is being proposed and what its value is to knowledge under a set of assumptions that can be interrogated by funders.</p>
<p>Funding applications often aim to communicate what research design is being proposed; why learning answers from the design would be useful, important, or interesting to scholars, the public, policymakers, or another audience; how the research design provides credible answers to the question; that the researcher is capable of executing the design; and that there is value-for-money in the design and the answers it provides.</p>
<p>A new section of funding applications that would aid in communicating about each of these questions is declaring the MIDA of the design and presenting a diagnosis of the design. In addition to common diagnosands such as bias and efficiency, two special diagnosands may be valuable: cost and, related, value-for-money. Cost can be included for each design variant as a function of design features such as sample size, the number of treated units, and the duration of survey interviews. The cost may vary by these parameters and may vary across possible designs when, for example, the number of treated units is a random number. Simulating the design across possible realizations of the design, thus, provides a distribution of costs as a function of choices the researcher makes. Value for money is a diagnosand that is a function of cost and also the amount that is learned from the design. RMSE might be one value criterion, another would be the average difference between priors and posteriors under a Bayesian answer strategy (a direct measure of learning).</p>
<p>In some cases, funders request applicants to provide multiple options and multiple price points or to make clear how a design could be altered such that it could be funded at a lower (or higher) level. Redesigning a design with differing sample sizes would communicate how the researcher conceptualizes these options, but also provide the funder with an understanding of tradeoffs between the amount of learning and cost in these design variants. Applicants could use redesign to justify the high cost of their request and to ask for additional funding.</p>
<p>Ex ante power analyses, required by an increasing number of funders, illustrate the crux of the misaligned incentives between applicants and funders. A power analysis can demonstrate that almost any design is “sufficiently powered” by changing expected effect sizes and noise. By clarifying the assumptions of the power analysis in code, researchers can more easily defend these choices. Funders can more easily interrogate these assumptions. Power analyses using standard power calculators online have difficult-to-interrogate assumptions built in and cannot accommodate the specifics of many common designs. As a result, many return incorrect estimates of power for these designs <span class="citation">(Blair et al. <a href="references.html#ref-bccmapsr" role="doc-biblioref">2020</a>)</span>.</p>
<p>Funders who request design declarations can compare funding applications on common scales: root mean-squared-error, bias, and power. Of course, they also want to weigh considerations like the importance of the question and the fit with their funding program. But moving design considerations onto a common scale takes guesswork out of the process and reduces reliance on researcher claims about properties.</p>
<!-- -- funders often request power analysis, but these are typically described in words and thus the assumptions behind them cannot be interrogated fully. (a) not in code, so not specific; (b) user power calculators that are wrong (cite paper); (c) do not provide the details funders need to verify whether they agree with the assumptions.  -->
<!-- -- for funders, providing MIDA declared in code allows them to change the parameters of the design and test how the properties of the design change with beliefs about the world in M or data strategy parameters in D such as sample size, rather than having to rely on claims by applicants -->
<!-- -- often funders require regular reporting on changes in plan -- MIDA and design declaration provides a way to communicate (a) what those changes are and (b) how they change the values of diagnosands.  -->
<!-- -- value for money as a diagnosand (cost of each design as a function of design parameters) -->
<!-- -- allows funders to compare on a common scale (the same set of diagnosands) funding proposals -- often trying to evaluate "quality" but hard to do that with narrative proposals -->
<!-- ### References -->

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
<!-- start post here, do not edit above -->
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="brainstorming.html"><span class="header-section-number">21</span> Brainstorming</a></div>
<div class="next"><a href="during.html"><span class="header-section-number">23</span> During</a></div>
</div></main><div id="on-this-page-nav" class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#planning"><span class="header-section-number">22</span> Planning</a></li>
<li>
<a class="nav-link" href="#ethics"><span class="header-section-number">22.1</span> Ethics</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#ethical-principles-as-diagnosands"><span class="header-section-number">22.1.1</span> Ethical principles as diagnosands</a></li></ul>
</li>
<li><a class="nav-link" href="#approvals"><span class="header-section-number">22.2</span> Approvals</a></li>
<li><a class="nav-link" href="#partners"><span class="header-section-number">22.3</span> Partners</a></li>
<li><a class="nav-link" href="#funding"><span class="header-section-number">22.4</span> Funding</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="download-code" href="./planning.R"><i class="far fa-file-code"></i> Download R code</a></li>
          
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Research Design: Declare, Diagnose, Redesign</strong>" was written by Graeme Blair, Alexander Coppock, and Macartan Humphreys. </p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
