---
title: "Archiving"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# Archiving

<!-- make sure to rename the section title below -->

```{r archiving, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

One of the biggest successes in the push for greater research transparency has been changing norms surrounding the sharing of data and analysis code after studies have been published. It has been become de rigeur at many journals to post these materials at publicly-available respositories like the OSF or dataverse. This development is undoubtedly a good thing. In older manuscripts, sometimes data or analyses are described as being "available upon request" but of course such requests are sometimes ignored. Furthermore, a century from now, study authors will no longer be with us even if they wanted to respond to such requests. Public respositories have a much better chance of preserving study information for the future.

What is in the typical replication archive. First, the data $d$ itself. Sometimes this is the raw data, sometimes it is only the "cleaned" data that is actually called by analysis scripts. Where ethicially possible, we think it is probably better to post as much of the raw data as possible alongside a cleaning script that ingests the raw data and outputs the cleaned data. Cleaning scripts might be considered a part of the data strategy $D$ in the sense that they complete the measurement procedures laid out in $D$. They might also be considered part of the answer strategy in the sense that they apply an interpretation to the data provided by the world. We do not take a hard stance on whether data cleaning procedures rightly belong in $D$ or $A$ since the choice likely varies from study to study.

Replication archives also include $A$, the set of fucntions applied to $d$ that produce $a^D$. It is vitally important that the *actual* analysis code is archived because the natural-language descriptions of $A$ that are typically given in papers are imprecise. As a small example, many articles describe their answer strategies as "ordinary least squares" but do not fully describe the set of covariates used or what flavor of standard errors were estimated. The actual analysis code makes $A$ explicit.

While typical replication archives include $d$ and $A$, we think that future replication archives should also include a design declaration that fully describes $M$, $I$, $D$, and $A$ -- that is, we should archive designs, not just data and analysis code. This should be done in code and words. In addition, a diagnosis should be included, demonstrating the properties as understood by the author and also indicating the diagnosands that the author considered in judging the quality of the design.

See also: alex's paper on active maintenance.

Graphic: file structure for a replication archive for the example introduced in planning with MIDA in it. Example is archive at OSF: https://osf.io/4vuqh
