---
title: "Implementation"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# Realization

## Implementation

<!-- make sure to rename the section title below -->

```{r implementation, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

Your design declaration is a road map for implementing your study. The data strategy tells you the procedure to use to sample units; how to assign treatments; and which variables to measure. Your answer strategy is the function that translates the realized data into a set of answers to your inquiries and statistics that communicate your confidence in those answers. If you have specified the data and answer strategies in sufficient detail in code, you can directly run the functions you declared to sample units and assign them to treatment and to analyze your data.

The road map is useful as a tool to learn where to go when things go right, but also to identify when you take a wrong turn and need to make decisions about how to get to an answer. In this sense, a design declaration should be a living document, updated to reflect the set of decisions you make along the way through the twists and turns of the research road. With the model of the world and an inquiry about it declared, when you are unable to collect a variable, treat a subset of units with treatment, or to reach some units for follow-up surveys, you can compare alternative ways of handling this deviation from your original plan on the same terms as you originally designed the experiment. These are all changes in your data strategy $D$. You can assess options and also guide your decision-making about whether to continue your study at all or use the money for another better purpose. You can also use the comparison of diagnosands under alternative options as a tool to communicate with your research partners about why a change to their practices is needed. You can also use it to defend your intermediate data strategy choices when you are finished to reviewers and readers.

As you make changes to $D$, changes in $A$ may also be required in order to follow the principle of analyzing as you sample, assign treatment, and measure. When you switch from an individual randomization to a cluster randomization because it is not logistically possible to individually assign units to treatment, you will typically want to adjust your answer strategy to account for clustering in the calculation of standard errors. By keeping both your data strategy and your answer strategy up to date as you implement your study, you may also identify new data that must be collected or new steps to take in order to still be able to provide credible answers.  

You also learn more about your design as you go along, not because anything goes wrong but as a natural progression of the research. For example, you may not know how many units there are per cluster or per block, key details in assigning treatments and analyzing data from experiments. When you learn these details, change your data and answer strategies to reflect these new details --- and diagnose the new design to be sure you still agree with your original choices. Beyond the data and answer strategies, you may also learn about new nodes or edges in the model during the course of research. When you learn about new confounders or mediators, update your model, but also consider whether changes to your data and answer strategy are necessitated to ensure you can answer your original inquiry.

In short, your design declaration is a living document that you can keep updated and use as a tool to guide you along the research path, not just as a document to write at the beginning of the study and revisit when you are writing up. This advice is in apparent tension with the idea of preanalysis plans, in which you precommit to your analysis choices before data is collected. It need not be. It is useful to keep your original design declaration and to preregister it, but it is also useful to keep the declaration updated as you make the changes along the way that inevitably happen. You will be in a better position to make good choices when things go awry, and also to communicate when and why you made changes to your design.

<!-- outline: -->
<!-- -- use your MIDA to figure out how to implement -->
<!-- -- redesign as you go when you haven't gotten specific enough -->
<!-- -- consider the whole design -- ex ante declared, then changed -- and what *could have happened*, which may be known to you ex ante (there could or could not be attrition) or unknown (you learn during implementation that there are some kinds of units that won't comply, so you need to think not just about which ones did or did not comply but which ones *could* have). the whole process is a function of your interventions in the world (treatment or measurement), so write down the whole process and potential outcomes to understand what you can and cannot learn. -->
<!-- -- use your MIDA to help you *make* logistical choices, help it prevent you from making bad decisions and use as tool to communicate with partners and implementers about why you do or do not wnat to make different changes (or BETTER evaluate tradeoffs in those decisions) -->

<!-- idea bin: -->
<!-- -- MIDA is a roadmap for how to implement the study -->
<!-- -- when there is a part not specified, redesign to specify and diagnose again -->

<!-- -- lots of choices you make after you start, because it was not clear what decisions would have to be made ex ante: how to make these choices? (redesign and diagnose!).  -->
<!-- -- often randomization procedure will have to take into account specifities of the number of units (odd numbers), blocks with differing numbers of units, cluster of varying sizes, etc. that require revising D -->
<!-- -- unexpected constraints come in that affect D, and may require changes to D but also A. redesign. -->
<!-- -- how to make decisions about unexpected changes when things going wrong? (even if you don't have a PAP). often want to create multiple variants of the design incorporating what went wrong. common example: unexpected noncompliance or unexpectedly high rates of attrition. may want to change analysis plan, and register it, so can use redesign to develop that new plan. but also may discover diagnosands are not good enough, so may want to change data strategy midstream to mitigate these problems. -->
<!-- -- what to do when you run out of money or feasibility of sample size or other aspects of D becomes clear. redesign mountain subject to new cost constraints. -->
<!-- -- make go-no go decisions about whether to continue -->
<!-- -- how to know if your inquiry is no longer answerable -->
<!-- -- updating your PAP -->

<!-- -- assessing what you can learn based on the implementation challenges: these are potential outcomes, i.e. could be affected by treatment, so are often informative about what we can learn about the original research question -->

<!-- -- often need to convince partners to not change plans -- MIDA can be a tool for helping assess tradeoffs in learning and doing -->

<!-- -- Projects that succeed have direct researcher invovlement. Outsourcing too much of the design can lead to big troubles. -->


**Related readings**.

- Failure (@karlan2018failing)




