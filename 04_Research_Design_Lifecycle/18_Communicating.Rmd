---
title: "Communicating"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

<!-- make sure to rename the section title below -->

```{r communicating, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

## Communicating

The findings from studies are communicated to other scholars through academic publications. But some of the most important audiences -- policymakers, businesses, journalists, and the public at large -- do not read academic journals. These audiences learn about the study in other in other ways. Authors write opeds, blog posts, and policy reports that translate research for nonspecialist audiences. Press offices pitch research studies for coverage by the media. Researchers present findings directly to decisionmakers and to their research partners.

These new outputs are for different audiences, so they are necessarily diverse in their tone and approach. Some things don't change: we still need to communicate the quality of the research design and what we learn from the study. But some things do: we need to translate specialist language about the substance of the study to a nonspecialist audience, and translate the features of the research design in a way that nonspecialists can understand. If we do not communicate outside of academic publications, research findings will not influence public debates, policymaking, or real-world decisions by people and firms.

Too often, a casualty of translating the study from academic to other audiences is the design information. When researchers write for popular blogs or give interviews, emphasis is placed on the study results, not on the reasons why the results of the study are to be believed. In sharing the research for nonspecialist audiences, we revert to saying *that* the findings are true and not *why we know* the findings are true. Explaining why we know requires explaining the research design, which in our view ought to be part of any public-facing communication about research.

Of course, even when authors do emphasize design, journalists do not always care. Science reporting is commonly criticized for ignoring study design when picking which studies to publicize, so weak studies are not appropriately filtered out of coverage. Furthermore, journalists emphasize results they believe will drive people to pick up a newspaper or click on a headline. Flashy, surprising, or pandering findings receive far more attention than deserved, with the result that boring but correct findings are crowded out of the media spotlight. 

In a review we conducted of recent studies published in *The New York Times* Well section on health and fitness, we found that two dimensions of design quality were commonly ignored. First, experimental studies on new fitness regimens with tiny samples, sometimes fewer than 10 units, are commonly highlighted. When both academic journals and reporters promote tiny studies, the likely result is that the published record is full of statistical flukes driven by noise, not new discoveries. Second, very large studies that draw observational comparisons between large samples of dieters and non-dieters with millions of observations receive outsize attention. These designs are prone to bias from confounding, but these concerns are swept under the rug. 

This state of affairs is not entirely or even mostly the journalists' fault, since, in the absence of design information, it can be challenging to separate the weak designs from the strong ones. Study results and the stamp of approval from peer review are too-easy heuristics to follow. 

How can we improve this scientific communication dilemma? The market incentives for both journalists and authors reward flash over substance, and any real solution to the problem would require addressing those incentives. Short of that, we recommend that authors who wish to communicate the high quality of their designs to the media do so by providing the design information in *M*, *I*, *D*, and *A* in lay terms. Science communicators should clearly state the research question (*I*) and explain why applying the data and answer strategies is likely to yield a good answer to the question. The actual result is, of course, also important to communicate, but *why* it  is a credible answer to the research question is just as important to share. Building confidence in scientific results requires building confidence in scientific practice.





<!-- Science reporting often has two aims: surface new discoveries, and inform decisions we make. New discoveries, such as a new cure on the horizon or better idea for reducing poverty, may not immediately affect decisions we make: we need further confirmation of their effectiveness in followup studies.  -->

<!-- A single study is typically not enough to immediately change decisions. Rather, most decisions should be based on the current scientific consensus on an inquiry (which itself could be informed by a single study, but in concert with those that came before). Scholars, therefore, may usefully consider how their study will or will not impact the scholarly consensus while designing a study.  -->

<!-- Conducting a meta-analysis or systematic review of past studies on the inquiry and considering what kind of evidence would shift the consensus -- either in terms of the sign or magnitude of the evidence or its strength. Each can be specified as a diagnosand: how much does the prior consensus on the average effectiveness of an intervention shift in response to evidence from this study.  -->

<!-- Design choices the researcher makes, such as sample size but also how they present the results in relation to past evidence, impact these diagnosands. On the flip side, reporters can then use the information about the current consensus and its strength (i.e., the standard deviation of the posterior), not only the findings from a single study.  -->

<!-- -- should we publish working papers -->
<!-- -- which papers should we *seek* reporting on -->
<!-- -- should reporters select  -->

<!-- - research design diagnosis can provide a tool for the media to assess the quality of evidence from a study they are considering for publication -->
<!-- - two kinds of bias: tiny samples and p-hacking; and confounding in observational studies -->
<!-- - susceptibility to clickbait  -->

<!-- - media publication bias from communication  -->
<!-- - bias from what scholars are able to and decide to share research outside  -->
<!-- - problem that lay audiences cannot distinguish quality of evidence -->
<!-- - communicating one study vs meta-analysis -->
<!-- - communicating scholarly consensus -->


