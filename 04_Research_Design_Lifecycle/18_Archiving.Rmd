---
title: "Archiving"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

## Archiving

<!-- make sure to rename the section title below -->

```{r archiving, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

One of the biggest successes in the push for greater research transparency has been changing norms surrounding the sharing of data and analysis code after studies have been published. It has been become de rigeur at many journals to post these materials at publicly-available repositories like the OSF or Dataverse. This development is undoubtedly a good thing. In older manuscripts, sometimes data or analyses are described as being "available upon request" but of course such requests are sometimes ignored. Furthermore, a century from now, study authors will no longer be with us even if they wanted to respond to such requests. Public repositories have a much better chance of preserving study information for the future.

<!-- That's the promise of publicly-posted replication archives, but the mundane reality of replication archives often falls short. We see many archives that are disorganized, poorly documented, and contain dozens of bugs and inconsistencies.  -->

What belongs in a replication archive? Enough documentation, data, and design detail that those who wish to reanalyze, replicate, and synthesize results can do so without contacting the authors.

**Data.** First, the data $d$ itself. Sometimes this is the raw data, sometimes it is only the "cleaned" data that is actually called by analysis scripts. Where ethically possible, we think it is preferable to post as much of the raw data as possible, for example after removing information like IP address or geographic location that could be used to identify a subject. We usually consider data processing scripts that clean and prepare data for analysis as part of the data strategy $D$ in the sense that they complete the measurement procedures laid out in $D$. That said, cleaning scripts might also be considered part of the answer strategy $A$ in the sense that they apply an interpretation to the data provided by the world. The output of cleaning scripts -- the cleaned data -- should be included in the replication archive as well. 

Reanalyses often reexamine and extend studies by exploring the use of alternative outcomes, varying sets of variables to control for, and new ways of grouping data. To enable these analyses, replication data ideally includes all data that was collected by the authors even if, in the final published results, those variables are not used. Often authors exclude these to preserve their own ability to publish on these other variables or because they are worried alternative analyses will uncover a lack of robustness. We hope norms will change such that study authors instead want to enable future researchers to build on their research by being expansive in what information is included.

**Analysis code.** Replication archives also include $A$, or the set of functions that produce $a^d$ when applied to $d$. We need the actual analysis code because the natural-language descriptions of $A$ that are typically given in written reports are imprecise. As a small example, many articles describe their answer strategies as "ordinary least squares" but do not fully describe the set of covariates included or the particular approach to variance estimation. These choice can substantively affect the quality of the research design -- and nothing makes these choices explicit like the actual analysis code. Analysis code is needed not only for reanalysis, but also replication and meta-analysis. Reanalyses may directly reuse or modify analysis code and replication projects need to know the exact details of analyses to ensure they can implement the same analyses on the data they collect. Meta-analysis authors may take the estimates from the past studies directly, in which case understanding the exact analyses conducted is important to understand what is being estimated. Other times, meta-analyses reanalyze data to ensure comparability in estimation. Conducting analyses with and without covariates, with clustering when it was appropriate, or with a single statistical model when they vary across studies all require having the exact analysis code. 

**Data strategy materials.** Increasingly, replication archives include the materials needed to implement treatments and measurement strategies. Without the survey questionnaires in their original languages and formats, we cannot exactly replicate them in future studies, which hinders our ability to build on and adapt them. The treatment stimuli used in the study should be included also. A central problem in replicating studies in the Many Labs projects was that the exact stimuli were either not retained or no longer available for past psychology studies. This loss led to confusion over whether failures to replicate were due to changes in the stimuli, different populations, or the underpowered original study designs. Data strategies are needed for reanalyses and meta-analyses too: answer strategies should respect data strategies, so understanding the details of sampling, treatment assignment, and measurement can shape reanalysts' decisions and meta-analysis authors decisions about what studies to include and which estimates to synthesize.

**Design declaration.** While typical replication archives include $d$ and $A$, we think that future replication archives should also include a design declaration that fully describes $M$, $I$, $D$, and $A$ -- that is, we should archive designs, not just data and analysis code. This should be done in code and words. In addition, a diagnosis should be included, demonstrating the properties as understood by the author and also indicating the diagnosands that the author considered in judging the quality of the design. 

Design details help future scholars not only assess, but replicate, reanalyze, and extend the study. Reanalysts need to understand the answer strategy so they can modify or extend it but also the data strategy that was used in order to ensure that their new analysis respects the details of the sampling, treatment assignment, and measurement procedures. Data and analysis sharing enables reanalysts to adopt or adapt the analysis strategy, but a declaration of the data strategy as well would help more. The same is true of meta-analysis authors, who need to understand the details of the designs to make good decisions about which studies to include and how to analyzet hem. Replicators who wish to exactly replicate, or even just to provide an answer to the same inquiry, need to understand the inquiry, data strategy, and answer strategy. Replication practice today involves inferring most of these details from descriptions in text. The result is disputes that result after the replication is sent out for peer review, when the original authors may not agree with inferences the replicators made about what the inquiry or data strategy or answer strategy actually were. To protect the original authors as well as the replicators, including a research design declaration specifying each of these elements resolves these issues so that replication and extension can focus on the substance of the research question and innovation in research design. 

<!-- Figure \@ref(fig:filestructure)  -->
The Figure below shows the file structure for an example replication. Our view on replication archives shares much in common with the [TIER protocol](https://www.projecttier.org). It includes raw data in a platform-independent format (.csv) and cleaned data in a language-specific format (.rds, a format for R data files), so that data features like labels, attributes, and factor levels are preserved when imported by the analysis scripts. The analysis scripts are labeled by the outputs they create, such as figures and tables. A master script is included that runs the cleaning and analysis scripts in the correct order. The documents folder includes the paper, the supplemental appendix, the pre-analysis plan, the populated analysis plan, and codebooks that describe the data. A README file explains each part of the replication archive. We also suggest that authors include a script that includes a design declaration and diagnosis. 

![File structure for archiving\label{filestructure}](figures/file_structure.png)


**Further Reading**

- @peer_orr_coppock_2021 who propose that researchers should "actively maintain" their replication archives by checking that they still run and making updates to obsolete code. In this way, the information about $A$ that is contained in the replication archive stays current and scientifically useful.

- @elman2018 argues that the benefits of data transparency in political science outweigh its costs.

- @Bowers2011 describes how good archiving is like collaborating with your future self

- @alvarez2018 provide guidance on how to create good replication archives.





<!-- Example is archive at OSF: https://osf.io/4vuqh -->
