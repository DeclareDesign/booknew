---
title: "Implementation"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

## Implementation

<!-- make sure to rename the section title below -->

```{r implementation, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```


outline:
-- use your MIDA to figure out how to implement
-- redesign as you go when you haven't gotten specific enough
-- redesign when new features of M come up
-- redesign when you can't do the D you planned (and make sure A fits it)
-- consider the whole design -- ex ante declared, then changed -- and what *could have happened*, which may be known to you ex ante (there could or could not be attrition) or unknown (you learn during implementation that there are some kinds of units that won't comply, so you need to think not just about which ones did or did not comply but which ones *could* have). the whole process is a function of your interventions in the world (treatment or measurement), so write down the whole process and potential outcomes to understand what you can and cannot learn.
-- use your MIDA to help you *make* logistical choices, help it prevent you from making bad decisions and use as tool to communicate with partners and implementers about why you do or do not wnat to make different changes (or BETTER evaluate tradeoffs in those decisions)

idea bin:
-- MIDA is a roadmap for how to implement the study
-- when there is a part not specified, redesign to specify and diagnose again

-- lots of choices you make after you start, because it was not clear what decisions would have to be made ex ante: how to make these choices? (redesign and diagnose!). 
-- often randomization procedure will have to take into account specifities of the number of units (odd numbers), blocks with differing numbers of units, cluster of varying sizes, etc. that require revising D
-- unexpected constraints come in that affect D, and may require changes to D but also A. redesign.
-- how to make decisions about unexpected changes when things going wrong? (even if you don't have a PAP). often want to create multiple variants of the design incorporating what went wrong. common example: unexpected noncompliance or unexpectedly high rates of attrition. may want to change analysis plan, and register it, so can use redesign to develop that new plan. but also may discover diagnosands are not good enough, so may want to change data strategy midstream to mitigate these problems.
-- what to do when you run out of money or feasibility of sample size or other aspects of D becomes clear. redesign mountain subject to new cost constraints.
-- make go-no go decisions about whether to continue
-- how to know if your inquiry is no longer answerable
-- updating your PAP

-- assessing what you can learn based on the implementation challenges: these are potential outcomes, i.e. could be affected by treatment, so are often informative about what we can learn about the original research question

-- often need to convince partners to not change plans -- MIDA can be a tool for helping assess tradeoffs in learning and doing
























