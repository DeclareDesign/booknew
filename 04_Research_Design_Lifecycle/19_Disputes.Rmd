---
title: "Resolving Disputes"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

## Resolving Disputes

<!-- make sure to rename the section title below -->

```{r resolving_disputes, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

<!-- The problem: Acrimonious debates arise; Hard to interpret contribution of replication and reanalysis; First main task (accumulation of knowledge) damaged. -->

<!-- The solution: Some of this comes down to basic disagreements we can't resolve. But some of it comes down to a lack of principles guiding how design decisions are made and how the results they produce should be interpreted. And a lack of procedures for understanding the consequences of decisions. -->

<!-- - Principles for making design choices, tailored to the distinct challenges posed to reanalysis and to replication -->
<!-- - Some changes are justifiable / encouraged, some things are not justifiable / discouraged, conditions for justification are clarified -->
<!-- - Procedures for putting principles into practice  -->
<!-- - Declaration and diagnosis through DD -->

<!-- Current practice for replication is to exactly replicate data strategy and analysis strategy in new or same context. This is not needed! Standard should be: best answer to same inquiry in new or same context! But how can we justify changes to D and A that give a "better" answer to same inquiry? -->

Disputes arise when reanalyses and replication studies are conducted and claims are made about the past studies or what we learn from the pair. The realized data from the two studies, $d$ and $d^{\prime}$, as well as the two designs $MIDA$ and $MIDA^{\prime}$, together inform what we learn from the two studies. Disputes arise over whether changes to $M^{\prime}$, $I^{\prime}$, $D^{\prime}$, or $A^{\prime}$ in the new study mean $d^{\prime}$ can be informative about the original $I$.

$M$ always changes. When a reanalysis or replication is conducted after the original study, we have by definition learned at least about $a^w$ from $a^d$ in the first study. We often have learned much more, about the distribution of variables, the existence of new nodes and edges, and sometimes much more when other related studies have been published in between. However, the original author may not agree with *how* the new author updated $M$. These disputes are substantive. 

We offer five rules for resolving disputes about changes to $I$, $D$, and $A$. 

Replacing them with alternative practices justified by design simulation

1. M always changes! (you have more information on $\tau$ or $\sd(\tau)$)
2. Home ground dominance: Change A or D-and-A if A$^\prime$ > A under M
3. Robustness to alternative models: Change A or D-and-A if A$^\prime$ $\geq$ A under M AND A$^\prime$ > A under M$^\prime$ E.g. change from simple to complete RA
4. Model plausibility: If A$^\prime$ < A under M AND A$^\prime$ > A under M$^\prime$, then change to A$^\prime$ or D-and-A IFF M$^\prime$ is more plausible than M E.g. switching to balanced design if you believe variances equal across treatment groups
5. Undefined inquiries. Change I to I$^\prime$ if I is undefined under M If I is defined under M: You can't change to I$^\prime$, You can’t change D to D$^\prime$ if that means I unidentifiable.

<!-- Disorganized thoughts: -->
<!-- - Changes to D include both interventions (sampling and randomization), as well as the inclusion of different / new datasets on the same model (this is common in econ reanalyses at state-level). The collection of "different" data through a change in question wording also fits into this. Need to think about a good typology of data strategies. -->
<!-- - There’s often a broader research question that’s being answered, and when I changes sometimes both are answering the same broader question. But focuses debate on whether that claim is true that I and I’ answer the same broader I.  -->
<!-- - In replication can you use data from study 1 to assess the plausibility of M? -->
<!-- - When does changing outcomes change the inquiry? -->
<!-- Example: you used z-scores in your original analysis in order to measure an effect on five different measures of some latent construct. I show that taking a simple average has better properties (e.g., statistical power), and use this instead of z-score. Have I changed estimand? If so, are there any instances of "recoding" or even "rewording" of outcome measures that we would be OK with, insofar as they get better answers to the inquiry without changing the inquiry? -->
<!-- One way of looking at this: inquiry is in reference to summary of a latent variable, which stays constant, but D changes which is different measurement of the latent variable -->
<!-- Point to keep in mind from this: D change might be in sampling/treat assignment or measurement -->
<!-- Key thing we are saying here: there are two dimensions of change with measurement. (1) are you changing estimands because the latent construct is changing implicitly; (2) are you changing to a better/worse measurement of the same latent construct. -->


