---
title: "Reconciliation"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

## Reconciliation {#p4reconciliation}

<!-- make sure to rename the section title below -->

```{r reconciliation, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

Inevitably, the research design as implemented will differ in some way from the research design as planned. Treatments cannot be implemented as conceived, some people cannot be found to interview, and sometimes what we learn from baseline measures informs how we measure later. An understanding of how your research design changed from conception to implementation is crucial to understanding what was learned from the design. 

Suppose the original design described a three-arm trial: one control and two treatments, but the design as implemented drops all subjects assigned to the second treatment. Sometimes this is an entirely appropriate and reasonable design modification: perhaps it turns out that due to an implementation failure, the second treatment was simply not delivered. Other times, these modification is less benign -- perhaps the estimate of the effect of the second treatment does not achieve statistical significance, so the author simply omits it from the analysis. 

For this reason, explicitly **reconciling** the design as planned with the design as implemented should be the first step to writing up a paper. Having a publicly-posted preanalysis plan can make the reconciliation process especially credible -- we know for sure what the planned design was because the preanalysis plan describes it pre-implementation. However, a preanalysis plan is not a prerequisite for engaging in reconciliation. The scientific enterprise is built in large measure on trust: we are ready to believe researchers who say, here is the design I though I would implement but due to unanticipated developements, here is the design I ended up implementing. 

In some cases, reconciliation will lead to additional learning beyond what can be inferred from the final design itself. When some units could refused to be included in the study sample or some units refused measurement, we learn that important features about those units. Understanding sample exclusions, noncompliance, and attrition not only may inform future research design planning choices, but contribute substantively to our understanding of the social setting. A policy implemented in the same way the study would likely also not be able to work in the units who refused to participate, and future research could examine why or how to convince them of the policy's benefits.

What belongs in a reconciliation? At a minimum, we need a full description of the planned design, a full description of the implemented design, and a list of the differences. This can be made explicit through the declaration of both design in computer code, then comparing the two design objects line-by-line.

In `DeclareDesign` we take the first steps of comparing designs for you:

```{r, eval = FALSE}
design1 <- declare_population(N = 100, U = rnorm(N)) +
  declare_potential_outcomes(Y ~ Z + U) +
  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +
  declare_sampling(n = 75) +
  declare_assignment(m = 50) +
  reveal_outcomes(Y, Z) +
  declare_estimator(Y ~ Z, estimand = "ATE")

design2 <- declare_population(N = 200, U = rnorm(N)) +
  declare_potential_outcomes(Y ~ 0.5*Z + U) +
  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +
  declare_sampling(n = 100) +
  declare_assignment(m = 25) +
  reveal_outcomes(Y, Z) +
  declare_estimator(Y ~ Z, model = lm_robust, estimand = "ATE")

compare_designs(design1, design2)
compare_design_code(design1, design2)
compare_design_summaries(design1, design2)
compare_design_data(design1, design2)
compare_design_estimates(design1, design2)
compare_design_estimands(design1, design2)
```


### Example

In Section \@ref(p4planning), we described the preanalysis plan registered by @bonilla_tillery_2020. We reconcile the set of conditional average treatment effect analyses planned in that PAP, the analyses reported in the paper, and those reported on in the appendix at the request of reviewers in Table \@ref(tab:reconciliation). In column two, we see that the authors planned four CATE estimates: effects by familiarity with Black Lives Matter; by gender; LGBT status; and linked fate. Only two of those are reported on in the paper, the others may have been excluded for space reasons. The two excluded analyses are not especially informative; they were not excluded on the basis of statistical significance. Another way to handle these uninteresting results would be to presented them in a populated PAP posted on their Web site or in the paper's appendix.

In their appendix, the authors report on a set of analyses requested by reviewers. We see this as a perfect example of transparently presenting the set of planned analyses and highlighting the analyses that were added afterward and why they were added. They write:

> We have been asked to consider other pertinent moderations beyond gender and LGBTQ+ status. They are contained in the four following sections.


| Covariate            | In the preanalysis plan | In the paper            | In the appendix (at the request of reviewers) |
| -------------------- | ----------------------- | ----------------------- | --------------------------------------------- |
| Familiarity with BLM | $\checkmark$            |                         |                                               |
| Gender               | $\checkmark$            | $\checkmark$            |                                               |
| LGBT status          | $\checkmark$            | $\checkmark$            |                                               |
| Linked fate          | $\checkmark$            |                         |                                               |
| Religiosity          |                         |                         | $\checkmark$                                  |
| Region               |                         |                         | $\checkmark$                                  |
| Age                  |                         |                         | $\checkmark$                                  |
| Education            |                         |                         | $\checkmark$                                  |

Table: (\#tab:reconciliation) Reconciliation of Bonilla and Tillery preanalysis plan.


<!-- ### Scattered thoughts -->

<!-- - @ofosu2019pre -->



