---
title: "Planning"
output:
  html_document: default
  pdf_document: default
bibliography: ../bib/book.bib
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->


## Preanalysis Plan {#p4pap}

<!-- make sure to rename the section title below -->

```{r writing_pap, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

In many research communities, it is becoming standard practice to publicly register a pre-analysis plan (PAP) prior to the implementation of some or all of the data strategy. PAPs serve many functions, but most importantly, they clarify which design choices were made before data collection and which were made afterward. Sometimes -- perhaps every time! -- we conduct a research study, aspects of $M$, $I$, $D$, and $A$ shift along the way. A concern is that they shift in ways that invalidate the apparent conclusions of the study. For example, "$p$-hacking" is the shady practice of trying out many regression specifications until the $p$-value associated with an important test attains statistical significance. PAPs protect researchers by communicating to skeptics *when* design decisions were made: if the regression specification was detailed in a PAP posted before any data were collected, the test cannot be the result of a $p$-hack. 

PAPs are sometimes misinterpreted as a binding commitment to report all pre-registered analyses and nothing but. This view is unrealistic and unnecessarily rigid. While we think that researchers should report all pre-registered analyses *somewhere* (see Section \@ref(p4populatedpap) on "populated PAPs"), study write-ups inevitably deviate in some way from the PAP -- and that's a good thing. Researchers learn more by conducting research; this learning can and should be reflected in the finalized answer strategy. Even if not binding, the main point of publicly posting pre-analysis plan is to communicate at what stage in the research process choices were made.

Our hunch is that the main consequence of actually writing a PAP is the improvement of the research design itself. Just like research design declaration forces us to think through the details of our model, inquiry, data strategy, and answer strategy, describing those choices in a publicly-posted document surely causes deeper reflection about the design. In this way, the main audience for a PAP is the study authors themselves.

What belongs in a PAP? Recommendations for the set of decisions that should be specified in a PAP remain remarkably unclear and inconsistent across research communities. PAP templates and checklists are proliferating, and the number of items they suggest ranges from nine to sixty. PAPs themselves are becoming longer and more detailed, with some in the American Economic Association (AEA) and Evidence in Governance and Politics (EGAP) study registries reaching hundreds of pages as researchers seek to be ever more comprehensive. Some registries emphasize the registration of the hypotheses to be tested, while others emphasize the registration of the tests that will used. We read many PAPs and found it is hard to assess whether these detailed plans actually contain the key analytically-relevant details. 

Our view is, minimally, a PAP should include a design declaration. A good deal of the discussion of what goes in a PAP centers on the answer strategy $A$ -- what estimator to use, what covariates to condition on, what subsets of the data to include. But of course, we also need to know the details of $D$ -- how units were sampled, how treatments were assigned, how the outcomes will be measured. We need these details to assess the properties of the design and also to gauge whether the principles of analysis as you randomize are being followed. We need to know about $I$ because we need to know the target of inference.^[A major concern in medical trials is "outcome switching," wherein the eventual report focuses on different health outcomes than originally intended. When we switch outcomes, we switch inquiries!] We need enough of $M$ to describe $I$ in sufficient detail. In short, a design declaration is what belongs in a PAP, because a design declaration specifies all of the analytically-relevant design decisions. 

In addition to a design declaration, a PAP should include mock analyses conducted on simulated data. If the design declaration is done formally in code, creating simulated data that resemble the eventual realized data is quite straightforward. We think researchers should run their answer strategy on the mock data, creating mock figures and tables that will eventually be made with real data. In our experience, *this* is the step that really causes researchers to think hard about all aspects of their design.

Strictly speaking, preanalysis plans should include design declaration, but they do not *require* design diagnosis. But since the design that is finally settled on as the design to be implemented is usually chosen as the result of a diagnosis, it can be informative to describe, in a preanalysis plan, the reasons why the particular design was chosen. For this reason, a PAP might include estimates of diagnosands like power, RMSE, or bias. If a researcher writes in a PAP that the power to detect a very small effect is large, then if the study comes back null, the eventual writeup can much more credibly rule out "low power" as an explanation for the null. Moreover, ex ante design diagnosis communicates the assumptions under which they thought the design was a good one before they ran the study. These reasons are often the basis on which we convince skeptics of the value of the design, so writing them down *before* the results are known increases the faith we put in them.

### Example

In this section, we provide an example of how to supplement a PAP with design declaration. We follow the actual PAP for @bonilla_tillery_2020, which was posted to the As Predicted registry here: https://aspredicted.org/q56qq.pdf. The goal of the study is to estimate the causal effects of alternative framings of the Black Lives Matter (BLM) movement on support for the movement among Black Americans overall as well as among subsets of the Black community. These study authors are models of research transparency: they prominently link to the PAP in the published article, they conduct no non-preregistered analyses except those requested during the review process, and their replication archive includes all materials required to confirm their analyses, all of which we were able to reproduce exactly with minimal effort. Our goal with this section is to show how design declaration can supplement and complement existing planning practices.

#### Model

The authors write in their PAP: 

> We hypothesize that: H1: Black Nationalist frames of the BLM movement will increase perceived effectiveness of BLM among African American test subjects. H2: Feminist frames of the BLM movement will increase perceived effectiveness of BLM among African American women, but decrease perceived effectiveness in male subjects. H3: LGBTQ and Intersectional frames of the BLM movement will have no effect (or a demobilizing effect) on the perceived effectiveness of BLM African American subjects.

These hypotheses reflect a model of coalition politics that emphasizes the tensions induced by overlapping identities. Framing the BLM movement as feminist or pro-LGBTQ may increase support among Black women or Black LGBTQ identifiers, but that increase may come at the expense of support among Black men or Blacks who do not identify as LGBTQ. Similarly, this model predicts that subjects with stronger attachment to their Black identity will have a larger response to a Black nationalist framing of BLM than those with weaker attachments. 

The model also includes beliefs about the distributions of gender, LGBTQ status, and Black identity strength. In the Data strategy, Black identity will be measured with the standard linked fate measure. Other background characteristics that may be correlated with support for BLM include age, religiosity, income, education, and familiarity with the movement, so these are included in the model as well.

The focus of the study will be on the causal effects of the nationalism, feminism, and intersectional frames relative to a general description of the Black Lives Matter movement. Model beliefs about treatment effect heterogeneity are embedded in the `declare_potential_outcomes` call. The effect of the nationalism treatment is hypothesized to be stronger, the greater subjects' sense of linked fate; the effect of the feminism treatment should be negative for men but positive for women; the effect of the intersectionality treatment should be positive for LGBTQ identifies, but negative for non-identifiers. 

```{r}
rescale <- function(x) (x - min(x)) / (max(x) - min(x))
likert_cut <- function(x)  as.numeric(cut(x, breaks = c(-100, 0.1, 0.3, 0.6, 0.8, 100), labels = 1:5))

model <- 
  declare_model(
    N = 800,
    female = rbinom(N, 1, prob = 0.51),
    lgbtq = rbinom(N, 1, prob = 0.05),
    linked_fate = sample(1:5, N, replace = TRUE, 
                         prob = c(0.05, 0.05, 0.15, 0.25, 0.5)),
    age = sample(18:80, N, replace = TRUE),
    religiosity = sample(1:6, N, replace = TRUE),
    income = sample(1:12, N, replace = TRUE),
    college = rbinom(N, 1, prob = 0.5),
    blm_familiarity = sample(1:4, N, replace = TRUE),
    U = runif(N),
    blm_support_latent = rescale(
      U + 0.1 * blm_familiarity + 
        0.45 * linked_fate + 
        0.001 * age + 
        0.25 * lgbtq + 
        0.01 * income + 
        0.1 * college + 
        -0.1 * religiosity)
  ) + 
  declare_potential_outcomes(
    blm_support_Z_general = 
      likert_cut(blm_support_latent),
    blm_support_Z_nationalism = 
      likert_cut(blm_support_latent + 0.01 + 
                   0.01 * linked_fate + 
                   0.01 * blm_familiarity),
    blm_support_Z_feminism = 
      likert_cut(blm_support_latent - 0.02 + 
                   0.07 * female + 
                   0.01 * blm_familiarity),
    blm_support_Z_intersectional = 
      likert_cut(blm_support_latent  - 0.05 + 
                   0.15 * lgbtq + 
                   0.01 * blm_familiarity)
  )
```

#### Inquiry
  
The inquiries for this study naturally include the average effects of all three treatments relative to the "general" framing, as well as the differences in average effects for subgroups. When describing their planned analyses, the authors write:

> We will also look at differences in responses between those indicating a pre-treatment familiarity BLM (4-Extensive knowledge to 1-Never heard of BLM), gender (particularly on the Feminist treatment), linked fate (particularly on the Nationalist treatment), and LGBT+ affiliation (particularly on the LGBT+ treatment), though we are not necessarily expecting these moderations to have a strong effect because samples may lack adequate representation.
  
In the code below, we specify how each treatment effect changes with its corresponding covariate $X$ with $\frac{\mathrm{cov}(\tau_i, X)}{\V(X)}$, which is identical to the difference-in-difference for the binary covariates (`female` and `lgbtq`) and is the slope of the best linear predictor of how the effect changes over the range of `linked_fate`, and `blm_familiarity` which we are treating as quasi-continuous here. 

```{r}
slope <- function(y, x) cov(y, x) / var(x)

inquiry <-  
  declare_inquirys(
    # Average effects
    ATE_nationalism = 
      mean(blm_support_Z_nationalism - blm_support_Z_general),
    ATE_feminism = 
      mean(blm_support_Z_feminism - blm_support_Z_general),
    ATE_intersectional = 
      mean(blm_support_Z_intersectional - blm_support_Z_general),
    
    # Overall heterogeneity w.r.t. blm_familiarity
    DID_nationalism_familiarity = 
      slope(blm_support_Z_nationalism - blm_support_Z_general, 
            blm_familiarity),
    DID_feminism_familiarity = 
      slope(blm_support_Z_feminism - blm_support_Z_general, 
            blm_familiarity),
    DID_intersectional_familiarity = 
      slope(blm_support_Z_intersectional - blm_support_Z_general, 
            blm_familiarity),
    
    # Treatment-specific heterogeneity
    DID_nationalism_linked_fate = 
      slope(blm_support_Z_nationalism - blm_support_Z_general, 
            linked_fate),
    DID_feminism_gender = 
      slope(blm_support_Z_feminism - blm_support_Z_general,
            female),
    DID_intersectional_lgbtq = 
      slope(blm_support_Z_intersectional - blm_support_Z_general, 
            lgbtq)
  )
```


#### Data strategy

The subjects for this study are 800 Black Americans recruited by the survey firm Qualtrics using a quota sampling procedure. We elide this sampling step in our declaration -- the 800 subjects are described by the `declare_population` call. The reason is that, as is common practice in the analysis of survey experiments on convenience samples, authors do not formally extrapolate out from their data to make generalizations about the population of Black Americans. The inquiries they study are sample average effects. If the authors had used a different sampling strategy, using random sampling through random digit dialing for example, we would have defined the population from which they were sampling and the random sampling procedure. 

After subjects' background characteristics are measured, they were assigned to one of four treatment conditions. Since the survey was conducted on Qualtrics, we assume that the authors used the built-in randomization tools, which typically use simple (Bernoulli) random assignment.

```{r}
data_strategy <- 
  declare_assignment(
    conditions = 
      c("general", "nationalism", "feminism", "intersectional"), 
    simple = TRUE
  ) + 
  declare_reveal(blm_support, Z) 
```

#### Answer strategy

The authors write: 

> We will run an OLS regression predicting the support for, effectiveness of, and trust in BLM on each treatment condition. [...] We will also look at differences in responses between those indicating a pre-treatment familiarity BLM (4-Extensive knowledge to 1-Never heard of BLM), gender (particularly on the Feminist treatment), linked fate (particularly on the Nationalist treatment), and LGBT+ affiliation (particularly on the LGBT+ treatment), though we are not necessarily expecting these moderations to have a strong effect because samples may lack adequate representation. We plan to conduct analyses without controls. As we will check for between group balance, we may also run OLS analyses with demographic controls (age, linked fate, gender, sexual orientation, religiosity, income, education, and ethnic or multi-racial backgrounds), and will report differences in OLS results.

In DeclareDesign, this corresponds to five estimators, with two shooting at the ATEs and three shooting at the differences-in-CATEs. We use OLS for all five -- the majority of the code is bookkeeping to ensure we match the right regression coefficient with the appropriate estimand.

```{r}
answer_strategy <-
  declare_estimator(
    blm_support ~ Z,
    term = c("Znationalism", "Zfeminism", "Zintersectional"),
    model = lm_robust,
    inquiry = 
      c("ATE_nationalism", "ATE_feminism", "ATE_intersectional"),
    label = "OLS"
  ) +
  declare_estimator(
    blm_support ~ Z + age + female + as.factor(linked_fate) + lgbtq,
    term = c("Znationalism", "Zfeminism", "Zintersectional"),
    inquiry = 
      c("ATE_nationalism", "ATE_feminism", "ATE_intersectional"),
    model = lm_robust,
    label = "OLS with controls"
  ) +
    declare_estimator(
    blm_support ~ Z*blm_familiarity,
    term = c("Znationalism:blm_familiarity", 
             "Zfeminism:blm_familiarity", 
             "Zintersectional:blm_familiarity"),
    model = lm_robust,
    inquiry = c("DID_nationalism_familiarity", 
                 "DID_feminism_familiarity", 
                 "DID_intersectional_familiarity"),
    label = "DID_familiarity"
  ) +
  declare_estimator(
    blm_support ~ Z * linked_fate,
    term = "Zfeminism:linked_fate",
    model = lm_robust,
    inquiry = "DID_nationalism_linked_fate",
    label = "DID_nationalism_linked_fate"
  ) +
  declare_estimator(
    blm_support ~ Z * female,
    term = "Zfeminism:female",
    model = lm_robust,
    inquiry = "DID_feminism_gender",
    label = "DID_feminism_gender"
  ) +
  declare_estimator(
    blm_support ~ Z * lgbtq,
    term = "Zintersectional:lgbtq",
    model = lm_robust,
    inquiry = "DID_intersectional_lgbtq",
    label = "DID_intersectional_lgbtq"
  )
```

#### Mock analysis

Putting it all together, we can declare the complete design and draw mock data from it. 

```{r}
design <- model + inquiry + data_strategy + answer_strategy
mock_data <- draw_data(design)
```

```{r, echo = FALSE}
mock_data %>% 
head(5) %>% 
kable(digits = 3, caption = "Mock analysis from Bonilla and Tillery design.", booktabs = TRUE)
```

Table \@ref(tab:bonillatilleryregtable) shows a mock analysis average effects (estimated with and without covariate adjustment) as well as the heterogeneous effects analyses with respect to the quasicontinuous moderators.

```{r bonillatilleryregtable, echo = FALSE, results = "asis"}
fit_1 <- lm_robust(blm_support ~ Z, data = mock_data)
fit_2 <- lm_robust(blm_support ~ Z + female + lgbtq + age + 
                     religiosity + income + college + linked_fate + 
                     blm_familiarity, data = mock_data)
fit_3 <- lm_robust(blm_support ~ Z * linked_fate, data = mock_data)
fit_4 <- lm_robust(blm_support ~ Z * blm_familiarity, data = mock_data)

bookreg(l = list(fit_1, fit_2, fit_3, fit_4), include.ci = FALSE, digits = 3, caption = "Mock regression table from Bonilla and Tillery design.")
```

```{r bonillatilleryresultsfig, fig.cap = "Mock coefficient plot from Bonilla and Tillery design.", echo = FALSE, fig.width = 5, fig.height = 5}
female_df <-
  mock_data %>%
  group_by(female) %>%
  do(tidy(lm_robust(blm_support ~ Z, data = .))) %>%
  filter(term != "(Intercept)")

female_int <- lm_robust(blm_support  ~ Z*female, data = mock_data) %>% 
  tidy() %>% filter(str_detect(term, ":")) %>%
  mutate(female = 2,
         term = str_remove(term, ":female")) 

gg_df_1 <-
  female_df %>% 
  bind_rows(female_int) %>%
  mutate(facet = factor(female, 0:2, c("Men", "Women", "Difference")),
         term = str_to_sentence(str_remove(term, "Z")))

lgbtq_df <-
  mock_data %>%
  group_by(lgbtq) %>%
  do(tidy(lm_robust(blm_support ~ Z, data = .))) %>%
  filter(term != "(Intercept)")

lgbtq_int <- lm_robust(blm_support ~ Z*lgbtq, data = mock_data) %>% 
  tidy() %>% 
  filter(str_detect(term, ":")) %>%
  mutate(lgbtq = 2,
         term = str_remove(term, ":lgbtq")) 

gg_df_2 <-
  lgbtq_df %>% 
  bind_rows(lgbtq_int) %>%
  mutate(facet = factor(lgbtq, 0:2, c("LGTBQ", "Non-LGBTQ", "Difference")),
         term = str_to_sentence(str_remove(term, "Z")))


g <- 
ggplot(data = NULL, aes(estimate, term)) +
  geom_point() +
  geom_vline(xintercept = 0, color = gray(0.5), linetype = "dashed") +
  geom_linerange(aes(xmin = conf.low, xmax = conf.high)) +
  facet_wrap(~facet, ncol = 1) +
  theme(axis.title.y = element_blank())

(g %+% gg_df_1) +
  (g %+% gg_df_2) 
```

#### Design diagnosis

Finally, while a design diagnosis is not a necessary component of a preanalysis plan, it can be useful to show readers why a particular design was chosen over others. This diagnosis shows that the design produces unbiased estimates of all estimands, but that we are better powered from some inquires than others (under the above assumptions about effect size, which were our own and not the original authors'). We are well-powered for the average effects, and the power increases when we include covariate controls. The design is probably too small for most of the heterogeneous effect analyses, which is a point directly conceded in the authors' original PAP. 

```{r, echo = FALSE, eval = do_diagnosis & !exists("do_bookdown")}
diagnosis <- diagnose_design(design, sims = sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("02_Planning.Rmd"), "/diagnosis.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnosis, path = rds_file_path)
}
diagnosis <- read_rds(rds_file_path)
```

```{r bonillatillerydiagnosis, echo = FALSE}
diagnosis %>% 
  get_simulations %>% 
  group_by(inquiry_label, estimator_label) %>% 
  summarize(
    bias = mean(estimate - estimand, na.rm = TRUE),
    power = mean(p.value <= 0.05, na.rm = TRUE)
  ) %>% 
  mutate(inquiry_label = str_replace_all(inquiry_label, "_", " "),
         estimator_label = str_replace_all(estimator_label, "_", " ")) %>% 
  kable(digits = 3, col.names = c("Estimand", "Estimator", "Bias", "Power"), 
        caption = "Design diagnosis for Bonilla and Tillery design.", booktabs = TRUE)
```


<!-- ## Debates over the value of PAPs -->

<!-- PAPs are often described as a tool for tying researchers' hand and reducing "researcher degrees of freedom" to seek out congenial analyses. PAPs are sometimes criticized because (A) they don't *actually* constrain what researchers do and (B) they *shouldn't*.   -->

<!-- PAPs might not actually constrain researchers because most journals and pressess do not check against PAPs. As reviewers, we have sometimes requested that authors send in the PAPs referenced in their papers. In nearly every case, some analyses promised in the PAP were not included, even in an appendix. Some analyses that appear in the paper were not included in the PAP. As we discuss in greater detail in section XXX on reconciliation, current practice is clearly too causal when distinguishing which analyses were pre-specified and which were not. -->

<!-- Some critics of PAPs charge that we shouldn't pre-register our analysis plans because we should be open to discovery. We learn once we arrive in the field what the interesting questions are, so we shouldn't be constrained by what we thought would happen when sitting in our offices. Furthermore, sometimes people pre-register analyses that are biased, misspecified, or are otherwise inappropriate, so they shouldn't be required to present them. We agree with all the above points -- researchers should create post-implementation reports (CITE JENS in PA) that follow the PAP. Analyses over and above the PAP should simply be labeled as such. -->

<!-- Our take is that PAPs are helpful tools for researchers to plan research better and they are useful for clarifying what the researcher was thinking at each stage.  -->

<!-- ## Other benefits of PAPS -->

<!-- - involving partners: agreeing on the analysis procedure ex ante reduces ex post conflict -->
<!-- - frontloading research design decisions (get more specific, identify problems) -->
<!-- - Anticipating what might go wrong: attrition, noncompliance, study failure, missing covariate data, etc. -->
<!-- - Faithful reanalysis. Reanalysts lose a degree of freedom in determining what an author might have intended by a given analysis.  -->
<!-- - Easier design comparison. If a design is declared at the preanalysis plan phase, then it enables direct comparison with the design as implemented in the final write-up. Side-by-side comparison of the code neatly clarifies which design choices were made ex-ante and ex-post. Side-by-side comparison of the performance of a planned and implemented designs clarify under what conditions deviations from plans are defensible improvements. -->
<!-- - ethics (summarize and cite Jay's ideas here: http://www.jasonlyall.com/wp-content/uploads/2020/08/PreregisterYourEthics.pdf) -->
<!-- - ethical outcomes are potential outcomes, so we need to think about them ex ante not just on the basis of revealed outcomes -->

<!-- ### countering objections: -->

<!-- - Time-consuming. Yes but in our experience we only pay this cost for failed studies. Fur successful studies, nearly all work that goes in to the pap pays off in terms of higher qualtity design, literal words we already wrote, and written-in-advance analysis code. -->

<!-- - Won't stop determined cheaters. Yes -- remember that the goal is not to prevent fraud, it's to help *researchers* improve their designs. Science depends on trust. -->

<!-- - Replication is better (@Coffman2015). These are complements, not substitutes. -->

<!-- - I can't preregister what I will do because I don't know what I will find. That's fine too, just write down how you will go about "finding" things so we (and YOU) can understand your own process. -->

<!-- ### Other thoughts -->

<!-- - when is the right moment to write a pap? -->
<!-- - relationship to registered reports? -->
<!-- - SOPs -->

<!-- ### citations on paps -->

**Further reading**.

- @Casey2012: early entry
- @Olken2015: halfway skeptical
- @Green2015: Standard operating procedures
- @Christensen2018: review
- @Coffman2015: a skeptical take (prefer replications).
- @Humphreys2013a: nonbinding
- @Miguel2014: distinguishes between disclosure and PAP
- @Ofosu2020: apparently paps hinder publication?
- @Rennie2004
- @Zarin2008
- @Nosek2015a
- @Findley:2016 on results-blind review
