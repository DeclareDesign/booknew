---
title: "Partners"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

## Partners

<!-- make sure to rename the section title below -->

```{r partners, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```


Partnering with organizations in research --- cooperating to intervene in the world or to measure outcomes --- is increasingly common in the social sciences. Organizations can learn about how to achieve their (private) goals better by partnering with social scientists to study current programs and policies. In some cases these private goals overlap with public goods (e.g., a government learning how to expand access to healthcare), in others they are largely private (e.g., Uber learning about how to improve its revenues). Social scientists' have their own goals: to produce knowledge and publish it. Understanding the private goals of the partner and of the researcher is essential to selecting a research design amenable to both parties. MIDA can help in doing this, by formalizing tradeoffs between the two sets of goals.

One common divergence between partner and researcher goals is that partner organizations often do want to learn, but their primary mission is focused on doing. Learning how to do their work better is a goal, but it may be in the short term secondary to trying to doing the work. Donors and other constituents expect to see *outputs* rather than *outcomes*, often understandably because it is simpler to monitor the outputs of an organization (trainings held, doors knocked on) than *outcomes* (quality services provided, votes changed). Because of this at least short-term divergence in goals, a central tradeoff in partnering with organizations is a learning-doing tradeoff. Both researchers and organizations wish to "do" (produce outputs) and "learn" (learn how to better translate outputs into outcomes), but because of the difference in goals the weight they place on these two activities may differ. This tradeoff is commonly referred to as the exploration-exploitation tradeoff (cite), and motivates a large literature on adaptive trials that aim to directly optimize doing but in the most effective way.

Research design diagnosis can help navigate the learning-doing tradeoff. One instance of the tradeoff is that the proportion of units who receive a treatment (e.g., a medicine) represents the rate of "doing" and also affects the amount of learning in that in the extreme if all units are treated there can (typically) be no learning about the *effect* of the treatment. The tradeoff here is represented in a graph of the RMSE vs. the proportion treated. In the absence of partners, researchers might simply ignore the proportion treated axis and select the proportion with the lowest RMSE. With a partner organization, the researcher might use this graph in conversation with the partner to jointly select the design that has the lowest RMSE that has a sufficiently high proportion treated to meet the partner's needs to satisfy its stakeholders. 

Choosing the proportion treated is one example of integrating partner constraints into research designs to generate feasible designs. A second common problem is that there are a set of units that *must* be treated, for ethical or political reasons (e.g., the home district of a government partner must receive the treatment), or that must not be treated. If these constraints are discovered after treatment assignment, they lead to noncompliance, which may substantially complicate analysis of the experiment and even prevent providing an answer to the original inquiry. Considerable thought has been given to avoiding this type of noncompliance. Gerber and Green recommend, before randomizing treatment, exploring possible treatment assignments with the partner organization and using this exercise to elicit the set of units that must or cannot be treated. King et al. (cite) describe a ``politically-robust'' design, which uses pair-matched block randomization and when any unit is dropped due to political constraints the pair is dropped from the study^[Note the inquiry that can be answered from such a design is complex, reflecting effects for units in pairs that would not be dropped in either possible random assignment within the pair] 

Design diagnosis can help in these circumstances by providing a mechanism to specify possible patterns of noncompliance and diagnosing alternative designs to mitigate the negative consequences. In addition, they can be used to communicate with partners about the consequences of noncompliance for the research to help make better decisions together about how to avoid noncompliance once treatment is randomized and the research is in progress.

