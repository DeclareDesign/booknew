---
title: "Replication"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

## Replication

<!-- make sure to rename the section title below -->

```{r replication, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```


After your study is completed, it may one day be replicated. Replication differs from reanalysis in that a replication study involves the specification of a new MIDA and collection of new data to study the same inquiry. As discussed in the previous, a reanalysis may re-specify parts of the research design, but always re-uses the original data $d$ in some way.

So-called "exact" replications hold key features of I, D, and A fixed, but draw a new dataset $d_{\rm new}$ from $D()$ and apply the same $A$ to the new $d$ in order to produce a fresh answer $a_{\rm new}^D$. Replications are said to "succeed" when $a_{\rm old}^D$ and $a_{\rm new}^D$ are similar and to "fail" when they are not. Dichotomizing replication attempts into successes and failures is usually not that helpful, and it would be better to simply characterize how similar $a_{\rm old}^D$ and $a_{\rm new}^D$ are.

Of course, exact replication is impossible: at least some elements of M have changed between the first study and the replication. Specifying how they might have changed, e.g., how outcomes vary with time, will help judge differences observed between $a_{\rm old}^D$ and $a_{\rm new}^D$. Statistical noise will also play a role.

Replication studies benefit enormously from the knowledge gains produced by the original studies. For example, we learn a large amount about $M$ and the likely value of $a^M$ from the original study. The $M$ of the replication study can and should incorporate this new information. For example, if we learn from the original study that $a^M$ is positive but it might be small, the replication study could respond by changing $D$ in order to increase the sample size. Design diagnosis can help you learn about how to change the design of the replication study in light of the original study.

When changes to $D$ or $A$ can made to produce more informative answers about the same $I$, exact replication may not be preferred. Holding the treatment and outcomes the same may be required to provide an answer to the same $I$, but increasing the sample size or sampling individuals rather than villages or other changes may be preferable to exact replication. Replication designs can take advantage of new best practices in research design.

When designing **original** studies, you should anticipate that someday your work will be replicated. This improves your *ex ante* incentives. To the extent that you want future replication studies to arrive a similar answers to the original study you produce (i.e., you want their $a_{\rm new}^D$ to match your $a_{\rm old}^D$ as closely as possible), you will want to choose designs that bring $a_{\rm old}^D$ as close to $a^M$ as possible, under the presupposition that faithful replicators will also design their studies in such a way that $a_{\rm new}^D$ will also be close to $a^M$.

Replication studies necessarily differ from original studies -- it is literally impossible to reproduce the exact conditions of the original study in the same way it's impossible to step in the same river twice. Another way of putting that same statement is that $D_{\rm new}$ is necessarily different from $D_{\rm old}$. Theory (i.e., beliefs about $M$) is the tool we use to say that $D_{\rm old}$ is similar enough to $D_{\rm new}$ to consititute a close enough replication study. As a concrete example, many survey experimental replications involve using the exact same experimental stimuli but changing the study sample, e.g., from a nationally representative sample to a convenience sample.

So-called "conceptual" replications alter both $M$ and $D$, but keep $I$ and $A$ as similar as possible. That is, a conceptual replication tries to ascertain whether a relationship in one context ($I(M_{\rm old})$) also holds in a new context ($I(M_{\rm new}$). The trouble and promise of conceptual replications lies in the success of the designer at holding $I$ constant. Too often, a conceptual replication fails because in changing $M$, too much changes about $I$ such that too much changes about the "concept" under replication.

There should be a summary function for how to interpret the difference between $a_{\rm old}^D$ and $a_{\rm new}^D$. This may be take the new one and throw out the old if MIDA was poor in the first. It may be taking the average. It may be a precision-weighted average. Specifying this function ex ante may be useful, to avoid the choice of summary depending on the results of the replication. This summary function will be reflected in A and in the discussion section of the replication paper. 

**Further reading**.

- @Clemens2017 on distinctions between replication and reanalysis


