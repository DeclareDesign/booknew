---
title: "Piloting"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

## Piloting {#p4piloting}

<!-- make sure to rename the section title below -->

```{r piloting, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

<!-- -- can't learn causal effect -->
<!-- -- what can you learn? about Y0, about M, about se -->
<!-- -- bring in blog post -->

<!-- mini MIDA in each piloting exercise -->

Designing a research study always entails relying on a set of mostly informal beliefs. Choices like how many subjects to sample, which covariates to measure, which treatments to allocate, and depend on beliefs about treatment effects, the correlations of the covariates with the outcome, and the variance of the outcome. 

We may have reasonably educated guesses about these parameters from past studies or theory. Our understanding of the nodes and edges in the causal graph of *M*, expected effect sizes, the distribution of outcomes, feasible randomization schemes, and many other features are directly selected from past research or chosen based on a literature review of past studies. 

Even so, we remain uncertain about these values. One reason for the uncertainty is that our research context and inquiries often differ subtly from previous work. Even when replicating an existing study as closely as possible, difficult-to-intuit features of the research setting may have serious consequences for the design. Moreover, our uncertainty about a design parameter is often the very reason for conducting a study. We run experiments *because* we are uncertain about the average treatment effect. If we knew the ATE for sure, there would be no need to run the study. Frustratingly, we always have to design using parameters whose values we are unsure of.

The main goal of pilot studies is to reduce this uncertainty so that the main study can be designed, taking into account design parameters closer to the true values. Pilots take many forms: focus groups to learn how to ask survey questions, small-scale tests of measurement tools, even miniature versions of the main study on a smaller scale. We want to learn things like the distribution of outcomes, how covariates and outcomes might be correlated, or how feasible the assignment, sampling, and measurement strategies are. 

Almost by definition, pilot studies are inferentially weaker than main studies. We turn to them in response to fundamental constraints on our time, money, and capacity. If we were not constrained, we would run a first full-size study, learn what is wrong with our design, then run a corrected full-size study. Since running multiple full studies is too expensive or otherwise infeasible, we run either smaller mini-studies or test out only a subset of the elements of our planned design. Accordingly, the diagnosands of a pilot design will not measure up to those of the main design. Pilots have much lower statistical power and may suffer from higher measurement error and less generalizability. Accordingly, the goal of pilot studies should not be to obtain a preliminary answer to the main inquiry, but instead to learn the information that will make the main study a success. 

Like main studies, pilot studies can be declared and diagnosed -- but importantly, the diagnosands for main and pilot studies need not be the same. Statistical power for an average treatment effect may be an essential diagnosand for the main study, but owing to their small size, power for pilot studies will typically be abysmal. Pilot studies should be diagnosed with respect to the decisions they imply for the main study. 

Figure \@ref(fig:nrequired) shows the relationship between effect size and the sample size required to achieve 80\% statistical power for a two-arm trial using simple random assignment. Uncertainty about the true effect size has enormous design consequences. If the effect size is 0.2, we need about 800 subjects to achieve 80% power. If it's 0.1, we need 3200. 

```{r nrequired, fig.cap = "Minimum required sample sizes and uncertainty over effect size", fig.width=6.5, fig.height=6.5, echo=FALSE}
full_dist <-
  tibble(
    effect_size = c(seq(-0.25,-0.01, by = 0.01),
                    seq(0.01, 0.75, by = 0.01)),
    N_required = sapply(
      effect_size,
      FUN = function(x)
        power.t.test(
          delta = abs(x),
          sd = 1,
          power = 0.8
        )$n * 2
    ),
    density = dnorm(effect_size, 0.3, 0.1)
  )

label_df <-
  full_dist %>%
  filter(round(effect_size, 2) %in% c(0.10, 0.17, 0.3)) %>%
  mutate(label = c("3200 subjects required at 0.10",
                   "1100 subjects required at 0.17",
                   "350 subjects required at 0.30"),
         label2 = c("True effect size",
                   "10th percentile guess",
                   "Average (best) guess"))

g1 <- 
ggplot(label_df, aes(effect_size, N_required)) +
  geom_line(data = full_dist) +
  geom_point(aes(color = label2, shape = label2)) +
  geom_segment(aes(xend = effect_size, yend = 0, color = label2)) +
  geom_text(aes(label = label, color = label2), hjust = 0, nudge_x = 0.02) +
  coord_cartesian(xlim = c(-0.25, 0.75),
                  ylim = c(0, 5000)) +
  dd_theme() +
  theme(legend.position = "none") +
  labs(x = "Effect size in standard units",
       y = "Sample size needed for 80% power",
       title = "Minimum sample sizes for 80% power, by effect size")


g2 <-
ggplot(label_df, aes(effect_size, density)) +
    geom_point(aes(color = label2, shape = label2)) +
  geom_segment(aes(xend = effect_size, yend = 0, color = label2)) +
  geom_text(aes(label = label2, color = label2), hjust = 1, nudge_x = -0.02) +
  geom_line(data = full_dist) +
  coord_cartesian(xlim = c(-0.25, 0.75)) +
  dd_theme() +
    theme(legend.position = "none") +
  labs(x = "Effect size in standard units",
       y = "Density of prior belief distribution",
       title = "Researcher's prior beliefs about effect size")


g1 / g2

```


Suppose we have prior beliefs about the effect size that can be summarized as a normal distribution centered at 0.3 with a standard deviation of 0.1, as in the bottom panel of Figure \@ref(fig:nrequired). We could choose a design that corresponds to this best guess, the average of our prior belief distribution. If the true effect size is 0.3, then a study with 350 subjects will have 80% power. 

However, redesigning the study to optimize for the "best guess" is risky because the true effect could be much smaller than 0.3. Suppose we adopt the redesign heuristic of powering the study for an effect size at the 10th percentile of our prior belief distribution, which works out here to be an effect size of 0.17. Following this rule, we would select a design with 1100 subjects. Now suppose the true effect size is, in actuality, only 0.1, so we would need to sample 3200 subjects for 80% power. The power of our chosen 1100-subject design is a mere 38%. Here we see the consequences of having incorrect prior beliefs: our ex-ante guess of the effect size was too optimistic. Even taking what we thought of as a conservative choice -- the 10th percentile redesign heuristic -- we ended up with too small a study.

A pilot study can help researchers update their priors about important design parameters. Suppose we do a small scale pilot with 100 subjects; we'll get a noisy but unbiased estimate of the true effect size. We can update prior beliefs by taking a precision weighted average of our priors and the estimate from the pilot, where the weights are the inverse of the variance of each guess. Our posterior beliefs will be closer to the truth, and our posterior uncertainty will be smaller. If we then follow the heuristic of powering the 10th percentile of our (now posterior) beliefs about effect size, we will have come closer to correctly powering our study. Figure \@ref(fig:pilotupdate) shows how large the studies would be, depending on how the pilot study came out if we were to follow the 10th percentile decision rule. On average, the pilot leads us to design the main study with 1800 subjects, sometimes more and sometimes less.

This exercise reveals that a pilot study can be quite valuable. Without a pilot study, we would chose to sample 1100 subjects, but since the true effect size is only 0.1 (not our best guess of 0.3), the experiment would be underpowered. The pilot study helps us correct our diffuse and incorrect prior beliefs. However, since the pilot is small, we don't update our priors all the way to the truth. We still end up with a main study that is on average too small (1800), with a corresponding power of 56%. That said, a 56% chance of finding a statistically significant result is better than a 38% chance.


```{r, echo = FALSE}
prior <- 0.3
prior_sd <- 0.1

calculate_n_required <- function(delta, sd = 1, power = 0.8) {
  sapply(
    delta,
    FUN = function(x)
      power.t.test(
        delta = x,
        sd = sd,
        power = power
      )$n * 2
  )
  
}

pilot <-
  declare_model(N = 100,
                U = rnorm(N),
                potential_outcomes(Y ~ 0.1 * Z + U)) +
  declare_inquiry(SATE = mean(Y_Z_1 - Y_Z_0)) +
  declare_assignment(Z = complete_ra(N), legacy = FALSE) +
  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +
  declare_estimator(Y ~ Z, inquiry = "SATE")
```

```{r, echo = FALSE, eval = do_diagnosis}
pilot_sims <- 
  pilot %>%
  simulate_design(sims = 1000)

pilot_sims <-
  pilot_sims %>%
  mutate(
    prior_w = 1/(prior_sd^2),
    pilot_w = 1/(std.error^2),
    sum_w = prior_w + pilot_w,
    posterior = prior * prior_w/sum_w + estimate * pilot_w/sum_w,
    posterior_sd = sqrt(1/sum_w),
    conservative_guess = qnorm(0.1, posterior, posterior_sd),
    N_required = calculate_n_required(conservative_guess)
)
```


```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("06_Piloting.Rmd"), "/simulation_df.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(pilot_sims, file = rds_file_path)
}
pilot_sims <- read_rds(rds_file_path)
```

```{r pilotupdate, echo=FALSE, fig.width=6.5, fig.cap = "Distribution of post-pilot sample size choices"}
pilot_sims <-
  pilot_sims %>%
  filter(N_required <= 7500, !is.na(N_required))
choices_df <-
  tibble(
    xintercept = c(calculate_n_required(qnorm(0.1, prior, prior_sd)),
                   calculate_n_required(0.1),
                   mean(pilot_sims$N_required)),
    label = c("Choice of N without pilot",
              "Best choice of N",
              "Average choice of N with pilot"),
    x = xintercept + 100,
    y = c(130, 110, 95)
  )

ggplot(pilot_sims, aes(N_required)) + 
  geom_histogram(bins = 40) +
  geom_segment(data = choices_df, aes(x = xintercept, xend = xintercept, y = 0, yend = y - 5), color = dd_dark_blue) +
  geom_text(data = choices_df, aes(x = x, y = y, label = label), color = dd_dark_blue, hjust = 0.2) +
  dd_theme() +
  theme(legend.position = "none") +
  labs(x = "Distribution of choice of N, depending on results of pilot")
```


In summary, pilots are most useful when we are uncertain -- or outright wrong -- about important design parameters. This uncertainty can often be shrunk by quite a bit without running pilot studies by meta-analyzing past empirical studies. Some things are hard to learn by reading others' work; pilot studies are especially useful tools for learning about those things. 
<!-- add another point about the pilot design -- we need to select a good MIDA, make sure you are sampling from same population and measure same outcomes etc. -->

**Further reading**.

- @van2001importance
- @lancaster2004design
- @turner2005role
- @hertzog2008considerations
- @viechtbauer2015simple (how many subjects do you need to discover if a rare problem exisits)
- @avery2017informing
- @eldridge2016defining - distinguishes between "feasibility" studies and "pilot" studies. these differ in what is unknown about the main study. "Is it even possible"
- @bell2018guidance




<!-- Pilot studies share a lot in common with baseline measurement. A baseline measurement phase can sometimes be used instead of a pilot to learn about some empirical features. If our sample size is fixed and we are interested in learning whether some outcome measures vary across units or how they covary, we can measure them in the baseline and then make adjustments before a posttreatment survey. We will still control for our imperfect measures at baseline to improve efficiency. -->

<!-- Diagnosing the pilot study on its own provides stark insights, which amount to: we cannot provide answers to the inquiry in the main study, and should not try to do so. There are also aspects of the logistics of research that within time and financial constraints we simply cannot learn until we run the main study. Science is imperfect, and also iterative, but these mistakes or suboptimal design choices also often lead to discoveries. -->

<!-- -- how does it help to diagnose the design together? the properties of the main study *change* when we do a pilot. This is because if we run the pilot study, we are doing so to make decisions about how to run the main study, and so our *design* of the main study and thus its results may depend on the *results* (and design) of the pilot study.  -->

<!-- In this section, we illustrate several general principles that flow from diagnosing pilot studies.  -->

<!-- Purposes of pilot studies: -->

<!-- Existence proofs: -->
<!-- -- is there variation in Y -->
<!-- -- is there variation in X -->
<!-- -- what are nodes in M -->
<!-- -- what are feasible D's, what are feasible treatments / can you implement the treatment (existence proof) -->

<!-- Harder questions requiring bigger sample sizes: -->
<!-- -- what is the distribution of X (helps select stratification proportions etc.) -->
<!-- -- what is the standard deviation of Y0 -->

<!-- #### Assessing a pilot design -->

<!-- declare pilot itself and diagnose just as if it were the main study -->

<!-- if you can't learn the answer, don't make any decisions based on it -->

<!-- #### Assessing a sequenced design -->

<!-- if you are making decisions about MIDA for main study based on pilot, diagnose the procedure of two studies, think about POs of pilot -->

<!-- #### Pilots and baselines -->

<!-- Designs can be reassessed after baselines and before treatment assignment -- so some of the questions you might do a pilot for can just be answered in a baseline -->


<!-- #### BLOG material -->

<!-- Data collection is expensive, and we often only get one bite at the apple. In response, we often conduct an inexpensive (and small) pilot test to help better design the study. Pilot studies have many virtues, including practicing the logistics of data collection and improving measurement tools. But using pilots to get noisy estimates in order to determine sample sizes for scale up comes with risks. -->

<!-- Pilot studies are often used to get a guess of the average effect size, which is then plugged into power calculators when designing the full study. -->

<!-- The procedure is: -->

<!-- 1. Conduct a small pilot study (say, N = 50) -->
<!-- 2. Obtain an estimate of the effect size (this is noisy, but better than nothing!) -->
<!-- 3. Conduct a power analysis for a larger study (say, N = 500) on the basis of the estimated effect size in the pilot -->

<!-- We show in this post that this procedure turns out to be dangerous: at common true effect sizes found in the social sciences, you are at risk of selecting an underpowered design based on the noisy effect estimate in your pilot study. -->

<!-- A different procedure has better properties: -->

<!-- 1. Conduct a small pilot study (say, N = 50) -->
<!-- 2. Obtain an estimate of the **standard deviation of the outcome variable** (again, this is a noisy estimate but better than nothing!) -->
<!-- 3. Estimate the minimum detectable effect (MDE) for a larger study (say, N = 500), using the estimated standard deviation -->

<!-- We show what happens in each procedure, using DeclareDesign. In each case, we'll think about a decision the researcher wants to make based on the pilot: should I move forward with my planned study, or should I go back to the drawing board? We'll rely on power to make that decision in the first procedure and the MDE in the second procedure. -->


<!-- [omitting code] -->


<!-- For each true effect size, the simulations will give us a distribution of estimated effects that a researcher might use as a basis for power analysis. For example, for a true effect size of 0 the researcher might still estimate an effect of 0.10, and so conduct their power analysis assuming that the true effect is 0.10. For each true effect, we can thus construct a distribution of *power estimates* a researcher might obtain from *estimated* effects. Since we know the true power for the true underlying effect, we can compare the distribution of post-hoc power estimates to the true power one would estimate if one knew the true effect size. -->


<!-- What did we find? In the plot, we show our guesses for the power of the main study based on our pilot effect size estimates.  -->

<!-- At high true effect sizes (top row), we do pretty well. Most of our guesses are above 80\% power, leading us to the correct decision that the study is powered. Indeed we often *underestimate* our power in these cases meaning that we run larger studies than we need to. -->

<!-- However, at low true effect sizes (bottom row) we show we are equally likely to find that the design is in fact powered as underpowered. We are equally likely to guess the power of the design is 90% as 10%. There is a good chance that we will falsely infer that our design is well powered just because we happened to get a high estimate from a noisy pilot. -->

<!-- ### How about estimating the standard deviation of the outcome? -->

<!-- Now, let's look at the second approach. Here, instead of using our pilot study to estimate the effect size for a power calculation, we estimate the **standard deviation of the outcome** and use this to calculate the main study's minimum detectable effect. The decision we want to make is: is this MDE small enough to be able to rule out substantively important effects? -->

<!-- We calculate the minimum detectable effect size using the approximation from [@gelman2006data, pg. 441], 2.8 times the estimated standard error. We estimate the standard error using Equation 3.6 from @gerber2012field.  -->



<!-- In summary, pilot studies can be valuable in planning research for many reasons, but power calculations based on noisy effect size estimates can be misleading. A better approach is to use the pilot to learn about the distribution of outcome variables. The variability of the outcome variable can then be plugged into MDE formulas or even power calculations with, say, the smallest effect size of political, economic, or social importance. -->

<!-- In the same spirit, pilot studies could also be used to learn the strength of the correlation between pre-treatment covariates and the outcome variable. With this knowledge in hand, researchers can develop their expectations about how much precision there is to be gained from covariate control or blocking. -->






