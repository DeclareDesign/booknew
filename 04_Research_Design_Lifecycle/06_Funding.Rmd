---
title: "Funding"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

## Funding

<!-- make sure to rename the section title below -->

```{r funding, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

<!-- point 1 -->
Higher quality designs usually come with higher costs. Costs are a function both of the data strategy (*D*) and the particular realization of the data strategy ($d$). Collecting original data is more expensive than analyzing existing data, but collecting new data may be more or less costly depending on the ease of contacting subjects or conducting measurements. As a result, including cost diagnosands in research design diagnosis is essential, and those diagnosands may usefully include both average cost and maximum cost. Researchers may make different decisions about cost: in some cases, the researcher will select the "best" design in terms of research design quality subject to a budget constraint. Others will choose the cheapest among similar quality designs to save money for future research. Diagnosis can help identify each set and decide among them.

To relax the budget constraint, researchers apply for funding. Funding applications must communicate what research design is being proposed; why learning answers from the design would be useful, important, or interesting to scholars, the public, or policymakers; how the research design provides credible answers to the question; that the researcher is capable of executing the design; and that there is value-for-money in the design and the answers it provides. 

Researchers and funders have an information problem. Applicants wish to obtain as large a grant as possible for their design but have difficulty credibly communicating the quality of their design given the subjectivity of the exercise. On the flip side, funders wish to get the most value for money in the set of proposals they decide to fund and have difficulty assessing the quality of proposed research. Design declaration and diagnosis provide a partial solution to the information problem. A common language for communicating the proposed design and its properties can communicate the value of the research under design assumptions that -- crucially -- can be understood and interrogated by funders. 

Funding applications should include a declaration and diagnosis of the proposed design. In addition to common diagnosands such as bias and efficiency, two special diagnosands may be valuable: cost and (relatedly) value-for-money. The cost can be included for each design variant as a function of design features such as sample size, the number of treated units, and the duration of survey interviews. The cost may vary by these parameters and may vary across possible designs when, for example, the number of treated units is random. Simulating the design across possible realizations of the design, thus, provides a distribution of costs as a function of choices the researcher makes. Value for money is a diagnosand that is a function of cost and the amount learned from the design. RMSE might be one value criterion. Another would be the average difference between priors and posteriors under a Bayesian answer strategy (a direct measure of learning). 

In some cases, funders request applicants to provide multiple options and multiple price points or make clear how a design could be altered so that it could be funded at a lower level. Redesigning over differing sample sizes communicates how the researcher conceptualizes these options and provides the funder with an understanding of tradeoffs between the amount of learning and cost in these design variants. Applicants could use the redesign process to justify their request's high cost and ask for additional funding.

Ex-ante power analyses, required by an increasing number of funders, illustrate the crux of the misaligned incentives between applicants and funders. A power analysis can demonstrate that almost any design is "sufficiently powered" by changing expected effect sizes and variances. By clarifying the assumptions of the power analysis in code, researchers can more easily defend these choices. Power analyses using standard power calculators online have difficult-to-interrogate assumptions built in and cannot accommodate the specifics of many common designs. As a result, many return incorrect estimates of power for these designs [@bccmapsr].

Funders who request design declarations can compare funding applications on standard scales: root mean-squared-error, bias, and power. They also want to weigh considerations like importance and fit. Moving design considerations onto a common scale takes some of the guesswork out of the process and reduces reliance on researcher claims about properties.

<!-- **Further readings.** -->

<!-- -  -->

<!-- -- funders often request power analysis, but these are typically described in words and thus the assumptions behind them cannot be interrogated fully. (a) not in code, so not specific; (b) user power calculators that are wrong (cite paper); (c) do not provide the details funders need to verify whether they agree with the assumptions.  -->

<!-- -- for funders, providing MIDA declared in code allows them to change the parameters of the design and test how the properties of the design change with beliefs about the world in M or data strategy parameters in D such as sample size, rather than having to rely on claims by applicants -->

<!-- -- often funders require regular reporting on changes in plan -- MIDA and design declaration provides a way to communicate (a) what those changes are and (b) how they change the values of diagnosands.  -->

<!-- -- value for money as a diagnosand (cost of each design as a function of design parameters) -->

<!-- -- allows funders to compare on a common scale (the same set of diagnosands) funding proposals -- often trying to evaluate "quality" but hard to do that with narrative proposals -->

<!-- ### References -->


