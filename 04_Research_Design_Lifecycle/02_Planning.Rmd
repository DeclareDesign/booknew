---
title: "Planning"
output: html_document
bibliography: ../bib/book.bib
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# Planning

<!-- make sure to rename the section title below -->

```{r writing_pap, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

When embarking on a prospective research design, it is becoming standard practice in many research communities to publicly register a pre-analysis plans (PAP) prior to the implementation of some or all of the Data strategy. PAPs serve many functions, but most importantly, they clarify which design choices were made before data collection and which were made afterward. Sometimes -- perhaps everytime! -- we conduct a research study, aspects of $M$, $I$, $D$, and $A$ shift along the way. A concern is that they shift in ways that invalidate the apparent conclusions of the study. For example, "$p$-hacking" is the shady practice of trying out many regression specifications until the $p$-value associated with an important test attains statistical significance. PAPs help researchers to credibly communicate to skeptics when design decisions were made: if the regression specification was detailed in a PAP posted before any data were collected, the test cannot be the result of a $p$-hack.

What belongs in a PAP? Thus far, the set of decisions that should be specified in a PAP remains remarkably unclear. PAP templates and checklists are proliferating, and the number of items they suggest range from nine to sixty. PAPs themselves are becoming longer and more detailed, with some in the American Economic Association (AEA) and Evidence in Governance and Politics (EGAP) study registries reaching hundreds of pages, as researchers seek to be ever more comprehensive. Some registries emphasize the registration of the hypotheses to be tested, while others emphasize the registration of the tests that will used. We read many PAPs -- it is often hard to assess whether these detailed plans actually contain the key analytically-relevant details. 

Our view is that, minimally, a PAP should include a design declaration. A good deal of the discussion of what goes in a PAP centers on the answer strategy $A$ -- what estimator to use, what covariates to condition on, what subsets of the data to include. But of course we also need to know the details of $D$ -- how units were sampled, how treatments were assigned, how the outcomes will be measured. We need to know about $I$ because we need to know what the target of inference is^[A major concern in medical trials is "outcome switching," wherein the eventual report focuses on different health outcomes that originally intended. When we switch outcomes, we switch inquiries!]. We need enough of $M$ to describe $I$ in sufficient detail. In short, a design declaration is what belongs in a PAP, because a design declaration specifies all of the analytically-relevent design decisions. 

In addition to a design declaration, a PAP should include mock analyses conducted on simulated data. If the design declaration is done formally in code, creating simulated data that resemble the eventual realized data is quite straightforward. We think researchers should run their Answer strategy on the mock data, creating mock figures and tables that will eventually be made with real data. In our experience, *this* is the step that really causes researchers to think hard about all aspects of their design.

Strictly speaking, preanalysis plans should include design declaration, but they do not *require* design diagnosis. But since the design that is finally settled on as the design to be implemented is usually chosen as the result of a diagnosis, it can be informative to describe, in a preanalysis plan, the reasons why the particular design was chosen. For this reason, a PAP might include estimates of diagnosands like power, rmse, or bias. If a researcher writes in a PAP that the power to detect a very small effect is large, then if the study comes back null, the eventual writeup can much more credibly rule out "low power" as an explanation for the null. Moreover, ex ante design diagnosis communicates the assumptions under which they thought the design was a good one before they ran the study. These reasons are often the basis on which we convince skeptics of the value of the design, so writing them down *before* the results are known increases the faith we put in them.

## Example

- This section will be an example PAP. 
- We will take a (relatively simple) PAP from the EGAP website (with permission) and declare the design in code. 
- We will make fake figures and tables using mock data
- We will choose wisely, because we will use the same study for the section on reconciliation. We want there to be a few differences (enough to make the point) but that the differences don't shame the author
- Very open to suggestions on a good study.

Possibility: @bonilla_tillery_2020 from the APSR on the effects of intersectional framing on support for the BLM movement. The takeaway is that the feminist and LGBTQ frames cause a decrease in support among the men in the sample, but not the women.

This is a 4-arm (control vs. three treatments) experiment among Black americans recruited via qualtrics. 4 dependent variables. Almost everything they do in the article and appendix was pre-registered. They don't do everything they said they would do (het fx by linked fate) but close. This study seems complex enough to be worth doing, but simple enough to make it seem "easy" to do design declaration.  A takeaway is that the study was probably underpowered for most of the het fx estimation, a point that gets conceeded post-hoc in the article. 

The data are posted on 
  




## Debates over the value of PAPs

PAPs are often described as a tool for tying researchers' hand and reducing "researcher degrees of freedom" to seek out congenial analyses. PAPs are sometimes criticized because (A) they don't *actually* constrain what researchers do and (B) they *shouldn't*.  

PAPs might not actually constrain researchers because most journals and pressess do not check against PAPs. As reviewers, we have sometimes requested that authors send in the PAPs referenced in their papers. In nearly every case, some analyses promised in the PAP were not included, even in an appendix. Some analyses that appear in the paper were not included in the PAP. As we discuss in greater detail in section XXX on reconciliation, current practice is clearly too causal when distinguishing which analyses were pre-specified and which were not.

Some critics of PAPs charge that we shouldn't pre-register our analysis plans because we should be open to discovery. We learn once we arrive in the field what the interesting questions are, so we shouldn't be constrained by what we thought would happen when sitting in our offices. Furthermore, sometimes people pre-register analyses that are biased, misspecified, or are otherwise inappropriate, so they shouldn't be required to present them. We agree with all the above points -- researchers should create post-implementation reports (CITE JENS in PA) that follow the PAP. Analyses over and above the PAP should simply be labeled as such.

Our take is that PAPs are helpful tools for researchers to plan research better and they are useful for clarifying what the researcher was thinking at each stage. 

## Other benefits of PAPS

- involving partners: agreeing on the analysis procedure ex ante reduces ex post conflict
- frontloading research design decisions (get more specific, identify problems)
- Anticipating what might go wrong: attrition, noncompliance, study failure, missing covariate data, etc.
- Faithful reanalysis. Reanalysts lose a degree of freedom in determining what an author might have intended by a given analysis. 
- Easier design comparison. If a design is declared at the preanalysis plan phase, then it enables direct comparison with the design as implemented in the final write-up. Side-by-side comparison of the code neatly clarifies which design choices were made ex-ante and ex-post. Side-by-side comparison of the performance of a planned and implemented designs clarify under what conditions deviations from plans are defensible improvements.
- ethics (summarize and cite Jay's ideas here: http://www.jasonlyall.com/wp-content/uploads/2020/08/PreregisterYourEthics.pdf)
- ethical outcomes are potential outcomes, so we need to think about them ex ante not just on the basis of revealed outcomes

### countering objections:

- Time-consuming. Yes but in our experience we only pay this cost for failed studies. Fur successful studies, nearly all work that goes in to the pap pays off in terms of higher qualtity design, literal words we already wrote, and written-in-advance analysis code.

- Won't stop determined cheaters. Yes -- remember that the goal is not to prevent fraud, it's to help *researchers* improve their designs. Science depends on trust.

- Replication is better (@Coffman2015). These are complements, not substitutes.

- I can't preregister what I will do because I don't know what I will find. That's fine too, just write down how you will go about "finding" things so we (and YOU) can understand your own process.

### Other thoughts

- when is the right moment to write a pap?
- relationship to registered reports?
- SOPs

### citations on paps

- @Casey2012: early entry
- @Olken2015: halfway skeptical
- @Green2015: Standard operating procedures
- @Christensen2018: review
- @Coffman2015: a skeptical take (prefer replications). 
- @Humphreys2013a: nonbinding
- @Miguel2014: distinguishes between disclosure and PAP
- @Ofosu2020: apparently paps hinder publication?
