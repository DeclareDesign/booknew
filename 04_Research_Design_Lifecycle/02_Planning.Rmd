---
title: "Planning"
output:
  html_document: default
  pdf_document: default
bibliography: ../bib/book.bib
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# Planning

<!-- make sure to rename the section title below -->

```{r writing_pap, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

In many research communities, it is becoming standard practice to publicly register a pre-analysis plan (PAP) prior to the implementation of some or all of the Data strategy. PAPs serve many functions, but most importantly, they clarify which design choices were made before data collection and which were made afterward. Sometimes -- perhaps every time! -- we conduct a research study, aspects of $M$, $I$, $D$, and $A$ shift along the way. A concern is that they shift in ways that invalidate the apparent conclusions of the study. For example, "$p$-hacking" is the shady practice of trying out many regression specifications until the $p$-value associated with an important test attains statistical significance. PAPs help researchers to credibly communicate to skeptics *when* design decisions were made: if the regression specification was detailed in a PAP posted before any data were collected, the test cannot be the result of a $p$-hack. 

PAPs are sometimes misinterpreted as a binding commitment to report all pre-registered analyses and nothing but. This view is unrealistic and unnecessarily rigid. While we think that researchers should report all pre-registered analyses *somewhere* (see Section XXX on "populated PAPs"), study write-ups inevitably deviate in some way from the PAP -- and that's a good thing. Researchers learn more by conducting research; this learning can and should be reflected in the finalized Answer strategy. Even if not binding, the main point of publicly posting pre-analysis plan is to communicate at what stage in the research process choices were made.

Our hunch is that the main consequence of actually writing a PAP is improvement to the research design itself. Just like research design declaration forces us to think through the details of our model, inquiry, data strategy, and answer strategy, describing those choices in a publicly-posted document surely causes deeper reflection about the design. In this way, the main audiencce for a PAP is the study authors themselves.

What belongs in a PAP? Thus far, recommendations for the set of decisions that should be specified in a PAP remain remarkably unclear and inconsistent across research communities. PAP templates and checklists are proliferating, and the number of items they suggest range from nine to sixty. PAPs themselves are becoming longer and more detailed, with some in the American Economic Association (AEA) and Evidence in Governance and Politics (EGAP) study registries reaching hundreds of pages as researchers seek to be ever more comprehensive. Some registries emphasize the registration of the hypotheses to be tested, while others emphasize the registration of the tests that will used. We read many PAPs -- it is often hard to assess whether these detailed plans actually contain the key analytically-relevant details. 

Our view is, minimally, a PAP should include a design declaration. A good deal of the discussion of what goes in a PAP centers on the answer strategy $A$ -- what estimator to use, what covariates to condition on, what subsets of the data to include. But of course we also need to know the details of $D$ -- how units were sampled, how treatments were assigned, how the outcomes will be measured. We need to know about $I$ because we need to know what the target of inference is^[A major concern in medical trials is "outcome switching," wherein the eventual report focuses on different health outcomes that originally intended. When we switch outcomes, we switch inquiries!]. We need enough of $M$ to describe $I$ in sufficient detail. In short, a design declaration is what belongs in a PAP, because a design declaration specifies all of the analytically-relevent design decisions. 

In addition to a design declaration, a PAP should include mock analyses conducted on simulated data. If the design declaration is done formally in code, creating simulated data that resemble the eventual realized data is quite straightforward. We think researchers should run their Answer strategy on the mock data, creating mock figures and tables that will eventually be made with real data. In our experience, *this* is the step that really causes researchers to think hard about all aspects of their design.

Strictly speaking, preanalysis plans should include design declaration, but they do not *require* design diagnosis. But since the design that is finally settled on as the design to be implemented is usually chosen as the result of a diagnosis, it can be informative to describe, in a preanalysis plan, the reasons why the particular design was chosen. For this reason, a PAP might include estimates of diagnosands like power, rmse, or bias. If a researcher writes in a PAP that the power to detect a very small effect is large, then if the study comes back null, the eventual writeup can much more credibly rule out "low power" as an explanation for the null. Moreover, ex ante design diagnosis communicates the assumptions under which they thought the design was a good one before they ran the study. These reasons are often the basis on which we convince skeptics of the value of the design, so writing them down *before* the results are known increases the faith we put in them.

## Example

In this section, we provide an example of a PAP using a design declaration. We follow the actual PAP for @bonilla_tillery_2020, which was posted to the As Predicted registry here: https://aspredicted.org/q56qq.pdf. The goal of the study is to estimate the causal effects of alternative framings of the Black Lives Matter (BLM) movement on support for the movement among Black Americans overall as well as among subsets of the Black community. These study authors are models of research transparency: they prominently link to the PAP in the published article, they conduct no non-preregistered analyses except those requested during the review process, and their replication archive includes all materials required to confirm their analyses, all of which we were able to reproduce exactly with minimal effort. 

### Model

The authors write in their PAP: 

> We hypothesize that: H1: Black Nationalist frames of the BLM movement will increase perceived effectiveness of BLM among African American test subjects. H2: Feminist frames of the BLM movement will increase perceived effectiveness of BLM among African American women, but decrease perceived effectiveness in male subjects. H3: LGBTQ and Intersectional frames of the BLM movement will have no effect (or a demobilizing effect) on the perceived effectiveness of BLM African American subjects.

These hypotheses reflect a model of coalition politics that emphasizes the tensions induced by overlapping identities. Framing the BLM movement as feminist or pro-LGBTQ may increase support among Black women or Black LGBTQ identifiers, but that increase may come at the expense of support among Black men or Blacks who do not identify as LGBTQ. Similarly, this model predicts that subjects with stronger attachment to their Black identity will have a larger response to a Black nationalist framing of BLM than those with weaker attachements. 

The model also includes beliefs about the distributions of gender, LGBTQ status, and Black identity strength. In the Data strategy, Black identity will be measured with the standard linked fate measure. Other background characteristics that may be correlated with support for BLM include age, religiosity, income, eductation, and familiarity with the movement, so these are included in the model as well.

The focus of the study will be on the causal effects of the nationalism, feminism, and intersectional frames relative to a general description of the Black Lives Matter movement. Model beliefs about treatment effect heterogeneity are embedded in the `declare_potential_outcomes` call. The effect of the nationalism treatment is hypothesized to be stronger, the greater subjects' sense of linked fate; the effect of the feminism treatment should be negative for men but postive for women; the effect of the intersectionality treatment should be positive for LGBTQ identifies, but negative for non-identifiers. The authors did not specify their predictions of the magnitudes of the differences, so we came up with our own guesses.

```{r, echo = FALSE}
rescale <- function(x) (x - min(x)) / (max(x) - min(x))
likert_cut <- function(x)  as.numeric(cut(x, breaks = c(-100, 0.1, 0.3, 0.6, 0.8, 100), labels = 1:5))
```

```{r}
model <- 
  declare_population(
    N = 800,
    female = rbinom(N, 1, prob = 0.51),
    lgbtq = rbinom(N, 1, prob = 0.05),
    linked_fate = sample(1:5, N, replace = TRUE, prob = c(0.05, 0.05, 0.15, 0.25, 0.5)),
    age = sample(18:80, N, replace = TRUE),
    religiosity = sample(1:6, N, replace = TRUE),
    income = sample(1:12, N, replace = TRUE),
    college = rbinom(N, 1, prob = 0.5),
    blm_familiarity = sample(1:4, N, replace = TRUE),
    U = runif(N),
    blm_support_latent = rescale(
      U + 0.1 * blm_familiarity + 0.45 * linked_fate + 0.001 * age + 
        0.25 * lgbtq + 0.01 * income + 0.1 * college + -0.1 * religiosity)
  ) + 
  declare_potential_outcomes(
    blm_support_Z_general = likert_cut(blm_support_latent),
    blm_support_Z_nationalism = likert_cut(blm_support_latent + 0.01 + linked_fate * 0.01),
    blm_support_Z_feminism = likert_cut(blm_support_latent - 0.02 + female * 0.07),
    blm_support_Z_intersectional = likert_cut(blm_support_latent  - 0.05 + lgbtq * 0.15)
  )
```

### Inquiry
  
The inquiries for this study are parameters implied by the three hypotheses quoted above and by the answer strategies described by the authors. We are interested in the average affects of all three treatments relative to the "general" framing, as well as the difference in effectiveness of each treatment by its corresponding covariate. We write these estimands as $\frac{cov(\tau_i, X)}{X}$, which is identical to the difference-in-difference for the binary covariates (`female` and `lgbtq`) and is the slope of the best linear predictor of how the effect changes over the range of `linked_fate`, which we are treating as quasi-continuous here.

```{r}
inquiry <-  
  declare_estimands(
    ATE_nationalism = mean(blm_support_Z_nationalism - blm_support_Z_general),
    ATE_feminism = mean(blm_support_Z_feminism - blm_support_Z_general),
    ATE_intersectional = mean(blm_support_Z_intersectional - blm_support_Z_general),
    DID_nationalism_linked_fate = 
      cov(blm_support_Z_nationalism - blm_support_Z_general, linked_fate)/var(linked_fate),
    DID_feminism_gender = 
      cov(blm_support_Z_feminism - blm_support_Z_general, female)/var(female),
    DID_intersectional_lgbtq = 
      cov(blm_support_Z_intersectional - blm_support_Z_general, lgbtq)/var(lgbtq)
  )
```


### Data strategy

The subjects for this study are 800 Black Americans recruited by the survey firm Qualtrics using a quota sampling procedure. We elide this sampling step in our declaration -- the 800 subjects are described by the `declare_population` call. 

After subjects' background charcteristics are measured, they will be assigned to one of four treatment conditions. Since the survey was conducted on Qualtrics, we assume that the authors used the built-in randomization tools, which typically use simple (Bernoulli) random assignment.


```{r}
data_strategy <- 
  declare_assignment(conditions = c("general", "nationalism", "feminism", "intersectional"), simple = TRUE) + 
  declare_reveal(blm_support, Z) 
```

```{r}
answer_strategy <-
  declare_estimator(
    blm_support ~ Z,
    term = c("Znationalism", "Zfeminism", "Zintersectional"),
    model = lm_robust,
    estimand = c("ATE_nationalism", "ATE_feminism", "ATE_intersectional"),
    label = "unadjusted"
  ) +
  declare_estimator(
    blm_support ~ Z + age + female + as.factor(linked_fate) + lgbtq,
    term = c("Znationalism", "Zfeminism", "Zintersectional"),
    estimand = c("ATE_nationalism", "ATE_feminism", "ATE_intersectional"),
    model = lm_robust,
    label = "adjusted"
  ) +
  declare_estimator(
    blm_support ~ Z * linked_fate,
    term = "Zfeminism:linked_fate",
    model = lm_robust,
    estimand = "DID_nationalism_linked_fate",
    label = "het-fx-nationalism-linked-fate"
  ) +
  declare_estimator(
    blm_support ~ Z * female,
    term = "Zfeminism:female",
    model = lm_robust,
    estimand = "DID_feminism_gender",
    label = "het-fx-feminism-female"
  ) +
  declare_estimator(
    blm_support ~ Z * lgbtq,
    term = "Zfeminism:lgbtq",
    model = lm_robust,
    estimand = "DID_intersectional_lgbtq",
    label = "het-fx-intersectional-lgbtq"
  )
```

```{r}
design <- model + inquiry + data_strategy + answer_strategy
```

```{r, echo = FALSE, eval = do_diagnosis & !exists("do_bookdown")}
diagnosis <- diagnose_design(design, sims = sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("02_Planning.Rmd"), "/diagnosis.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnosis, path = rds_file_path)
}
diagnosis <- read_rds(rds_file_path)
```

```{r, echo = FALSE}
diagnosis %>% 
  get_simulations %>% 
  group_by(estimand_label, estimator_label) %>% 
  summarize(
    bias = mean(estimate - estimand, na.rm = TRUE),
    rmse = sqrt(mean((estimate - estimand)^2, na.rm = TRUE)),
    power = mean(p.value <= 0.05, na.rm = TRUE)
  ) %>% 
  mutate(estimand_label = str_replace_all(estimand_label, "_", " "),
         estimator_label = str_replace_all(estimand_label, "_", " ")) %>% 
  kable(digits = 2, col.names = c("Estimand", "Estimator", "Bias", "RMSE", "Power"))
```

```{r, echo = FALSE, results = "asis"}
dat <- draw_data(design) %>% as_tibble

fit_1 <- lm_robust(blm_support ~ Z, data = dat)
fit_2 <- lm_robust(blm_support ~ Z + female + lgbtq + age + religiosity + income + college + linked_fate + blm_familiarity, data = dat)
fit_3 <- lm_robust(blm_support ~ Z*linked_fate, data = dat)
fit_4 <- lm_robust(blm_support ~ Z*blm_familiarity, data = dat)

bookreg(l = list(fit_1, fit_2, fit_3, fit_4), include.ci = FALSE)
```

```{r, echo = FALSE, fig.width = 5, fig.height = 5}
female_df <-
  dat %>%
  group_by(female) %>%
  do(tidy(lm_robust(blm_support ~ Z, data = .))) %>%
  filter(term != "(Intercept)")

female_int <- lm_robust(blm_support  ~ Z*female, data = dat) %>% 
  tidy() %>% filter(str_detect(term, ":")) %>%
  mutate(female = 2,
         term = str_remove(term, ":female")) 

gg_df_1 <-
  female_df %>% 
  bind_rows(female_int) %>%
  mutate(facet = factor(female, 0:2, c("Men", "Women", "Difference")),
         term = str_to_sentence(str_remove(term, "Z")))

g1 <- 
ggplot(gg_df_1, aes(estimate, term)) +
  geom_point() +
  geom_vline(xintercept = 0, color = gray(0.5), linetype = "dashed") +
  geom_linerange(aes(xmin = conf.low, xmax = conf.high)) +
  facet_wrap(~facet, ncol = 1) +
  theme(axis.title.y = element_blank())

lgbtq_df <-
  dat %>%
  group_by(lgbtq) %>%
  do(tidy(lm_robust(blm_support ~ Z, data = .))) %>%
  filter(term != "(Intercept)")

lgbtq_int <- lm_robust(blm_support ~ Z*lgbtq, data = dat) %>% 
  tidy() %>% 
  filter(str_detect(term, ":")) %>%
  mutate(lgbtq = 2,
         term = str_remove(term, ":lgbtq")) 

gg_df_2 <-
  lgbtq_df %>% 
  bind_rows(lgbtq_int) %>%
  mutate(facet = factor(lgbtq, 0:2, c("LGTBQ", "Non-LGBTQ", "Difference")),
         term = str_to_sentence(str_remove(term, "Z")))

g2 <- 
ggplot(gg_df_2, aes(estimate, term)) +
  geom_point() +
  geom_vline(xintercept = 0, color = gray(0.5), linetype = "dashed") +
  geom_linerange(aes(xmin = conf.low, xmax = conf.high)) +
  facet_wrap(~facet, ncol = 1) +
  theme(axis.title.y = element_blank())

g1 + g2
```

## Debates over the value of PAPs

PAPs are often described as a tool for tying researchers' hand and reducing "researcher degrees of freedom" to seek out congenial analyses. PAPs are sometimes criticized because (A) they don't *actually* constrain what researchers do and (B) they *shouldn't*.  

PAPs might not actually constrain researchers because most journals and pressess do not check against PAPs. As reviewers, we have sometimes requested that authors send in the PAPs referenced in their papers. In nearly every case, some analyses promised in the PAP were not included, even in an appendix. Some analyses that appear in the paper were not included in the PAP. As we discuss in greater detail in section XXX on reconciliation, current practice is clearly too causal when distinguishing which analyses were pre-specified and which were not.

Some critics of PAPs charge that we shouldn't pre-register our analysis plans because we should be open to discovery. We learn once we arrive in the field what the interesting questions are, so we shouldn't be constrained by what we thought would happen when sitting in our offices. Furthermore, sometimes people pre-register analyses that are biased, misspecified, or are otherwise inappropriate, so they shouldn't be required to present them. We agree with all the above points -- researchers should create post-implementation reports (CITE JENS in PA) that follow the PAP. Analyses over and above the PAP should simply be labeled as such.

Our take is that PAPs are helpful tools for researchers to plan research better and they are useful for clarifying what the researcher was thinking at each stage. 

## Other benefits of PAPS

- involving partners: agreeing on the analysis procedure ex ante reduces ex post conflict
- frontloading research design decisions (get more specific, identify problems)
- Anticipating what might go wrong: attrition, noncompliance, study failure, missing covariate data, etc.
- Faithful reanalysis. Reanalysts lose a degree of freedom in determining what an author might have intended by a given analysis. 
- Easier design comparison. If a design is declared at the preanalysis plan phase, then it enables direct comparison with the design as implemented in the final write-up. Side-by-side comparison of the code neatly clarifies which design choices were made ex-ante and ex-post. Side-by-side comparison of the performance of a planned and implemented designs clarify under what conditions deviations from plans are defensible improvements.
- ethics (summarize and cite Jay's ideas here: http://www.jasonlyall.com/wp-content/uploads/2020/08/PreregisterYourEthics.pdf)
- ethical outcomes are potential outcomes, so we need to think about them ex ante not just on the basis of revealed outcomes

### countering objections:

- Time-consuming. Yes but in our experience we only pay this cost for failed studies. Fur successful studies, nearly all work that goes in to the pap pays off in terms of higher qualtity design, literal words we already wrote, and written-in-advance analysis code.

- Won't stop determined cheaters. Yes -- remember that the goal is not to prevent fraud, it's to help *researchers* improve their designs. Science depends on trust.

- Replication is better (@Coffman2015). These are complements, not substitutes.

- I can't preregister what I will do because I don't know what I will find. That's fine too, just write down how you will go about "finding" things so we (and YOU) can understand your own process.

### Other thoughts

- when is the right moment to write a pap?
- relationship to registered reports?
- SOPs

### citations on paps

- @Casey2012: early entry
- @Olken2015: halfway skeptical
- @Green2015: Standard operating procedures
- @Christensen2018: review
- @Coffman2015: a skeptical take (prefer replications). 
- @Humphreys2013a: nonbinding
- @Miguel2014: distinguishes between disclosure and PAP
- @Ofosu2020: apparently paps hinder publication?
