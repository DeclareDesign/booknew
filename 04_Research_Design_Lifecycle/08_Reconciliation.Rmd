---
title: "Reconciliation"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# Reconciliation

<!-- make sure to rename the section title below -->

```{r reconciliation, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

Inevitably, the research design as implemented will differ in some way from the research design as planned. When they diverge, misleading readers to believe that the design as implemented was the design as planned all along is an error. We should be transparent about how the design changed over the course of the research process -- first of all, it's the honest thing to do, and second, those changes condition how we interpret the research.

Suppose the original design described a three-arm trial: one control and two treatments, but the design as implemented drops all subjects assigned to the second treatment. Sometimes this is an entirely appropriate and reasonable design modification: perhaps it turns out that due to an implementation failure, the second treatment was simply not delivered. Other times, these modification is less benign -- perhaps the estimate of the effect of the second treatment does not achieve statistical significance, so the author simply omits it from the analysis. 

For this reason, explicitly **reconciling** the design as planned with the design as implemented can be useful. Having a publicly-posted preanalysis plan can make the reconciliation process especially credible -- we know for sure what the planned design was because the preanalysis plan describes it (in greater or lesser detail) pre-implementation. However, a preanalysis plan is not a prerequisite for engaging in reconciliation. The scientific enterprise is built in large measure on trust: we are ready to believe researchers who say, here is the design I though I would implement but due to unanticipated developements, here is the design I ended up implementing. 

In some cases, reconciliation will lead to additional learning beyond what can be inferred from the final design itself. When some units could refused to be included in the study sample or some units refused measurement, we learn that important features about those units. Understanding sample exclusions, noncompliance, and attrition not only may inform future research design planning choices, but contribute substantively to our understanding of the social setting. A policy implemented in the same way the study would likely also not be able to work in the units who refused to participate, and future research could examine why or how to convince them of the policy's benefits.

What belongs in a reconciliation? At a minimum, we need a full description of the planned design, a full description of the implemented design, and a list of the differences. 

This can be made explicit through the declaration of both design in computer code, then comparing the two design objects line-by-line.

In `DeclareDesign` we do the comparison for you:

```{r, eval = FALSE}
design1 <- declare_population(N = 100, u = rnorm(N)) +
  declare_potential_outcomes(Y ~ Z + u) +
  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +
  declare_sampling(n = 75) +
  declare_assignment(m = 50) +
  declare_reveal(Y, Z) +
  declare_estimator(Y ~ Z, estimand = "ATE")

design2 <- declare_population(N = 200, u = rnorm(N)) +
  declare_potential_outcomes(Y ~ 0.5*Z + u) +
  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +
  declare_sampling(n = 100) +
  declare_assignment(m = 25) +
  declare_reveal(Y, Z) +
  declare_estimator(Y ~ Z, model = lm_robust, estimand = "ATE")

compare_designs(design1, design2)
compare_design_code(design1, design2)
compare_design_summaries(design1, design2)
compare_design_data(design1, design2)
compare_design_estimates(design1, design2)
compare_design_estimands(design1, design2)
```


<!-- ### Scattered thoughts -->

<!-- - @ofosu2019pre -->



