---
title: "Reanalysis"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

## Reanalysis

<!-- make sure to rename the section title below -->

```{r reanalysis, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

A reanalysis of an existing study is a follow-up study in which $d$, the original realized data, is fixed and changes to the answer strategy $A$ and sometimes the model $M$ or inquiry $I$ are proposed. Given $d$ is fixed, so too is the data strategy $D$. The results given the new MIDA, which may differ from the original study's results, are reported. We can learn from reanalyses in several ways. 

We can confirm that there were no errors in the analysis strategy. Many reanalyses correct simple mathematical errors, typos in data transcription, or failures to analyze following the data strategy faithfully. These reanalyses show whether results do or do not depend on these corrections. 

We can reassess what is known about the same inquiry, using new information about the world that was learned after the original study was published. Here, we may learn about new confounders or alternative causal channels that undermine the credibility of the original answer strategy. When reanalyzed, demonstrating the results do (or do not) change improves our understanding of the true inquiry. 

Many reanalyses show that original findings are not "robust" to alternative answer strategies. These are better conceptualized as claims about robustness to alternative models: one model may imply one answer strategy and a different model, with another confounder, implies another. If both models are plausible, a good answer strategy should be robust to both and even help to distinguish between them and a reanalysis could uncover robustness to these alternative models or lack thereof. 

Reanalyses may also aim to answer new questions that were not considered by the original study, but for which the realized data can provide useful answers. For example, authors may analyze outcomes not originally analyzed.

Reanalyses are themselves research designs. Whether a reanalysis is a good design, and how much it can contribute to our knowledge about the original inquiry, depend on *possible* realizations of the data not just the realized data. Because $d$ is fixed in a reanalysis, analysts are often instead tempted to judge the reanalysis based on whether it overturns or confirms the results of the original study. A successful reanalysis in this way of thinking demonstrates, by showing that the original results are changed under an alternative answer strategy, that the results are not robust to other plausible models. This way of thinking can lead to incorrect assessments of reanalyses. We need to consider what answers would obtain under the original answer strategy $A$ and the reanalysis strategy $A^{\prime}$ under many *possible* realizations of the data. A good reanalysis strategy reveals with high probability the set of models of the world under which we can make credible claims about the inquiry. Whether or not the results from the fixed $d$ that was realized change under the answer strategies $A$ and $A^{\prime}$ tells us little about this probability. It is only one draw. 

To diagnose a reanalysis, we need to define two answer strategies --- $A$ and $A^{\prime}$ --- but also a new diagnostic-statistic. We need to decide how we summarize the answers from the two answer strategies. If one returns TRUE and one FALSE, what do we conclude about the inquiry? The function we define to summarize the two results depends on the inquiry and the goals of the reanalysis. But our diagnosis of the reanalysis should assess the properties of this summary of the two studies under possible realizations of the data. If the goal of the reanalysis is instead to learn about a new question, then we should simply construct a new MIDA altogether, but holding constant $D$ from the original study, which we cannot change because we already collected $d$ using it.

We illustrate the flaw in assessing reanalyses on the basis of changing significance of results from realized data below. We demonstrate how to assess the properties of reanalysis plans, comparing the properties of original answer strategies to proposed reanalysis answer strategies. 

The design we consider is an observational study with a binary treatment $Z$ that may or may not be confounded by a covariate $X$. The original answer strategy is a difference-in-means in the outcome $Y$ between the treatment and control group defined by $Z$, call it `A`. The reanalyst collects the covariate $X$ and proposes to control for it in a linear regression; call that strategy `A_prime`. 

```{r}
A <- declare_estimator(Y ~ Z, model = lm_robust, label = "A")
A_prime <- declare_estimator(Y ~ Z + X, model = lm_robust, label = "A_prime")
```

We set up the model in the shoes of the renalyst, who believes $Z$ and $Y$ are confounded by $X$:

```{r}
# X is a confounder and is measured pretreatment
model_1 <- 
  declare_model(
    N = 100,
    U = rnorm(N),
    X = rnorm(N), # X is pretreatment
    Z = rbinom(N, 1, prob = plogis(0.5 + X)),
    potential_outcomes(Y ~ 0.1 * Z + 0.25 * X + U),
    Y = reveal_outcomes(Y ~ Z)
  ) 
```

Drawing data from this strategy and applying the two answer strategies, we get differing results:

```{r, eval = FALSE}
draw_estimates(model_1 + A + A_prime)
```

```{r, echo = FALSE}
set.seed(3)
est <- draw_estimates(model_1 + A + A_prime)
est %>% select(estimator_label, estimate, std.error, p.value) %>% kable(digits = 3)
```

Commonly, renalysts would infer from this two things: that there is not an average treatment effect, and that the answer strategy `A_prime` is preferred. It is preferred because it is unbiased under confounding and also unbiased (and lower variance than `A`) even when there is no confounding. As we show below, these claims depend on the validity of other parts of the model $M$. In particular, we define two other possible models, both where $X$ is not a confounder. In model 2, $X$ is measured before treatment, as in the first model. In model 3, however, $X$ is measured *post*treatment, and is affected by $Y$. In many observational settings, it is now always clear when $X$ is realized or measured relative to treatment, so it is useful to explore the implications of the timing of measurement on the diagnosands of the design as we do below.

```{r}
# X is not a confounder and is measured pretreatment
model_2 <- 
  declare_model(
    N = 100,
    U = rnorm(N),
    X = rnorm(N),
    Z = rbinom(N, 1, prob = plogis(0.5)),
    potential_outcomes(Y ~ 0.1 * Z + 0.25 * X + U),
    Y = reveal_outcomes(Y ~ Z)
  ) 

# X is not a confounder and is measured posttreatment
model_3 <- 
  declare_model(
    N = 100,
    U = rnorm(N),
    Z = rbinom(N, 1, prob = plogis(0.5)),
    potential_outcomes(Y ~ 0.1 * Z + U),
    Y = reveal_outcomes(Y ~ Z),
    X = 0.1 * Z + 5 * Y + rnorm(N)
  ) 

design_1 <- model_1 + I + A + A_prime
design_2 <- model_2 + I + A + A_prime
design_3 <- model_3 + I + A + A_prime
```

```{r, echo = FALSE, eval = do_diagnosis}
diagnosis <- diagnose_design(design_1, design_2, design_3, sims = sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("17_Reanalysis"), "/diagnosis.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnosis, file = rds_file_path)
}
diagnosis <- read_rds(rds_file_path)
```

What we see in the diagnosis below is that `A_prime` is only preferred if we know that $X$ is measured pretreatment. In design 2, where $X$ is measured posttreatment, `A` is preferred, because controlling for $X$ leads to posttreatment bias. Including the $X$ variable in the regression misattributes some of the effect of $Z$ on $Y$ to $X$. The reanalyst, this diagnosis indicates, needs to justify their beliefs about when $X$ is measured in order to claim that `A_prime` is preferred to `A`. It is not always preferred, as they might have concluded just by looking at the realized estimates from the two answer strategies.

```{r, echo = FALSE}
get_diagnosands(diagnosis) %>% 
  select(design_label, estimator_label, bias)
```


<!-- how do we update from the reanalysis research design (the original design plus the reanalysis of d)? -->
<!-- -- the design is the research design from before with two sets of estimates from two different A's -->
<!-- -- need an aggregation function (decisionmaking function) that converts the two sets of results into a decision or posterior -->

<!-- what can be learned from reanalysis? -->
<!-- (1) confirm there were not errors (consider changing A only) -->
<!-- (2) reassess what is known about the same inquiry, using new information about the world (change M, change A to suit new M) -->
<!-- (3) learn something new from the data about another node or edge or a different summary about the same ones (change I and possibly A to match it; possibly M if a node was missing; possibly add data) -->
<!-- (4) assess "robustness" of findings - point to discussion of this in answer strategy (or move it here) (change A) -->
<!-- (5) update M based on new research and assess what d can tell us from this study (change M and possibly I, possibly A to fit changed M and I) -->

<!-- how can we assess the properties of a *reanalysis*? diagnose changed MIDA. important to not condition on d, the design includes the actual D, and we need to consider what results d' we would get from the reanalysis under different realizations of D. -->

<!-- there are now two A's, so need to specify a decision function about how to integrate the two findings. this could be throw away the old a, or combine them in some way. if it's a "robustness" to alternative A, then you may want to combine not throw out for example. it's crucial to specify how you do that, that's part of the answer strategy. -->

<!-- ## Example -->

<!-- Knox, Lowe, and Mummolo (2020) (https://www.cambridge.org/core/journals/american-political-science-review/article/administrative-records-mask-racially-biased-policing/66BC0F9998543868BB20F241796B79B8) study the statistical biases that accompany estimates of racial bias in police use of force when presence in the dataset (being stopped by police) is conditioned on an outcome that is a downstream consequence of race. They show the estimate is not identified unless additional modelling assumptions are brought to bear. -->

<!-- Gaebler et al. (2020) (https://5harad.com/papers/post-treatment-bias.pdf) study the same question and make such modeling assumptions (subset ignorability, definition 2). -->

<!-- In a twitter thread (https://twitter.com/jonmummolo/status/1275790509647241222?s=20), Mummolo shows the three DAGs that are compatible with subset ignorability. We agree with Mummolo that these DAGs assume away causal paths that are very plausible. -->

<!-- ![DAG](figures/mummolo_dag.png) -->

<!-- This document provides a design declaration for this setting and shows how estimates of the controlled direct effect (effect of race on force among the stopped) are biased unless those paths are set to zero by assumption. -->

<!-- Design Declaration -->

<!-- There are four variables: (D: minority, M: stop, U: suspicion (unobserved), Y: force) and five paths: -->

<!-- ```{r} -->
<!-- D_M = 1 # effect of minority on stop -->
<!-- U_M = 1 # effect of suspicion on stop -->
<!-- D_Y = 1 # effect of minority on force -->
<!-- U_Y = 1 # effect of suspicion on force -->
<!-- M_Y = 1 # effect of stop on force -->
<!-- ``` -->

<!-- This basic design allows all five paths. -->

<!-- ```{r} -->
<!-- design_1 <- -->
<!--   declare_model(N = 1000, -->
<!--                      D = rbinom(N, size = 1, prob = 0.5), -->
<!--                      U = rnorm(N)) + -->
<!--   declare_potential_outcomes(M ~ rbinom(N, size = 1, prob = pnorm(D_M * -->
<!--                                                                     D + U_M * U)), -->
<!--                              assignment_variable = "D") + -->
<!--   declare_reveal(M, D) + -->
<!--   declare_potential_outcomes(Y ~ rnorm(N, D_Y * D + M_Y * M + U_Y * U), -->
<!--                              conditions = list(D = c(0, 1), M = c(0, 1))) + -->
<!--   declare_reveal(outcome_variables = "Y", -->
<!--                  assignment_variables = c("D", "M")) + -->
<!--   declare_inquiry(CDE = mean(Y_D_1_M_1 - Y_D_0_M_1)) + -->
<!--   declare_estimator(Y ~ D, subset = M == 1, inquiry = "CDE") -->
<!-- ``` -->

<!-- We redesign the design 3 times, removing one path at a time, then simulate all four designs. -->

<!-- ```{r, message=FALSE} -->
<!-- # no effect of D on M -->
<!-- design_2 <- redesign(design_1, D_M = 0) -->

<!-- # no effect of U on M -->
<!-- design_3 <- redesign(design_1, U_M = 0) -->

<!-- # no effect of U on Y -->
<!-- design_4 <- redesign(design_1, U_Y = 0) -->
<!-- ``` -->

<!-- This chunk is set to `echo = TRUE` and `eval = do_diagnosis` -->
<!-- ```{r, eval = do_diagnosis & !exists("do_bookdown")} -->
<!-- simulations <- simulate_designs(design_1, design_2, design_3, design_4, sims = sims) -->
<!-- ``` -->

<!-- Right after you do simulations, you want to save the simulations rds. -->

<!-- ```{r, echo = FALSE, purl = FALSE} -->
<!-- # figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file -->
<!-- rds_file_path <- paste0(get_dropbox_path("policing"), "/simulations_policing.RDS") -->
<!-- if (do_diagnosis & !exists("do_bookdown")) { -->
<!--   write_rds(simulations, path = rds_file_path) -->
<!-- } -->
<!-- simulations <- read_rds(rds_file_path) -->
<!-- ``` -->


<!-- ```{r, echo=FALSE, message = FALSE} -->

<!-- simulations <- -->
<!--   simulations %>% -->
<!--   mutate(`Assumed DAG` = factor( -->
<!--     design_label, -->
<!--     levels = c("design_1", "design_2", "design_3", "design_4"), -->
<!--     labels = c( -->
<!--       "All paths possible", -->
<!--       "no effect of D on M", -->
<!--       "no effect of U on M", -->
<!--       "no effect of U on Y" -->
<!--     ) -->
<!--   )) -->

<!-- summary_df <- -->
<!--   simulations %>% -->
<!--   group_by(`Assumed DAG`) %>% -->
<!--   summarise( -->
<!--     mean_estimand = mean(estimand), -->
<!--     mean_estimate = mean(estimate), -->
<!--     bias = mean(estimate - estimand) -->
<!--   ) %>% -->
<!--   pivot_longer(cols = c("mean_estimand", "mean_estimate")) -->
<!-- ``` -->

<!-- This plot confirms that unless one of those implausible assumptions hold, estimates of the CDE are biased. -->

<!-- ```{r, echo=FALSE} -->
<!-- ggplot(simulations, aes(estimate)) + -->
<!--   geom_histogram(bins = 50) + -->
<!--   geom_vline(data = summary_df, aes(xintercept = value, color = name)) + -->
<!--   facet_wrap(~`Assumed DAG`) + -->
<!--   xlab("Simulated CDE estimates") + -->
<!--   theme_bw() + -->
<!--   theme(legend.position = "bottom", -->
<!--         strip.background = element_blank(), -->
<!--         axis.title.y = element_blank(), -->
<!--         legend.title = element_blank()) -->
<!-- ``` -->

<!-- ### Grab bag -->

<!-- - @Clemens2017 on taxonomy of these kinds of efforts -->
<!-- - if you're going to use d to learn about a different M for a different I, you need to understand their D -->
