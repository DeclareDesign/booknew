---
title: "Communicating"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

<!-- make sure to rename the section title below -->

```{r communicating, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

# Integration

After publication, research studies leave the hands of researchers and enter the public domain. 

Most immediately, authors share their findings with the public through the media and with decisionmakers who may incorporate the results in specialist publications and executive summaries. Consideration of how the findings will eventually be communicated --- and how policymakers, businesses, and regular people will use the findings to make decisions --- may change the research design. New inquiries beyond those of interest to scientists can be added to design declarations. They can be diagnosed with respect to how lay readers will interpret evidence, which may differ from the interpretation in scientific publications. Ex-ante consideration of communication plans will ensure designs target both scientific publication and decisionmaking by nonscientists.

Researchers can prepare for the integration of their studies into scholarly debates through better archiving practices and better reporting of research designs in the published article. Future researchers may build on the results of a past study in three ways. First, they may *reanalyze* the original data. Reanalysts must be cognizant of the original data strategy *D* when working with the realized data $d$. Changes to the the answer strategy *A* must respect *D*, regardless of whether the purpose of the reanalysis is to answer the original inquiry *I* or to answer a different inquiry $I'$. Second, future researchers may *replicate* the design. Typically, replicators provide a new answer to the same *I* with new data, possibly improving elements of *D* and *A* along the way. If the inquiry of the replication is too different from the inquiry of the original study, the fidelity of the replication study may be compromised. Lastly, future researchers may *meta-analyze* study's answer $a^d$ with other past studies. Meta-analysis is most meaningful when all of the included studies target a similar enough inquiry and when all studies rely on credible design. Otherwise, the procedure produces a meta-analytic average that is difficult to interpret.

All three of these activities depend on an accurate understanding of the study design. Reanalysts, replicators, and meta-analysts all need access to the study data and materials, of course. They also need to be sure of the critical design information in *M*, *I*, *D*, and *A*. Later in this section, we outline how archiving procedures that preserve study data and study design can enable new scientific purposes and describe strategies for doing each of these three particular integration tasks.



## Communicating

The findings from studies are communicated to other scholars through academic publications. But some of the most important audiences -- policymakers, businesses, journalists, and the public -- do not read academic journals. These audiences learn about the study in other in other ways. Authors write opeds, blog posts, and policy reports that translate research for nonspecialist audiences. Press offices pitch research studies for coverage by the media. Researchers present findings directly to decisionmakers and to their research partners.

These new outputs are for different audiences, so they are necessarily diverse in their tone and approach. Some things don't change: we still need to communicate the quality of the research design and what we learn from the study. But some things do: we need to translate specialist language about the substance of the study to a nonspecialist audience, and translate the features of the research design in a way that nonspecialists can understand. If we do not communicate outside of academic publications, research findings will not influence public debates, policymaking, or real-world decisions by people and firms.

Too often, a casualty of translating the study from academic to other audiences is the design information. When researchers write for popular blogs or give interviews, emphasis is placed on the study results, not on the reasons why the results of the study are to be believed. In "dumbing down" the research for nonspecialist audiences, we revert to saying *that* the findings are true and not *why we know* the findings are true. Explaining why we know requires explaining the research design, which in our view ought to be part of any public-facing communication about research.

Of course, even when authors do emphasize design, journalists do not always care. Science reporting is commonly criticized for ignoring study design when picking which studies to publicize, so weak studies are not appropriately filtered out of coverage. Furthermore, journalists emphasize results they believe will drive people to pick up a newspaper or click on a headline. Flashy, surprising, or pandering findings receive far more attention than deserved, with the result that boring but correct findings are crowded out of the media spotlight. 

In a review we conducted of recent studies published in the New York Times Well section on health and fitness, we found that two dimensions of design quality were commonly ignored. First, experimental studies on new fitness regimens with tiny samples, sometimes fewer than 10 units, are commonly highlighted. When both academic journals and reporters promote tiny studies with flashy or unexpected findings, the likely result is that the published record is full of statistical flukes driven by noise, not new discoveries. Second, very large studies that draw observational comparisons between large samples of dieters and non-dieters with millions of observations on diet receive outsize attention. These designs are prone to bias from confounding, but these concerns are swept under the rug. 

This state of affairs is not entirely or even mostly the journalists' fault, since, in the absence of design information, it can be challenging to separate the weak designs from the strong ones. Study results and the stamp of approval from peer review are easy heuristics to follow. 

How can we improve this scientific communication dilemma? The market incentives for both journalists and authors reward style over substance, and any real solution to the problem would require addressing those incentives. Short of that, we recommend that authors who wish to communicate the high quality of their designs to the media can do so by providing the design information in *M*, *I*, *D*, and *A* in lay terms. Science communicators should clearly state the research question (*I*) and explain why applying the data and answer strategies is likely to yield a good answer to the question. The actual result ($a^d$) is, of course, also important to communicate, but *why* $a^d$ is a credible answer to the research question is just as important to share. Building confidence in scientific results requires building confidence in scientific practice.





<!-- Science reporting often has two aims: surface new discoveries, and inform decisions we make. New discoveries, such as a new cure on the horizon or better idea for reducing poverty, may not immediately affect decisions we make: we need further confirmation of their effectiveness in followup studies.  -->

<!-- A single study is typically not enough to immediately change decisions. Rather, most decisions should be based on the current scientific consensus on an inquiry (which itself could be informed by a single study, but in concert with those that came before). Scholars, therefore, may usefully consider how their study will or will not impact the scholarly consensus while designing a study.  -->

<!-- Conducting a meta-analysis or systematic review of past studies on the inquiry and considering what kind of evidence would shift the consensus -- either in terms of the sign or magnitude of the evidence or its strength. Each can be specified as a diagnosand: how much does the prior consensus on the average effectiveness of an intervention shift in response to evidence from this study.  -->

<!-- Design choices the researcher makes, such as sample size but also how they present the results in relation to past evidence, impact these diagnosands. On the flip side, reporters can then use the information about the current consensus and its strength (i.e., the standard deviation of the posterior), not only the findings from a single study.  -->

<!-- -- should we publish working papers -->
<!-- -- which papers should we *seek* reporting on -->
<!-- -- should reporters select  -->

<!-- - research design diagnosis can provide a tool for the media to assess the quality of evidence from a study they are considering for publication -->
<!-- - two kinds of bias: tiny samples and p-hacking; and confounding in observational studies -->
<!-- - susceptibility to clickbait  -->

<!-- - media publication bias from communication  -->
<!-- - bias from what scholars are able to and decide to share research outside  -->
<!-- - problem that lay audiences cannot distinguish quality of evidence -->
<!-- - communicating one study vs meta-analysis -->
<!-- - communicating scholarly consensus -->


