---
title: "Communicating"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

<!-- make sure to rename the section title below -->

```{r communicating, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

# Integration

After publication, research studies leave the hands of researchers and enter the public domain. 

Most immediately, authors consider sharing their findings with the public through the media as well as decisionmakers who may make use of the findings through specialist publications and executive summaries. Considering plans for communicating scientific findings --- and how policymakers, businesses, and regular people will use the findings of the study to make decisions --- may change the research design. New inquiries beyond those of interest to scientists can be added to design declarations and they can be diagnosed with respect to how lay readers will interpret evidence, which may differ from the interpretation in scientific publications. Ex ante consideration of communication plans will ensure designs target both scientific publication and decisionmaking by nonscientists.

Researchers can prepare for the integration of their studies into scholarly debates through better archiving practices, as well as better reporting of research designs in the published article. Later researchers may build on the results of a past study in three ways: reanalyze the original data, holding $d$ constant but changing $A$ (reanalysis); providing a new answer to the same $I$ with new data, possibly changing $D$ and $A$ (replication); or synthesizing the study's answer $a^d$ to the inquiry along with other past studies (meta-analysis). In this section, we outline archiving procedures to enable later researchers to reanalyze, replicate, or synthesize; and we describe procedures for doing each of these three integration tasks.

## Communicating

The findings from studies are communicated to other scholars through academic publication, but to policymakers, businesses, and the public in other ways. Authors write opeds, blog posts, and policy reports that translate research for nonspecialist audiences; pitch their research for coverage by the media; and present findings directly to decisionmakers. After spending so much time and effort honing the description of the research design and results for publication, these are new outputs for different audiences. Some goals are shared in common: communicating the quality of the research design and what we learn from the study. But some differ: we need to translate specialist language about the substance of the study to a nonspecialist audience, and translate the features of the research design in a way that nonspecialists can understand. If we do not communicate outside of academic publications, research findings will not influence public debates, policymaking, or real world decisions by people and firms.

Media reporting on science is commonly criticized for two pathologies in how studies are selected for publication: study design is ignored, allowing low quality designs to published; and selective reporting of findings that will drive people to pick up a newspaper or click through to an article online. Indeed, one of the causes of publication bias in scientific venues is the desire to get covered in the media and the belief that flashy findings will result in coverage. 

Design declaration and diagnosis can help authors communicate the high quality of their designs, and science reporters who wish to increase the quality of reporting to select only high quality designs. In a review we conducted of recent studies published in the New York Times Well section on health and fitness, two dimensions of design quality are commonly ignored. First, experimental studies on new fitness regimens with tiny samples, sometimes fewer than 10 units, are commonly published. When both academic journals and reporters are selecting among tiny studies for flashy or unexpected findings, the likely result is that what is published are statistical flukes driven by noise not new discoveries. Second, very large studies with millions of observations on diets --- meta-analyses or longitudinal studies of large populations --- are commonly published that draw observational comparisons between dieters and non-dieters. Typically with little or no consideration for whether ignorability assumptions are met, these findings are likely to be biased due to confounding. A design declaration could provide information both on the statistical power and possible effects of confounding on the bias of findings that reporters could use in selecting studies to report on.

Science reporting often has two aims: surface new discoveries, and inform decisions we make. New discoveries, such as a new cure on the horizon or better idea for reducing poverty, may not immediately affect decisions we make: we need further confirmation of their effectiveness in followup studies. A higher level of scrutiny should be placed on reporting that aims to (or may have the effect of) affecting decisions people or organizations make. A single study is typically not enough to immediately change decisions. Rather, most decisions should be based on the current scientific consensus on an inquiry (which itself could be informed by a single study, but in concert with those that came before). Scholars, therefore, may usefully consider how their study will or will not impact the scholarly consensus while designing a study. Conducting a meta-analysis or systematic review of past studies on the inquiry and considering what kind of evidence would shift the consensus -- either in terms of the sign or magnitude of the evidence or its strength. Each can be specified as a diagnosand: how much does the prior consensus on the average effectiveness of an intervention shift in response to evidence from this study. Design choices the researcher makes, such as sample size but also how they present the results in relation to past evidence, impact these diagnosands. On the flip side, reporters can then use the information about the current consensus and its strength (i.e., the standard deviation of the posterior), not only the findings from a single study. 

<!-- -- should we publish working papers -->
<!-- -- which papers should we *seek* reporting on -->
<!-- -- should reporters select  -->

<!-- - research design diagnosis can provide a tool for the media to assess the quality of evidence from a study they are considering for publication -->
<!-- - two kinds of bias: tiny samples and p-hacking; and confounding in observational studies -->
<!-- - susceptibility to clickbait  -->

<!-- - media publication bias from communication  -->
<!-- - bias from what scholars are able to and decide to share research outside  -->
<!-- - problem that lay audiences cannot distinguish quality of evidence -->
<!-- - communicating one study vs meta-analysis -->
<!-- - communicating scholarly consensus -->


