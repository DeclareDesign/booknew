---
title: "Resolving Disputes"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

## Resolving Disputes

<!-- make sure to rename the section title below -->

```{r resolving_disputes, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

The problem: Acrimonious debates arise; Hard to interpret contribution of replication and reanalysis; First main task (accumulation of knowledge) damaged.

The solution: Some of this comes down to basic disagreements we can't resolve. But some of it comes down to a lack of principles guiding how design decisions are made and how the results they produce should be interpreted. And a lack of procedures for understanding the consequences of decisions.

- Principles for making design choices, tailored to the distinct challenges posed to reanalysis and to replication
- Some changes are justifiable / encouraged, some things are not justifiable / discouraged, conditions for justification are clarified
- Procedures for putting principles into practice 
- Declaration and diagnosis through DD

Current practice for replication is to exactly replicate data strategy and analysis strategy in new or same context. This is not needed! Standard should be: best answer to same inquiry in new or same context! But how can we justify changes to D and A that give a "better" answer to same inquiry?


Replacing them with alternative practices justified by design simulation
1. M always changes! (you have more information on tau or sd(tau))
2. Home ground dominance: Change A or D-and-A if A’ > A under M
3. Robustness to alternative models: Change A or D-and-A if A’ ≥ A under M AND A’ > A under M’ E.g. change from simple to complete RA
4. Model plausibility:If A’ < A under M AND A’ > A under M’, then change to A' or D-and-A IFF M' is more plausible than M E.g. switching to balanced design if you believe variances equal across treatment groups
5. Undefined inquiries. Change I to I’ if I is undefined under M If I is defined under M: You can’t change to I’, You can’t change D to D’ if that means I unidentifiable.

Disorganized thoughts:
- Changes to D include both interventions (sampling and randomization), as well as the inclusion of different / new datasets on the same model (this is common in econ reanalyses at state-level). The collection of "different" data through a change in question wording also fits into this. Need to think about a good typology of data strategies.
- There’s often a broader research question that’s being answered, and when I changes sometimes both are answering the same broader question. But focuses debate on whether that claim is true that I and I’ answer the same broader I. 
- In replication can you use data from study 1 to assess the plausibility of M?
- When does changing outcomes change the inquiry?
Example: you used z-scores in your original analysis in order to measure an effect on five different measures of some latent construct. I show that taking a simple average has better properties (e.g., statistical power), and use this instead of z-score. Have I changed estimand? If so, are there any instances of "recoding" or even "rewording" of outcome measures that we would be OK with, insofar as they get better answers to the inquiry without changing the inquiry?
One way of looking at this: inquiry is in reference to summary of a latent variable, which stays constant, but D changes which is different measurement of the latent variable
Point to keep in mind from this: D change might be in sampling/treat assignment or measurement
Key thing we are saying here: there are two dimensions of change with measurement. (1) are you changing estimands because the latent construct is changing implicitly; (2) are you changing to a better/worse measurement of the same latent construct.

### Example policing

Knox, Lowe, and Mummolo (2020) (https://www.cambridge.org/core/journals/american-political-science-review/article/administrative-records-mask-racially-biased-policing/66BC0F9998543868BB20F241796B79B8) study the statistical biases that accompany estimates of racial bias in police use of force when presence in the dataset (being stopped by police) is conditioned on an outcome that is a downstream consequence of race. They show the estimate is not identified unless additional modelling assumptions are brought to bear.

Gaebler et al. (2020) (https://5harad.com/papers/post-treatment-bias.pdf) study the same question and make such modeling assumptions (subset ignorability, definition 2).

In a twitter thread (https://twitter.com/jonmummolo/status/1275790509647241222?s=20), Mummolo shows the three DAGs that are compatible with subset ignorability. We agree with Mummolo that these DAGs assume away causal paths that are very plausible.



<!-- ```{r echo=FALSE, out.width='100%'} -->
<!-- knitr::include_graphics('../../figures/mummolo_dag.png') -->
<!-- ``` -->

This document provides a design declaration for this setting and shows how estimates of the controlled direct effect (effect of race on force among the stopped) are biased unless those paths are set to zero by assumption.

Design Declaration

There are four variables: (D: minority, M: stop, U: suspicion (unobserved), Y: force) and five paths:

```{r}
D_M = 1 # effect of minority on stop
U_M = 1 # effect of suspicion on stop
D_Y = 1 # effect of minority on force
U_Y = 1 # effect of suspicion on force
M_Y = 1 # effect of stop on force
```

This basic design allows all five paths.

```{r}
design_1 <-
  declare_population(N = 1000,
                     D = rbinom(N, size = 1, prob = 0.5),
                     U = rnorm(N)) +
  declare_potential_outcomes(M ~ rbinom(N, size = 1, prob = pnorm(D_M *
                                                                    D + U_M * U)),
                             assignment_variable = "D") +
  declare_reveal(M, D) +
  declare_potential_outcomes(Y ~ rnorm(N, D_Y * D + M_Y * M + U_Y * U),
                             conditions = list(D = c(0, 1), M = c(0, 1))) +
  declare_reveal(outcome_variables = "Y",
                 assignment_variables = c("D", "M")) +
  declare_estimand(CDE = mean(Y_D_1_M_1 - Y_D_0_M_1)) +
  declare_estimator(Y ~ D, subset = M == 1, estimand = "CDE")
```

We redesign the design 3 times, removing one path at a time, then simulate all four designs.

```{r, message=FALSE}
# no effect of D on M
design_2 <- redesign(design_1, D_M = 0)

# no effect of U on M
design_3 <- redesign(design_1, U_M = 0)

# no effect of U on Y
design_4 <- redesign(design_1, U_Y = 0)
```

This chunk is set to `echo = TRUE` and `eval = do_diagnosis`
```{r, eval = do_diagnosis & !exists("do_bookdown")}
simulations <- simulate_designs(design_1, design_2, design_3, design_4, sims = sims)
```

Right after you do simulations, you want to save the simulations rds.

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("policing"), "/simulations_policing.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(simulations, path = rds_file_path)
}
simulations <- read_rds(rds_file_path)
```


```{r, echo=FALSE, message = FALSE}

simulations <-
  simulations %>%
  mutate(`Assumed DAG` = factor(
    design_label,
    levels = c("design_1", "design_2", "design_3", "design_4"),
    labels = c(
      "All paths possible",
      "no effect of D on M",
      "no effect of U on M",
      "no effect of U on Y"
    )
  ))


summary_df <-
  simulations %>%
  group_by(`Assumed DAG`) %>%
  summarise(
    mean_estimand = mean(estimand),
    mean_estimate = mean(estimate),
    bias = mean(estimate - estimand)
  ) %>%
  pivot_longer(cols = c("mean_estimand", "mean_estimate"))
```

This plot confirms that unless one of those implausible assumptions hold, estimates of the CDE are biased.

```{r, echo=FALSE}
ggplot(simulations, aes(estimate)) +
  geom_histogram(bins = 50) +
  geom_vline(data = summary_df, aes(xintercept = value, color = name)) +
  facet_wrap(~`Assumed DAG`) +
  xlab("Simulated CDE estimates") +
  theme_bw() +
  theme(legend.position = "bottom",
        strip.background = element_blank(),
        axis.title.y = element_blank(),
        legend.title = element_blank())
```


