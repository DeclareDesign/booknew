---
title: "Decisionmaking"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

<!-- make sure to rename the section title below -->

```{r decisionmaking, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
library(metafor)
library(broom)
```

## Decisionmaking

Policymakers, businesses, humanitarian organizations, and regular people make decisions based on social science research. Research designs, however, are often constructed without considering who will be informed by the evidence and how they will use evidence in decisions. We can optimize our designs for both scientific publication and decisionmaking. The first step is eliciting the inquiries decisionmakers have, and the second is diagnosing how decisions change depending on the results. How often would the decisionmaker make the right decision, with and without the study? A design that exhibits high statistical power and a high rate of making the right decision will influence not only the scientific literature but also decisions made by the public. 

We illustrate this process by declaring an experimental design that compares a status quo policy with an alternative. As the researcher, your inquiry is the average treatment effect, but the policymaker has a subtlely different inquiry. The policymaker would like to know which policy to implement: the status quo or the alternative. Imagine you meet with the policymaker and ask how they would use the evidence you plan to produce. The policymaker says that they would like to switch to the alternative if it is better than the status quo. However, they face a switching cost to adopt the new policy, so for now, they would like to adopt the treatment only if it is at least 0.1 standard deviations better than the status quo. 

In your design declaration, you add two new components to assess your design's statistical power for estimating the average treatment effect as well as the probability of the policymaker making the right decision. The first is you add a new inquiry, which is, is the treatment at least 0.1 standard deviations better than the control condition? In addition, you add a statistical test to target this inquiry, which tests whether the treatment effect is larger than 0.1. It does so by calculating a linear combination of the treatment effect estimate: the null hypothesis is $\widehat\tau - 0.1 = 0$.

```{r}
# compare status quo to a new proposed policy, 
#   given cost of switching 
N <- 100
effect_size <- 0.1

decision_function <-
  function(data) {
    data %>%
      lh_robust(Y ~ Z, linear_hypothesis = "Z - 0.1 = 0", data = .) %>%
      tidy %>%
      filter(term == "Z - 0.1 = 0")
  }

design <-
  declare_model(
    N = N,
    U = rnorm(N),
    potential_outcomes(Y ~ effect_size * Z + U)
  ) + 
  declare_inquiry(
    ATE = mean(Y_Z_1 - Y_Z_0),
    alternative_better_than_sq = if_else(ATE > 0.1, TRUE, FALSE)
  ) + 
  declare_assignment(Z = complete_ra(N), legacy = FALSE) +
  declare_measurement(Y = reveal_outcomes(Y ~ Z)) + 
  declare_estimator(Y ~ Z, model = difference_in_means, inquiry = "ATE", label = "dim") + 
  declare_test(handler = label_estimator(decision_function), label = "decision", 
               inquiry = "alternative_better_than_sq")
```

<!-- switch to this later, there is a bug in lh_robust that needs to be fixed -->
<!-- # declare_estimator(Y ~ Z, model = lh_robust, linear_hypothesis = "Z - 0.05 = 0", se_type = "HC2", label = "decision") -->

In addition to power, we set up a diagnosand for the proportion of times the policymaker will make the right decision given the evidence you provide. We redesign to consider alternative sample sizes and we diagnose under different possible true effect sizes, some negative (in which case the policymaker should retain the status quo); no difference (status quo should be retained because the effect is not a big enough improvement to justify switching costs); and positive with different sizes. 

```{r, echo = FALSE, eval = do_diagnosis}
diagnosands <- declare_diagnosands(
  power = mean(p.value <= 0.05),
  proportion_choose_control = mean(estimate < 0 | p.value > 0.05),
  proportion_choose_treatment = mean(estimate > 0 & p.value <= 0.05)
)

designs <-
  redesign(
    design,
    N = seq(from = 100, to = 3000, by = 500),
    effect_size = seq(from = 0, to = 0.25, by = 0.05)
  )

diagnosis <- diagnose_design(designs, diagnosands = diagnosands, sims = 250)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("20_Decisions"), "/diagnosis.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnosis, file = rds_file_path)
}
diagnosis <- read_rds(rds_file_path)
```

In Figure \@ref(fig:decisionplot), we show the probability of retaining the status quo policy (left facet) and the probability of switching to the treatment (right) by different true effect sizes. On the left, we see that there is a very high probability of selecting the right policy when the true effect size is very low. This pattern occurs because when the effect size is low, we are likely to fail to reject the null. With small sample sizes, we are likely to also select the status quo even when we should not, because of the imprecision of the estimates. Looking at the right graph, even when the true effect size is large (i.e., 0.25), we need to have a large sample size, about 1500, to achieve 80% probability of correctly choosing the treatment.

The sample size we might choose based on this analysis of the policymaker's choice is different than if we only considered statistical power. This is because the decision curve only reaches 80% power at 1500, while power reaches 80% at just over 500. The reason for the divergence is to make a correct decision, we need evidence that the treatment effect is greater than 0.1, as compared to statistical power which considers whether the effect is greater than 0.0.

```{r decisionplot, echo = FALSE, fig.cap = "WIP: Research design diagnosis for study of effectiveness of a policy change compared to the status quo, where a policymaker wishes to switch to the treatment policy only if it is at least 0.05 standard deviations better than the status quo. On the left, we display the statistical power of the study to detect an effect in either direction. On the right, we display the rate of making the right decision to switch policies or not."}
# the colors here are messed up
gg1 <- diagnosis %>% 
  get_diagnosands %>% 
  filter(estimator_label == "dim") %>% 
  ggplot(aes(N, power, group = effect_size, color = as.factor(effect_size))) + 
    geom_hline(yintercept = 0.8, lty = "dashed", color = "darkgray") +
  geom_line(size = 1) + 
  coord_cartesian(ylim = c(0, 1)) + 
  scale_color_discrete("True effect size", type = c('#b2182b','#ef8a62','#f7f7f7','#d1e5f0','#67a9cf','#2166ac'),
                       labels = c("0 (should retain status quo)", "0.05", "0.1 (indifferent)", "0.15", "0.2", "0.25 (should switch to treatment)")) + 
  labs(y = "Statistical power") + 
  theme(legend.position = "bottom") + 
  guides(colour = guide_legend(nrow = 1))

gg2 <- diagnosis %>% 
  get_diagnosands %>% 
  filter(estimator_label == "decision") %>% 
  ggplot(aes(N, proportion_choose_control, group = effect_size, color = as.factor(effect_size))) + 
  geom_hline(yintercept = 0.8, lty = "dashed", color = "darkgray") +
  geom_line(size = 1) + 
  coord_cartesian(ylim = c(0, 1)) + 
  scale_color_discrete("True effect size", type = rev(c('#b2182b','#ef8a62','#f7f7f7','#d1e5f0','#67a9cf','#2166ac'))) + 
  labs(y = "Probability of Choosing the Status Quo Policy") + 
  theme(legend.position = "bottom") + 
  guides(colour = guide_legend(nrow = 1))

gg3 <- diagnosis %>% 
  get_diagnosands %>% 
  filter(estimator_label == "decision") %>% 
  ggplot(aes(N, proportion_choose_treatment, group = effect_size, color = as.factor(effect_size))) + 
  geom_hline(yintercept = 0.8, lty = "dashed", color = "darkgray") +
  geom_line(size = 1) + 
  coord_cartesian(ylim = c(0, 1)) + 
  scale_color_discrete("True effect size", type = c('#b2182b','#ef8a62','#f7f7f7','#d1e5f0','#67a9cf','#2166ac'),
                       labels = c("0 (should retain status quo)", "0.05", "0.1 (indifferent)", "0.15", "0.2", "0.25 (should switch to treatment)")) + 
  labs(y = "Probability of Choosing the Alternative Policy") + 
  theme(legend.position = "bottom") + 
  guides(colour = guide_legend(nrow = 1))

gg1 + gg3 + plot_layout(guides = "collect")  & theme(legend.position = "bottom")
```

<!-- - different set of inquiries -- we should directly consider these in designs -->
<!-- - different set of diagnosands -->
<!-- - requires conversation with decisionmakers *ex ante* to understand what decisions they make on the basis of this evidence *and* how they will interpret evidence (diagnostic statistic) -->


