---
title: "Publication"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# After

## Publication

<!-- make sure to rename the section title below -->

```{r publication, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

<!-- Publication in peer-reviewed journal is a major goal of many (but not all) research projects. The peer review process is demanding, slow, overly-critical, and, in the minds of many academics, broken. The reasons reviewers and editors provide when rejecting your paper never seem fair and moreover, never seem like the *real* reasons. It's a frustrating process. -->

<!-- The advice we gave in the previous section on writing papers was to build the case for your findings by grounding your conclusions in the specifics of the research design. By detailing $M$, $I$, $D$, and $A$ in ways that leave little room for confusion or ambiguity, you greatly improve the chances that reviewers and, later, readers will understand your paper. -->

<!-- Eventually, you *will* receive that coveted invitation revise and resubmit your paper, but the reviwers will have made suggestions that are mostly not optional. You will receive comments about all four aspects of the research design. Comments about framing, additional literature to cite, and theoretical distinctions are all about the model. Comments about the research question itself (is the answer already known, is it the right question to be asking) are about the Inquiry. Criticism of data strategy will include comments about the sample quality, the measurement properties of the survey instrument, or requests for replication studies. Many reviewer comments are about specifics of the answer strategy -- you ran OLS, but they want logit; you ran logit so they want to see OLS just to be sure.  -->

<!-- The goal is to respond to reviewer comments in a way that does not compromise the essential strength of your design: do not let the review process make your paper worse. -->

Declaring the MIDA of the research design can help editors and reviewers assess the quality of a research design. When reviewers request changes to your study, declaration of the proposed changes and diagnosis can be used to compare the two possible designs yours and theirs.

### Reviewing papers

Reviewers and editors must decide whether to devote scarce space and editing bandwidth to publishing a paper. Criteria may include the topic fit with the journal, the importance of the question, and how much we learned from the research. The problem of the publication filter --- publishing only studies with statistically-significant or splashy results --- has long been recognized as a cause both of "false" findings making their way into the literature and bias in the literature simply from the missing null findings. One remedy for this problem is results-blind review of studies: selecting studies without knowing the nature of the results. 

Results-blind review can take place in multiple ways. Journals have instituted alternative submission paths for "registered reports," in which authors lay out how they will analyze their data and often include mock tables and figures based on simulated data. These reports can be constructed before or after data is collected. They are closely related to preanalysis plans, in fact often PAPs consist of a registered report.

But results-blind review does not require institutional setup: reviewers can review papers without regard for their results themselves. Reviewers can print out papers and hide results or simply aim to write comments only on the basis of the importance of the question and the quality of the research design.

In both cases, what is needed from authors is a clear statement about the research design and how they will report on and interpret results. Results-blind review need not ignore features of the research design such as how data are displayed and whether authors are overclaiming based on their results. Authors can write plans for how they will visualize and what claims they will make based on different patterns of findings. Journals who institute results-blind review can ensure reviewers are able to judge the quality of the research design by requesting details about $M$, $I$, $D$, and $A$ from authors.

### Responding to reviewers

The research design of a study is not set in stone until the final version is posted on a journal Web site or published in print. Until then, journal editors and reviewers may ask for changes to answer strategies or even data strategies or inquiries that would, in their view, improve the paper. 

The reviewer may make three kinds of requests: an alternative analysis strategy to replace or augment the original; additional data collection; the addition of a new inquiry, based on the same realized data or new data collection; a change to the model in the form of adding or dropping a node or an edge, and with it related changes to other parts of the design; or a change to the inquiry that the reviewer feels can be better answered with the data and answer strategy the author selected.

You can understand how making the changes requested by reviewers alters the design. If the change improves the design, then adopting the suggestions is easy. Some changes are irrelevant to the design -- like reporting the reviewer's preferred descriptive statistics -- in which case, following the advice is also easy. The trouble comes when reviewers propose changes that actively undermine the design? If so, diagnosing the reviewer's alternative design can be an effective way to demonstrate that the proposed changes would harm the research design. 

<!-- For example, reviewers sometimes ask authors to explore treatment effect heterogeneity by a number of additional covariates (as in the Bonilla) example. -->

<!-- Three big ideas: -->
<!-- - understand whether a reviewer suggestion improves or diminishes the quality of your design through diagnosis -->
<!-- - use diagnosis and your original declared design (ideally preregistered, but need not be) to defend against reviewer criticisms that *diminish* the quality -->



<!-- - Notes: could also do mediation analysis? What's a good example here? -->


<!-- ### Publication bias -->

<!-- change this to a section on how to review a paper (results blind!) and idea of registered reports -->


<!--  Here we look at risks from publication bias and illustrate two distinct types of upwards bias that arise from  a "[significance filter](https://andrewgelman.com/2011/09/10/the-statistical-significance-filter/)." A journal for publishing null results might help, but the results in there are *also* likely to be biased, *downwards*. -->

<!-- Two distinct problems arise if only significant results are published: -->

<!-- * The results of published studies will be *biased* towards larger magnitudes.  -->
<!-- * The published studies will be *unrepresentative* of the distribution of true effects in the relevant population of studies.  -->

<!-- These two problems are quite distinct. The first problem is more familiar: conditional on any true effect size, larger estimates have an easier time passing the statistical significance filter, so the distribution of published results will be biased upwards because it will be missing all of the smaller estimates.  The second problem is more subtle. If different studies seek to measure effects that are of different size, conditioning on statistical significance means that we are more likely to learn from places that have large effects than from places that have small effects. The significance filter means that our answers to any particular question will be biased *and* it means that the set of questions we see answers to will be biased as well. The *Journal of Significant Results* is a poor guide to the true distribution of causal effects. -->

<!-- What about a *Journal of Null Results*? Such a journal would condition acceptance on *failing* to achieve statistical significance. The set of articles published in such a journal would *also* be biased. -->

<!-- Looking first at the *Journal of Significant Results*, we see the familiar problem: the average estimate is biased away from the true value of the estimand. This problem is greatly helped by increasing the sample size. But we can also see the second problem -- the distribution of estimands (the true effects under study) is also biased towards larger effects, this problem is also allayed, though less dramatically, by larger sample sizes. -->

<!-- The *Journal of Null Results* suffers from a parallel problem, only in reverse. Now estimands are smaller than is typical in the population and, on average, estimates are biased down relative to these estimands. Strikingly, the bias in estimand selection is *worse* at the larger sample size (though downwards bias within the set of published studies is smaller). -->

<!-- Now, we agree that proactively publishing null results may help when considering entire research literatures as a whole, and for this reason alone a *Journal of Null Results* is probably a good thing.  -->

<!-- But, better would be to not do any conditioning at all. The *Journal of Interesting Designs* would condition only on the question being interesting and the design being appropriate to answering the question. We see that the distribution of estimates and estimands are both centered on the correct average value.  -->

<!-- Idea of results-blind review  -->

<!-- Idea of registered reports -->




