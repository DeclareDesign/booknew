---
title: "Pivoting"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

## Pivoting

<!-- make sure to rename the section title below -->

```{r pivoting, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

When something goes wrong or you learn things work differently from how you expect, you need to pivot. You face two decisions: go/no-go, and if go, should you alter your research design to account for the new reality? Redesigning the study and diagnosing the possible new designs can help you make these decisions. Your design declaration is a living document that you can keep updated and use as a tool to guide you along the research path, not just as a document to write at the beginning of the study and revisit when you are writing up. Keeping the declaration updated as you make the changes along the way will make it easier to reconcile the planned design with the implemented design.

We illustrate two real choices we made, one in which we abandoned the study and one in which we changed the design radically. We link to design declarations and diagnoses of the studies pre- and post-pivot.

One of us (Coppock) was involved with a get-out-the-vote canvassing-experiment-gone-wrong during the 2014 Senate race in New Hampshire. We randomly assigned 4,230 of 8,530 subjects to treatment. However, approximately two weeks before the election, canvassers had only attempted 746 subjects (17.6\% of the treatment group) and delivered treatment to just 152 subjects (3.6\%). In essence, the implementer was overly optimistic about the number of subjects they would be able to contact in time for the election. Upon reflection, the organization estimated that they would only be able to attempt to contact 900 more voters and believed that their efforts would be best spent on voters with above-median vote propensities.

We faced a choice: should we spend (1) the remaining organizational capacity on treating 900 of the 3,484 remaining unattempted treatment group subjects or should we (2) conduct a new random assignment among above-median propensity voters only? The inquiry for both designs is a complier average causal effect (CACE), but who is classified as a never-taker or a complier differs across the two designs. The organization successfully contacts approximately 20\% of those it attempts to contact. In the first design, those who are never even attempted are nevertakers (through no fault of their own!), and further deflate the intention-to-treat effect. We can't just drop them from design 1, because we don't know which units in the control group wouldn't have been attempted, had they been in the control group. In design 2, we conduct a brand-new assignment and the treatment group is only as large as the organization thinks it can handle. A design diagnosis (link) reveals a clear course of action. Even though it decreases the overall sample size, restricting the study to the above-median propensity voters substantially increases the precision of the design. This conclusion follows the logic of the placebo-controlled design described in section XX: our goal is to restrict the experimental sample to compliers only. 

Another of us (Blair) faced another kind of noncompliance problem in a study in Nigeria: failure to deliver the correct treatment. We launched a cluster-randomized placebo-controlled 2x2 factorial trial of a film treatment and a text message blast treatment. A few days after treatment delivery began, we noticed that the number of replies was extremely similar in treatment and placebo communities, counter to our expectation. We discovered that our research partner, the cell phone company, delivered the treatment message to all communities, so placebo communities received the wrong treatment. But by that time, treatments had been delivered to 106 communities (about half the sample). 

We faced a choice to abandon the study or pivot and adapt the study. We quickly agreed that we could not continue research in the 106 communities, because they had received at least partial treatment. We were left with 109 from our original sample of 200 plus 15 alternates that were selected in the same random sampling process. We determined we could not retain all four treatment conditions and the pure control. We decided that at most we could have two conditions, with about 50 units in each. But which ones? We were reticent to lose the text message or the film treatments, as both tested two distinct theoretical mechanisms for how to encourage prosocial behaviors. We decided to drop the pure control group, the fifth condition, as well as the placebo text message condition. In this way, we could learn about the effect of the film (compared to placebo) and about the effect of the text messages (compared to none).^[We randomized half of the communities to receive the treatment film and half the placebo. We then used an over-time stepped-wedge design to study the effect of the text message, randomizing how many days after the film was distributed the text message was sent.]


**Related readings**.

- Failure (@karlan2018failing)




<!-- outline: -->
<!-- -- use your MIDA to figure out how to implement -->
<!-- -- redesign as you go when you haven't gotten specific enough -->
<!-- -- consider the whole design -- ex ante declared, then changed -- and what *could have happened*, which may be known to you ex ante (there could or could not be attrition) or unknown (you learn during implementation that there are some kinds of units that won't comply, so you need to think not just about which ones did or did not comply but which ones *could* have). the whole process is a function of your interventions in the world (treatment or measurement), so write down the whole process and potential outcomes to understand what you can and cannot learn. -->
<!-- -- use your MIDA to help you *make* logistical choices, help it prevent you from making bad decisions and use as tool to communicate with partners and implementers about why you do or do not wnat to make different changes (or BETTER evaluate tradeoffs in those decisions) -->

<!-- idea bin: -->
<!-- -- MIDA is a roadmap for how to implement the study -->
<!-- -- when there is a part not specified, redesign to specify and diagnose again -->

<!-- -- lots of choices you make after you start, because it was not clear what decisions would have to be made ex ante: how to make these choices? (redesign and diagnose!).  -->
<!-- -- often randomization procedure will have to take into account specifities of the number of units (odd numbers), blocks with differing numbers of units, cluster of varying sizes, etc. that require revising D -->
<!-- -- unexpected constraints come in that affect D, and may require changes to D but also A. redesign. -->
<!-- -- how to make decisions about unexpected changes when things going wrong? (even if you don't have a PAP). often want to create multiple variants of the design incorporating what went wrong. common example: unexpected noncompliance or unexpectedly high rates of attrition. may want to change analysis plan, and register it, so can use redesign to develop that new plan. but also may discover diagnosands are not good enough, so may want to change data strategy midstream to mitigate these problems. -->
<!-- -- what to do when you run out of money or feasibility of sample size or other aspects of D becomes clear. redesign mountain subject to new cost constraints. -->
<!-- -- make go-no go decisions about whether to continue -->
<!-- -- how to know if your inquiry is no longer answerable -->
<!-- -- updating your PAP -->

<!-- -- assessing what you can learn based on the implementation challenges: these are potential outcomes, i.e. could be affected by treatment, so are often informative about what we can learn about the original research question -->

<!-- -- often need to convince partners to not change plans -- MIDA can be a tool for helping assess tradeoffs in learning and doing -->

<!-- -- Projects that succeed have direct researcher invovlement. Outsourcing too much of the design can lead to big troubles. -->






<!-- We tested distributing the treatment film that encouraged reporting corruption in half of communities and a placebo version with the same content but for the reporting corruption. We crossed this with sending a treatment message to half of communities and a placebo message to the other half. In addition, we had a 40-community pure control group with no treatments to assess whether there was an effect of the placebo condition.  -->

<!-- Though our hypotheses might have been wrong, this led us to investigate whether the right treatment was being delivered. It was not.   -->

<!-- We could not switch regions in Nigeria, because we produced a feature-length film in the specific region where the study took place and the text message treatment was tied to the film. Moreover, we had already developed and piloted a measurement strategy in the regional dialect of Pidgin English. The first choice, then, was whether to redraw a sample of communities from the region or to work within the subset of our sample where the treatments had not yet been rolled out. If we worked in that subset, we could no longer use our sampling design to draw conclusions about the population average effect for communities in the region. We did not randomize the order in which communities received the treatment for cost reasons: rural sample communities were far-flung and hard to reach across rivers and off-road travel. We attempted to construct model-based weights based on the process our data collection team took in ordering the communities, but we were not confident they exactly represented the process. We then could redraw a sample, which would include some communities that were treated and rely on the experimental property of balance in expectation or block on past randomization in our randomization scheme to avoid imbalances in past treatment status. However, we worried that there could be an interaction effect of the treatment with past treatment status that could introduce bias into our estimates. Given the sampling procedure we chose -- a restricted random sample that identified communities separated in distance to avoid geographic-based spillovers -- a resample would produce a high proportion of previously treated communities (see map). As a result, we chose to focus on designs within the 109 communities that had not yet been treated. -->

<!-- But what randomization procedure and which treatments within those 109?  -->

<!-- Our final decision was whether to continue. We decided that we had sufficient statistical power for the effect of the film treatment and for the effect of the text message treatment with the new design. And we decided that although we had hoped to produce estimates that directly estimated the population average effect in communities in the region, that we did not expect large heterogeneity in effects across different types of communities so the bias from focusing on the 109 communities that differ from a random sample was not a first order concern.  -->


<!-- Nollywood -->
<!-- - 215 cellphone towers randomly sampled from the universe in four states in Southeastern Nigeria; restricted sampling procedure to prevent samples with proximate towers to reduce the risk of geographic spillovers. randomization design was five conditions: two-by-two factorial treat vs placebo film X treat vs placebo SMS plus pure control with measurement cluster randomized at the tower level.  -->
<!-- - learned as data collection began that the SMS treatment had been misallocated, and the treatment SMS was sent to all conditions accidentally by our partner. over one hundred of the sampled towers were treated. 109 towers were untreated.  -->
<!-- - three dimensions of choice: continue or abandon; resample and randomize; retain or reduce number of treatments -->
<!-- - constraints: treatment was out there in many places and had been text message blasted to every subscriber of the major phone company in the area; the film was produced for this region and filmed in the region, so we could not switch areas with the film treatment. (the whole measurement strategy was region-specific too, in its choice of language and pretesting.) -->
<!-- - ultimate choice: step-wedge design for the SMS crossed with two arm trial of the film.  -->

<!-- # ```{r, eval = FALSE} -->
<!-- # design <- -->
<!-- #   declare_model(N = 744, -->
<!-- #                      U = rnorm(N), -->
<!-- #                      potential_outcomes(Y ~ 0.1 * Z + U, conditions = list( -->
<!-- #                        Z = c( -->
<!-- #                          "backups", -->
<!-- #                          "pure_control", -->
<!-- #                          "film_placebo_text_placebo", -->
<!-- #                          "film_placebo_text_treat", -->
<!-- #                          "film_treat_text_placebo", -->
<!-- #                          "film_treat_text_treat" -->
<!-- #                        ) -->
<!-- #                      ))) + -->
<!-- #   declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) + -->
<!-- #   declare_sampling(n = 215) + -->
<!-- #   declare_assignment(Z = complete_ra( -->
<!-- #     N, -->
<!-- #     m_each = c(15, 40, 40, 40, 40, 40), -->
<!-- #     conditions = c( -->
<!-- #       "backups", -->
<!-- #       "pure_control", -->
<!-- #       "film_placebo_text_placebo", -->
<!-- #       "film_placebo_text_treat", -->
<!-- #       "film_treat_text_placebo", -->
<!-- #       "film_treat_text_treat" -->
<!-- #     ) -->
<!-- #   ), handler = fabricate) + -->
<!-- #   declare_measurement(Y = reveal_outcomes(Y ~ Z)) + -->
<!-- #   declare_estimator(Y ~ Z, model = lm_robust) -->
<!-- #  -->
<!-- # draw_data(design) -->
<!-- #    -->
<!-- # ``` -->
<!-- #  -->
<!-- # ```{r, eval = FALSE} -->
<!-- # design <- -->
<!-- #   declare_model( -->
<!-- #     towers = add_level( -->
<!-- #       N = 744, -->
<!-- #       U_tower = rnorm(N) -->
<!-- #     ), -->
<!-- #     citizens = add_level( -->
<!-- #       N = 14, -->
<!-- #       U = rnorm(N), -->
<!-- #       potential_outcomes(Y ~ 0.1 * Z + U + U_tower) -->
<!-- #     ) -->
<!-- #   ) +  -->
<!-- #   declare_sampling(clusters = towers) +  -->
<!-- #   declare_sampling(n = 109, order_by = U_tower, handler = slice_max) +  -->
<!-- #   declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +  -->
<!-- #   declare_assignment(Z_film = cluster_ra(N, clusters = towers, prob = 0.5), handler = fabricate) +  -->
<!-- #   declare_assignment(Z_sms = cluster_ra(N, clusters = towers, conditions = 7:14), handler = fabricate) +  -->
<!-- #   declare_measurement(Y = reveal_outcomes(Y ~ Z)) +  -->
<!-- #   declare_estimator(Y ~ Z, clusters = towers, model = lm_robust) -->
<!-- #  -->
<!-- # draw_data(design) -->
<!-- # ``` -->


