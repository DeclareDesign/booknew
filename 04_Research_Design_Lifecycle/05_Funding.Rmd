---
title: "Funding"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# Funding

<!-- make sure to rename the section title below -->

```{r funding, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

<!-- point 1 -->
In Section XX we discuss the diagnosands related to cost. In any design, there will be a tradeoffs between diagnosands about the value of the research (in terms of RMSE or average movemenent from priors to posteriors for example) as well as costs. Cost are potential outcomes, so may vary across possible implementations of a study (e.g., when the number of treated units vary and treatment is costly), and thus diagnosing the average or maximum cost may be as important as understanding the benefits.

<!-- post 2 -->
To relax the budget constraint, researchers apply for funding. Funding applicants wish to obtain as large a grant as possible for their design, but have difficulty credibly communicating the quality of their design given the subjectivity of the exercise. Funders, on the flip side, wish to get the most value for money in the set of proposals they decide to fund, and have difficulty assessing the quality of proposed research. MIDA and design declaration provide a tool for speaking in a common language that can be more easily verified by both sides about what design is being proposed and what its value is to knowledge under a set of assumptions that can be interrogated by funders. 

Funding applications often aim to communicate what research design is being proposed; why learning answers from the design would be useful, important, or interesting to scholars, the public, policymakers, or another audience; how the research design provides credible answers to the question; that the researcher is capable of executing the design; and that there is value-for-money in the design and the answers it provides. 

A new section of funding applications that would aid in communicating about each of these questions is declaring the MIDA of the design and presenting a diagnosis of the design. In addition to common diagnosands such as bias and efficiency, two special diagnosands may be valuable: cost and, related, value-for-money. Cost can be included for each design variant as a function of design features such as sample size, the number of treated units, and the duration of survey interviews. The cost may vary by these parameters and may vary across possible designs when, for example, the number of treated units is a random number. Simulating the design across possible realizations of the design, thus, provides a distribution of costs as a function of choices the researcher makes. Value for money is a diagnosand that is a function of cost and also the amount that is learned from the design. RMSE might be one value criterion, another would be the average difference between priors and posteriors under a Bayesian answer strategy (a direct measure of learning). 

In some cases, funders request applicants to provide multiple options and multiple price points or to make clear how a design could be altered such that it could be funded at a lower (or higher) level. Redesigning a design with differing sample sizes would communicate how the researcher conceptualizes these options, but also provide the funder with an understanding of tradeoffs between RMSE and cost in these design variants. Applicants could use redesign to justify the high cost of their request and to ask for additional funding.

Ex ante power analyses, required by an increasing number of funders, illustrate the crux of the misaligned incentives between applicants and funders. A power analysis can demonstrate almost any design is "sufficiently powered" by changing expected effect sizes and noise.
-- funders often request power analysis, but these are typically described in words and thus the assumptions behind them cannot be interrogated fully. (a) not in code, so not specific; (b) user power calculators that are wrong (cite paper); (c) do not provide the details funders need to verify whether they agree with the assumptions. 

-- for funders, providing MIDA declared in code allows them to change the parameters of the design and test how the properties of the design change with beliefs about the world in M or data strategy parameters in D such as sample size, rather than having to rely on claims by applicants

-- often funders require regular reporting on changes in plan -- MIDA and design declaration provides a way to communicate (a) what those changes are and (b) how they change the values of diagnosands. 

-- value for money as a diagnosand (cost of each design as a function of design parameters)

-- allows funders to compare on a common scale (the same set of diagnosands) funding proposals -- often trying to evaluate "quality" but hard to do that with narrative proposals

### References

- no idea, but arnold has a document on how to prepare proposals for them.


