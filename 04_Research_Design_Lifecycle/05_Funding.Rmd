---
title: "Funding"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

## Funding

<!-- make sure to rename the section title below -->

```{r funding, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

<!-- point 1 -->
In any design, there will be tradeoffs between diagnosands about the quality of the research as well as about its costs. Costs are a function both of the data strategy (some are more expensive than others!) and how the data are realized each time. Collecting original data is more expensive than analyzing existing data, but collecting new data may be more and less expensive depending on how easy it is to reach specific individuals to interview them. As a result, diagnosing research designs including cost diagnosands is important, and those diagnosands may usefully include both average cost and maximum cost. Researchers may make different decisions about cost: in some cases, the researcher will select the "best" design in terms of research design quality subject to a budget constraint, others will choose the cheapest among similar quality designs in order to save money for future research. Diagnosis can help identify each set and decide among them.

To relax the budget constraint, researchers apply for funding. Funding applicants wish to obtain as large a grant as possible for their design, but have difficulty credibly communicating the quality of their design given the subjectivity of the exercise. Funders, on the flip side, wish to get the most value for money in the set of proposals they decide to fund, and have difficulty assessing the quality of proposed research. MIDA and design declaration provide a tool for speaking in a common language that can be more easily verified by both sides about what design is being proposed and what its value is to knowledge under a set of assumptions that can be interrogated by funders. 

Funding applications often aim to communicate what research design is being proposed; why learning answers from the design would be useful, important, or interesting to scholars, the public, policymakers, or another audience; how the research design provides credible answers to the question; that the researcher is capable of executing the design; and that there is value-for-money in the design and the answers it provides. 

A new section of funding applications that would aid in communicating about each of these questions is declaring the MIDA of the design and presenting a diagnosis of the design. In addition to common diagnosands such as bias and efficiency, two special diagnosands may be valuable: cost and, related, value-for-money. Cost can be included for each design variant as a function of design features such as sample size, the number of treated units, and the duration of survey interviews. The cost may vary by these parameters and may vary across possible designs when, for example, the number of treated units is a random number. Simulating the design across possible realizations of the design, thus, provides a distribution of costs as a function of choices the researcher makes. Value for money is a diagnosand that is a function of cost and also the amount that is learned from the design. RMSE might be one value criterion, another would be the average difference between priors and posteriors under a Bayesian answer strategy (a direct measure of learning). 

In some cases, funders request applicants to provide multiple options and multiple price points or to make clear how a design could be altered such that it could be funded at a lower (or higher) level. Redesigning a design with differing sample sizes would communicate how the researcher conceptualizes these options, but also provide the funder with an understanding of tradeoffs between the amount of learning and cost in these design variants. Applicants could use redesign to justify the high cost of their request and to ask for additional funding.

Ex ante power analyses, required by an increasing number of funders, illustrate the crux of the misaligned incentives between applicants and funders. A power analysis can demonstrate that almost any design is "sufficiently powered" by changing expected effect sizes and noise. By clarifying the assumptions of the power analysis in code, researchers can more easily defend these choices. Funders can more easily interrogate these assumptions. Power analyses using standard power calculators online have difficult-to-interrogate assumptions built in and cannot accommodate the specifics of many common designs. As a result, many return incorrect estimates of power for these designs [@bccmapsr].

Funders who request design declarations can compare funding applications on common scales: root mean-squared-error, bias, and power. Of course, they also want to weigh considerations like the importance of the question and the fit with their funding program. But moving design considerations onto a common scale takes guesswork out of the process and reduces reliance on researcher claims about properties.

<!-- -- funders often request power analysis, but these are typically described in words and thus the assumptions behind them cannot be interrogated fully. (a) not in code, so not specific; (b) user power calculators that are wrong (cite paper); (c) do not provide the details funders need to verify whether they agree with the assumptions.  -->

<!-- -- for funders, providing MIDA declared in code allows them to change the parameters of the design and test how the properties of the design change with beliefs about the world in M or data strategy parameters in D such as sample size, rather than having to rely on claims by applicants -->

<!-- -- often funders require regular reporting on changes in plan -- MIDA and design declaration provides a way to communicate (a) what those changes are and (b) how they change the values of diagnosands.  -->

<!-- -- value for money as a diagnosand (cost of each design as a function of design parameters) -->

<!-- -- allows funders to compare on a common scale (the same set of diagnosands) funding proposals -- often trying to evaluate "quality" but hard to do that with narrative proposals -->

<!-- ### References -->


