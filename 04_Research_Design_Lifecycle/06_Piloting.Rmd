---
title: "Piloting"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

## Piloting

<!-- make sure to rename the section title below -->

```{r piloting, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

-- can't learn causal effect
-- what can you learn? about Y0, about M, about se
-- bring in blog post

The designs and results of past studies are important guides for selecting M, I, D, and A. Our understanding of the nodes and edges in the causal graph of M, expected effect sizes, the distribution of outcomes, feasible randomization schemes, and many other features are directly selected from past research or chosen based on a literature review of the distribution over past studies. However, researchers face a problem in being guided by past research: the research context and our inquiries often differ in at least subtle ways from any past study. Even when we are replicating a past study, we are collecting data in a different time period and if effects vary over time then aspects of M may differ from the original study. To deal with this, we often run pilot studies. These take many forms: focus groups to learn about features of M or to learn how to ask survey questions; small-scale tests of measurement tools to verify our data collection technology works; up to mini studies with the planned design but on a smaller scale. 

Pilot studies are constrained by our time and by money. If we were not constrained, we would run the full study and learn what is wrong with our design and then run a corrected design for the main study. Since we cannot due to our constraints, we run either smaller mini studies or test out only a subset of the elements of our planned design. This places us in a bind: we are running a design smaller or less complete than the study we imagine conducting, and so the properties of the pilot design will not measure up. 

MIDA provides a framework for thinking about two aspects of piloting: what can be learned from a pilot? And what are the properties of the joint research design of pilot plus main study?

Diagnosing the pilot study on its own provides stark insights, which amount to: we cannot provide answers to the inquiry in the main study, and should not try to do so. There are also aspects of the logistics of research that within time and financial constraints we simply cannot learn until we run the main study. Science is imperfect, and also iterative, but these mistakes or suboptimal design choices also often lead to discoveries.

-- how does it help to diagnose the design together? the properties of the main study *change* when we do a pilot. This is because if we run the pilot study, we are doing so to make decisions about how to run the main study, and so our *design* of the main study and thus its results may depend on the *results* (and design) of the pilot study. 

In this section, we illustrate several general principles that flow from diagnosing pilot studies. 

Purposes of pilot studies:

Existence proofs:
-- is there variation in Y
-- is there variation in X
-- what are nodes in M
-- what are feasible D's, what are feasible treatments / can you implement the treatment (existence proof)

Harder questions requiring bigger sample sizes:
-- what is the distribution of X (helps select stratification proportions etc.)
-- what is the standard deviation of Y0

#### Assessing a pilot design

declare pilot itself and diagnose just as if it were the main study

if you can't learn the answer, don't make any decisions based on it

#### Assessing a sequenced design

if you are making decisions about MIDA for main study based on pilot, diagnose the procedure of two studies, think about POs of pilot

#### Pilots and baselines

Designs can be reassessed after baselines and before treatment assignment -- so some of the questions you might do a pilot for can just be answered in a baseline




