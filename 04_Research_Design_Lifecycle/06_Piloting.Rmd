---
title: "Piloting"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

## Piloting {#p4piloting}

<!-- make sure to rename the section title below -->

```{r piloting, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

<!-- -- can't learn causal effect -->
<!-- -- what can you learn? about Y0, about M, about se -->
<!-- -- bring in blog post -->

The designs and results of past studies are important guides for selecting M, I, D, and A. Our understanding of the nodes and edges in the causal graph of M, expected effect sizes, the distribution of outcomes, feasible randomization schemes, and many other features are directly selected from past research or chosen based on a literature review of the distribution over past studies. However, researchers face a problem in being guided by past research: the research context and our inquiries often differ in at least subtle ways from any past study. Even when we are replicating a past study, we are collecting data in a different time period and if effects vary over time then aspects of M may differ from the original study. To deal with this, we often run pilot studies. These take many forms: focus groups to learn about features of M or to learn how to ask survey questions; small-scale tests of measurement tools to verify our data collection technology works; up to mini studies with the planned design but on a smaller scale. 

Pilot studies are constrained by our time and by money. If we were not constrained, we would run the full study and learn what is wrong with our design and then run a corrected design for the main study. Since we cannot due to our constraints, we run either smaller mini studies or test out only a subset of the elements of our planned design. This places us in a bind: we are running a design smaller or less complete than the study we imagine conducting, and so the properties of the pilot design will not measure up. 

MIDA provides a framework for thinking about what can be learned from a pilot through research design diagnosis. Just like for a full study, we can define inquiries about the decisions we would make and the parameter estimates we would draw on in designing the full study.

In Figure \@ref(fig:pilotingfig), we display the results of a diagnosis of a 50-unit pilot study that we are conducting to prepare for a larger main study. We consider two strategies: (1) determining the sample size from a power analysis of the main study, selecting the minimum $N$ such that the study is 80% powered to detect the pilot study's effect size); (2) setting a fixed $N$ determined by our budget constraint, in this case to 500, and using the standard deviation of units in the treated and control group from the pilot to determine the minimum detectable effect size of our 500-unit main study. 

In the left panel is the sampling distribution of effect size estimates, i.e., a histogram of the effect estimates from the pilot. In the design, the standard deviation of the outcome is set to one, so effect estimates are in standard deviation units. The true effect size is set to 0.2. We can see that the sampling distribution has a huge range, from nearly -0.5 to nearly 0.75. The first problem with the sampling distribution is that many estimates, in fact nearly a quarter of them, are negative (the wrong sign!). This might lead us not only to choose the wrong sample size but to choose one-sided tests in the wrong direction. The second is that we have a high likelihood of guessing the effect size is *much* higher than it really is. If we obtain one of the estimates over 0.75 or even over 0.5, we would choose an $N$ too small to detect the true effect size of 0.2. In short, our estimates of the effect size from our 50-person pilot study are simple too variable to be useful in designing our main study.

However, there is good news: we can learn a lot about the power of our main study from the pilot study, just not from the effect estimates. In the right panel of Figure \@ref(fig:pilotingfig), we estimate the minimum detectable effect size of a 500-unit main study, relying on the estimated standard deviation in the control group and the estimated standard deviation in the treatment group to calculate the estimated standard error of the effect estimate in the main study. We then calculate the minimum detectable effect size using the approximation from @gelman2006data: 2.8 times the estimated standard error (pg. 441). We find that our estimates of the MDE for the full study are much more precise, tightly centered around 0.25. Since we don't know if that is larger or smaller than the true effect size, we then must make an argument based on past studies' effect sizes to justify whether that minimum size is sufficiently large or whether we should increase the sample size in order to detect even smaller effects. The reason the MDE is more precisely estimated is that the standard deviation of the control group is a much less variable estimate of the true standard deviation of the control potential outcome than the effect size estimate is of the true effect size. 

```{r, echo = FALSE}
sd_estimator <- function(data){
  data.frame(sd_y_0_hat = sd(data$Y[data$Z == 0]), sd_y_1_hat = sd(data$Y[data$Z == 1]))
}

design <- 
  declare_population(N = 50, U = rnorm(N, mean = 0, sd = 1)) + 
  declare_potential_outcomes(Y ~ 0.2 * Z + U) + 
  declare_assignment() + 
  declare_estimator(Y ~ Z) + 
  declare_estimator(handler = label_estimator(sd_estimator), label = "sd_estimator")
```

```{r, echo = FALSE, eval = do_diagnosis}
simulation_df <- simulate_design(design, sims = sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("06_Piloting.Rmd"), "/simulation_df.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(simulation_df, path = rds_file_path)
}
simulation_df <- read_rds(rds_file_path)
```

```{r pilotingfig, echo = FALSE, fig.cap = "Learning from pilot studies.", fig.width = 6.5, fig.height = 3}
gg_df <- simulation_df %>% 
  mutate(
    se_hat_full_study = sqrt( sd_y_0_hat^2 / 250  + sd_y_1_hat^2 / 250),
    mde_hat_full_study = 2.8 * se_hat_full_study
  ) %>% 
  select(sim_ID, estimate, mde_hat_full_study) %>% 
  pivot_longer(cols = c(estimate, mde_hat_full_study), names_to = "statistic", values_to = "value") %>% 
  na.omit %>% 
  mutate(statistic = factor(statistic, levels = c("estimate", "mde_hat_full_study"), labels = c("Estimated effect size", "Estimated MDE of full study")))

ggplot(gg_df, aes(value)) + 
  geom_histogram() + 
  facet_wrap(~statistic) + 
  xlab("") + ylab("Number of simulations")
```

By diagnosing our pilot studies in this way, we can learn what decisions can be made with confidence from pilot data and what should be shaped instead by expectations from past studies and qualitative knowledge. Diagnosis can also help us to decide how large a pilot study we need in order to estimate quantities like the MDE of the full study with precision. 

Beyond estimating the MDE of studies, other facts that can often be usefully learned from pilot studies take the form of existence proofs. We often wish to study how variation in $D$ (a treatment) affects variation in $Y$ (an outcome), but in the absence of past data from these two variables we may not know even if there is variation in $Y$ to explain. In experimental studies, we can learn whether a treatment *can* be implemented, and in an observational study we can learn whether there is variation in the treatment variable.

Baseline measurement may often be used instead of a pilot study to learn about some empirical features. If our sample size is fixed and we are interested in learning whether some outcome measures vary across units or how they covary, we can measure them in the baseline and then make adjustments before a posttreatment survey. We will still control for our imperfect measures at baseline to improve efficiency.

<!-- Diagnosing the pilot study on its own provides stark insights, which amount to: we cannot provide answers to the inquiry in the main study, and should not try to do so. There are also aspects of the logistics of research that within time and financial constraints we simply cannot learn until we run the main study. Science is imperfect, and also iterative, but these mistakes or suboptimal design choices also often lead to discoveries. -->

<!-- -- how does it help to diagnose the design together? the properties of the main study *change* when we do a pilot. This is because if we run the pilot study, we are doing so to make decisions about how to run the main study, and so our *design* of the main study and thus its results may depend on the *results* (and design) of the pilot study.  -->

<!-- In this section, we illustrate several general principles that flow from diagnosing pilot studies.  -->

<!-- Purposes of pilot studies: -->

<!-- Existence proofs: -->
<!-- -- is there variation in Y -->
<!-- -- is there variation in X -->
<!-- -- what are nodes in M -->
<!-- -- what are feasible D's, what are feasible treatments / can you implement the treatment (existence proof) -->

<!-- Harder questions requiring bigger sample sizes: -->
<!-- -- what is the distribution of X (helps select stratification proportions etc.) -->
<!-- -- what is the standard deviation of Y0 -->

<!-- #### Assessing a pilot design -->

<!-- declare pilot itself and diagnose just as if it were the main study -->

<!-- if you can't learn the answer, don't make any decisions based on it -->

<!-- #### Assessing a sequenced design -->

<!-- if you are making decisions about MIDA for main study based on pilot, diagnose the procedure of two studies, think about POs of pilot -->

<!-- #### Pilots and baselines -->

<!-- Designs can be reassessed after baselines and before treatment assignment -- so some of the questions you might do a pilot for can just be answered in a baseline -->


<!-- #### BLOG material -->

<!-- Data collection is expensive, and we often only get one bite at the apple. In response, we often conduct an inexpensive (and small) pilot test to help better design the study. Pilot studies have many virtues, including practicing the logistics of data collection and improving measurement tools. But using pilots to get noisy estimates in order to determine sample sizes for scale up comes with risks. -->

<!-- Pilot studies are often used to get a guess of the average effect size, which is then plugged into power calculators when designing the full study. -->

<!-- The procedure is: -->

<!-- 1. Conduct a small pilot study (say, N = 50) -->
<!-- 2. Obtain an estimate of the effect size (this is noisy, but better than nothing!) -->
<!-- 3. Conduct a power analysis for a larger study (say, N = 500) on the basis of the estimated effect size in the pilot -->

<!-- We show in this post that this procedure turns out to be dangerous: at common true effect sizes found in the social sciences, you are at risk of selecting an underpowered design based on the noisy effect estimate in your pilot study. -->

<!-- A different procedure has better properties: -->

<!-- 1. Conduct a small pilot study (say, N = 50) -->
<!-- 2. Obtain an estimate of the **standard deviation of the outcome variable** (again, this is a noisy estimate but better than nothing!) -->
<!-- 3. Estimate the minimum detectable effect (MDE) for a larger study (say, N = 500), using the estimated standard deviation -->

<!-- We show what happens in each procedure, using DeclareDesign. In each case, we'll think about a decision the researcher wants to make based on the pilot: should I move forward with my planned study, or should I go back to the drawing board? We'll rely on power to make that decision in the first procedure and the MDE in the second procedure. -->


<!-- [omitting code] -->


<!-- For each true effect size, the simulations will give us a distribution of estimated effects that a researcher might use as a basis for power analysis. For example, for a true effect size of 0 the researcher might still estimate an effect of 0.10, and so conduct their power analysis assuming that the true effect is 0.10. For each true effect, we can thus construct a distribution of *power estimates* a researcher might obtain from *estimated* effects. Since we know the true power for the true underlying effect, we can compare the distribution of post-hoc power estimates to the true power one would estimate if one knew the true effect size. -->


<!-- What did we find? In the plot, we show our guesses for the power of the main study based on our pilot effect size estimates.  -->

<!-- At high true effect sizes (top row), we do pretty well. Most of our guesses are above 80\% power, leading us to the correct decision that the study is powered. Indeed we often *underestimate* our power in these cases meaning that we run larger studies than we need to. -->

<!-- However, at low true effect sizes (bottom row) we show we are equally likely to find that the design is in fact powered as underpowered. We are equally likely to guess the power of the design is 90% as 10%. There is a good chance that we will falsely infer that our design is well powered just because we happened to get a high estimate from a noisy pilot. -->

<!-- ### How about estimating the standard deviation of the outcome? -->

<!-- Now, let's look at the second approach. Here, instead of using our pilot study to estimate the effect size for a power calculation, we estimate the **standard deviation of the outcome** and use this to calculate the main study's minimum detectable effect. The decision we want to make is: is this MDE small enough to be able to rule out substantively important effects? -->

<!-- We calculate the minimum detectable effect size using the approximation from [@gelman2006data, pg. 441], 2.8 times the estimated standard error. We estimate the standard error using Equation 3.6 from @gerber2012field.  -->



<!-- In summary, pilot studies can be valuable in planning research for many reasons, but power calculations based on noisy effect size estimates can be misleading. A better approach is to use the pilot to learn about the distribution of outcome variables. The variability of the outcome variable can then be plugged into MDE formulas or even power calculations with, say, the smallest effect size of political, economic, or social importance. -->

<!-- In the same spirit, pilot studies could also be used to learn the strength of the correlation between pre-treatment covariates and the outcome variable. With this knowledge in hand, researchers can develop their expectations about how much precision there is to be gained from covariate control or blocking. -->






