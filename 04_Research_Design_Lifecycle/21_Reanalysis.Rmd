---
title: "Reanalysis"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

## Reanalysis

<!-- make sure to rename the section title below -->

```{r reanalysis, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

A reanalysis of an existing study is a follow-up study that reuses $d$ the original realized data for some new purpose. The reanalysis is a study with a research design that can be described in terms of *M*, *I*, *D*, and *A*. Reanalyses are fundamentally constrained by the data strategy of the original study. The data strategy *D* and the resulting data $d$ are set in stone -- but reanalysts can make changes to the answer strategy *A* -- sometimes also to the model *M* or inquiry *I*.

We can learn from reanalyses in several ways. First, we can fix errors in the original answer strategy. Reanalyses fixed simple mathematical errors, typos in data transcription, or failures to account for features of the data strategy when analyzing the data. These reanalyses show whether the original results do or do not depend on these corrections. Second, we can reassess the study in light of new information about the world learned after the original study was published. That is, sometimes *M* changes in ways that color our interpretation of past results. Perhaps we learned about new confounders or alternative causal channels that undermine the original answer strategy's credibility. When reanalyzed, demonstrating the results do (or do not) change when new model features are incorporated improves our understanding of the inquiry. 

Third, many reanalyses show that original findings are not "robust" to alternative answer strategies. These are better conceptualized as claims about robustness to alternative models: one model may imply one answer strategy, and a different model, with another confounder, suggests another. If both models are plausible, a good answer strategy should be robust to both and even help distinguish between them. A reanalysis could uncover robustness to these alternative models or lack thereof. 

Lastly, reanalyses may also aim to answer new questions that were not considered by the original study but for which the realized data can provide useful answers. For example, authors may analyze outcomes not originally analyzed, or data collected during an experimental design to answer a causal question might help answer a descriptive question.

Reanalyses are themselves research designs. Just like any design, whether a reanalysis is a strong research design depends on *possible* realizations of the data (as determined by the data strategy), not just the realized data. Because $d$ is fixed in a reanalysis, analysts are often instead tempted to judge the reanalysis based on whether it overturns or confirms the original study's results. A successful reanalysis in this way of thinking demonstrates, by showing that the original results are changed under an alternative answer strategy, that the results are not robust to other plausible models. This way of thinking can lead to incorrect assessments of reanalyses. We need to consider what answers would obtain under the original answer strategy *A* and the reanalysis strategy $A^{\prime}$ under many *possible* realizations of the data. A good reanalysis strategy reveals with high probability the set of models of the world under which we can make credible claims about the inquiry. Whether or not the results from the fixed $d$ that was realized change under the answer strategies *A* and $A^{\prime}$ tells us little about this probability because $d$ is only one draw. 

To diagnose a reanalysis, we need to define two answer strategies --- *A* and $A^{\prime}$--and a new diagnostic-statistic. We need to decide how we summarize the answers from the two answer strategies. If one returns TRUE and one FALSE, what do we conclude about the inquiry? The function we define to summarize the two results depends on the inquiry and the goals of the reanalysis. But our diagnosis of the reanalysis should assess the properties of this summary of the two studies under possible realizations of the data. If the goal of the reanalysis is instead to learn about a new question, then we should simply construct a new MIDA altogether, but holding constant *D* from the original study, which we cannot change because we already collected $d$ using it.

We illustrate the flaw in assessing reanalyses based on changing significance of results from realized data below. We demonstrate how to assess the properties of reanalysis plans, comparing the properties of original answer strategies to proposed reanalysis answer strategies. 

The design we consider is an observational study with a binary treatment $Z$ that may or may not be confounded by a covariate $X$. The original answer strategy `A` is regression of the outcome $Y$ on the the treatment $Z$. The reanalyst collects the covariate $X$ and proposes to control for it in a linear regression; call that strategy `A_prime`. 

```{r}
A <- declare_estimator(Y ~ Z, model = lm_robust, label = "A")
A_prime <- declare_estimator(Y ~ Z + X, model = lm_robust, label = "A_prime")
```

We set up the model taking the perspective of the reanalyst, who believes $Z$ and $Y$ are confounded by $X$:

```{r}
# X is a confounder and is measured pretreatment
model_1 <- 
  declare_model(
    N = 100,
    U = rnorm(N),
    X = rnorm(N),
    Z = rbinom(N, 1, prob = plogis(0.5 + X)),
    potential_outcomes(Y ~ 0.1 * Z + 0.25 * X + U),
    Y = reveal_outcomes(Y ~ Z)
  ) 
```

Drawing data from this strategy and applying the two answer strategies, we get differing results:

```{r, eval = FALSE}
draw_estimates(model_1 + A + A_prime)
```

```{r, echo = FALSE}
set.seed(3)
est <- draw_estimates(model_1 + A + A_prime)
est %>% select(estimator_label, estimate, std.error, p.value) %>% kable(digits = 3)
```

Commonly, reanalysts would infer from this two things: the ATE is nonsignificant, and that the answer strategy `A_prime` is preferred. It is preferred because it is unbiased under confounding and unbiased (and lower variance than `A`) when there is no confounding. As we show below, these claims depend on the validity of other parts of the model *M*. In particular, we define two other possible models. In model 2, $X$ is measured before treatment, as in the first model, but is not a confounder. In model 3, however, $X$ is affected by $Y$ and is measured *post*treatment. In many observational settings, it is now always clear when $X$ is realized or measured relative to treatment, so it is useful to explore the implications of the timing of measurement on the design's diagnosands as we do below.

```{r}
# X is not a confounder and is measured pretreatment
model_2 <- 
  declare_model(
    N = 100,
    U = rnorm(N),
    X = rnorm(N),
    Z = rbinom(N, 1, prob = plogis(0.5)),
    potential_outcomes(Y ~ 0.1 * Z + 0.25 * X + U),
    Y = reveal_outcomes(Y ~ Z)
  ) 

# X is not a confounder and is measured posttreatment
model_3 <- 
  declare_model(
    N = 100,
    U = rnorm(N),
    Z = rbinom(N, 1, prob = plogis(0.5)),
    potential_outcomes(Y ~ 0.1 * Z + U),
    Y = reveal_outcomes(Y ~ Z),
    X = 0.1 * Z + 5 * Y + rnorm(N)
  ) 

design_1 <- model_1 + I + A + A_prime
design_2 <- model_2 + I + A + A_prime
design_3 <- model_3 + I + A + A_prime
```

```{r, echo = FALSE, eval = do_diagnosis}
diagnosis <- diagnose_design(design_1, design_2, design_3, sims = sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("17_Reanalysis"), "/diagnosis.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnosis, file = rds_file_path)
}
diagnosis <- read_rds(rds_file_path)
```

What we see in the diagnosis below is that `A_prime` is only preferred if we know for sure that $X$ is measured pretreatment. In design 2, where $X$ is measured posttreatment, `A` is preferred, because controlling for $X$ leads to posttreatment bias. Including the $X$ variable in the regression misattributes some of the effect of $Z$ on $Y$ to $X$. The reanalyst, this diagnosis indicates, needs to justify their beliefs about when $X$ is measured in order to claim that `A_prime` is preferred to `A`. It is not always preferred, as they might have concluded just by looking at the realized estimates from the two answer strategies.

```{r, echo = FALSE}
get_diagnosands(diagnosis) %>% 
  select(design_label, estimator_label, bias) %>% 
  kable(digits = 3)
```

Three principles emerge from the idea that changes from answer strategy *A* to $A^{\prime}$ should be justified by diagnosis, not the comparison of the results $a^d$ to $a^{\prime d}$:

1. **Home ground dominance.** Holding the original *M* constant (i.e., the home ground of the original study) as well as *I*, if you can show that a new answer strategy $A^{\prime}$ yields better diagnosands than the original *A*, then the new $A^{\prime}$ can be justified by a reanalyst based on home ground dominance. In this situation, we would prefer $A^{\prime}$ and the estimates produced by it to *A* and the original estimates produced by it, regardless of what those realized estimates are. In the example above, model 2 is the "home ground," and $A^{\prime}$ is preferred to *A* on this home ground.

<!-- parallel idea about finding a replication with a new D that rules out confounders --> 
2. **Robustness to alternative models.** A second justification for a change in answer strategy is that you can show that a new answer strategy is robust to both the original model *M* and a new, also plausible, $M^{\prime}$. In observational studies, commonly we are uncertain about many features of the model, such as the existence of unobserved confounders. In the example above, $A^{\prime}$ is robust to models 1 and 2 but is not robust to model 3. By contrast, *A* is robust to models 2 and 3 but not to model 1. 

3. **Model plausibility.** If the diagnosands for a design with $A^{\prime}$ are worse than those with *A* under *M* but better under $M^{\prime}$, then the switch to $A^{\prime}$ can only be justified by a claim or demonstration that $M^{\prime}$ is more plausible than *M*. As we saw in the example, there was a third possible model in which $X$ was realized posttreatment and so controlling for $X$ led to posttreatment bias. In that case, neither controlling for $X$ or not controlling for $X$ is robust to all three alternative models. The justification of model plausibility would have to be used in this case to justify controlling for $X$: substantive knowledge or additional data would have to be brought to bear to rule out the third model with posttreatment measurement of $X$. The reanalyst could demonstrate that data collection of $X$ took place before the treatment was realized, for example. 


<!-- 1. M always changes! (you have more information on $\tau$ or $\sd(\tau)$) -->
<!-- 2. Home ground dominance: Change A or D-and-A if A$^\prime$ > A under M -->
<!-- 3. Robustness to alternative models: Change A or D-and-A if A$^\prime$ $\geq$ A under M AND A$^\prime$ > A under M$^\prime$ E.g. change from simple to complete RA -->
<!-- 4. Model plausibility: If A$^\prime$ < A under M AND A$^\prime$ > A under M$^\prime$, then change to A$^\prime$ or D-and-A IFF M$^\prime$ is more plausible than M E.g. switching to balanced design if you believe variances equal across treatment groups -->
<!-- 5. Undefined inquiries. Change I to I$^\prime$ if I is undefined under M If I is defined under M: You can't change to I$^\prime$, You canâ€™t change D to D$^\prime$ if that means I unidentifiable. -->


<!-- how do we update from the reanalysis research design (the original design plus the reanalysis of d)? -->
<!-- -- the design is the research design from before with two sets of estimates from two different A's -->
<!-- -- need an aggregation function (decisionmaking function) that converts the two sets of results into a decision or posterior -->

<!-- what can be learned from reanalysis? -->
<!-- (1) confirm there were not errors (consider changing A only) -->
<!-- (2) reassess what is known about the same inquiry, using new information about the world (change M, change A to suit new M) -->
<!-- (3) learn something new from the data about another node or edge or a different summary about the same ones (change I and possibly A to match it; possibly M if a node was missing; possibly add data) -->
<!-- (4) assess "robustness" of findings - point to discussion of this in answer strategy (or move it here) (change A) -->
<!-- (5) update M based on new research and assess what d can tell us from this study (change M and possibly I, possibly A to fit changed M and I) -->

<!-- how can we assess the properties of a *reanalysis*? diagnose changed MIDA. important to not condition on d, the design includes the actual D, and we need to consider what results d' we would get from the reanalysis under different realizations of D. -->

<!-- there are now two A's, so need to specify a decision function about how to integrate the two findings. this could be throw away the old a, or combine them in some way. if it's a "robustness" to alternative A, then you may want to combine not throw out for example. it's crucial to specify how you do that, that's part of the answer strategy. -->

<!-- ## Example -->

<!-- Knox, Lowe, and Mummolo (2020) (https://www.cambridge.org/core/journals/american-political-science-review/article/administrative-records-mask-racially-biased-policing/66BC0F9998543868BB20F241796B79B8) study the statistical biases that accompany estimates of racial bias in police use of force when presence in the dataset (being stopped by police) is conditioned on an outcome that is a downstream consequence of race. They show the estimate is not identified unless additional modelling assumptions are brought to bear. -->

<!-- Gaebler et al. (2020) (https://5harad.com/papers/post-treatment-bias.pdf) study the same question and make such modeling assumptions (subset ignorability, definition 2). -->

<!-- In a twitter thread (https://twitter.com/jonmummolo/status/1275790509647241222?s=20), Mummolo shows the three DAGs that are compatible with subset ignorability. We agree with Mummolo that these DAGs assume away causal paths that are very plausible. -->

<!-- ![DAG](figures/mummolo_dag.png) -->

<!-- This document provides a design declaration for this setting and shows how estimates of the controlled direct effect (effect of race on force among the stopped) are biased unless those paths are set to zero by assumption. -->

<!-- Design Declaration -->

<!-- There are four variables: (D: minority, M: stop, U: suspicion (unobserved), Y: force) and five paths: -->

<!-- ```{r} -->
<!-- D_M = 1 # effect of minority on stop -->
<!-- U_M = 1 # effect of suspicion on stop -->
<!-- D_Y = 1 # effect of minority on force -->
<!-- U_Y = 1 # effect of suspicion on force -->
<!-- M_Y = 1 # effect of stop on force -->
<!-- ``` -->

<!-- This basic design allows all five paths. -->

<!-- ```{r} -->
<!-- design_1 <- -->
<!--   declare_model(N = 1000, -->
<!--                      D = rbinom(N, size = 1, prob = 0.5), -->
<!--                      U = rnorm(N)) + -->
<!--   declare_potential_outcomes(M ~ rbinom(N, size = 1, prob = pnorm(D_M * -->
<!--                                                                     D + U_M * U)), -->
<!--                              assignment_variable = "D") + -->
<!--   declare_reveal(M, D) + -->
<!--   declare_potential_outcomes(Y ~ rnorm(N, D_Y * D + M_Y * M + U_Y * U), -->
<!--                              conditions = list(D = c(0, 1), M = c(0, 1))) + -->
<!--   declare_reveal(outcome_variables = "Y", -->
<!--                  assignment_variables = c("D", "M")) + -->
<!--   declare_inquiry(CDE = mean(Y_D_1_M_1 - Y_D_0_M_1)) + -->
<!--   declare_estimator(Y ~ D, subset = M == 1, inquiry = "CDE") -->
<!-- ``` -->

<!-- We redesign the design 3 times, removing one path at a time, then simulate all four designs. -->

<!-- ```{r, message=FALSE} -->
<!-- # no effect of D on M -->
<!-- design_2 <- redesign(design_1, D_M = 0) -->

<!-- # no effect of U on M -->
<!-- design_3 <- redesign(design_1, U_M = 0) -->

<!-- # no effect of U on Y -->
<!-- design_4 <- redesign(design_1, U_Y = 0) -->
<!-- ``` -->

<!-- This chunk is set to `echo = TRUE` and `eval = do_diagnosis` -->
<!-- ```{r, eval = do_diagnosis & !exists("do_bookdown")} -->
<!-- simulations <- simulate_designs(design_1, design_2, design_3, design_4, sims = sims) -->
<!-- ``` -->

<!-- Right after you do simulations, you want to save the simulations rds. -->

<!-- ```{r, echo = FALSE, purl = FALSE} -->
<!-- # figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file -->
<!-- rds_file_path <- paste0(get_dropbox_path("policing"), "/simulations_policing.RDS") -->
<!-- if (do_diagnosis & !exists("do_bookdown")) { -->
<!--   write_rds(simulations, path = rds_file_path) -->
<!-- } -->
<!-- simulations <- read_rds(rds_file_path) -->
<!-- ``` -->


<!-- ```{r, echo=FALSE, message = FALSE} -->

<!-- simulations <- -->
<!--   simulations %>% -->
<!--   mutate(`Assumed DAG` = factor( -->
<!--     design_label, -->
<!--     levels = c("design_1", "design_2", "design_3", "design_4"), -->
<!--     labels = c( -->
<!--       "All paths possible", -->
<!--       "no effect of D on M", -->
<!--       "no effect of U on M", -->
<!--       "no effect of U on Y" -->
<!--     ) -->
<!--   )) -->

<!-- summary_df <- -->
<!--   simulations %>% -->
<!--   group_by(`Assumed DAG`) %>% -->
<!--   summarise( -->
<!--     mean_estimand = mean(estimand), -->
<!--     mean_estimate = mean(estimate), -->
<!--     bias = mean(estimate - estimand) -->
<!--   ) %>% -->
<!--   pivot_longer(cols = c("mean_estimand", "mean_estimate")) -->
<!-- ``` -->

<!-- This plot confirms that unless one of those implausible assumptions hold, estimates of the CDE are biased. -->

<!-- ```{r, echo=FALSE} -->
<!-- ggplot(simulations, aes(estimate)) + -->
<!--   geom_histogram(bins = 50) + -->
<!--   geom_vline(data = summary_df, aes(xintercept = value, color = name)) + -->
<!--   facet_wrap(~`Assumed DAG`) + -->
<!--   xlab("Simulated CDE estimates") + -->
<!--   theme_bw() + -->
<!--   theme(legend.position = "bottom", -->
<!--         strip.background = element_blank(), -->
<!--         axis.title.y = element_blank(), -->
<!--         legend.title = element_blank()) -->
<!-- ``` -->

<!-- ### Grab bag -->

<!-- - @Clemens2017 on taxonomy of these kinds of efforts -->
<!-- - if you're going to use d to learn about a different M for a different I, you need to understand their D -->
