---
title: "Publication"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

## Publication

<!-- make sure to rename the section title below -->

```{r publication, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

Publication in a peer-reviewed journal is a major goal of many (but not all) research projects. The advice we gave in the previous section on writing papers was to build the case for your findings by grounding your conclusions in the specifics of the research design. By detailing *M*, *I*, *D*, and *A* in ways that leave little room for confusion or ambiguity, you greatly improve the chances that reviewers and, later, readers will understand your paper.

Ideally, studies would be selected for publication on the basis of the design rather than on the basis of the results. This ideal can be hard to achieve. Reviewers and editors must decide whether to devote scarce journal space and editing bandwidth to publishing a paper. Criteria may include the topic fit with the journal, the importance of the question, and how much was learned from the research. The publication filter problem -- publishing only studies with statistically-significant or splashy results -- has long been recognized as a cause both of "false" findings making their way into the literature and publication bias due to missing null findings. 

One major barrier to fixing this problem is that design quality is hard to convey to reviewers, so they substitute their judgments of the results. When the estimate turns out to be statistically significant, reviewers infer that the design must have been well-enough-powered to discover a statistically-significant result. The trouble with this approach is that a significant result might come from a study with 80% power or it might be a lucky draw from a study with just 10% power. 

Formal design declaration and diagnosis is one way to communicate study quality separately from results. The theory and design sections of a paper should describe *M*, *I*, *D*, and *A* in enough detail that reviewers can understand the empirical thrust of the study. If in addition to this information, authors provide diagnostic information about the ability of the study to generate credible inferences, we may be to induce reviewers to evaluate studies on the basis of design, not results.

<!-- ### Responding to reviewers -->

A study's research design is not set in stone until the final version is posted on a journal Web site or published in print. Until then, journal editors and reviewers may ask for design changes that would, in their view, improve the paper. If the changes improve the design, then adopting their suggestions is easy. Some changes are irrelevant to the design -- like reporting the reviewer's preferred descriptive statistics -- in which case, following the advice is also easy. The trouble comes when reviewers propose changes that actively undermine the design. When this happens, diagnosing the reviewer's alternative design can effectively demonstrate that the proposed changes would harm the research design. 

<!-- For example, reviewers sometimes ask authors to explore treatment effect heterogeneity by a number of additional covariates (as in the Bonilla) example. -->

<!-- Three big ideas: -->
<!-- - understand whether a reviewer suggestion improves or diminishes the quality of your design through diagnosis -->
<!-- - use diagnosis and your original declared design (ideally preregistered, but need not be) to defend against reviewer criticisms that *diminish* the quality -->



<!-- - Notes: could also do mediation analysis? What's a good example here? -->


<!-- ### Publication bias -->

<!-- change this to a section on how to review a paper (results blind!) and idea of registered reports -->


<!--  Here we look at risks from publication bias and illustrate two distinct types of upwards bias that arise from  a "[significance filter](https://andrewgelman.com/2011/09/10/the-statistical-significance-filter/)." A journal for publishing null results might help, but the results in there are *also* likely to be biased, *downwards*. -->

<!-- Two distinct problems arise if only significant results are published: -->

<!-- * The results of published studies will be *biased* towards larger magnitudes.  -->
<!-- * The published studies will be *unrepresentative* of the distribution of true effects in the relevant population of studies.  -->

<!-- These two problems are quite distinct. The first problem is more familiar: conditional on any true effect size, larger estimates have an easier time passing the statistical significance filter, so the distribution of published results will be biased upwards because it will be missing all of the smaller estimates.  The second problem is more subtle. If different studies seek to measure effects that are of different size, conditioning on statistical significance means that we are more likely to learn from places that have large effects than from places that have small effects. The significance filter means that our answers to any particular question will be biased *and* it means that the set of questions we see answers to will be biased as well. The *Journal of Significant Results* is a poor guide to the true distribution of causal effects. -->

<!-- What about a *Journal of Null Results*? Such a journal would condition acceptance on *failing* to achieve statistical significance. The set of articles published in such a journal would *also* be biased. -->

<!-- Looking first at the *Journal of Significant Results*, we see the familiar problem: the average estimate is biased away from the true value of the estimand. This problem is greatly helped by increasing the sample size. But we can also see the second problem -- the distribution of estimands (the true effects under study) is also biased towards larger effects, this problem is also allayed, though less dramatically, by larger sample sizes. -->

<!-- The *Journal of Null Results* suffers from a parallel problem, only in reverse. Now estimands are smaller than is typical in the population and, on average, estimates are biased down relative to these estimands. Strikingly, the bias in estimand selection is *worse* at the larger sample size (though downwards bias within the set of published studies is smaller). -->

<!-- Now, we agree that proactively publishing null results may help when considering entire research literatures as a whole, and for this reason alone a *Journal of Null Results* is probably a good thing.  -->

<!-- But, better would be to not do any conditioning at all. The *Journal of Interesting Designs* would condition only on the question being interesting and the design being appropriate to answering the question. We see that the distribution of estimates and estimands are both centered on the correct average value.  -->

<!-- Idea of results-blind review  -->

<!-- Idea of registered reports -->




<!-- Eventually, you *will* receive that coveted invitation revise and resubmit your paper, but the reviwers will have made suggestions that are mostly not optional. You will receive comments about all four aspects of the research design. Comments about framing, additional literature to cite, and theoretical distinctions are all about the model. Comments about the research question itself (is the answer already known, is it the right question to be asking) are about the Inquiry. Criticism of data strategy will include comments about the sample quality, the measurement properties of the survey instrument, or requests for replication studies. Many reviewer comments are about specifics of the answer strategy -- you ran OLS, but they want logit; you ran logit so they want to see OLS just to be sure.  -->

<!-- The goal is to respond to reviewer comments in a way that does not compromise the essential strength of your design: do not let the review process make your paper worse. -->

<!-- Declaring the MIDA of the research design can help editors and reviewers assess the quality of a research design. When reviewers request changes to your study, declaration of the proposed changes and diagnosis can be used to compare the two possible designs: yours and theirs. -->