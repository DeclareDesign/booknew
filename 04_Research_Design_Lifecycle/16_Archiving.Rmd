---
title: "Archiving"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# Integration

Researchers may build on the results of a past study in three ways: reanalyze the original data, holding $d$ constant but changing $A$ (reanalysis); providing a new answer to the same $I$ with new data, possibly changing $D$ and $A$ (replication); or synthesizing the study's answer $a^d$ to the inquiry along with other past studies (meta-analysis). In this section, we outline archiving procedures to enable later researchers to reanalyze, replicate, or synthesize; and we describe procedures for doing each of these three integration tasks.

## Archiving

<!-- make sure to rename the section title below -->

```{r archiving, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

One of the biggest successes in the push for greater research transparency has been changing norms surrounding the sharing of data and analysis code after studies have been published. It has been become de rigeur at many journals to post these materials at publicly-available repositories like the OSF or Dataverse. This development is undoubtedly a good thing. In older manuscripts, sometimes data or analyses are described as being "available upon request" but of course such requests are sometimes ignored. Furthermore, a century from now, study authors will no longer be with us even if they wanted to respond to such requests. Public repositories have a much better chance of preserving study information for the future.

<!-- That's the promise of publicly-posted replication archives, but the mundane reality of replication archives often falls short. We see many archives that are disorganized, poorly documented, and contain dozens of bugs and inconsistencies.  -->

What belongs in a replication archive? Enough detail so that those who wish to reanalyze, replicate, and synthesize results can do so without contacting the authors.

**Data.** First, the data $d$ itself. Sometimes this is the raw data, sometimes it is only the "cleaned" data that is actually called by analysis scripts. Where ethically possible, we think it is preferable to post as much of the raw data as possible, for example after removing information like IP address or geographic location that could be used to identify a subject. We usually consider data processing scripts that clean and prepare data for analysis as part of the data strategy $D$ in the sense that they complete the measurement procedures laid out in $D$. Cleaning scripts might also be considered part of the answer strategy in the sense that they apply an interpretation to the data provided by the world. The output of cleaning scripts -- the cleaned data -- should be included in the replication archive as well.

**Analysis code.** Replication archives also include $A$, or the set of functions applied to $d$ that produce $a^D$. It is vitally important that the *actual* analysis code is archived because the natural-language descriptions of $A$ that are typically given in papers are imprecise. As a small example, many articles describe their answer strategies as "ordinary least squares" but do not fully describe the set of covariates used or what flavor of standard errors was estimated. These differences can substantively affect the quality of the research design. The actual analysis code makes $A$ explicit.

**Data strategy materials.** Increasingly, replication archives include the materials needed to implement treatments and measurement strategies. Without the survey questionnaires in the languages and formats they were enumerated, we cannot exactly replicate them in future studies -- and we cannot build on and adapt them. The treatment stimuli used in the study should be included also. A central problem in replicating studies in the Many Labs projects was that the exact stimuli were either not retained or no longer availble for past psychology studies. This loss led to confusion over whether failures to replicate were due to changes in the stimuli, different populations, or the underpowered original study designs.

**Design declaration.** While typical replication archives include $d$ and $A$, we think that future replication archives should also include a design declaration that fully describes $M$, $I$, $D$, and $A$ -- that is, we should archive designs, not just data and analysis code. This should be done in code and words. In addition, a diagnosis should be included, demonstrating the properties as understood by the author and also indicating the diagnosands that the author considered in judging the quality of the design.

Design details help future scholars not only assess, but replicate, reanalyze, and extend the study. Reanalysts need to understand the answer strategy so they can modify or extend it but also the data strategy that was used in order to ensure that their new analysis respects the details of the sampling, treatment assignment, and measurement procedures. Data and analysis sharing enables reanalysts to adopt or adapt the analysis strategy, but a declaration of the data strategy as well would help more. Replicators who wish to exactly replicate, or even just to provide an answer to the same inquiry, need to understand the inquiry, data strategy, and answer strategy. Replication practice today involves inferring most of these details from descriptions in text. The result is disputes that result after the replication is sent out for peer review, when the original authors may not agree with inferences the replicators made about what the inquiry or data strategy or answer strategy actually were. To protect the original authors as well as the replicators, including a research design declaration specifying each of these elements resolves these issues so that replication and extension can focus on the substance of the research question and innovation in research design. 

<!-- Figure \@ref(fig:filestructure)  -->
The Figure below shows the file structure for an example replication. Our view on replication archives shares much in common with the [TIER protocol](https://www.projecttier.org). It includes raw data in a platform-independent format (.csv) and cleaned data in a language-specific format (.rds, a format for R data files), so that data features like labels, attributes, and factor levels are preserved when imported by the analysis scripts. The analysis scripts are labeled by the outputs they create, such as figures and tables. A master script is included that runs the cleaning and analysis scripts in the correct order. The documents folder includes the paper, the supplemental appendix, the pre-analysis plan, the populated analysis plan, and codebooks that describe the data. A README file explains each part of the replication archive. We also suggest that authors include a script that includes a design declaration and diagnosis. 

![File structure for archiving\label{filestructure}](figures/file_structure.png)


<!-- Example is archive at OSF: https://osf.io/4vuqh -->
