---
title: "Pivoting"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

## Pivoting

<!-- make sure to rename the section title below -->

```{r pivoting, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

When something goes wrong or you learn things work differently than you expect, you need to pivot. You face two decisions: go/no-go, and if go, should you alter your research design to account for the new reality? 

<!-- -- Alex's example -->
<!-- -- Nollywood -->

We faced another kind of noncompliance problem in a study in Nigeria: failure to deliver the correct treatment. We launched a cluster-randomized placebo-controlled factorial trial of a film treatment and an text message blast treatment. After a baseline study in 200 rural and urban communities randomly sampled from a list of all communities covered by our cell phone company partner for the text message treatment, we launched a 2x2 factorial cluster-randomized trial. We tested distributing the treatment film that encouraged reporting corruption in half of communities and a placebo version with the same content but for the reporting corruption. We crossed this with sending a treatment message to half of communities and a placebo message to the other half. In addition, we had a 40-community pure control group with no treatments to assess whether there was an effect of the placebo condition. A few days after the treatment began to be delivered, it became clear that the number of replies were extremely similar in treatment and placebo communities, counter to our expectation. Though our hypotheses might have been wrong, this led us to investigate whether the right treatment was being delivered. It was not. Our research partner, the cell phone company, had delivered the treatment message to all communities, so communities assigned to placebo received the wrong treatment. By the time the partner admitted the mistake, some activities --- distributing the film or sending the SMS blast --- had already taken place in 106 communities or about half the sample. We faced the two pivot decisions: abandon the study, or continue; and if we continued, what research design to adopt. We quickly agreed that we could not continue research in the 106 communities, because they had received at least partial treatment. We were left with 109 from our original sample of 200 plus 15 alternates that were selected in the same random sampling process in case there were problems in the 200 that necessitated replacing a community. 

We could not switch regions in Nigeria, because we produced a feature-length film in the specific region where the study took place and the text message treatment was tied to the film. Moreover, we had already developed and piloted a measurement strategy in the regional dialect of Pidgin English. The first choice, then, was whether to redraw a sample of communities from the region or to work within the subset of our sample where the treatments had not yet been rolled out. If we worked in that subset, we could no longer use our sampling design to draw conclusions about the population average effect for communities in the region. We did not randomize the order in which communities received the treatment for cost reasons: rural sample communities were far-flung and hard to reach across rivers and off-road travel. We attempted to construct model-based weights based on the process our data collection team took in ordering the communities, but we were not confident they exactly represented the process. We then could redraw a sample, which would include some communities that were treated and rely on the experimental property of balance in expectation or block on past randomization in our randomization scheme to avoid imbalances in past treatment status. However, we worried that there could be an interaction effect of the treatment with past treatment status that could introduce bias into our estimates. Given the sampling procedure we chose -- a restricted random sample that identified communities separated in distance to avoid geographic-based spillovers -- a resample would produce a high proportion of previously treated communities (see map). As a result, we chose to focus on designs within the 109 communities that had not yet been treated.

TODO: add map here

But what randomization procedure and which treatments within those 109? Using an online power calculator, we realized that we could not retain all four factorial conditions and the pure control. We decided that at most we could have two conditions, with about 50 units in each. But which ones? We were reticent to lose the text message or the film treatments, as both tested two different theoretical mechanisms for how to encourage prosocial behaviors. We settled on a step-wedge design crossed with the film treatment to solve this problem. We randomized half of communities to receive the treatment version of the film and half to the placebo version. Then, the stepped-wedge component was that we randomized each community, treatment and placebo, to receive the text blast between 7 and 14 days after the film was distributed. In this way, we could learn from between-community evidence about the effect of the film and within-community over time evidence about the effect of the text messages. 

Our final decision was whether to continue. We decided that we had sufficient statistical power for the effect of the film treatment and for the effect of the text message treatment with the new design. And we decided that although we had hoped to produce estimates that directly estimated the population average effect in communities in the region, that we did not expect large heterogeneity in effects across different types of communities so the bias from focusing on the 109 communities that differ from a random sample was not a first order concern. 


<!-- Nollywood -->
<!-- - 215 cellphone towers randomly sampled from the universe in four states in Southeastern Nigeria; restricted sampling procedure to prevent samples with proximate towers to reduce the risk of geographic spillovers. randomization design was five conditions: two-by-two factorial treat vs placebo film X treat vs placebo SMS plus pure control with measurement cluster randomized at the tower level.  -->
<!-- - learned as data collection began that the SMS treatment had been misallocated, and the treatment SMS was sent to all conditions accidentally by our partner. over one hundred of the sampled towers were treated. 109 towers were untreated.  -->
<!-- - three dimensions of choice: continue or abandon; resample and randomize; retain or reduce number of treatments -->
<!-- - constraints: treatment was out there in many places and had been text message blasted to every subscriber of the major phone company in the area; the film was produced for this region and filmed in the region, so we could not switch areas with the film treatment. (the whole measurement strategy was region-specific too, in its choice of language and pretesting.) -->
<!-- - ultimate choice: step-wedge design for the SMS crossed with two arm trial of the film.  -->

```{r, eval = FALSE}
design <-
  declare_model(N = 744,
                     U = rnorm(N),
                     potential_outcomes(Y ~ 0.1 * Z + U, conditions = list(
                       Z = c(
                         "backups",
                         "pure_control",
                         "film_placebo_text_placebo",
                         "film_placebo_text_treat",
                         "film_treat_text_placebo",
                         "film_treat_text_treat"
                       )
                     ))) +
  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +
  declare_sampling(n = 215) +
  declare_assignment(Z = complete_ra(
    N,
    m_each = c(15, 40, 40, 40, 40, 40),
    conditions = c(
      "backups",
      "pure_control",
      "film_placebo_text_placebo",
      "film_placebo_text_treat",
      "film_treat_text_placebo",
      "film_treat_text_treat"
    )
  ), handler = fabricate) +
  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +
  declare_estimator(Y ~ Z, model = lm_robust)

draw_data(design)
  
```

```{r, eval = FALSE}
design <-
  declare_model(
    towers = add_level(
      N = 744,
      U_tower = rnorm(N)
    ),
    citizens = add_level(
      N = 14,
      U = rnorm(N),
      potential_outcomes(Y ~ 0.1 * Z + U + U_tower)
    )
  ) + 
  declare_sampling(clusters = towers) + 
  declare_sampling(n = 109, order_by = U_tower, handler = slice_max) + 
  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) + 
  declare_assignment(Z_film = cluster_ra(N, clusters = towers, prob = 0.5), handler = fabricate) + 
  declare_assignment(Z_sms = cluster_ra(N, clusters = towers, conditions = 7:14), handler = fabricate) + 
  declare_measurement(Y = reveal_outcomes(Y ~ Z)) + 
  declare_estimator(Y ~ Z, clusters = towers, model = lm_robust)

draw_data(design)
```


