---
title: "Reanalysis"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

## Reanalysis

<!-- make sure to rename the section title below -->

```{r reanalysis, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

A reanalysis of an existing study is a followup study in which $d$, the original realized data, is fixed and changes to $A$ and sometimes $M$ or $I$ are proposed. Given $d$ is fixed, so too is the data strategy $D$. The results given the new MIDA, which may differ from the original study's results, are reported. 

We can learn from reanalyses in at least five ways. We can confirm that there were not errors in the analysis strategy. Many reanalyses correct simple mathematical errors, typos in data transcription, or failures to analyze following the data strategy faithfully. These reanalyses show whether results do or do not depend on these corrections. 

We can reassess what is known about the same $I$, using new information about the world that was learned after the original study was published. Here, we may learn about new confounders or alternative causal channels that undermine the credibility of the original answer strategy. When reanalyzed, demonstrating the results do (or do not) change improves our understanding of $a^W$. 

Many reanalyses show that original findings are not "robust" to alternative answer strategies. These are better conceptualized as claims about robustness to alternative models: one model may imply one answer strategy and a different model, with another confounder, implies another. If both models are plausible, a good answer strategy should be robust to both and even help to distinguish between them and a reanalysis could uncover robustness to these alternative models or lack thereof. 

Reanalyses may also aim to answer new questions that were not considered by the original study, but for which the realized data can provide useful answers. For example, authors may analyze outcomes not originally analyzed.

Reanalyses are themselves research designs. Whether a reanalysis is a good design, and how much it can contribute to our knowledge about the original inquiry, depend on possible realizations of the data. Because $d$ is fixed in a reanalysis, analysts are often instead tempted to judge the reanalysis based on whether it overturns or confirms the results of the original study. A successful reanalysis in this way of thinking demonstrates, by showing that the original results are changed under an alternative $A$, that the results are not robust to other plausible models. This way of thinking can lead to incorrect assessments of reanalyses. We need to consider what answers would obtain under the original answer strategy $A$ and the reanalysis strategy $A^{\prime}$ under many *possible* realizations of the data. A good reanalysis strategy reveals with high probability the set of models of the world under which we can make credible claims about $I$. Whether or not the results from the fixed $d$ that was realized change under $A$ and $A^{\prime}$ tells us little about this probability. It is only one draw. 

To diagnose a reanalysis, we need to define two answer strategies --- $A$ and $A^{\prime}$ --- but also a new diagnostic-statistic. We need to decide how we summarize the answers from the two answer strategies. If one returns TRUE and one FALSE, what do we conclude about the inquiry? The function we define to summarize the two results depends on the inquiry and the goals of the reanalysis. But our diagnosis of the reanalysis should assess the properties of this summary of the two studies under possible realizations of the data. If the goal of the reanalysis is instead to learn about a new question, then we should simply construct a new MIDA altogether, but holding constant $D$ from the original study, which we cannot change because we already collected $d$ using it.

<!-- how do we update from the reanalysis research design (the original design plus the reanalysis of d)? -->
<!-- -- the design is the research design from before with two sets of estimates from two different A's -->
<!-- -- need an aggregation function (decisionmaking function) that converts the two sets of results into a decision or posterior -->

<!-- what can be learned from reanalysis? -->
<!-- (1) confirm there were not errors (consider changing A only) -->
<!-- (2) reassess what is known about the same inquiry, using new information about the world (change M, change A to suit new M) -->
<!-- (3) learn something new from the data about another node or edge or a different summary about the same ones (change I and possibly A to match it; possibly M if a node was missing; possibly add data) -->
<!-- (4) assess "robustness" of findings - point to discussion of this in answer strategy (or move it here) (change A) -->
<!-- (5) update M based on new research and assess what d can tell us from this study (change M and possibly I, possibly A to fit changed M and I) -->

<!-- how can we assess the properties of a *reanalysis*? diagnose changed MIDA. important to not condition on d, the design includes the actual D, and we need to consider what results d' we would get from the reanalysis under different realizations of D. -->

<!-- there are now two A's, so need to specify a decision function about how to integrate the two findings. this could be throw away the old a, or combine them in some way. if it's a "robustness" to alternative A, then you may want to combine not throw out for example. it's crucial to specify how you do that, that's part of the answer strategy. -->

<!-- ## Example -->

<!-- Knox, Lowe, and Mummolo (2020) (https://www.cambridge.org/core/journals/american-political-science-review/article/administrative-records-mask-racially-biased-policing/66BC0F9998543868BB20F241796B79B8) study the statistical biases that accompany estimates of racial bias in police use of force when presence in the dataset (being stopped by police) is conditioned on an outcome that is a downstream consequence of race. They show the estimate is not identified unless additional modelling assumptions are brought to bear. -->

<!-- Gaebler et al. (2020) (https://5harad.com/papers/post-treatment-bias.pdf) study the same question and make such modeling assumptions (subset ignorability, definition 2). -->

<!-- In a twitter thread (https://twitter.com/jonmummolo/status/1275790509647241222?s=20), Mummolo shows the three DAGs that are compatible with subset ignorability. We agree with Mummolo that these DAGs assume away causal paths that are very plausible. -->

<!-- ![DAG](figures/mummolo_dag.png) -->

<!-- This document provides a design declaration for this setting and shows how estimates of the controlled direct effect (effect of race on force among the stopped) are biased unless those paths are set to zero by assumption. -->

<!-- Design Declaration -->

<!-- There are four variables: (D: minority, M: stop, U: suspicion (unobserved), Y: force) and five paths: -->

<!-- ```{r} -->
<!-- D_M = 1 # effect of minority on stop -->
<!-- U_M = 1 # effect of suspicion on stop -->
<!-- D_Y = 1 # effect of minority on force -->
<!-- U_Y = 1 # effect of suspicion on force -->
<!-- M_Y = 1 # effect of stop on force -->
<!-- ``` -->

<!-- This basic design allows all five paths. -->

<!-- ```{r} -->
<!-- design_1 <- -->
<!--   declare_population(N = 1000, -->
<!--                      D = rbinom(N, size = 1, prob = 0.5), -->
<!--                      U = rnorm(N)) + -->
<!--   declare_potential_outcomes(M ~ rbinom(N, size = 1, prob = pnorm(D_M * -->
<!--                                                                     D + U_M * U)), -->
<!--                              assignment_variable = "D") + -->
<!--   reveal_outcomes(M, D) + -->
<!--   declare_potential_outcomes(Y ~ rnorm(N, D_Y * D + M_Y * M + U_Y * U), -->
<!--                              conditions = list(D = c(0, 1), M = c(0, 1))) + -->
<!--   reveal_outcomes(outcome_variables = "Y", -->
<!--                  assignment_variables = c("D", "M")) + -->
<!--   declare_estimand(CDE = mean(Y_D_1_M_1 - Y_D_0_M_1)) + -->
<!--   declare_estimator(Y ~ D, subset = M == 1, estimand = "CDE") -->
<!-- ``` -->

<!-- We redesign the design 3 times, removing one path at a time, then simulate all four designs. -->

<!-- ```{r, message=FALSE} -->
<!-- # no effect of D on M -->
<!-- design_2 <- redesign(design_1, D_M = 0) -->

<!-- # no effect of U on M -->
<!-- design_3 <- redesign(design_1, U_M = 0) -->

<!-- # no effect of U on Y -->
<!-- design_4 <- redesign(design_1, U_Y = 0) -->
<!-- ``` -->

<!-- This chunk is set to `echo = TRUE` and `eval = do_diagnosis` -->
<!-- ```{r, eval = do_diagnosis & !exists("do_bookdown")} -->
<!-- simulations <- simulate_designs(design_1, design_2, design_3, design_4, sims = sims) -->
<!-- ``` -->

<!-- Right after you do simulations, you want to save the simulations rds. -->

<!-- ```{r, echo = FALSE, purl = FALSE} -->
<!-- # figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file -->
<!-- rds_file_path <- paste0(get_dropbox_path("policing"), "/simulations_policing.RDS") -->
<!-- if (do_diagnosis & !exists("do_bookdown")) { -->
<!--   write_rds(simulations, path = rds_file_path) -->
<!-- } -->
<!-- simulations <- read_rds(rds_file_path) -->
<!-- ``` -->


<!-- ```{r, echo=FALSE, message = FALSE} -->

<!-- simulations <- -->
<!--   simulations %>% -->
<!--   mutate(`Assumed DAG` = factor( -->
<!--     design_label, -->
<!--     levels = c("design_1", "design_2", "design_3", "design_4"), -->
<!--     labels = c( -->
<!--       "All paths possible", -->
<!--       "no effect of D on M", -->
<!--       "no effect of U on M", -->
<!--       "no effect of U on Y" -->
<!--     ) -->
<!--   )) -->

<!-- summary_df <- -->
<!--   simulations %>% -->
<!--   group_by(`Assumed DAG`) %>% -->
<!--   summarise( -->
<!--     mean_estimand = mean(estimand), -->
<!--     mean_estimate = mean(estimate), -->
<!--     bias = mean(estimate - estimand) -->
<!--   ) %>% -->
<!--   pivot_longer(cols = c("mean_estimand", "mean_estimate")) -->
<!-- ``` -->

<!-- This plot confirms that unless one of those implausible assumptions hold, estimates of the CDE are biased. -->

<!-- ```{r, echo=FALSE} -->
<!-- ggplot(simulations, aes(estimate)) + -->
<!--   geom_histogram(bins = 50) + -->
<!--   geom_vline(data = summary_df, aes(xintercept = value, color = name)) + -->
<!--   facet_wrap(~`Assumed DAG`) + -->
<!--   xlab("Simulated CDE estimates") + -->
<!--   theme_bw() + -->
<!--   theme(legend.position = "bottom", -->
<!--         strip.background = element_blank(), -->
<!--         axis.title.y = element_blank(), -->
<!--         legend.title = element_blank()) -->
<!-- ``` -->

<!-- ### Grab bag -->

<!-- - @Clemens2017 on taxonomy of these kinds of efforts -->
<!-- - if you're going to use d to learn about a different M for a different I, you need to understand their D -->
