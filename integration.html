<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 25 Integration | Research Design: Declare, Diagnose, Redesign</title>
<meta name="author" content="Graeme Blair, Alexander Coppock, and Macartan Humphreys">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.4.9002/tabs.js"></script><script src="libs/bs3compat-0.2.4.9002/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<link href="libs/bs4_book-1.0.0/dd_imgpopup.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/bs4_book-1.0.0/dd_imgpopup.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://hypothes.is/embed.js" async></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS --><link rel="stylesheet" href="headers/style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Research Design: Declare, Diagnose, Redesign</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li class="book-part">Introduction</li>
<li><a class="" href="preamble.html"><span class="header-section-number">1</span> Preamble</a></li>
<li><a class="" href="defining-research-designs.html"><span class="header-section-number">2</span> Defining research designs</a></li>
<li><a class="" href="research-design-principles.html"><span class="header-section-number">3</span> Research design principles</a></li>
<li><a class="" href="primer.html"><span class="header-section-number">4</span> Software primer</a></li>
<li><a class="" href="part-i-exercises.html"><span class="header-section-number">5</span> Part I Exercises</a></li>
<li class="book-part">Declaration, Diagnosis, Redesign</li>
<li><a class="" href="declaration-1.html"><span class="header-section-number">6</span> Declaration</a></li>
<li><a class="" href="specifying-the-model.html"><span class="header-section-number">7</span> Specifying the model</a></li>
<li><a class="" href="defining-the-inquiry.html"><span class="header-section-number">8</span> Defining the inquiry</a></li>
<li><a class="" href="crafting-a-data-strategy.html"><span class="header-section-number">9</span> Crafting a data strategy</a></li>
<li><a class="" href="choosing-an-answer-strategy.html"><span class="header-section-number">10</span> Choosing an answer strategy</a></li>
<li><a class="" href="p2diagnosis.html"><span class="header-section-number">11</span> Diagnosis</a></li>
<li><a class="" href="redesign-2.html"><span class="header-section-number">12</span> Redesign</a></li>
<li><a class="" href="part-ii-exercises.html"><span class="header-section-number">13</span> Part II Exercises</a></li>
<li class="book-part">Research Design Library</li>
<li><a class="" href="research-design-library.html"><span class="header-section-number">14</span> Research Design Library</a></li>
<li><a class="" href="observational-descriptive.html"><span class="header-section-number">15</span> Observational | descriptive</a></li>
<li><a class="" href="observational-causal.html"><span class="header-section-number">16</span> Observational | causal</a></li>
<li><a class="" href="experimental-causal.html"><span class="header-section-number">17</span> Experimental | causal</a></li>
<li><a class="" href="experimental-descriptive.html"><span class="header-section-number">18</span> Experimental | descriptive</a></li>
<li><a class="" href="complex-designs-1.html"><span class="header-section-number">19</span> Complex designs</a></li>
<li><a class="" href="part-iii-exercises.html"><span class="header-section-number">20</span> Part III Exercises</a></li>
<li class="book-part">Research Design Lifecycle</li>
<li><a class="" href="research-design-lifecycle.html"><span class="header-section-number">21</span> Research Design Lifecycle</a></li>
<li><a class="" href="brainstorming.html"><span class="header-section-number">22</span> Brainstorming</a></li>
<li><a class="" href="planning.html"><span class="header-section-number">23</span> Planning</a></li>
<li><a class="" href="realization.html"><span class="header-section-number">24</span> Realization</a></li>
<li><a class="active" href="integration.html"><span class="header-section-number">25</span> Integration</a></li>
<li><a class="" href="part-iv-exercises.html"><span class="header-section-number">26</span> Part IV Exercises</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="integration" class="section level1">
<h1>
<span class="header-section-number">25</span> Integration<a class="anchor" aria-label="anchor" href="#integration"><i class="fas fa-link"></i></a>
</h1>
<p>After publication, research studies leave the hands of researchers and enter the public domain.</p>
<p>Most immediately, authors share their findings with the public through the media and with decisionmakers who may incorporate the results in specialist publications and executive summaries. Consideration of how the findings will eventually be communicated — and how policymakers, businesses, and regular people will use the findings to make decisions — may change the research design. New inquiries beyond those of interest to scientists can be added to design declarations. They can be diagnosed with respect to how lay readers will interpret evidence, which may differ from the interpretation in scientific publications. Ex-ante consideration of communication plans will ensure designs target both scientific publication and decisionmaking by nonscientists.</p>
<p>Researchers can prepare for the integration of their studies into scholarly debates through better archiving practices and better reporting of research designs in the published article. Future researchers may build on the results of a past study in three ways. First, they may <em>reanalyze</em> the original data. Reanalysts must be cognizant of the original data strategy <em>D</em> when working with the realized data <span class="math inline">\(d\)</span>. Changes to the the answer strategy <em>A</em> must respect <em>D</em>, regardless of whether the purpose of the reanalysis is to answer the original inquiry <em>I</em> or to answer a different inquiry <span class="math inline">\(I'\)</span>. Second, future researchers may <em>replicate</em> the design. Typically, replicators provide a new answer to the same <em>I</em> with new data, possibly improving elements of <em>D</em> and <em>A</em> along the way. If the inquiry of the replication is too different from the inquiry of the original study, the fidelity of the replication study may be compromised. Lastly, future researchers may <em>meta-analyze</em> study’s answer <span class="math inline">\(a^d\)</span> with other past studies. Meta-analysis is most meaningful when all of the included studies target a similar enough inquiry and when all studies rely on credible design. Otherwise, the procedure produces a meta-analytic average that is difficult to interpret.</p>
<p>All three of these activities depend on an accurate understanding of the study design. Reanalysts, replicators, and meta-analysts all need access to the study data and materials, of course. They also need to be sure of the critical design information in <em>M</em>, <em>I</em>, <em>D</em>, and <em>A</em>. Later in this section, we outline how archiving procedures that preserve study data and study design can enable new scientific purposes and describe strategies for doing each of these three particular integration tasks.</p>
<div id="communicating" class="section level2">
<h2>
<span class="header-section-number">25.1</span> Communicating<a class="anchor" aria-label="anchor" href="#communicating"><i class="fas fa-link"></i></a>
</h2>
<p>The findings from studies are communicated to other scholars through academic publications. But some of the most important audiences – policymakers, businesses, journalists, and the public – do not read academic journals. These audiences learn about the study in other in other ways. Authors write opeds, blog posts, and policy reports that translate research for nonspecialist audiences. Press offices pitch research studies for coverage by the media. Researchers present findings directly to decisionmakers and to their research partners.</p>
<p>These new outputs are for different audiences, so they are necessarily diverse in their tone and approach. Some things don’t change: we still need to communicate the quality of the research design and what we learn from the study. But some things do: we need to translate specialist language about the substance of the study to a nonspecialist audience, and translate the features of the research design in a way that nonspecialists can understand. If we do not communicate outside of academic publications, research findings will not influence public debates, policymaking, or real-world decisions by people and firms.</p>
<p>Too often, a casualty of translating the study from academic to other audiences is the design information. When researchers write for popular blogs or give interviews, emphasis is placed on the study results, not on the reasons why the results of the study are to be believed. In “dumbing down” the research for nonspecialist audiences, we revert to saying <em>that</em> the findings are true and not <em>why we know</em> the findings are true. Explaining why we know requires explaining the research design, which in our view ought to be part of any public-facing communication about research.</p>
<p>Of course, even when authors do emphasize design, journalists do not always care. Science reporting is commonly criticized for ignoring study design when picking which studies to publicize, so weak studies are not appropriately filtered out of coverage. Furthermore, journalists emphasize results they believe will drive people to pick up a newspaper or click on a headline. Flashy, surprising, or pandering findings receive far more attention than deserved, with the result that boring but correct findings are crowded out of the media spotlight.</p>
<p>In a review we conducted of recent studies published in the New York Times Well section on health and fitness, we found that two dimensions of design quality were commonly ignored. First, experimental studies on new fitness regimens with tiny samples, sometimes fewer than 10 units, are commonly highlighted. When both academic journals and reporters promote tiny studies with flashy or unexpected findings, the likely result is that the published record is full of statistical flukes driven by noise, not new discoveries. Second, very large studies that draw observational comparisons between large samples of dieters and non-dieters with millions of observations on diet receive outsize attention. These designs are prone to bias from confounding, but these concerns are swept under the rug.</p>
<p>This state of affairs is not entirely or even mostly the journalists’ fault, since, in the absence of design information, it can be challenging to separate the weak designs from the strong ones. Study results and the stamp of approval from peer review are easy heuristics to follow.</p>
<p>How can we improve this scientific communication dilemma? The market incentives for both journalists and authors reward style over substance, and any real solution to the problem would require addressing those incentives. Short of that, we recommend that authors who wish to communicate the high quality of their designs to the media can do so by providing the design information in <em>M</em>, <em>I</em>, <em>D</em>, and <em>A</em> in lay terms. Science communicators should clearly state the research question (<em>I</em>) and explain why applying the data and answer strategies is likely to yield a good answer to the question. The actual result (<span class="math inline">\(a^d\)</span>) is, of course, also important to communicate, but <em>why</em> <span class="math inline">\(a^d\)</span> is a credible answer to the research question is just as important to share. Building confidence in scientific results requires building confidence in scientific practice.</p>
<!-- Science reporting often has two aims: surface new discoveries, and inform decisions we make. New discoveries, such as a new cure on the horizon or better idea for reducing poverty, may not immediately affect decisions we make: we need further confirmation of their effectiveness in followup studies.  -->
<!-- A single study is typically not enough to immediately change decisions. Rather, most decisions should be based on the current scientific consensus on an inquiry (which itself could be informed by a single study, but in concert with those that came before). Scholars, therefore, may usefully consider how their study will or will not impact the scholarly consensus while designing a study.  -->
<!-- Conducting a meta-analysis or systematic review of past studies on the inquiry and considering what kind of evidence would shift the consensus -- either in terms of the sign or magnitude of the evidence or its strength. Each can be specified as a diagnosand: how much does the prior consensus on the average effectiveness of an intervention shift in response to evidence from this study.  -->
<!-- Design choices the researcher makes, such as sample size but also how they present the results in relation to past evidence, impact these diagnosands. On the flip side, reporters can then use the information about the current consensus and its strength (i.e., the standard deviation of the posterior), not only the findings from a single study.  -->
<!-- -- should we publish working papers -->
<!-- -- which papers should we *seek* reporting on -->
<!-- -- should reporters select  -->
<!-- - research design diagnosis can provide a tool for the media to assess the quality of evidence from a study they are considering for publication -->
<!-- - two kinds of bias: tiny samples and p-hacking; and confounding in observational studies -->
<!-- - susceptibility to clickbait  -->
<!-- - media publication bias from communication  -->
<!-- - bias from what scholars are able to and decide to share research outside  -->
<!-- - problem that lay audiences cannot distinguish quality of evidence -->
<!-- - communicating one study vs meta-analysis -->
<!-- - communicating scholarly consensus -->

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
<!-- start post here, do not edit above -->
<!-- make sure to rename the section title below -->
</div>
<div id="decisionmaking" class="section level2">
<h2>
<span class="header-section-number">25.2</span> Decisionmaking<a class="anchor" aria-label="anchor" href="#decisionmaking"><i class="fas fa-link"></i></a>
</h2>
<p>Policymakers, businesses, humanitarian organizations, and regular people make decisions based on social science research. Research designs, however, are often constructed without considering who will be informed by the evidence and how they will use evidence in decisions. We can optimize our designs for both scientific publication and decisionmaking. The first step is eliciting the inquiries decisionmakers have, and the second is diagnosing how decisions change depending on the results. How often would the decisionmaker make the right decision, with and without the study? A design that exhibits high statistical power and a high rate of making the right decision will influence not only the scientific literature but also decisions made by the public.</p>
<p>We illustrate this process by declaring an experimental design that compares a status quo policy with an alternative. As the researcher, your inquiry is the average treatment effect, but the policymaker has a subtlely different inquiry. The policymaker would like to know which policy to implement: the status quo or the alternative. Imagine you meet with the policymaker and ask how they would use the evidence you plan to produce. The policymaker says that they would like to switch to the alternative if it is better than the status quo. However, they face a switching cost to adopt the new policy, so for now, they would like to adopt the treatment only if it is at least 0.1 standard deviations better than the status quo.</p>
<p>In your design declaration, you add two new components to assess your design’s statistical power for estimating the average treatment effect as well as the probability of the policymaker making the right decision. The first is you add a new inquiry, which is, is the treatment at least 0.1 standard deviations better than the control condition? In addition, you add a statistical test to target this inquiry, which tests whether the treatment effect is larger than 0.1. It does so by calculating a linear combination of the treatment effect estimate: the null hypothesis is <span class="math inline">\(\widehat\tau - 0.1 = 0\)</span>.</p>
<div class="sourceCode" id="cb144"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># compare status quo to a new proposed policy, </span>
<span class="co">#   given cost of switching </span>
<span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">100</span>
<span class="va">effect_size</span> <span class="op">&lt;-</span> <span class="fl">0.1</span>

<span class="va">decision_function</span> <span class="op">&lt;-</span>
  <span class="kw">function</span><span class="op">(</span><span class="va">data</span><span class="op">)</span> <span class="op">{</span>
    <span class="va">data</span> <span class="op">%&gt;%</span>
      <span class="fu">lh_robust</span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">Z</span>, linear_hypothesis <span class="op">=</span> <span class="st">"Z - 0.1 = 0"</span>, data <span class="op">=</span> <span class="va">.</span><span class="op">)</span> <span class="op">%&gt;%</span>
      <span class="va">tidy</span> <span class="op">%&gt;%</span>
      <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">term</span> <span class="op">==</span> <span class="st">"Z - 0.1 = 0"</span><span class="op">)</span>
  <span class="op">}</span>

<span class="va">design</span> <span class="op">&lt;-</span>
  <span class="fu"><a href="https://rdrr.io/pkg/DeclareDesign/man/declare_model.html">declare_model</a></span><span class="op">(</span>
    N <span class="op">=</span> <span class="va">N</span>,
    U <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span>,
    <span class="fu">potential_outcomes</span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">effect_size</span> <span class="op">*</span> <span class="va">Z</span> <span class="op">+</span> <span class="va">U</span><span class="op">)</span>
  <span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://rdrr.io/pkg/DeclareDesign/man/declare_inquiry.html">declare_inquiry</a></span><span class="op">(</span>
    ATE <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">Y_Z_1</span> <span class="op">-</span> <span class="va">Y_Z_0</span><span class="op">)</span>,
    alternative_better_than_sq <span class="op">=</span> <span class="fu">if_else</span><span class="op">(</span><span class="va">ATE</span> <span class="op">&gt;</span> <span class="fl">0.1</span>, <span class="cn">TRUE</span>, <span class="cn">FALSE</span><span class="op">)</span>
  <span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://rdrr.io/pkg/DeclareDesign/man/declare_assignment.html">declare_assignment</a></span><span class="op">(</span>Z <span class="op">=</span> <span class="fu">complete_ra</span><span class="op">(</span><span class="va">N</span><span class="op">)</span>, legacy <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://rdrr.io/pkg/DeclareDesign/man/declare_measurement.html">declare_measurement</a></span><span class="op">(</span>Y <span class="op">=</span> <span class="fu">reveal_outcomes</span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">Z</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://rdrr.io/pkg/DeclareDesign/man/declare_estimator.html">declare_estimator</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">Z</span>, model <span class="op">=</span> <span class="va">difference_in_means</span>, inquiry <span class="op">=</span> <span class="st">"ATE"</span>, label <span class="op">=</span> <span class="st">"dim"</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://rdrr.io/pkg/DeclareDesign/man/declare_test.html">declare_test</a></span><span class="op">(</span>handler <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/DeclareDesign/man/declare_estimator.html">label_estimator</a></span><span class="op">(</span><span class="va">decision_function</span><span class="op">)</span>, label <span class="op">=</span> <span class="st">"decision"</span>, 
               inquiry <span class="op">=</span> <span class="st">"alternative_better_than_sq"</span><span class="op">)</span></code></pre></div>
<!-- switch to this later, there is a bug in lh_robust that needs to be fixed -->
<!-- # declare_estimator(Y ~ Z, model = lh_robust, linear_hypothesis = "Z - 0.05 = 0", se_type = "HC2", label = "decision") -->
<p>In addition to power, we set up a diagnosand for the proportion of times the policymaker will make the right decision given the evidence you provide. We redesign to consider alternative sample sizes and we diagnose under different possible true effect sizes, some negative (in which case the policymaker should retain the status quo); no difference (status quo should be retained because the effect is not a big enough improvement to justify switching costs); and positive with different sizes.</p>
<p>In Figure <a href="integration.html#fig:decisionplot">25.1</a>, we show the probability of retaining the status quo policy (left facet) and the probability of switching to the treatment (right) by different true effect sizes. On the left, we see that there is a very high probability of selecting the right policy when the true effect size is very low. This pattern occurs because when the effect size is low, we are likely to fail to reject the null. With small sample sizes, we are likely to also select the status quo even when we should not, because of the imprecision of the estimates. Looking at the right graph, even when the true effect size is large (i.e., 0.25), we need to have a large sample size, about 1500, to achieve 80% probability of correctly choosing the treatment.</p>
<p>The sample size we might choose based on this analysis of the policymaker’s choice is different than if we only considered statistical power. This is because the decision curve only reaches 80% power at 1500, while power reaches 80% at just over 500. The reason for the divergence is to make a correct decision, we need evidence that the treatment effect is greater than 0.1, as compared to statistical power which considers whether the effect is greater than 0.0.</p>
<div class="figure">
<span id="fig:decisionplot"></span>
<img src="book_files/figure-html/decisionplot-1.svg" alt="WIP: Research design diagnosis for study of effectiveness of a policy change compared to the status quo, where a policymaker wishes to switch to the treatment policy only if it is at least 0.05 standard deviations better than the status quo. On the left, we display the statistical power of the study to detect an effect in either direction. On the right, we display the rate of making the right decision to switch policies or not." width="100%"><p class="caption">
Figure 25.1: WIP: Research design diagnosis for study of effectiveness of a policy change compared to the status quo, where a policymaker wishes to switch to the treatment policy only if it is at least 0.05 standard deviations better than the status quo. On the left, we display the statistical power of the study to detect an effect in either direction. On the right, we display the rate of making the right decision to switch policies or not.
</p>
</div>
<!-- - different set of inquiries -- we should directly consider these in designs -->
<!-- - different set of diagnosands -->
<!-- - requires conversation with decisionmakers *ex ante* to understand what decisions they make on the basis of this evidence *and* how they will interpret evidence (diagnostic statistic) -->

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
<!-- start post here, do not edit above -->
</div>
<div id="archiving" class="section level2">
<h2>
<span class="header-section-number">25.3</span> Archiving<a class="anchor" aria-label="anchor" href="#archiving"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<p>One of the biggest successes in the push for greater research transparency has been changing norms surrounding data sharing and analysis code after studies have been published. It has been become de rigeur at many journals to post these materials at publicly-available repositories like the OSF or Dataverse. This development is undoubtedly a good thing. In older manuscripts, sometimes data or analyses are described as being “available upon request,” but of course, such requests are sometimes ignored. Furthermore, a century from now, study authors will no longer be with us even if they wanted to respond to such requests. Public repositories have a much better chance of preserving study information for the future.</p>
<!-- That's the promise of publicly-posted replication archives, but the mundane reality of replication archives often falls short. We see many archives that are disorganized, poorly documented, and contain dozens of bugs and inconsistencies.  -->
<p>What belongs in a replication archive? Enough documentation, data, and design detail that those who wish to reanalyze, replicate, and synthesize results can do so without contacting the authors.</p>
<p><strong>Data.</strong> First, the data <span class="math inline">\(d\)</span> itself. Sometimes this is the raw data. Sometimes it is only the “cleaned” data that is actually used by analysis scripts. Where ethically possible, we think it is preferable to post as much of the raw data as possible after removing information like IP address or geographic location that could be used to identify a subject. We usually consider data processing scripts that clean and prepare data for analysis as part of the data strategy <em>D</em> in the sense that they complete the measurement procedures laid out in <em>D</em>. That said, cleaning scripts might also be considered part of the answer strategy <em>A</em> in the sense that they apply an interpretation to the data provided by the world. The output of cleaning scripts – the cleaned data – should also be included in the replication archive.</p>
<p>Reanalyses often reexamine and extend studies by exploring the use of alternative outcomes, varying sets of variables to control for, and new ways of grouping data. As a result, replication data ideally includes all data collected by the authors even if the variables are not used in the final published results. Often authors exclude these to preserve their own ability to publish on these other variables or because they are worried alternative analyses will uncover a lack of robustness. We hope norms will change such that study authors instead want to enable future researchers to build on their research by being expansive in what information is included.</p>
<p><strong>Analysis code.</strong> Replication archives also include the answer strategy <em>A</em>, or the set of functions that produce <span class="math inline">\(a^d\)</span> when applied to <span class="math inline">\(d\)</span>. We need the actual analysis code because the natural-language descriptions of <em>A</em> that are typically given in written reports are imprecise. As a small example, many articles describe their answer strategies as “ordinary least squares” but do not fully describe the set of covariates included or the particular approach to variance estimation. These choices can substantively affect the quality of the research design – and nothing makes these choices explicit like the actual analysis code. Analysis code is needed not only for reanalysis but also replication and meta-analysis. Reanalyses may directly reuse or modify analysis code and replication projects need to know the exact details of analyses to ensure they can implement the same analyses on the data they collect. Meta-analysis authors may take the estimates from the past studies directly, so understanding the exact analyses conducted is important to understand what is being estimated. Other times, meta-analyses reanalyze data to ensure comparability in estimation. Conducting analyses with and without covariates, with clustering when it was appropriate, or with a single statistical model when they vary across studies all require having the exact analysis code.</p>
<p><strong>Data strategy materials.</strong> Increasingly, replication archives include the materials needed to implement treatments and measurement strategies. Without the survey questionnaires in their original languages and formats, we cannot exactly replicate them in future studies, which hinders our ability to build on and adapt them. The treatment stimuli used in the study should also be included. A central problem in replicating studies in the Many Labs projects was that the exact stimuli were either not retained or were no longer available for past psychology studies. This loss led to confusion over whether failures to replicate were due to changes in the stimuli, different populations, or the underpowered original study designs. Data strategies are needed for reanalyses and meta-analyses too: answer strategies should respect data strategies, so understanding the details of sampling, treatment assignment, and measurement can shape reanalysts’ decisions and meta-analysis authors’ decisions about what studies to include and which estimates to synthesize.</p>
<p><strong>Design declaration.</strong> While typical replication archives include <span class="math inline">\(d\)</span> and <em>A</em>, we think that future replication archives should also have a design declaration that fully describes <em>M</em>, <em>I</em>, <em>D</em>, and <em>A</em> – that is, we should archive designs, not just data and analysis code. This should be done in code and words. A diagnosis should be included, demonstrating the properties as understood by the author and indicating the diagnosands that the author considered in judging the quality of the design.</p>
<p>Design details help future scholars not only assess, but replicate, reanalyze, and extend the study. Reanalysts need to understand the answer strategy to modify or extend it and the data strategy used to ensure that their new analysis respects the details of the sampling, treatment assignment, and measurement procedures. Data and analysis sharing enables reanalysts to adopt or adapt the analysis strategy, but a declaration of the data strategy would help more. The same is true of meta-analysis authors, who need to understand the designs’ details to make good decisions about which studies to include and how to analyze them. Replicators who wish to exactly replicate or even just provide an answer to the same inquiry need to understand the inquiry, data strategy, and answer strategy. Replication practice today involves inferring most of these details from descriptions in text. The result is disputes that result after the replication is sent out for peer review. The original authors may disagree with inferences the replicators made about the inquiry or data strategy or answer strategy. To protect the original authors and the replicators, including a research design declaration specifying each of these elements resolves these issues so that replication and extension can focus on the substance of the research question and innovation in research design.</p>
<!-- Figure \@ref(fig:filestructure)  -->
<p>The Figure below shows the file structure for an example replication. Our view on replication archives shares much in common with the <a href="https://www.projecttier.org">TIER protocol</a>. It includes raw data in a platform-independent format (.csv) and cleaned data in a language-specific format (.rds, a format for R data files). Data features like labels, attributes, and factor levels are preserved when imported by the analysis scripts. The analysis scripts are labeled by the outputs they create, such as figures and tables. A master script is included that runs the cleaning and analysis scripts in the correct order. The documents folder consists of the paper, the supplemental appendix, the pre-analysis plan, the populated analysis plan, and codebooks that describe the data. A README file explains each part of the replication archive. We also suggest that authors include a script that consists of a design declaration and diagnosis.</p>
<div class="figure">
<img src="figures/file_structure.png" alt=""><p class="caption">File structure for archiving</p>
</div>
<p><strong>Further Reading</strong></p>
<ul>
<li><p><span class="citation">Peer, Orr, and Coppock (<a href="references.html#ref-peer_orr_coppock_2021" role="doc-biblioref">2021</a>)</span> who propose that researchers should “actively maintain” their replication archives by checking that they still run and making updates to obsolete code. In this way, the information about <em>A</em> that is contained in the replication archive stays current and scientifically useful.</p></li>
<li><p><span class="citation">Elman, Kapiszewski, and Lupia (<a href="references.html#ref-elman2018" role="doc-biblioref">2018</a>)</span> argues that the benefits of data transparency in political science outweigh its costs.</p></li>
<li><p><span class="citation">(<span class="citeproc-not-found" data-reference-id="Bowers2011"><strong>???</strong></span>)</span> describes how good archiving is like collaborating with your future self</p></li>
<li><p><span class="citation">(<span class="citeproc-not-found" data-reference-id="alvarez2018"><strong>???</strong></span>)</span> provide guidance on how to create good replication archives.</p></li>
</ul>
<!-- Example is archive at OSF: https://osf.io/4vuqh --><!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book--><!-- start post here, do not edit above -->
</div>
<div id="reanalysis" class="section level2">
<h2>
<span class="header-section-number">25.4</span> Reanalysis<a class="anchor" aria-label="anchor" href="#reanalysis"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<p>A reanalysis of an existing study is a follow-up study that reuses <span class="math inline">\(d\)</span> the original realized data for some new purpose. The reanalysis is a study with a research design that can be described in terms of <em>M</em>, <em>I</em>, <em>D</em>, and <em>A</em>. Reanalyses are fundamentally constrained by the data strategy of the original study. The data strategy <em>D</em> and the resulting data <span class="math inline">\(d\)</span> are set in stone – but reanalysts can make changes to the answer strategy <em>A</em> – sometimes also to the model <em>M</em> or inquiry <em>I</em>.</p>
<p>We can learn from reanalyses in several ways. First, we can fix errors in the original answer strategy. Reanalyses fixed simple mathematical errors, typos in data transcription, or failures to account for features of the data strategy when analyzing the data. These reanalyses show whether the original results do or do not depend on these corrections. Second, we can reassess the study in light of new information about the world learned after the original study was published. That is, sometimes <em>M</em> changes in ways that color our interpretation of past results. Perhaps we learned about new confounders or alternative causal channels that undermine the original answer strategy’s credibility. When reanalyzed, demonstrating the results do (or do not) change when new model features are incorporated improves our understanding of the inquiry.</p>
<p>Third, many reanalyses show that original findings are not “robust” to alternative answer strategies. These are better conceptualized as claims about robustness to alternative models: one model may imply one answer strategy, and a different model, with another confounder, suggests another. If both models are plausible, a good answer strategy should be robust to both and even help distinguish between them. A reanalysis could uncover robustness to these alternative models or lack thereof.</p>
<p>Lastly, reanalyses may also aim to answer new questions that were not considered by the original study but for which the realized data can provide useful answers. For example, authors may analyze outcomes not originally analyzed, or data collected during an experimental design to answer a causal question might help answer a descriptive question.</p>
<p>Reanalyses are themselves research designs. Just like any design, whether a reanalysis is a strong research design depends on <em>possible</em> realizations of the data (as determined by the data strategy), not just the realized data. Because <span class="math inline">\(d\)</span> is fixed in a reanalysis, analysts are often instead tempted to judge the reanalysis based on whether it overturns or confirms the original study’s results. A successful reanalysis in this way of thinking demonstrates, by showing that the original results are changed under an alternative answer strategy, that the results are not robust to other plausible models. This way of thinking can lead to incorrect assessments of reanalyses. We need to consider what answers would obtain under the original answer strategy <em>A</em> and the reanalysis strategy <span class="math inline">\(A^{\prime}\)</span> under many <em>possible</em> realizations of the data. A good reanalysis strategy reveals with high probability the set of models of the world under which we can make credible claims about the inquiry. Whether or not the results from the fixed <span class="math inline">\(d\)</span> that was realized change under the answer strategies <em>A</em> and <span class="math inline">\(A^{\prime}\)</span> tells us little about this probability because <span class="math inline">\(d\)</span> is only one draw.</p>
<p>To diagnose a reanalysis, we need to define two answer strategies — <em>A</em> and <span class="math inline">\(A^{\prime}\)</span>–and a new diagnostic-statistic. We need to decide how we summarize the answers from the two answer strategies. If one returns TRUE and one FALSE, what do we conclude about the inquiry? The function we define to summarize the two results depends on the inquiry and the goals of the reanalysis. But our diagnosis of the reanalysis should assess the properties of this summary of the two studies under possible realizations of the data. If the goal of the reanalysis is instead to learn about a new question, then we should simply construct a new MIDA altogether, but holding constant <em>D</em> from the original study, which we cannot change because we already collected <span class="math inline">\(d\)</span> using it.</p>
<p>We illustrate the flaw in assessing reanalyses based on changing significance of results from realized data below. We demonstrate how to assess the properties of reanalysis plans, comparing the properties of original answer strategies to proposed reanalysis answer strategies.</p>
<p>The design we consider is an observational study with a binary treatment <span class="math inline">\(Z\)</span> that may or may not be confounded by a covariate <span class="math inline">\(X\)</span>. The original answer strategy <code>A</code> is regression of the outcome <span class="math inline">\(Y\)</span> on the the treatment <span class="math inline">\(Z\)</span>. The reanalyst collects the covariate <span class="math inline">\(X\)</span> and proposes to control for it in a linear regression; call that strategy <code>A_prime</code>.</p>
<div class="sourceCode" id="cb145"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">A</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/DeclareDesign/man/declare_estimator.html">declare_estimator</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">Z</span>, model <span class="op">=</span> <span class="va">lm_robust</span>, label <span class="op">=</span> <span class="st">"A"</span><span class="op">)</span>
<span class="va">A_prime</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/DeclareDesign/man/declare_estimator.html">declare_estimator</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">Z</span> <span class="op">+</span> <span class="va">X</span>, model <span class="op">=</span> <span class="va">lm_robust</span>, label <span class="op">=</span> <span class="st">"A_prime"</span><span class="op">)</span></code></pre></div>
<p>We set up the model taking the perspective of the reanalyst, who believes <span class="math inline">\(Z\)</span> and <span class="math inline">\(Y\)</span> are confounded by <span class="math inline">\(X\)</span>:</p>
<div class="sourceCode" id="cb146"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># X is a confounder and is measured pretreatment</span>
<span class="va">model_1</span> <span class="op">&lt;-</span> 
  <span class="fu"><a href="https://rdrr.io/pkg/DeclareDesign/man/declare_model.html">declare_model</a></span><span class="op">(</span>
    N <span class="op">=</span> <span class="fl">100</span>,
    U <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span>,
    X <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span>,
    Z <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">N</span>, <span class="fl">1</span>, prob <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Logistic.html">plogis</a></span><span class="op">(</span><span class="fl">0.5</span> <span class="op">+</span> <span class="va">X</span><span class="op">)</span><span class="op">)</span>,
    <span class="fu">potential_outcomes</span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="fl">0.1</span> <span class="op">*</span> <span class="va">Z</span> <span class="op">+</span> <span class="fl">0.25</span> <span class="op">*</span> <span class="va">X</span> <span class="op">+</span> <span class="va">U</span><span class="op">)</span>,
    Y <span class="op">=</span> <span class="fu">reveal_outcomes</span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">Z</span><span class="op">)</span>
  <span class="op">)</span> </code></pre></div>
<p>Drawing data from this strategy and applying the two answer strategies, we get differing results:</p>
<div class="sourceCode" id="cb147"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/DeclareDesign/man/draw_functions.html">draw_estimates</a></span><span class="op">(</span><span class="va">model_1</span> <span class="op">+</span> <span class="va">A</span> <span class="op">+</span> <span class="va">A_prime</span><span class="op">)</span></code></pre></div>
<div class="inline-table"><table class="table table-sm">
<thead><tr>
<th style="text-align:left;">
estimator_label
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
std.error
</th>
<th style="text-align:right;">
p.value
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
A
</td>
<td style="text-align:right;">
0.385
</td>
<td style="text-align:right;">
0.176
</td>
<td style="text-align:right;">
0.031
</td>
</tr>
<tr>
<td style="text-align:left;">
A_prime
</td>
<td style="text-align:right;">
0.219
</td>
<td style="text-align:right;">
0.188
</td>
<td style="text-align:right;">
0.246
</td>
</tr>
</tbody>
</table></div>
<p>Commonly, reanalysts would infer from this two things: the ATE is nonsignificant, and that the answer strategy <code>A_prime</code> is preferred. It is preferred because it is unbiased under confounding and unbiased (and lower variance than <code>A</code>) when there is no confounding. As we show below, these claims depend on the validity of other parts of the model <em>M</em>. In particular, we define two other possible models. In model 2, <span class="math inline">\(X\)</span> is measured before treatment, as in the first model, but is not a confounder. In model 3, however, <span class="math inline">\(X\)</span> is affected by <span class="math inline">\(Y\)</span> and is measured <em>post</em>treatment. In many observational settings, it is now always clear when <span class="math inline">\(X\)</span> is realized or measured relative to treatment, so it is useful to explore the implications of the timing of measurement on the design’s diagnosands as we do below.</p>
<div class="sourceCode" id="cb148"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># X is not a confounder and is measured pretreatment</span>
<span class="va">model_2</span> <span class="op">&lt;-</span> 
  <span class="fu"><a href="https://rdrr.io/pkg/DeclareDesign/man/declare_model.html">declare_model</a></span><span class="op">(</span>
    N <span class="op">=</span> <span class="fl">100</span>,
    U <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span>,
    X <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span>,
    Z <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">N</span>, <span class="fl">1</span>, prob <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Logistic.html">plogis</a></span><span class="op">(</span><span class="fl">0.5</span><span class="op">)</span><span class="op">)</span>,
    <span class="fu">potential_outcomes</span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="fl">0.1</span> <span class="op">*</span> <span class="va">Z</span> <span class="op">+</span> <span class="fl">0.25</span> <span class="op">*</span> <span class="va">X</span> <span class="op">+</span> <span class="va">U</span><span class="op">)</span>,
    Y <span class="op">=</span> <span class="fu">reveal_outcomes</span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">Z</span><span class="op">)</span>
  <span class="op">)</span> 

<span class="co"># X is not a confounder and is measured posttreatment</span>
<span class="va">model_3</span> <span class="op">&lt;-</span> 
  <span class="fu"><a href="https://rdrr.io/pkg/DeclareDesign/man/declare_model.html">declare_model</a></span><span class="op">(</span>
    N <span class="op">=</span> <span class="fl">100</span>,
    U <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span>,
    Z <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">N</span>, <span class="fl">1</span>, prob <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Logistic.html">plogis</a></span><span class="op">(</span><span class="fl">0.5</span><span class="op">)</span><span class="op">)</span>,
    <span class="fu">potential_outcomes</span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="fl">0.1</span> <span class="op">*</span> <span class="va">Z</span> <span class="op">+</span> <span class="va">U</span><span class="op">)</span>,
    Y <span class="op">=</span> <span class="fu">reveal_outcomes</span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">Z</span><span class="op">)</span>,
    X <span class="op">=</span> <span class="fl">0.1</span> <span class="op">*</span> <span class="va">Z</span> <span class="op">+</span> <span class="fl">5</span> <span class="op">*</span> <span class="va">Y</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span>
  <span class="op">)</span> 

<span class="va">design_1</span> <span class="op">&lt;-</span> <span class="va">model_1</span> <span class="op">+</span> <span class="va">I</span> <span class="op">+</span> <span class="va">A</span> <span class="op">+</span> <span class="va">A_prime</span>
<span class="va">design_2</span> <span class="op">&lt;-</span> <span class="va">model_2</span> <span class="op">+</span> <span class="va">I</span> <span class="op">+</span> <span class="va">A</span> <span class="op">+</span> <span class="va">A_prime</span>
<span class="va">design_3</span> <span class="op">&lt;-</span> <span class="va">model_3</span> <span class="op">+</span> <span class="va">I</span> <span class="op">+</span> <span class="va">A</span> <span class="op">+</span> <span class="va">A_prime</span></code></pre></div>
<p>What we see in the diagnosis below is that <code>A_prime</code> is only preferred if we know for sure that <span class="math inline">\(X\)</span> is measured pretreatment. In design 2, where <span class="math inline">\(X\)</span> is measured posttreatment, <code>A</code> is preferred, because controlling for <span class="math inline">\(X\)</span> leads to posttreatment bias. Including the <span class="math inline">\(X\)</span> variable in the regression misattributes some of the effect of <span class="math inline">\(Z\)</span> on <span class="math inline">\(Y\)</span> to <span class="math inline">\(X\)</span>. The reanalyst, this diagnosis indicates, needs to justify their beliefs about when <span class="math inline">\(X\)</span> is measured in order to claim that <code>A_prime</code> is preferred to <code>A</code>. It is not always preferred, as they might have concluded just by looking at the realized estimates from the two answer strategies.</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr>
<th style="text-align:left;">
design_label
</th>
<th style="text-align:left;">
estimator_label
</th>
<th style="text-align:right;">
bias
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
design_1
</td>
<td style="text-align:left;">
A
</td>
<td style="text-align:right;">
0.213
</td>
</tr>
<tr>
<td style="text-align:left;">
design_1
</td>
<td style="text-align:left;">
A_prime
</td>
<td style="text-align:right;">
-0.002
</td>
</tr>
<tr>
<td style="text-align:left;">
design_2
</td>
<td style="text-align:left;">
A
</td>
<td style="text-align:right;">
0.003
</td>
</tr>
<tr>
<td style="text-align:left;">
design_2
</td>
<td style="text-align:left;">
A_prime
</td>
<td style="text-align:right;">
0.002
</td>
</tr>
<tr>
<td style="text-align:left;">
design_3
</td>
<td style="text-align:left;">
A
</td>
<td style="text-align:right;">
0.013
</td>
</tr>
<tr>
<td style="text-align:left;">
design_3
</td>
<td style="text-align:left;">
A_prime
</td>
<td style="text-align:right;">
0.016
</td>
</tr>
</tbody>
</table></div>
<p>Three principles emerge from the idea that changes from answer strategy <em>A</em> to <span class="math inline">\(A^{\prime}\)</span> should be justified by diagnosis, not the comparison of the results <span class="math inline">\(a^d\)</span> to <span class="math inline">\(a^{\prime d}\)</span>:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Home ground dominance.</strong> Holding the original <em>M</em> constant (i.e., the home ground of the original study) as well as <em>I</em>, if you can show that a new answer strategy <span class="math inline">\(A^{\prime}\)</span> yields better diagnosands than the original <em>A</em>, then the new <span class="math inline">\(A^{\prime}\)</span> can be justified by a reanalyst based on home ground dominance. In this situation, we would prefer <span class="math inline">\(A^{\prime}\)</span> and the estimates produced by it to <em>A</em> and the original estimates produced by it, regardless of what those realized estimates are. In the example above, model 2 is the “home ground,” and <span class="math inline">\(A^{\prime}\)</span> is preferred to <em>A</em> on this home ground.</li>
</ol>
<!-- parallel idea about finding a replication with a new D that rules out confounders --><ol start="2" style="list-style-type: decimal">
<li><p><strong>Robustness to alternative models.</strong> A second justification for a change in answer strategy is that you can show that a new answer strategy is robust to both the original model <em>M</em> and a new, also plausible, <span class="math inline">\(M^{\prime}\)</span>. In observational studies, commonly we are uncertain about many features of the model, such as the existence of unobserved confounders. In the example above, <span class="math inline">\(A^{\prime}\)</span> is robust to models 1 and 2 but is not robust to model 3. By contrast, <em>A</em> is robust to models 2 and 3 but not to model 1.</p></li>
<li><p><strong>Model plausibility.</strong> If the diagnosands for a design with <span class="math inline">\(A^{\prime}\)</span> are worse than those with <em>A</em> under <em>M</em> but better under <span class="math inline">\(M^{\prime}\)</span>, then the switch to <span class="math inline">\(A^{\prime}\)</span> can only be justified by a claim or demonstration that <span class="math inline">\(M^{\prime}\)</span> is more plausible than <em>M</em>. As we saw in the example, there was a third possible model in which <span class="math inline">\(X\)</span> was realized posttreatment and so controlling for <span class="math inline">\(X\)</span> led to posttreatment bias. In that case, neither controlling for <span class="math inline">\(X\)</span> or not controlling for <span class="math inline">\(X\)</span> is robust to all three alternative models. The justification of model plausibility would have to be used in this case to justify controlling for <span class="math inline">\(X\)</span>: substantive knowledge or additional data would have to be brought to bear to rule out the third model with posttreatment measurement of <span class="math inline">\(X\)</span>. The reanalyst could demonstrate that data collection of <span class="math inline">\(X\)</span> took place before the treatment was realized, for example.</p></li>
</ol>
<!-- 1. M always changes! (you have more information on $\tau$ or $\sd(\tau)$) --><!-- 2. Home ground dominance: Change A or D-and-A if A$^\prime$ > A under M --><!-- 3. Robustness to alternative models: Change A or D-and-A if A$^\prime$ $\geq$ A under M AND A$^\prime$ > A under M$^\prime$ E.g. change from simple to complete RA --><!-- 4. Model plausibility: If A$^\prime$ < A under M AND A$^\prime$ > A under M$^\prime$, then change to A$^\prime$ or D-and-A IFF M$^\prime$ is more plausible than M E.g. switching to balanced design if you believe variances equal across treatment groups --><!-- 5. Undefined inquiries. Change I to I$^\prime$ if I is undefined under M If I is defined under M: You can't change to I$^\prime$, You can’t change D to D$^\prime$ if that means I unidentifiable. --><!-- how do we update from the reanalysis research design (the original design plus the reanalysis of d)? --><!-- -- the design is the research design from before with two sets of estimates from two different A's --><!-- -- need an aggregation function (decisionmaking function) that converts the two sets of results into a decision or posterior --><!-- what can be learned from reanalysis? --><!-- (1) confirm there were not errors (consider changing A only) --><!-- (2) reassess what is known about the same inquiry, using new information about the world (change M, change A to suit new M) --><!-- (3) learn something new from the data about another node or edge or a different summary about the same ones (change I and possibly A to match it; possibly M if a node was missing; possibly add data) --><!-- (4) assess "robustness" of findings - point to discussion of this in answer strategy (or move it here) (change A) --><!-- (5) update M based on new research and assess what d can tell us from this study (change M and possibly I, possibly A to fit changed M and I) --><!-- how can we assess the properties of a *reanalysis*? diagnose changed MIDA. important to not condition on d, the design includes the actual D, and we need to consider what results d' we would get from the reanalysis under different realizations of D. --><!-- there are now two A's, so need to specify a decision function about how to integrate the two findings. this could be throw away the old a, or combine them in some way. if it's a "robustness" to alternative A, then you may want to combine not throw out for example. it's crucial to specify how you do that, that's part of the answer strategy. --><!-- ## Example --><!-- Knox, Lowe, and Mummolo (2020) (https://www.cambridge.org/core/journals/american-political-science-review/article/administrative-records-mask-racially-biased-policing/66BC0F9998543868BB20F241796B79B8) study the statistical biases that accompany estimates of racial bias in police use of force when presence in the dataset (being stopped by police) is conditioned on an outcome that is a downstream consequence of race. They show the estimate is not identified unless additional modelling assumptions are brought to bear. --><!-- Gaebler et al. (2020) (https://5harad.com/papers/post-treatment-bias.pdf) study the same question and make such modeling assumptions (subset ignorability, definition 2). --><!-- In a twitter thread (https://twitter.com/jonmummolo/status/1275790509647241222?s=20), Mummolo shows the three DAGs that are compatible with subset ignorability. We agree with Mummolo that these DAGs assume away causal paths that are very plausible. --><!-- ![DAG](figures/mummolo_dag.png) --><!-- This document provides a design declaration for this setting and shows how estimates of the controlled direct effect (effect of race on force among the stopped) are biased unless those paths are set to zero by assumption. --><!-- Design Declaration --><!-- There are four variables: (D: minority, M: stop, U: suspicion (unobserved), Y: force) and five paths: --><!-- ```{r} --><!-- D_M = 1 # effect of minority on stop --><!-- U_M = 1 # effect of suspicion on stop --><!-- D_Y = 1 # effect of minority on force --><!-- U_Y = 1 # effect of suspicion on force --><!-- M_Y = 1 # effect of stop on force --><!-- ``` --><!-- This basic design allows all five paths. --><!-- ```{r} --><!-- design_1 <- --><!--   declare_model(N = 1000, --><!--                      D = rbinom(N, size = 1, prob = 0.5), --><!--                      U = rnorm(N)) + --><!--   declare_potential_outcomes(M ~ rbinom(N, size = 1, prob = pnorm(D_M * --><!--                                                                     D + U_M * U)), --><!--                              assignment_variable = "D") + --><!--   declare_reveal(M, D) + --><!--   declare_potential_outcomes(Y ~ rnorm(N, D_Y * D + M_Y * M + U_Y * U), --><!--                              conditions = list(D = c(0, 1), M = c(0, 1))) + --><!--   declare_reveal(outcome_variables = "Y", --><!--                  assignment_variables = c("D", "M")) + --><!--   declare_inquiry(CDE = mean(Y_D_1_M_1 - Y_D_0_M_1)) + --><!--   declare_estimator(Y ~ D, subset = M == 1, inquiry = "CDE") --><!-- ``` --><!-- We redesign the design 3 times, removing one path at a time, then simulate all four designs. --><!-- ```{r, message=FALSE} --><!-- # no effect of D on M --><!-- design_2 <- redesign(design_1, D_M = 0) --><!-- # no effect of U on M --><!-- design_3 <- redesign(design_1, U_M = 0) --><!-- # no effect of U on Y --><!-- design_4 <- redesign(design_1, U_Y = 0) --><!-- ``` --><!-- This chunk is set to `echo = TRUE` and `eval = do_diagnosis` --><!-- ```{r, eval = do_diagnosis & !exists("do_bookdown")} --><!-- simulations <- simulate_designs(design_1, design_2, design_3, design_4, sims = sims) --><!-- ``` --><!-- Right after you do simulations, you want to save the simulations rds. --><!-- ```{r, echo = FALSE, purl = FALSE} --><!-- # figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file --><!-- rds_file_path <- paste0(get_dropbox_path("policing"), "/simulations_policing.RDS") --><!-- if (do_diagnosis & !exists("do_bookdown")) { --><!--   write_rds(simulations, path = rds_file_path) --><!-- } --><!-- simulations <- read_rds(rds_file_path) --><!-- ``` --><!-- ```{r, echo=FALSE, message = FALSE} --><!-- simulations <- --><!--   simulations %>% --><!--   mutate(`Assumed DAG` = factor( --><!--     design_label, --><!--     levels = c("design_1", "design_2", "design_3", "design_4"), --><!--     labels = c( --><!--       "All paths possible", --><!--       "no effect of D on M", --><!--       "no effect of U on M", --><!--       "no effect of U on Y" --><!--     ) --><!--   )) --><!-- summary_df <- --><!--   simulations %>% --><!--   group_by(`Assumed DAG`) %>% --><!--   summarise( --><!--     mean_estimand = mean(estimand), --><!--     mean_estimate = mean(estimate), --><!--     bias = mean(estimate - estimand) --><!--   ) %>% --><!--   pivot_longer(cols = c("mean_estimand", "mean_estimate")) --><!-- ``` --><!-- This plot confirms that unless one of those implausible assumptions hold, estimates of the CDE are biased. --><!-- ```{r, echo=FALSE} --><!-- ggplot(simulations, aes(estimate)) + --><!--   geom_histogram(bins = 50) + --><!--   geom_vline(data = summary_df, aes(xintercept = value, color = name)) + --><!--   facet_wrap(~`Assumed DAG`) + --><!--   xlab("Simulated CDE estimates") + --><!--   theme_bw() + --><!--   theme(legend.position = "bottom", --><!--         strip.background = element_blank(), --><!--         axis.title.y = element_blank(), --><!--         legend.title = element_blank()) --><!-- ``` --><!-- ### Grab bag --><!-- - @Clemens2017 on taxonomy of these kinds of efforts --><!-- - if you're going to use d to learn about a different M for a different I, you need to understand their D --><!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book--><!-- start post here, do not edit above -->
</div>
<div id="replication" class="section level2">
<h2>
<span class="header-section-number">25.5</span> Replication<a class="anchor" aria-label="anchor" href="#replication"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<p>After your study is completed, it may one day be replicated. Replication differs from reanalysis in that a replication study involves collecting new data to study the same inquiry. A new model, data strategy, or answer strategy may also be proposed. By contrast, a reanalysis may re-specify parts of the research design but always re-uses the original data <span class="math inline">\(d\)</span> somehow.</p>
<p>So-called “exact” replications hold key features of I, D, and A fixed, but draw a new dataset <span class="math inline">\(d_{\rm new}\)</span> from data strategy <span class="math inline">\(D()\)</span> and apply the same answer strategy <em>A</em> to the new <span class="math inline">\(d\)</span> to produce a fresh answer <span class="math inline">\(a_{\rm new}^D\)</span>. Replications are said to “succeed” when <span class="math inline">\(a_{\rm old}^D\)</span> and <span class="math inline">\(a_{\rm new}^D\)</span> are similar and to “fail” when they are not. Dichotomizing replication attempts into successes and failures is usually not that helpful, and it would be better to simply characterize how similar <span class="math inline">\(a_{\rm old}^D\)</span> and <span class="math inline">\(a_{\rm new}^D\)</span> are. Literal exact replication is impossible: at least some elements of M have changed between the first study and the replication. Specifying how they might have changed, e.g., how outcomes vary with time, will help judge differences observed between <span class="math inline">\(a_{\rm old}^D\)</span> and <span class="math inline">\(a_{\rm new}^D\)</span>.</p>
<p>Replication studies can benefit enormously from the knowledge gains produced by the original studies. For example, we learn a large amount about the model <em>M</em> and the value of the estimand <span class="math inline">\(a^m\)</span> from the original study. The <em>M</em> of the replication study can and should incorporate this new information. For example, if we learn from the original study that <span class="math inline">\(a^m\)</span> is positive, but it might be small, the replication study could respond by changing <em>D</em> to increase the sample size. Design diagnosis can help you learn about how to change the replication study’s design in light of the original research.</p>
<p>When changes to the data strategy <em>D</em> or answer strategy <em>A</em> can be made to produce more informative answers about the same inquiry <em>I</em>, exact replication may not be preferred. Holding the treatment and outcomes the same may be required to provide an answer to the same <em>I</em>, but increasing the sample size or sampling individuals rather than villages or other changes may be preferable to exact replication. Replication designs can also take advantage of new best practices in research design.</p>
<!-- When designing **original** studies, you should anticipate that someday your work will be replicated. To the extent that you want future replication studies to arrive at similar answers to the original study you produce (i.e., you want their $a_{\rm new}^D$ to match your $a_{\rm old}^D$ as closely as possible), you will want to choose designs that bring $a_{\rm old}^D$ as close to $a^M$ as possible, under the presupposition that faithful replicators will also design their studies in such a way that $a_{\rm new}^D$ will also be close to $a^M$. -->
<!-- Replication studies necessarily differ from original studies -- it is literally impossible to reproduce the exact conditions of the original study in the same way it's impossible to step in the same river twice. Another way of putting that same statement is that $D_{\rm new}$ is necessarily different from $D_{\rm old}$. Theory (i.e., beliefs about *M*) is the tool we use to say that $D_{\rm old}$ is similar enough to $D_{\rm new}$ to constitute a close enough replication study. As a concrete example, many survey experimental replications involve using the exact same experimental stimuli but changing the study sample, e.g., from a nationally representative sample to a convenience sample. -->
<p>So-called “conceptual” replications alter both <em>M</em> and <em>D</em>, but keep <em>I</em> and <em>A</em> as similar as possible. That is, a conceptual replication tries to ascertain whether a relationship in one context (<span class="math inline">\(I(M_{\rm old})\)</span>) also holds in a new context (<span class="math inline">\(I(M_{\rm new}\)</span>). The trouble and promise of conceptual replications lie in the designer’s success at holding <em>I</em> constant. Too often, a conceptual replication fails because in changing <em>M</em>, too much changes about <em>I</em>, muddying the “concept” under replication. The key is holding <em>I</em> fixed.</p>
<p>A summary function is needed to interpret the difference between <span class="math inline">\(a_{\rm old}^D\)</span> and <span class="math inline">\(a_{\rm new}^D\)</span>. This might take the new one and throw out the old if MIDA was poor in the first. It might be taking the average. It might be a precision-weighted average. Specifying this function ex-ante may be useful to avoid the choice of summary depending on the replication results. This summary function will be reflected in <em>A</em> and in the discussion section of the replication paper.</p>
<div id="example-15" class="section level3">
<h3>
<span class="header-section-number">25.5.1</span> Example:<a class="anchor" aria-label="anchor" href="#example-15"><i class="fas fa-link"></i></a>
</h3>
<p>Here we have an original study design with size 1000. The original study design’s true SATE is 0.2 because the original authors happened to study a very treatment-responsive population. We seek to replicate the original results, whatever they may be. We want to characterize the probability of concluding that we “failed” to replicate the original results. We have four alternative metrics for assessing this.</p>
<ol style="list-style-type: decimal">
<li><p>Are the original and replication estimates statistically significantly different from each other? If yes, we conclude that we failed to replicate the original results, and if no, we conclude that the study replicated.</p></li>
<li><p>Is the replication estimate within the original 95% CI?</p></li>
<li><p>Is the original estimate within the replication 95% CI?</p></li>
<li><p>Do we fail to affirm equivalence between the replication and original estimate, using a tolerance of 0.2?</p></li>
</ol>
<p>Figure @(fig:replications) shows that no matter how big we make the study, we find that the rate of concluding the difference-in-SATEs is nonzero only occurs about 10% of the time. Similarly, the replication estimate is rarely outside of the original confidence interval, because it’s rare to be more extreme than a wide confidence interval. The relatively high variance of the original study estimate means that it is so uncertain, it’s tough to distinguish it from any number in particular.</p>
<p>If we turn to the third metric, we become more and more likely to conclude that the study fails to replicate as the replication study’s size grows. At very large sample sizes, the replication confidence intervals become extremely small, so in the limit, it will always exclude the original study estimate.</p>
<p>The last metric, equivalence testing, has the nice property that as the sample size grows, we get closer to the correct answer – the true SATEs are indeed within 0.2 standard units together. However, again because the original study is so noisy, it is difficult to affirm its equivalence with anything, even when the replication study is quite large.</p>
<p>The upshot of this exercise is that, curiously, when original studies are weak (in that they generate imprecise estimates), it becomes <strong>harder</strong> to conclusively affirm that they did not replicate. This set of incentives is somewhat perverse: designers of original studies benefit from a lack of precision if it means they can’t “fail to replicate.”</p>
<div class="figure">
<span id="fig:replications"></span>
<img src="book_files/figure-html/replications-1.svg" alt="Rates of 'Failure to Replicate' according to four diagnosands" width="100%"><p class="caption">
Figure 25.2: Rates of ‘Failure to Replicate’ according to four diagnosands
</p>
</div>
<p><strong>Further reading</strong>.</p>
<ul>
<li>
<span class="citation">Clemens (<a href="references.html#ref-Clemens2017" role="doc-biblioref">2017</a>)</span> on distinctions between replication and reanalysis</li>
<li>
<span class="citation">Nosek et al. (<a href="references.html#ref-aac4716" role="doc-biblioref">2015</a>)</span> on estimating the reproducibility of psychological science, see <span class="citation">Gilbert et al. (<a href="references.html#ref-Gilbert1037" role="doc-biblioref">2016</a>)</span> for a response</li>
</ul>
<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book--><!-- start post here, do not edit above -->
</div>
</div>
<div id="meta-analysis" class="section level2">
<h2>
<span class="header-section-number">25.6</span> Meta-analysis<a class="anchor" aria-label="anchor" href="#meta-analysis"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<p>One of the last stages of the lifecycle of a research design is its eventual incorporation in to our common scientific understanding of the world. Research findings are synthesized into our broader scientific understanding through systematic reviews and meta analysis. In this section, we described how a research synthesis project itself comprises a new research design, whose properties we can investigate through declaration and diagnosis.</p>
<p>Research synthesis takes two basic forms. The first is meta-analysis, in which a series of answers <span class="math inline">\(a^d\)</span>s are analyzed together in order to better understand features of the distribution of answers obtained in the literature. Traditional meta-analysis focus on the average of k answers: <span class="math inline">\(a_1^{d_1}\)</span>,<span class="math inline">\(a_2^{d_2}\)</span>,…<span class="math inline">\(a_k^{d_k}\)</span>. Studies can be averaged together in ways that are better and worse. Sometimes the answers are averaged together according to their precision. A precision-weighted average gives more weight to precise estimates and less weight to studies that are noisy.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Precision-weighting is equivalent to fixed-effects meta-analysis. Random-effects meta-analysis also incorporates the precision of each study, but differs from fixed-effects meta analysis in its assumptions (in &lt;em&gt;M&lt;/em&gt;) about the true distribution of treatment effects. For much more on the distinctions between the two approaches, see CITE Hedges.&lt;/p&gt;"><sup>30</sup></a> Sometimes studies are “averaged” by counting up how many of the estimates are positive and significant, how many are negative and significant, and how many are null. This is the typical approach taken in a literature review. Regardless of the averaging approach, the goal of this kind of synthesis is to learn as much as possible about a particular inquiry <em>I</em> by drawing on evidence from many studies.</p>
<p>A second kind of synthesis is an attempt to bring together many <span class="math inline">\(a^d\)</span>s, each of which targets a different inquiry about a common model. This is the kind of synthesis that takes place across an entire research literature. Different scholars focus on different nodes and edges of the common model, so a synthesis needs to incorporate the diverse sources of evidence.</p>
<p>How can you best anticipate how your research findings will be synthesized? For the first kind of synthesis – meta-analysis – you must be cognizant of keeping a commonly understood <em>I</em> in mind. You want to select inquiries not for their novelty, but because of their commonly-understood importance. We want <em>many</em> studies on the effects of having women versus men elected officials on public goods because we want to understand this particular <em>I</em> in great detail and specificity. While the specifics of the model <em>M</em> might differ from study to study, the fact that the <em>I</em>s are all similar enough to be synthesized allows for a specific kind of knowledge accumulation.</p>
<p>For the second kind of synthesis – literature-wide progress on a full causal model – even greater care is required. Specific studies cannot make up bespoke models <em>M</em> but instead must understand how the specific <em>M</em> adopted in the study is a special case of some broader <em>M</em> that is in principle agreed to by a wider research community. The nonstop, neverending proliferation of study-specific theories is a threat to this kind of knowledge accumulation.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;&lt;span class="citation"&gt;McPhetres et al. (&lt;a href="references.html#ref-mcphetres_2020" role="doc-biblioref"&gt;2020&lt;/a&gt;)&lt;/span&gt; document that in a decade of research articles published in &lt;em&gt;Psychological Science&lt;/em&gt;, 359 specific theories were named, 70% were named only once and a further 12% were named just twice.&lt;/p&gt;'><sup>31</sup></a></p>
<p>Since either kind of synthesis is a research design of its own, declaring it and diagnosing its properties can be as informative. The data strategy for any research synthesis includes the process of collecting past studies. Search strategies are sampling strategies, and they can be biased in the same ways as convenience samples of individuals. Conducting a full census of past literature on a topic is usually not possible since not all research is made public, but selecting only published studies may reinforce publication biases. Proactively collecting working papers and soliciting unpublished or abandoned research on a topic are strategies to mitigate these risks. The choice of answer strategy for research synthesis depends on model assumptions about how studies are related. The model for declaring a research synthesis thus might include assumptions not only about how studies reach you as the synthesizer, but also how the contexts and units were selected in those original studies. Three common inquiries for meta-analysis include the average effect across contexts, the extent to which effects vary across contexts, and the best estimate of effects in specific contexts. Diagnosis can help assess the conditions under which your analysis strategies will provide unbiased, efficient estimates of true effects either in a subset of contexts which were studied or about a broader population.</p>
<p><strong>Further Reading</strong></p>
<ul>
<li>
<span class="citation">McPhetres et al. (<a href="references.html#ref-mcphetres_2020" role="doc-biblioref">2020</a>)</span> on the proliferation of theories in psychology</li>
<li>
<span class="citation">Samii (<a href="references.html#ref-Samii2016" role="doc-biblioref">2016</a>)</span> on the role of “causal empiricists,” as distinct from the role of theorists.</li>
</ul>
<!-- ^[Meta-analysis can sharpen individual study estimates, by leveraging information from other studies about how effects vary, as we describe in [a blog post](https://declaredesign.org/blog/meta-analysis-can-be-used-not-just-to-guess-about-effects-out-of-sample-but-also-to-re-evaluate-effects-in-sample.html).] --><!-- -- need to share materials --><!-- -- share all the data so meta-analyses can answer other questions from your data --><!-- -- meta-analysis is a design too!  --><!-- -- model: research designs of your constituent studies, including whether they have good designs! --><!-- -- inquiries: population average effect, best estimate of effects in specific contexts, how much effects vary across sites; effects need not be average effect, could be meta-analysis of heterogeneous effects --><!-- -- data strategy: how do you select studies, do you include only published studies, etc. --><!-- -- answer strategy:  --><!-- # ```{r} --><!-- # study_design_fixed <-  --><!-- #   declare_model(N = 100, U = rnorm(N), potential_outcomes(Y ~ 0.2 * Z + U)) +  --><!-- #   declare_inquiry(study_ATE = mean(Y_Z_1 - Y_Z_0)) +  --><!-- #   declare_assignment(Z = complete_ra(N, m = 50), legacy = FALSE) +  --><!-- #   declare_measurement(Y = reveal_outcomes(Y ~ Z)) +  --><!-- #   declare_estimator(Y ~ Z, model = difference_in_means) --><!-- #  --><!-- # draw_inquiries(study_design_fixed) --><!-- #  --><!-- # diagnose_design(study_design_fixed, sims = 500) --><!-- #  --><!-- # study_design_random <-  --><!-- #   declare_model(N = 100, U = rnorm(N), potential_outcomes(Y ~ rnorm(1, mean = 0.2, sd = 0.3) * Z + U)) +  --><!-- #   declare_inquiry(study_ATE = mean(Y_Z_1 - Y_Z_0)) +  --><!-- #   declare_assignment(Z = complete_ra(N, m = 50), legacy = FALSE) +  --><!-- #   declare_measurement(Y = reveal_outcomes(Y ~ Z)) +  --><!-- #   declare_estimator(Y ~ Z, model = difference_in_means) --><!-- #  --><!-- # draw_inquiries(study_design_random) --><!-- #  --><!-- # diagnose_design(study_design_random, sims = 500) --><!-- #  --><!-- #  --><!-- # model <- --><!-- #   declare_model(data = simulate_design(study_design_fixed, study_design_random, sims = 100)) --><!-- #  --><!-- # inquiry <-  --><!-- #   declare_inquiry(PATE = 0.2) --><!-- #  --><!-- # answer_strategy <-  --><!-- #   declare_estimator(handler = label_estimator(function(data){ --><!-- #     with(data, tidy(rma.uni(yi = estimate, sei = std.error, subset = design_label == "study_design_fixed", method = "REML"), conf.int = TRUE)) --><!-- #   }), label = "dFE-eRE", inquiry = "PATE") + --><!-- #   declare_estimator(handler = label_estimator(function(data){ --><!-- #     with(data, tidy(rma.uni(yi = estimate, sei = std.error, subset = design_label == "study_design_random", method = "REML"), conf.int = TRUE)) --><!-- #   }), label = "dRE-eRE", inquiry = "PATE") +  --><!-- #   declare_estimator(handler = label_estimator(function(data){ --><!-- #     with(data, tidy(rma.uni(yi = estimate, sei = std.error, subset = design_label == "study_design_fixed", method = "FE"), conf.int = TRUE)) --><!-- #   }), label = "dFE-eFE", inquiry = "PATE") +  --><!-- #   declare_estimator(handler = label_estimator(function(data){ --><!-- #     with(data, tidy(rma.uni(yi = estimate, sei = std.error, subset = design_label == "study_design_random", method = "FE"), conf.int = TRUE)) --><!-- #   }), label = "dRE-eFE", inquiry = "PATE") --><!-- #  --><!-- # design <- model + inquiry + answer_strategy --><!-- # ``` --><!-- #  --><!-- # ```{r} --><!-- # dat <- draw_data(design) --><!-- #  --><!-- # est <- get_estimates(design, data = dat) %>%  --><!-- #   mutate(y = if_else(str_detect(estimator_label, "eRE"), 0.07, -0.07), --><!-- #          plot_label = if_else(str_detect(estimator_label, "eRE"), "RE", "FE"), --><!-- #          design_label = if_else(str_detect(estimator_label, "dRE"), "study_design_random", "study_design_fixed")) --><!-- #  --><!-- # dat %>%  --><!-- #   ggplot(aes(x = estimate, y = 0, size = 1/std.error)) +  --><!-- #   geom_jitter(height = 0.02, width = 0, alpha = 0.5) +  --><!-- #   geom_errorbarh(data = est, aes(y = y, xmin = conf.low, xmax = conf.high), size = 0.5, height = 0.005) +  --><!-- #   geom_point(data = est, aes(y = y), size = 2.5) +  --><!-- #   geom_text(data = est, aes(y = y, label = plot_label), nudge_y = 0.015, size = 4) +  --><!-- #   coord_cartesian(ylim = c(-0.1, 0.1)) +  --><!-- #   facet_grid(design_label~ .) --><!-- # ``` --><!-- #  --><!-- # ```{r, echo = FALSE, eval = do_diagnosis} --><!-- # diagnosis <- diagnose_design(design, sims = sims) --><!-- # ``` --><!-- #  --><!-- # ```{r, echo = FALSE, purl = FALSE} --><!-- # # figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file --><!-- # rds_file_path <- paste0(get_dropbox_path("19_Synthesis"), "/diagnosis.RDS") --><!-- # if (do_diagnosis & !exists("do_bookdown")) { --><!-- #   write_rds(diagnosis, file = rds_file_path) --><!-- # } --><!-- # diagnosis <- read_rds(rds_file_path) --><!-- # ``` --><!-- #  --><!-- # ```{r} --><!-- # diagnosis %>%  --><!-- #   get_diagnosands %>%  --><!-- #   transmute( --><!-- #     model = if_else(str_detect(estimator_label, "dRE"), "RE", "FE"), --><!-- #     estimator = if_else(str_detect(estimator_label, "eRE"), "RE", "FE"), --><!-- #     bias, coverage --><!-- #   ) %>%  --><!-- #   kable(digits = 2) --><!-- # ``` --><!-- #  --><!-- A research synthesis is a "meta MIDA" --><!-- M: A model that subsumes portions of the sub Ms --><!-- I: This is a summary of all of the Is across the studies --><!-- D: This is the inclusion / exclusion criteria. Transformations of the study data. standardization. (sampling, measurement.) --><!-- A: things like random effects or fixed effects --><!-- $I_1 \approx I_2 \approx I_3$ --><!-- Not --><!-- $a^M_1 \approx a^M_2 \approx a^M_3$ --><!-- Meta-analysis can be used not just to guess about effects out-of-sample but also to re-evaluate effects in sample: https://declaredesign.org/blog/2018-12-11-meta-analysis.html --><!-- - don't select on DV --><!-- - select on high quality MIDAs (drop those with bias) --><!-- - precision weighting (accounting for the quality of the design indirectly!) --><!-- ## grab bag --><!-- -- systematic reviews are sign and significance, meta-analysis are point estimates --><!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book--><!-- start post here, do not edit above -->
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="realization.html"><span class="header-section-number">24</span> Realization</a></div>
<div class="next"><a href="part-iv-exercises.html"><span class="header-section-number">26</span> Part IV Exercises</a></div>
</div></main><div id="on-this-page-nav" class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#integration"><span class="header-section-number">25</span> Integration</a></li>
<li><a class="nav-link" href="#communicating"><span class="header-section-number">25.1</span> Communicating</a></li>
<li><a class="nav-link" href="#decisionmaking"><span class="header-section-number">25.2</span> Decisionmaking</a></li>
<li><a class="nav-link" href="#archiving"><span class="header-section-number">25.3</span> Archiving</a></li>
<li><a class="nav-link" href="#reanalysis"><span class="header-section-number">25.4</span> Reanalysis</a></li>
<li>
<a class="nav-link" href="#replication"><span class="header-section-number">25.5</span> Replication</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#example-15"><span class="header-section-number">25.5.1</span> Example:</a></li></ul>
</li>
<li><a class="nav-link" href="#meta-analysis"><span class="header-section-number">25.6</span> Meta-analysis</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="download-code" href="./integration.R"><i class="far fa-file-code"></i> Download R code</a></li>
          
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Research Design: Declare, Diagnose, Redesign</strong>" was written by Graeme Blair, Alexander Coppock, and Macartan Humphreys. </p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
