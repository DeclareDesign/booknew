[
["index.html", "Research Design: Declare, Diagnose, Redesign Welcome", " Research Design: Declare, Diagnose, Redesign Graeme Blair, Jasper Cooper, Alexander Coppock, and Macartan Humphreys Welcome Draft book manuscript. Comments welcomed. Please treat with caution. Copyright 2020 Graeme Blair, Jasper Cooper, Alexander Coppock, and Macartan Humphreys "],
["preamble.html", "Chapter 1 Preamble", " Chapter 1 Preamble With this book, we introduce a new way of thinking about research designs in the social sciences. We hope this way of thinking will make research designs more transparent and more robust. But we also hope it will make research design easier: easier to produce good designs, but also easier to share designs and build off of the designs that others have developed. The core idea is to start to think of a design as an object that can be interrogated. The object has four main characteristics, and we have to understand each of the four and also how they relate to one another. The design encodes your beliefs about the world, it describes your questions, and it lays out how you go about answering those questions, in terms both of what data you use and how you use it. A key idea is that all of these features can be provided in code and – if done right – the information provided is enough to estimate the quality of the design through simulation. We think this way of thinking about research design pays dividends at every point through the research lifecycle. Choosing a question, synthesizing previous answers, conducting an ethics review, piloting parts of the empirical strategy, crafting a preanalysis plan, implementing the study, summarizing the results, writing the paper, publishing the result, archiving the materials, engaging with critical scholarship — a clear understanding of research design is immensely value throughout every stage of the process. We think all empirical research designs can be described in terms of a model of the world, an inquiry about the world, a data strategy to gather information about the world, and an answer strategy that summarizes the collected data. If you can articulate those four aspects of the design, every part of the research process becomes clearer, easier, and better for accumulating scientific knowledge.\n"],
["how-to-read-this-book.html", "1.1 How to read this book", " 1.1 How to read this book We had multiple audiences in mind when writing this book. First, we’re thinking of college seniors who have to produce a course research paper. These students need a framework for thinking about the ways each part of the research process fit together. We’re also thinking of graduate students in seminar courses where the main purpose of the course is to read papers and discuss how well the theory matches the empirics. The MIDA framework introduced in Part I is a way to structure both tasks that accommodates many different empirical approaches: qualitative and quantitative, descriptive and causal, observational and experimental. If we only had 30 minutes with a person to try and get them to understand what our book is about, we would give them Part I. Part II is more involved. We provide the mathematical foundations of the MIDA framework. We walk though each component of a research design in detail: Models, Inquiries, Data strategies, and Answer Strategies. We describe the finer points of research design diagnosis and how to carry out a redesign. We imagine that Part II could be assigned early in a graduate course on research design in any of the social sciences. In Part III, we apply the general framework to specific research designs. The result is a library of common research designs. Many empirical research designs are included in the library, but not all. The set of entries covers a large portion of what we see in current empirical practice, but it is not meant to be exhaustive. We are thinking of two kinds of audiences for the design library. The first is the researcher who wants to know how a specific research design operates and what to watch out for. They turn to the ``regression discontinuity’’ entry and immediately learn the ten most important things to know about the design. These 10 most important things are not everything there is to know about the design. For that, we refer at the end of each entry to a set of the most up-to-date methodological treatments of the topic. The entry will help you simulate the design’s properties and explore design tradeoffs before implementing it. The second reader we have in mind is a person who is studying the entry not to learn the specifics of the regression discontinuity design necessarily, but instead to learn to become a research designer. The regression discontinuity entry is an instance of a research design from which general lessons about research design can be drawn. Readers who want to be research designers should read through the design library in its entirety. The last section of the book describes in detail how our framework can help at each step of the research process. Each of these sections of the book should be readable for anyone who has read Part I. The entry on preanalysis plans can be assigned in an experiments course as guidance for students filing their first PAP. The entry on research ethics could be shared among coauthors at the start of a project. The entry on writing a research paper could be assigned to college seniors trying to finish their essay on time. "],
["how-to-work-this-book.html", "1.2 How to work this book", " 1.2 How to work this book At many times throughout the book, we will describe research designs not just in words, but in computer code. You can read all of Part I without any coding, but some aspects of Part II and Part III are enhanced by engaging with the code. If you want to work though all of the code and exercises, fantastic. This path requires investment in R, the tidyverse and the DeclareDesign set of software packages. We think it is very rewarding, but we understand that there is a learning curve. You could, of course, tackle the declaration, diagnosis, and redesign processes using bespoke simulations in any computer language you like ,1 but it is easier in DeclareDesign. If you want nothing to do with the code, you can skip all the code and exercises to just focus on the text. We have self-consciously written the book so that understanding of the code is not required in order to understand research design concepts. If you choose this path, we promise our way of thinking about research design is useful even without any code. The free, online version of this book has many extra ways to engage with the material. You can download any section of code and you can play with interactive simulations. These bells and whistles are not necessary for understanding, but we all learn in different ways. On our Web site, we provide examples in R, Python, Stata, and Excel.↩ "],
["what-this-book-will-not-do.html", "1.3 What this book will not do", " 1.3 What this book will not do This is a research design textbook, not a statistics textbook. We will not derive estimators, we will provide no guarantees of the general optimality of designs, and we will present no mathematical proofs. By the same token – this is a research design textbook, not a handbook of best practices for all research designs. We can’t tell you the best thing to do in your particular research scenarios because we know far far less about your research than you do – you have to learn for yourself what the most important design considerations will be. What we do have is language to express research designs. We can help you to learn that language so you can describe your own design in it. When you can declare your design in this language, then you can diagnose it, then improve it through redesign. "],
["improving-research-designs.html", "Chapter 2 Improving research designs", " Chapter 2 Improving research designs This book offers a language for describing research designs and an algorithm for assessing their properties. Together, the language and the algorithm help researchers address two main problems. First, they have to select high quality research designs that can be relied upon to generate credible answers to their research questions. Without a way to measure the quality of design, it’s very difficult to choose strong ones over weak ones. Second, researchers need to communicate their designs to others. Without a way to describe a design in detail, it’s very difficult to explain to other scholars why they are high quality and why they are the right design for the question. Our language for research design helps with both problems: once a design is “declared” in simple enough language that a computer can understand it, then we can implement our “diagnosis” algorithm for assessing the properties of a design at the click of a button. This makes it easy to compare among alternative designs and pick a strong one. The same language we use to talk to the computer can be used to talk to others: reviewers, advisors, students, funders, journalists, and the public – and yes the computer too – need to know four basic things to understand your design.\n"],
["the-four-components-of-research-design.html", "2.1 The four components of research design", " 2.1 The four components of research design Empirical research designs share in common that they all have a model, an inquiry, a data strategy, and an answer strategy. The model is your theory of the system under study. It includes causal beliefs about what causes what and why. It includes beliefs about how important variables are distributed, how things are correlated, the sequences of events. Even though we don’t know for sure that every part of our model is true, we nevertheless it is. As research designers, we often have to proceed under difficult or impossible to verify assumptions about our theories for the plain reason that so little about the social world is known for sure. The model is the research question. The research question – what we’re calling the inquiry – is a feature of the model we want to measure. Our theories are rich, so in a single model, there may be many possible inquiries that a researcher could seek to learn about. A model may be a complex dance of 10 or more interrelated variables, but an inquiry is something like the average causal effect of a single variable on another, the decripitive distribution of a third variable, or a prediction about the value a single variable will take on some time in the future. Together, the model and data form the theoretical half of a research design. The second half is empirical, and its two components mirror those in the theoretical half. The data strategy is the full set of procedures we use to gather information from the world. This includes three basic sets of procedures: sampling, assignment, and measurement. Sampling refers to the fact that no empirical strategy is comprehensive – some units are sampled into the study and some units aren’t. Even research designs (like the census, for example) have a sampling strategy in that they don’t sample respondents in different years or in different countries. Assignment procedures describe how researchers generate variation in the world. If you ask some subjects one question, but other subjects a different question, you’ve generated variation on the basis of an assignment procedure. We think of assignment procedures most often when they are randomized, as in an randomized experiment, but many kinds of research designs engage in assignment procedures that are not randomized. Measurement procedures are the ways in which researchers reduce the complex and multidimensional social world in to a relatively parismonious set of data. These data need not be “quantitative data” in the sense of being numbers or values on a pre-defined scale – qualitative data are data too – but measurement is the vexing but necessary reduction of reality to a few choice representations. Measurement always carries with the possibility of measurement error, because this reduction is hard. The answer strategy is how you summarize the data that the data strategy produces. Just like the inquiry summarizes a part of the model, the answer strategy summarizes the data. Complex, multidimensional datasets don’t just speak for themselves – they need to be summarized and explained. You can think of answer strategy as function that takes in data and returns answers. For some research designs, this is a literal function like lm_robust that estimates an ordinary least squares regression with robust standard errors. For some research designs, the function is embodied by the researcher themself when they read case study documents and summarize their meaning. The theoretical and empirical halves of research design go hand-in-hand and no description of a research design is complete unless both halves have been communicated. If someone asks, “What’s your research design?” and you respond “It’s an regression discontinuity design,” we’ve maybe learned what your answer strategy might be, but we don’t yet have enough information to decide whether it’s a strong design until we learn more about the model and the inquiry and whether the data and answer strategies are indeed well suited for them. a data strategy to intervene in the world to learn an answer to the inquiry, including interventions that are implemented but also measurement that is collected, and an answer strategy that defines how we construct an answer to the inquiry from the data that results from implementing our data strategy. The four together, which we refer to as MIDA, represent both your suppositions about how the world works and the choices you make as the researcher to intervene in and learn about the world. The model defines a set of units, people or neighborhoods or social groups, that we wish to study. Often, this set of units is a large population of units we cannot afford to take measurements of, but we can nevertheless define and make inferences about through sampling and studying a subset of its units. The model then includes a set of baseline characteristics that describe each unit and the probability distributions of each characteristic (i.e., are heights normally distributed, or is there skew that comes from stunting in infants). Finally, the model includes a set of endogenous outcome variables that may be functions of exogenous (pretreatment) characteristics and the effects of interventions. Each endogenous outcome variable has a function that defines the variables that affect what values it takes on. When an outcome depends on an intervention, it will be a potential outcome, where we can define what value the outcome would take on if a unit received the treatment and what outcome that unit would take on if it did not receive the treatment. Typically, endogenous outcome variables are random variables, either because it is a function of an exogenous baseline variable for which we defined a probability distributions or because whether a unit is assigned to treatment or control is randomly assigned as part of the data strategy. Defining the model can feel like an odd exercise. Since researchers presumably want to learn about the model, declaring it in advance may seem to beg the question. Yet declaring a model is often unavoidable when declaring reserach designs. In practice, doing so is already familiar to any researcher who has calculated the power of a design, which requires the specification of effect sizes. The seeming arbitrariness of the declared model can be mitigated by assessing the sensitivity of diagnosis to alternative models and strategies (see Section XX). Further, researchers can inform their substantive models with existing data, such as baseline surveys. Just as power calculators focus attention on minimum detectable effects, design declaration offers a tool to demonstrate design properties and how they change depending on researcher assumptions. The second component of a research design is the inquiry, often known as the estimand. This is a quantity that represents the true answer to the research question we are asking. Inquiries may be causal, as in the average treatment effect, or descriptive, as in the proportion of units who hold a certain characteristic. An inquiry is a function of the exogenous characteristics of units, of endogenous outcome variables, or both. It may be defined over all units in the population defined by the model, as in the average treatment effect for all units, or it may be defined over a subset of units, as in the conditional average treatment effect for women. Because we defined the distribution of the variables in the model, we can define the probability distribution of inquiries, which are a function of those variables. The data strategy defines how the researcher, along with research partners, intervenes in the world to generate an answer to the question posed by the inquiry. The researcher must select a sampling strategy for measurement, which could be to take measurements of all units or to first sample a subset of units. It includes the treatments and treatment assignment procedures. And it includes the measurement procedure itself, defining the set of survey questions or administrative data that will be collected from selected units. In short, the data strategy is everything the researcher does to obtain a set of data or observations used to answer the inquiry about the world. With the data that results from the data strategy, the answer strategy defines a set of procedures the researcher uses to translate the data into an answer to the inquiry. It is not simply the choice of an estimator, such as OLS or logit, but the full set of procedures from receiving the dataset to providing the answer in words, tables, and graphs. This includes data cleaning, data transformation, estimation, plotting, and interpretation. Not only the choice of OLS must be defined, but that we will focus attention on the coefficient estimate from the Z variable and assess uncertainty using a confidence interval and construct a coefficient plot in a certain way to visualize the inference. The answer strategy also includes all of the if-then procedures that researchers implicitly or explicitly take depending on initial results and features of the data. In a stepwise regression procedure, the answer strategy is not the final regression that results from iterative model selection, but that procedure itself – because the answer will depend on features of that change depending on sampling variability. Just like the values of the inquiry, the values of the estimates that result from the answer strategy have a probability distribution, because they are the result of the variables defined in the model (which have probability distributions) and the data strategy (sampling and treatment assignment are defined by probability distributions). Declaring a design entails separating out which parts of your idea belong in \\(M\\), \\(I\\), \\(D\\), and \\(A\\). The declaration process can be a challenge because mapping your ideas and excitement about your project into MIDA is not always straightforward. We promise it is a rewarding task. When you can express your research design in terms of these four components you are newly able to think about its properties. 2.1.1 Example We illustrate the application of the MIDA framework to a study of the motivations of political office-seekers in Pakistan. Gulzar and Khan (n.d.) conducted a experiment to test whether prosocial or personal benefits were more important motivations for running for office. They randomly-assigned villages to receive different encouragements to run and measured how this affected the rate of running for office, the types of people who chose to run, and the congruence of their policy positions with the general population. The model describes the citizens in the villages they conduct the study, and which village they live in. It describes both their individual characteristics and their potential outcomes in response to the possible treatment assignments. The inquiry is the average treatment effect between receiving and not receiving an encouragement to run for office. The data strategy is (1) randomly sampling 50 citizens per village; (2) random assignment of villages into either an encouragement to run for office (treatment) or no encouragement (control); and (3) measurement via a posttreatment survey. The answer strategy is calculating the difference-in-means, with standard errors accounting for the clustering of the treatment assignment by village. References "],
["declaring-a-design-in-code.html", "2.2 Declaring a design in code", " 2.2 Declaring a design in code You can implement the MIDA framework in any software package. Indeed, a design could be declared in writing or mathematical notation and then diagnosed using analytical formula.2 Social scientists use a number of tools for conducting statistical analysis: Stata, R, Python, Julia, SPSS, SAS, Mathematica, and more. Stata and R are most commonly used. We wrote the companion software to the book, DeclareDesign, in the R statistical environment because of the availability of other tools for implementing research designs and because it is free-to-use. To illustrate declaring a design in code, we declare a simplified version of the Gulzar and Khan (n.d.) study. (In Part II we declare the complete design.) # we should turn this into a picture labeling MIDA simple_design &lt;- # M: model # 50 citizens in each of 100 villages declare_population( # 100 villages villages = add_level(N = 100, N_citizens_per_village = sample(20:100, N, replace = TRUE)), # 50 citizens in each village citizens = add_level(N = N_citizens_per_village, u = rnorm(N)) ) + # two potential outcomes, Y_Z_0 and Y_Z_1 # Y_Z_0 is the control potential outcome (what would happen if the unit is untreated) # it is equal to the unobserved shock &#39;u&#39; # Y_Z_1 is the treated potential outcome # it is equal to the control potential outcome plus a treatment effect of 0.25 declare_potential_outcomes( Y_Z_0 = u, Y_Z_1 = Y_Z_0 + 0.25) + # I: inquiry # we are interested in the average treatment effect in the population declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + # D: data strategy # sampling: we randomly sample 50 of the 100 villages in the population declare_sampling(n = 50, clusters = villages) + # assignment: we randomly assign half of the 50 sampled units to treatment (half to control) declare_assignment(prob = 0.5, clusters = villages) + # measurement: construct outcomes from the potential outcomes named Y depending on # the realized value of their assignment variable named Z # we measure a binary outcome Yobs from the unobserved, latent variable Y declare_measurement( Yobs = case_when( Z == 1 &amp; Y_Z_1 &gt; 0 ~ 1, Z == 1 &amp; Y_Z_1 &lt;= 0 ~ 0, Z == 0 &amp; Y_Z_0 &gt; 0 ~ 1, Z == 0 &amp; Y_Z_0 &lt;= 0 ~ 0) ) + declare_reveal(outcome_variables = Y, assignment_variables = Z) + # A: answer strategy # calculate the difference-in-means of Y depending on Z # we link this estimator to ATE because this is our estimate of our inquiry declare_estimator(Y ~ Z, clusters = villages, model = difference_in_means, estimand = &quot;ATE&quot;) For each design that we declare in the book, we will also present a graphical representation of the design. Below, we visualize the simplified form of the Gulzar Khan design: With the declared design, you can learn about it in a number of ways. You can “diagnose” or simulate its properties via diagnose_design(design), you can simulate mock data from it to explore possible estimation strategies before analyzing the real data with draw_data(design), you can obtain simulated estimates via draw_estimates(design), and after the data comes in you can apply the planned design to your data via get_estimates(design, data = study_data). We introduce the software in more detail in the next section. We have designed the rest of the book so that it can be read even if you do not use R, but you will have to translate the code into your own language of choice. On our Web site, we have a translation of parts of the declaration and diagnosis process into Stata, Python, and Excel. In addition, we link to a “Design wizard” that lets you declare and diagnose variations of standard designs via a web interface. References "],
["assessing-research-design-quality-design-diagnosis.html", "2.3 Assessing research design quality: design diagnosis", " 2.3 Assessing research design quality: design diagnosis Once you’ve declared your design, you can diagnose it. Design diagnosis is the process of simulating your research design in order to understand the range of possible ways the study could turn out. It is in the diagnosis stage that we define the design properties that are most desirable in our research setting. We let computers do the simulations for us because imagining how design choices influence sampling distributions is — to put it lightly — cognitively demanding. Diagnosis is an opportunity to write down what would make the study a success. For a long time, researchers have classified studies as successful or not based on statistical significance. Accordingly, statistical power (the probability of a statistically significant result) has been the most front-of-mind diagnosand when researchers have set to designing studies. As we learn more about the pathologies of relying on the statistical significance, we learn that diagnosands beyond power are just as, if not more important. For example, the “credibility revolution” throughout social science has trained a laser-like focus on the bias diagnosand. Studies are coming under new criticism for lacking “strong identification,” which usually implies that the data and answer strategies could lead to biased answers depending on how incorrect the model is. Randomized experimentation promises unbiased answers, at least when the data and answer strategies are implemented well. Design diagnosis relies on two further concepts, both functions of research designs. These are quantities that a researcher or a third party could calculate with respect to a design. The first is a diagnostic statistic, which is a summary statistic generated from a “run” of a design—that is, the results given a possible realization of variables, given the model and data strategy. For example the statistic: \\(e=\\) “difference between the estimated and the actual average treatment effect” depends on the model (since the ATE depends on the model’s assumptions about potential outcomes). The statistic \\(s = \\mathbb{1}(p \\leq 0.05)\\), interpreted as “the result is considered statistically significant at the 5% level,” does not depend on the model but it does presuppose an answer strategy that reports a \\(p\\)-value. Diagnostic statistics are governed by probability distributions that arise because both the model and the data generation, given the model, may be stochastic. Second, a diagnosand is a summary of the distribution of a diagnostic statistic. For example, is average value of the \\(e\\) statistic and is is the average value of the \\(s\\) statistic. Other diagnosands include things like root-mean-squared-error (RMSE), Type I, Type II, Type M, and Type S error rates. One especially important diagnosand is the “success rate,” which is the average value of the “success” diagnostic statistic. As a researcher, you get to decide what would make your study a success. What matters most in your research scenario? Is it statistical significance? If so, optimize your design with respect to power. Is what matters most in your research setting with the answer has the correct sign or not? Then diagnose how frequently your answer strategy yields an answer with the same sign as your inquiry. Diagnosis is an opportunity for you to articulate what would make your study a success and then to simulate how often you obtain that success. To diagnose the design, we first define a set of diagnosands (see Section XX), which are statistical properties of the design. In this case, we select the bias (difference between the estimate and the estimand, which is the PATE); the root mean-squared error; and the statistical power of the design. # Select diagnosands simple_design_diagnosands &lt;- declare_diagnosands(select = c(bias, rmse, power)) We then diagnose the design, which involves simulating the design and again and again, and then calculate the diagnosands based on the simulations data. # Diagnose the design simple_design_diagnosis &lt;- diagnose_design(simple_design, diagnosands = simple_design_diagnosands, sims = 500) estimand_label estimator_label bias rmse power ATE estimator -0.0001358 0.0373301 1 "],
["redesign.html", "2.4 Redesign", " 2.4 Redesign The subtitle of this book is “Declaration, Diagnosis, Redesign” to emphasize three important steps in the conceptualization of a research design. So far, we’ve outlined the first two points: declaration and diagnosis. Once your design has been declared, and you have learned to diagnose it with respect to the most important diagnosands, the last step is redesign. Redesign entails playing with each of the design parameters to understand the implications of each for your most important diagnosands. This can mean a variety of things. Many diagnosands (power, RMSE) depend on the size of the study. We can redesign the study, varying the “sample size” feature of the data strategy to determine how big it needs to be to achieve a target diagnosand: 90% power, say, or an RMSE of 0.02. We could also vary an aspect of the answer strategy, say, the covariates used to adjust a regression model. Sometimes the changes to the data and answer strategies interact: if we use better covariates to increase the precision of the estimates in the answer strategy, we have to collect that information as a part of the data strategy. The redesign question now becomes, is it better to collect pre-treatment information from all subjects or is the money better spent on increasing the total number of subjects? Finally, redesign sometimes means changing the model. That is, sometimes we want to understand whether our design yields the right inferences even when the underlying data generating processes shift beneath our feet. In summary, redesign entails enumerating a set of possible designs given resource and theoretical constraints then picking the best one. In DeclareDesign, the redesign() function replaces key inputs to the design to form a new design. "],
["relationship-to-other-ways-of-thinking-about-research-design.html", "2.5 Relationship to other ways of thinking about research design", " 2.5 Relationship to other ways of thinking about research design [This UNWRITTEN secion should be a bit of a history of how different fields think about research designs, including beyond political science ] We build on two influential research design frameworks. (King, Keohane, and Verba 1994, 13) enumerate four components of a research design: a theory, a research question, data, and an approach to using the data. Geddes (2003) articulates the links between theory formation, research question formulation, case selection and coding strategies, and strategies for case comparison and inference. In both cases, the set of components are closely aligned to those in the framework we propose. References "],
["putting-designs-to-use.html", "2.6 Putting designs to use", " 2.6 Putting designs to use In this book, we are asking that scholars add a new step to their workflow. We want scholars to formally declare and diagnose their research designs both in order to learn about them and to improve them. Much of the work of declaring and diagnosing designs is already part of how social scientists conduct research: grant proposals, IRB protocols, preanalysis plans, and dissertation prospectuses contain design information and justifications for why the design is appropriate for the question. The lack of a common language to describe designs and their properties, however, seriously hampers the utility of these practices for assessing and improving design quality. We hope that the inclusion of a declaration-diagnosis-redesign step to the research process can help address this basic difficulty. We outline three phases of the scientific process during which the MIDA and declaration-diagnosis-redesign framework can assist study authors, readers, and research funders. Making design choices. The move towards increasing credibility of research in the social sciences places a premium on considering alternative data strategies and analysis strategies at early stages of research projects, not only because it reduces researcher discretion, but more importantly because it can improve the quality of the final research design. While there is nothing new about the idea of determining features such as sampling and estimation strategies ex ante, in practice many designs are finalized late in the research process, after data are collected. Frontloading design decisions is difficult not only because existing tools are rudimentary and often misleading, but because it is not clear in current practice what features of a design must be considered ex ante. We provide a framework for identifying which features affect the assessment of a design’s properties, declaring designs and diagnosing their inferential quality, and frontloading design decisions. Declaring the design’s features in code enables direct exploration of alternative data and analysis strategies using simulated data; evaluating alternative strategies through diagnosis; and exploring the robustness of a chosen strategy to alternative models. Researchers can undertake each step before study implementation or data collection. Communicating design choices. Bias in published results can arise for many reasons. For example, researchers may deliberately or inadvertently select analysis strategies because they produce statistically significant results. Proposed solutions to reduce this kind of bias focus on various types of preregistration of analysis strategies by researchers (Rennie 2004; Zarin and Tse 2008; Casey, Glennerster, and Miguel 2012; Nosek et al. 2015; Green and Lin 2016). Study registries are now operating in numerous areas of social science, including those hosted by the American Economic Association, Evidence in Governance and Politics, and the Center for Open Science. Bias may also arise from reviewers basing publication recommendations on statistical significance. Results-blind review processes are being introduced in some journals to address this form of bias (e.g. Findley et al. 2016). However, the effectiveness of design registries and results-blind review in reducing the scope for either form of publication bias depends on clarity over which elements must be included to describe the design. In practice, some registries rely on checklists and preanalysis plans exhibit great variation, ranging from lists of written hypotheses to all-but-results journal articles. In our view, the solution to this problem does not lie in ever-more-specific questionnaires, but rather in a new way of characterizing designs whose analytic features can be diagnosed through simulation. The actions to be taken by researchers are described by the data strategy and the answer strategy; these two features of a design are clearly relevant elements of a preregistration document. In order to know which design choices were made ex ante and which were arrived at ex post, researchers need to communicate their data and answer strategies unambiguously. However, assessing whether the data and answer strategies are any good usually requires specifying a model and an inquiry. Design declaration can clarify for researchers and third parties what aspects of a study need to be specified in order to meet standards for effective preregistration. Declaration of a design in code also enables a final and infrequently practiced step of the registration process, in which the researcher ``reports and reconciles’’ the final with the planned analysis. Identifying how and whether the features of a design diverge between ex ante and ex post declarations highlights deviations from the preanalysis plan. The magnitude of such deviations determines whether results should be considered exploratory or confirmatory. At present, this exercise requires a review of dozens of pages of text, such that differences (or similarities) are not immediately clear even to close readers. Reconciliation of designs declared in code can be conducted automatically, by comparing changes to the code itself (e.g., a move from the use of a stratified sampling function to simple random sampling) and by comparing key variables in the design such as sample sizes. Challenging Design Choices. The independent replication of the results of studies after their publication is an essential component of the shift toward more credible science. Replication — whether verification, reanalysis of the original data, or reproduction using fresh studies — provides incentives for researchers to be clear and transparent in their analysis strategies, and can build confidence in findings.3 In addition to rendering the design more transparent, design declaration can allow for a different approach to the re-analysis and critique of published research. A standard practice for replicators engaging in reanalysis is to propose a range of alternative strategies and assess the robustness of the data-dependent estimates to different analyses. The problem with this approach is that, when divergent results are found, third parties do not have clear grounds to decide which results to believe. This issue is compounded by the fact that, in changing the analysis strategy, replicators risk departing from the estimand of the original study, possibly providing different answers to different questions. In the worst case scenario, it can be difficult to determine what is learned both from the original study and from the replication. A more coherent strategy facilitated by design simulations would be to use a design declaration to conduct “design replication.” In a design replication, a scholar restates the essential design characteristics to learn about what the study could have revealed, not just what the original author reports was revealed. This helps to answer the question: under what conditions are the results of a study to be believed? By emphasizing abstract properties of the design, design replication provides grounds to support alternative analyses on the basis of the original authors’ intentions and not on the basis of the degree of divergence of results. Conversely, it provides authors with grounds to question claims made by their critics. References "],
["avoiding-declaration-and-diagnosis-pitfalls.html", "2.7 Avoiding declaration and diagnosis pitfalls", " 2.7 Avoiding declaration and diagnosis pitfalls Declaring a design is just like writing out a recipe. You can cook without writing out a recipe, but when you do, you can think through the whole process start to finish, you can critique the process, and you can modify it. That said, designing high quality research is difficult and comes with many pitfalls, only a subset of which are addressed by the MIDA framework. Others we fail to help with entirely and, in some cases, we may even exacerbate them. We outline four concerns. The first is the worry that evaluative weight could get placed on essentially meaningless diagnoses. Given that design declaration includes declarations of conjectures about the world it is possible to choose inputs so that a design passes any diagnostic test set for it. For instance, a simulation-based claim to unbiasedness that incorporates all features of a design is still only good with respect to the precise conditions of the simulation (in contrast, analytic results, when available, may extend over general classes of designs). Still worse, simulation parameters might be selected because of their properties. A power analysis, for instance, may be useless if implausible parameters are chosen to raise power artificially. While MIDA may encourage more honest declarations, there is nothing in the framework that enforces them. As ever, garbage-in, garbage-out. Second, we see a risk that research may get evaluated on the basis of a narrow, but perhaps inappropriate set of diagnosands. Statistical power is often invoked as a key design feature – but even well-powered studies that are biased away from their targets of interest are of little theoretical use. The appropriateness of the diagnosand depends on the purposes of the study. As MIDA is silent on the question of a study’s purpose, it cannot guide researchers or critics to the appropriate set of diagnosands by which to evaluate a design. An advantage of the approach however is that the choice of diagnosands gets highlighted and new diagnosands can be generated in response to substantive concerns. Third, emphasis on the statistical properties of a design can obscure the substantive importance of a question being answered or other qualitative features of a design. A similar concern has been raised regarding the ``identification revolution’’ where a focus on identification risks crowding out attention to the importance of questions being addressed . Our framework can help researchers determine whether a particular design answers a question well (or at all), and it also nudges them to make sure that their questions are defined clearly and . It cannot, however, help researchers choose good questions. Finally, we see a risk that the variation in the suitability of design declaration to different research strategies may be taken as evidence of the relative superiority of different types of research strategies. While we believe that the range of strategies that can be declared and diagnosed is wider than what one might at first think possible, there is no reason to believe that all strong designs can be declared either ex ante or ex post. An advantage of our framework, we hope, is that it can help clarify when a strategy can or cannot be completely declared. When a design cannot be declared, nondeclarability is all the framework provides, and in such cases we urge caution in drawing conclusions about design quality. MIDA captures the analysis-relevant features of a design, but it does not describe substantive elements, such as how theories are derived or interventions are implemented. Yet many other aspects of a design that are not explicitly labeled in these features enter into this framework if they are analytically relevant. For example, logistical details of data collection such as the duration of time between a treatment being administered and endline data collection enter into the model if the longer time until data collection affects subject recall of the treatment. However, information in MIDA is typically insufficient to assess those substantive elements, an important and separate part of assessing the quality of a research study. "],
["software-primer.html", "Chapter 3 Software primer", " Chapter 3 Software primer In this section, we introduce you to DeclareDesign for R and describe how to use it to implement each step of the design-diagnose-redesign process.\n"],
["installing-r.html", "3.1 Installing R", " 3.1 Installing R You can download the statistical computing environment R for free from CRAN. We also recommend the free program RStudio, which provides a friendly interface to R.4 Once you have got RStudio installed, open it up and install DeclareDesign and its related packages. These include three packages that enable specific steps in the research process (fabricatr for simulating social science data; randomizr, for random sampling and random assignment; and estimatr for design-based estimators). You can also install DesignLibrary, which gets standard designs up-and-running in one line. To install them, you can type: install.packages(c(&quot;DeclareDesign&quot;, &quot;fabricatr&quot;, &quot;randomizr&quot;, &quot;estimatr&quot;, &quot;DesignLibrary&quot;)) We also recommend that you install and get to know the tidyverse suite of packages for data analysis, which we will use throughout the book: install.packages(&quot;tidyverse&quot;) For introductions to R and the tidyverse we especially recommend the free resource R for Data Science. Both R and RStudio are available on Windows, Mac, and Linux.↩ "],
["building-a-step-of-a-research-design.html", "3.2 Building a step of a research design", " 3.2 Building a step of a research design A research design is a concatenation of steps so the best way to learn how to build a design is learn how to make a step. We will start out by making—or declaring—a step that implements random assignment. Almost all steps take a dataset as input and return a dataset as output. We will will imagine input data that describes a set of voters in Los Angeles. The research project we are planning involves randomly assigning voters to receive (or not receive) a knock on their door from a canvasser. Our data look like this: ID age sex party precinct 001 66 M REP 9104 002 54 F DEM 8029 003 18 M GRN 8383 004 42 F DEM 2048 005 27 M REP 5210 006 53 M REP 2155 There are 100 voters in the dataset. We want a function that takes this dataset, implements a random assignment, adds it to the dataset, and then returns the new dataset containing the random assignment. You could write your own function to do that but you can also use one of the declare_* functions in DeclareDesign that are designed to write functions. Each one of these functions is a kind of function factory: it takes a set of parameters about your research design like the number of units and the random assignment probability as inputs, and returns a function as an output. For generating assignment functions we can use declare_assignment like this: simple_random_assignment_step &lt;- declare_assignment(prob = 0.6) The big idea to understand here is that the object we created, simple_random_assignment_step, is not a particular assignment, it is a function that creates an assignment. In particular it is a tidy function that summarizes how your design does assignment. You can run the function on data: simple_random_assignment_step(voter_file) Table 3.1: Data output following implementation of an assignment step. ID age sex party precinct Z Z_cond_prob 001 66 M REP 9104 0 0.4 002 54 F DEM 8029 0 0.4 003 18 M GRN 8383 1 0.6 004 42 F DEM 2048 0 0.4 005 27 M REP 5210 0 0.4 006 53 M REP 2155 0 0.4 The output here is an expanded dataset with a new column indicating treatment assignment (Z). As a bonus the data also includes the implied assignment propensity for each unit to the condition in which it is in (Z_cond). The most important thing to understand here is that implementing a step means taking a dataset in and sending a new dataset out. A few parts of this step declaration may seem a little bit odd. First, we did not tell R anything about the number of units in our dataset. Second, we did not give it the data. This is because a step declaration creates functions that are meant to be flexible, function that will work on any size dataset. We told declare_assignment that we want to assign treatment with probability 0.6 (and implicitly control with probability 1-0.6 = 0.4), regardless of how large the dataset is. We did not send the declaration the data because, although the assignment is a function of the data, the assignment function is not a function of the data. Put differently, the assignment function takes data as an argument, but the function to create the assignment function (declare_assignment()) does not. When you implement your research design after you have conducted it, you can use the exact same functions you generated in this design phase. In the same way when you diagnose your design you will use the same function many times. This is one of the reasons we declare the assignment step — because we will learn about the properties of your design with the same code you can actually use to randomly assign treatment. Every step of a research design in MIDA can be written using one of the declare_* functions. In the next section, we walk through each step and how to declare it using DeclareDesign. 3.2.1 Options and defaults Most of the declare_* functions have many options. In general you do not have to specify these as default values are usually provided (though you should be familiar with what these default values are). For instance you might have noticed above that when you ran the assignment step above, the new variable that was created was called Z. This is because declare_assignment has an argument assignment_variable that defaults to Z. You can change that of course to whatever you want. More subtly, the declare_* functions also default to “handlers” which have their own default arguments. These handlers are generally quite well developed sets of functions that implement the tasks needed by the declare_ function. For instance declare_assignment defaults to conduct_ra from the randomizr package as a handler and passes any additional arguments that you give it on to conduct_ra, and, by the same token, assumes by default the default values of the handler. In the example above we had prob = 0.6 as an argument. If you look at the documentation, prob is not an argument of declare_assignment but it is an argument of conduct_ra, with a default value of .5. If we had left this bit out we would have gotten a function that assigned treatment with probability .5. 3.2.2 Your own handlers The built in functions we provide in the DeclareDesign package are quite flexible and handle many major designs but the framework is built so that you are never constrained by what we provide. At any point rather than using the default handlers (such as conduct_ra), you can write a function that implements your own procedures. The only discipline that the framework imposes is that you write your procedure as a function that takes data in and send data back. Here is an example of how you turn your own functions into design steps. custom_assignment &lt;- function(data) mutate(data, X = rbinom(n = nrow(data), 1, prob = 0.5)) my_assignment_step &lt;- declare_assignment(handler = custom_assignment) my_assignment_step(voter_file) Table 3.2: Data generated using a custom function ID age sex party precinct X 001 66 M REP 9104 0 002 54 F DEM 8029 1 003 18 M GRN 8383 1 004 42 F DEM 2048 0 005 27 M REP 5210 1 006 53 M REP 2155 1 There is, of course, no great difference in this example between custom_assignment and my_assignment_step since my_assignment_step is just a function that applies custom_assignment. Even still, it is worth declaring this step formally as a design step using declare_assignment since this lets DeclareDesign know how the step fits into the whole design, how to interpret it, and when to call it. "],
["research-design-steps.html", "3.3 Research design steps", " 3.3 Research design steps In this section, we walk through how to declare each step of a research design using DeclareDesign. In the next section, we build those steps into a research design, and then describe how to interrogate the design. 3.3.1 Model The model defines the structure of the world, both its size and background characteristics as well as how interventions in the world determine outcomes. In DeclareDesign, we split the model into two main design steps: the population and potential outcomes steps. There 3.3.1.1 Population The population defines the number of units in the population, any multilevel structure to the data, and its background characteristics. We can define the population in several ways. In some cases, you may start a design with data on the population. When that happens, we do not to simulate it. We can simply declare the data as our population: declare_population(data = voter_file) ID age sex party precinct 001 66 M REP 9104 002 54 F DEM 8029 003 18 M GRN 8383 004 42 F DEM 2048 005 27 M REP 5210 006 53 M REP 2155 When we do not have complete data on the population, we simulate it. Relying on the data simulation functions from our fabricatr package, declare_population asks about the size and variables of the population. For instance if we want a function that generates a dataset with 100 units and a random variable u we write: declare_population(N = 100, u = rnorm(N)) When we run this population function, we will get a different 100-unit dataset each time: ID u 001 -0.3192017 002 1.1680298 003 1.6973635 004 0.9313464 005 -1.1518232 006 0.1216726 ID u 001 0.1887085 002 0.6912696 003 0.8218563 004 -0.9762560 005 -1.2951012 006 0.8627552 ID u 001 -1.2795357 002 1.8800277 003 0.5974647 004 -1.9631460 005 0.0840496 006 -1.6926872 The fabricatr package can simulate many different typs of data, including various types of categorical variables or different types of data structures, such as panel or multilevel strucures. You can read the fabricatr website to get started simulating your data structure. As an exmaple of a simple two-level data structure, we imagine a setting with 100 households and random number of individuals within each household. This two level structure could be declared as: declare_population( households = add_level(N = 100, individuals_per_hh = sample(1:10, N, replace = TRUE)), individuals = add_level(N = individuals_per_hh, age = sample(1:100, N, replace = TRUE)) ) Remember, in every step of the research design process, you can short-circuit our default way of doing things and bring in your own code. This is useful when you have a complex design, or when you have already written code for your design and you want to use it directly. It works by setting the handler: complex_population_function &lt;- function(data, N_units) { data.frame(u = rnorm(N_units)) } declare_population(handler = complex_population_function, N_units = 100) 3.3.1.2 Potential outcomes Defining potential outcomes is as easy as a single expression per potential outcome. These may be a function of background characteristics, other potential outcomes, or other R functions.5 declare_potential_outcomes( Y_Z_0 = u, Y_Z_1 = Y_Z_0 + 0.25) des &lt;- declare_population(N = 100, u = rnorm(N)) + declare_potential_outcomes(Y_Z_0 = u, Y_Z_1 = Y_Z_0 + 0.25) draw_data(des) ID u Y_Z_0 Y_Z_1 001 -0.3787828 -0.3787828 -0.1287828 002 -0.3463574 -0.3463574 -0.0963574 003 -0.6414364 -0.6414364 -0.3914364 004 0.4023661 0.4023661 0.6523661 005 0.3348888 0.3348888 0.5848888 006 0.0684464 0.0684464 0.3184464 We also have a simpler interface to define all the potential outcomes at once as a function of a treatment assignment variable. The names of the potential outcomes are constructed from the outcome name (here Y on the lefthand side of the formula) and from the assignment_variables argument (here Z). declare_potential_outcomes(Y ~ u + 0.25 * Z, assignment_variables = Z) Either way of creating potential outcomes works; one may be easier or harder to code up in a given research design setting. 3.3.2 Inquiry To define your inquiry, declare your estimand, which is a function of background characteristics from your population, potential outcomes, or both. We define the average treatment effect for the experiment in our simple design as follows: declare_estimand(PATE = mean(Y_Z_1 - Y_Z_0)) Notice that we defined the PATE (the population average treatment effect), but said nothing special related to the population. In fact, it looks like we just defined the average treatment effect. This is because where you define the estimand in your design is going to determine whether it refers to the population, sample, or other form of estimand. We will see how to do this in a moment. 3.3.3 Data strategy The data strategy constitutes one or more steps representing interventions the researcher makes in the world from sampling to assignment to measurement. Typically, this may include sampling and assignment. 3.3.3.1 Sampling The sampling step relies on the randomizr package to conduct random sampling. See Section XX for an overview of the many kinds of sampling that are possible. We define a simple 50-unit sample from the population as follows: declare_sampling(n = 50) When we draw data from our simple design at this point, it will be smaller: from 100 units in the population to a data frame of 50 units representing the sample. In the data frame, we have an inclusion probability, the probability of being included in the sample. randomizr includes this by default. In this case, every unit in the population had an equal 0.5 probability of inclusion. ID u Y_Z_0 Y_Z_1 S_inclusion_prob 1 001 2.4447631 2.4447631 2.6947631 0.5 8 008 0.4044073 0.4044073 0.6544073 0.5 9 009 0.6092110 0.6092110 0.8592110 0.5 10 010 -1.0616529 -1.0616529 -0.8116529 0.5 12 012 1.4102234 1.4102234 1.6602234 0.5 14 014 -0.5709666 -0.5709666 -0.3209666 0.5 Sampling could also be non-random, which could be accomplished by using a handler. 3.3.3.2 Assignment Assignment also relies, by default, on the randomizr package for random assignment. Here, we define assignment as a 50% probability of assignment to treatment and 50% to control. declare_assignment(prob = 0.5) Assignment results in a data frame with an additional indicator Z of the assignment as well as the probability of assignment. Again, here the assignment probabilities are constant, but in other designs described in Section XX they are not and this is crucial information for the analysis stage. ID u Y_Z_0 Y_Z_1 S_inclusion_prob Z Z_cond_prob 001 1.2666112 1.2666112 1.5166112 0.5 0 0.5 002 0.9343600 0.9343600 1.1843600 0.5 0 0.5 006 0.9265014 0.9265014 1.1765014 0.5 1 0.5 008 -0.6682399 -0.6682399 -0.4182399 0.5 0 0.5 011 1.6738920 1.6738920 1.9238920 0.5 1 0.5 017 -0.0781385 -0.0781385 0.1718615 0.5 1 0.5 3.3.3.3 Other data strategies Random sampling and random assignment are not the only kinds of data strategies. Others may include merging in fixed administrative data from other sources, collapsing data across months or days, and other operations. You can include these as steps in your design too, using declare_step. Here, you must define a handler, as we did for using a custom function in declare_population. Some handlers that may prove useful are the dplyr verbs such as mutate and summarize, and the fabricate function from our fabricatr package. To add a variable using fabricate: declare_step(handler = fabricate, add_variable = rnorm(N)) If you have district-month data you may want to analyze at the district level, collapsing across months:6 collapse_data &lt;- function(data, collapse_by) { data %&gt;% group_by({{ collapse_by }}) %&gt;% summarize_all(mean, na.rm = TRUE) } declare_step(handler = collapse_data, collapse_by = district) 3.3.4 Answer strategy Through our model and data strategy steps, we have simulated a dataset with two key inputs to the answer strategy: an assignment variable and an outcome. In other answer strategies, pretreatment characteristics from the model might also be relevant. The data look like this: ID u Y_Z_0 Y_Z_1 S_inclusion_prob Z Z_cond_prob Y 006 -0.5286968 -0.5286968 -0.2786968 0.5 0 0.5 -0.5286968 007 -0.2512728 -0.2512728 -0.0012728 0.5 0 0.5 -0.2512728 008 -0.6858477 -0.6858477 -0.4358477 0.5 0 0.5 -0.6858477 014 -0.0965727 -0.0965727 0.1534273 0.5 0 0.5 -0.0965727 016 1.5965742 1.5965742 1.8465742 0.5 0 0.5 1.5965742 017 -0.4513317 -0.4513317 -0.2013317 0.5 0 0.5 -0.4513317 Our estimator is the difference-in-means estimator, which compares outcomes between the group that was assigned to treatment and that assigned to control. We can calculate the difference-in-means estimate with a call to summarize from dplyr: simple_design_data %&gt;% summarize(DiM = mean(Y[Z == 1]) - mean(Y[Z == 0])) DiM 0.2834928 The estimatr package makes this easy and calculates the design-based standard error and a p-value and confidence interval for you: difference_in_means(Y ~ Z, data = simple_design_data) term estimate std.error statistic p.value conf.low conf.high df outcome Z 0.2834928 0.2895318 0.9791423 0.3324471 -0.2987359 0.8657215 47.72683 Y Now, in order to declare our estimator, we can send the name of a model to declare_estimator. R has many models that work with declare_estimator, including lm, glm, the ictreg package from the list package, etc. The design-based estimators from estimatr can all be used. declare_estimator(Y ~ Z, model = difference_in_means, estimand = &quot;PATE&quot;) In this declaration, we also define the estimand we are targeting with the difference-in-means estimator.7 Typically, you will have an estimand that you are targeting, and sometimes you may consider targeting more than one and assessing how good your estimator estimates them. For example, you may want to know how good a job your instrumental variables job is at targeting the complier average causal effect, but also how close it gets on average to the average treatment effect. Typically, we think of potential outcomes as fixed and not random, and move random variables to the population.↩ The {{ }} syntax is handy for writing functions in dplyr where you want to be able reuse the function with different variable names. Here, the collapse_data function will group_by the variable you send to the argument collapse_by, which in our declaration we set to district. The pipeline within the function then calculates the mean in each district.↩ Sometimes, you may be interested just in the properties of an estimator, such as calculating its power. In this case, you need not define an estimand.↩ "],
["building-a-design-from-design-steps.html", "3.4 Building a design from design steps", " 3.4 Building a design from design steps In the last section, we defined a set of individual research steps. We draw one version of them together here: population &lt;- declare_population(N = 100, u = rnorm(N)) potential_outcomes &lt;- declare_potential_outcomes(Y_Z_0 = u, Y_Z_1 = Y_Z_0 + 0.25) estimand &lt;- declare_estimand(PATE = mean(Y_Z_1 - Y_Z_0)) sampling &lt;- declare_sampling(n = 50) assignment &lt;- declare_assignment(prob = 0.5) reveal &lt;- declare_reveal(outcome_variables = Y, assignment_variables = Z) estimator &lt;- declare_estimator(Y ~ Z, model = difference_in_means, estimand = &quot;PATE&quot;) To construct a research design object that we can operate on — diagnose it, redesign it, draw data from it, etc. — we add them together with the + operator. The + creates a design object. simple_design &lt;- population + potential_outcomes + estimand + sampling + assignment + reveal + estimator In the book, we’ll use a more compact way of writing a design, where we define it all at once with the +: simple_design &lt;- declare_population(N = 100, u = rnorm(N)) + declare_potential_outcomes(Y_Z_0 = u, Y_Z_1 = Y_Z_0 + 0.25) + declare_estimand(PATE = mean(Y_Z_1 - Y_Z_0)) + declare_sampling(n = 50) + declare_assignment(prob = 0.5) + declare_reveal(outcome_variables = Y, assignment_variables = Z) + declare_estimator(Y ~ Z, model = difference_in_means, estimand = &quot;PATE&quot;) 3.4.1 Order matters When defining a design, the order steps are included in the design via the + operator matters. Think of the order of your design as the causal order in which steps take place. population + potential_outcomes + estimand + sampling + assignment + reveal + estimator The order encodes several important aspects of the design: - First, the fact that the estimand follows the potential outcomes and comes before sampling and assignment means it is a population estimand, the population average treatment effect. This is because it is calculated on the data created so far. - The estimator comes after the assignment and reveal outcomes steps. If it didn’t, our difference-in-means would not work, because it wouldn’t have access to the treatment variable and the realized outcomes. "],
["simulating-a-research-design.html", "3.5 Simulating a research design", " 3.5 Simulating a research design Diagnosing a research design — learning about its properties — requires first simulating running the design over and over. We need to simulate the data generating process, then calculate the estimands, then calculate the estimates that will result. With simple design defined as an object, we can easily learn about what kind of data it generates, the values of its estimand and estimates, and other features with simple funtions in DeclareDesign. They chain together functions in a similar way to a dplyr or ggplot pipeline. To draw simulated data based on the design, we use draw_data: draw_data(simple_design) ID u Y_Z_0 Y_Z_1 S_inclusion_prob Z Z_cond_prob Y 001 -1.1269042 -1.1269042 -0.8769042 0.5 1 0.5 -0.8769042 003 -0.9563091 -0.9563091 -0.7063091 0.5 0 0.5 -0.9563091 004 -0.6915299 -0.6915299 -0.4415299 0.5 0 0.5 -0.6915299 005 -0.4453528 -0.4453528 -0.1953528 0.5 1 0.5 -0.1953528 007 0.7036982 0.7036982 0.9536982 0.5 0 0.5 0.7036982 008 -0.6030058 -0.6030058 -0.3530058 0.5 0 0.5 -0.6030058 draw_data runs all of the “data steps” in a design, which are both from the model (population and potential outcomes) and from the data strategy (typically sampling and assignment). To simulate the estimands from a single run of the design, we use draw_estimands. This runs two operations at once: it draws the data, and calculates the estimands at the point defined by the design. For example, in our design the estimand comes just after the potential outcomes. In this design, draw_estimands will run the first two steps and then calculate the estimands from the estimand function we declared: draw_estimands(simple_design) estimand_label estimand PATE 0.25 Similarly, we can simulate the estimates from a single run with draw_estimates which draws data and at the appropriate moment calculates estimates. draw_estimates(simple_design) estimator_label term estimate std.error statistic p.value conf.low conf.high df outcome estimand_label estimator Z 0.3008934 0.3037054 0.9907412 0.3268775 -0.310058 0.9118449 47.0728 Y PATE To diagnose a design, we want a data frame that includes the estimates and estimands from many runs of a design. That is, we want to run the design, draw estimates and estimands, and then do that over and over and stack the results. This is exactly what simulate_design does: simulate_design(simple_design, sims = 500) design_label sim_ID estimand_label estimand estimator_label term estimate std.error statistic p.value conf.low conf.high df outcome simple_design 1 PATE 0.25 estimator Z 0.3089824 0.2920417 1.0580080 0.2953479 -0.2782083 0.8961732 47.99511 Y simple_design 2 PATE 0.25 estimator Z -0.2744487 0.2883450 -0.9518065 0.3459643 -0.8542075 0.3053102 47.99248 Y simple_design 3 PATE 0.25 estimator Z 0.2628113 0.2652493 0.9908085 0.3267977 -0.2706479 0.7962705 47.51853 Y simple_design 4 PATE 0.25 estimator Z 0.1554328 0.3017014 0.5151876 0.6087898 -0.4512005 0.7620662 47.93263 Y simple_design 5 PATE 0.25 estimator Z -0.2029857 0.2795080 -0.7262252 0.4712282 -0.7649777 0.3590063 47.98830 Y "],
["diagnosing-a-research-design.html", "3.6 Diagnosing a research design", " 3.6 Diagnosing a research design The simulations data frame we created allows us to diagnose the design (calculate summary statistics from the simulations) directly. We can now calculate the bias, root mean-squared error, and power for each estimator-estimand pair. In DeclareDesign, we do this in two steps. First, declare your diagnosands. These are functions of the simulations data. We have precoded several standard diagnosands (see Section XX). study_diagnosands &lt;- declare_diagnosands( select = c(bias, rmse, power), mse = mean((estimate - estimand)^2)) Next, take your simulations data and the diagnosands, and diagnose. This runs a single operation, which is to calculate the diagnosands on your simulations data. diagnose_design(simulations_df, diagnosands = study_diagnosands) design_label estimand_label estimator_label term mse se(mse) bias se(bias) rmse se(rmse) power se(power) n_sims simple_design PATE estimator Z 0.0985657 0.0515525 -0.2000416 0.1087173 0.3139517 0.0860497 0 0 5 We can also do this in a single step. When you send diagnose_design a design object, it will first run the simulations for you, then calculate the diagnosands from the simulations data frame that results. diagnose_design(simple_design, diagnosands = study_diagnosands) "],
["comparing-designs.html", "3.7 Comparing designs", " 3.7 Comparing designs In the diagnosis phase, you will often want to compare the properties of two designs to see which you prefer on the basis of the diagnosand values. We have two ways to compare. First, we can compare the designs themselves — what kinds of estimates and estimands do they produce, what steps are in the design. And we can compare the diagnoses. compare_designs(simple_design, redesigned_simple_design) To compare the diagnoses, we run a diagnosis for each one and then calculate the difference between each diagnosand for the two designs and conduct a statistical test of the null effect of no difference. compare_diagnoses(simple_design, redesigned_simple_design) 3.7.1 Comparing many variants of a design In the diagnosis phase, you will often want to compare the properties of two designs to see which you prefer on the basis of the diagnosand values. We have two ways to compare. First, we can compare the designs themselves — what kinds of estimates and estimands do they produce, what steps are in the design. And we can compare the diagnoses. We can do this using redesign: redesign(simple_design, N = c(100, 200, 300, 400, 500)) An alternative way to do this is to write a function that makes designs based on a set of these design inputs. This offers the researcher more flexibility in setting up the design variants. We call these functions designers. Here’s a simple designer based on our running example: simple_designer &lt;- function(sample_size, effect_size) { declare_population(N = sample_size, u = rnorm(N)) + declare_potential_outcomes(Y_Z_0 = u, Y_Z_1 = Y_Z_0 + effect_size) + declare_estimand(PATE = mean(Y_Z_1 - Y_Z_0)) + declare_sampling(n = 50) + declare_assignment(prob = 0.5) + declare_reveal(outcome_variables = Y, assignment_variables = Z) + declare_estimator(Y ~ Z, model = difference_in_means, estimand = &quot;PATE&quot;) } To create a single design, based on our original parameters of a 100-unit sample size and a treatment effect of 0.25, we can run: simple_design &lt;- simple_designer(sample_size = 100, effect_size = 0.25) Now to simulate multiple designs, we can use the DeclareDesign function expand_design. Here we examine our simple design under several possible sample sizes, which we might want to do to conduct a minimum power analysis. We hold the effect size constant. simple_designs &lt;- expand_design(simple_designer, sample_size = c(100, 500, 1000), effect_size = 0.25) Our simulation and diagnosis tools can take a set of expanded designs (an R list) and will simulate all of them at once, creating a column called design_label to keep them apart. For example: diagnose_design(simple_designs) 3.7.2 Library of designs In our DesignLibrary package, we have created a set of common designs as designers, so you can get started quickly and also easily set up a range of design variants for comparison. library(DesignLibrary) b_c_design &lt;- block_cluster_two_arm_designer(N = 1000, N_blocks = 10) diagnose_design(b_c_design) "],
["part-i-exercises.html", "Chapter 4 Part I Exercises", " Chapter 4 Part I Exercises "],
["formalizing-mida.html", "Chapter 5 Formalizing MIDA", " Chapter 5 Formalizing MIDA In Chapter 2, we gave a high-level overview of our framework for describing research designs in terms of their models, inquiries, data strategies, and answer strategies, our process for diagnosing their properties, and a general purpose approach for improving them to better fit research tasks. Now in this chapter, we place our approach on firmal formal footing. To do so, we employ elements from Pearl’s (2009) approach to causal modeling (directed acyclic graphs, or DAGs for short), which provides a syntax for mapping design inputs to design outputs. We also use the potential outcomes framework as presented, for example, in Imbens and Rubin (2015), which many social scientists use to clarify their inferential targets. Our goal here is not formalization for formalization’s sake. Describing a research design as a DAG helps us to see the fundamental symmetries across the theoretical (M and I) and empirical (D and A) halves of a research design. A recurring theme of our book is that research designs tend to be stronger when the relationship of M to I is mirrored by the relationship of D to A; the aim of this chapter is to make this somewhat mystical claim more concrete. References "],
["declaration.html", "5.1 Declaration", " 5.1 Declaration We define a research design as \\(\\Delta\\), with four elements \\(&lt;M,I,D,A&gt;\\). Describing a research design entails “declaring” each of these four elements. \\(M\\) is a theoretical model of how the world works. Following Pearl’s definition of a probabilistic causal model, \\(M\\) contains three core elements. The first is a specification of the variables \\(X\\) about which research is being conducted. This includes endogenous and exogenous variables (\\(V\\) and \\(U\\) respectively) and the ranges of these variables. In the formal literature this is sometimes called the signature of a model (Halpern 2000). The second element (\\(F\\)) a specification of how each endogenous variable depends on other variables. These can be considered functional relations or, as in Imbens and Rubin (2015), potential outcomes. The third and final element is a probability distribution over exogenous variables, written \\(P(U)\\). Since the model is probabalistic, we can think of \\(m\\) as a draw from the model \\(M\\). The inquiry \\(I\\) is a summary of the variables \\(X\\), perhaps given interventions on some variables. An inquiry might be the average value of an outcome \\(Y\\): \\(\\mathbb{E}[Y] = \\int{Y*Pr(Y)}\\), or the average value of the outcome conditional on the value of a treatment \\(Z\\): \\(\\mathbb{E}[Y|Z=1] = \\int{Y*Pr(Y|Z=1)}\\). Using Pearl’s notation we can distinguish between descriptive inquiries and causal inquiries. Causal inquiries are those that summarise distributions that would arise under interventions, as indicated by the \\(do()\\) operator, e.g., \\(\\Pr(Y | do(Z = 1))\\). Descriptive inquiries summarise distributions that arise without intervention, such as \\(\\Pr(Y | Z =1)\\). We let \\(a^m\\) denote the answer to \\(I\\) . Conditional on the model, \\(a^m\\) is the value of the estimand, the quantity that the researcher wants to learn about. The connection of \\(a^m\\) to the model can be seen in the following relationship: \\(a^m = I(m)\\). As the saying goes, models are wrong but some may perhaps be useful. We denote the true causal model of the world as \\(W\\) and the realized world \\(w\\) as a draw from this true causal model. The true answer, then, is \\(a^w = I(w)\\). The answer under the model \\(a^m\\) may be close or far from the true value \\(a^w\\), which is to say it could be wrong. If the model \\(M\\) is far from the \\(W\\), then of course \\(a^m\\) need not be correct. We note that \\(a^w\\) might be undefined, since inquiries can only be stated in terms of theoretical models. If the theoretical model is wrong enough, then the inquiry might be nonsensical when applied to the real world. A data strategy, \\(D\\), generates data \\(d\\). Data \\(d\\) arises, under model \\(M\\) with probability \\(P_M(d|D)\\). The data strategy includes sampling strategies and assignment strategies, which we denote with \\(P_S\\) and \\(P_Z\\) respectively. Measurement techniques are also a part of data strategies and can be thought of as a selection of observable variables that carry information about unobservable variables. The data strategy operates on \\(w\\) to produce the observed data: \\(D(w) = d\\). While \\(M\\) reflects our beliefs about true underlying causal processes at work where \\(D\\) is the procedure that results in the collection or creation of data. In this light, the phrase “data generating process” is imprecise. Scholars usually use the phrase “true DGP” to refer to \\(M\\), even though \\(M\\) doesn’t create data, \\(D\\) does when it is applied to the real world. The answer strategy, \\(A\\) generates answer \\(a^d\\) using data \\(d\\). We encode this relationship as \\(A(d) = a^d\\). The full set of causal relationships between \\(M\\), \\(I\\), \\(D\\), and \\(A\\) with respect to \\(m\\), \\(a^m\\), \\(d\\), \\(a^d\\), \\(w\\), and \\(a^w\\) can be seen in the DAG schematic representation of a research design. Figure 5.1: MIDA as a DAG Figure XX points out how a research design is a correspondence \\(I(m) = a^m\\) and \\(A(d) = a^d\\). The theoretical half of a research design produces an answer to the inquiry in theory. The emprical half of a research design produces an empirical estimate of the answer to the inquiry. Neither answer is necessarily close to the truth \\(a^w\\), but inescapable gamble of empirical research is that our theoretical models are close enough to the truth to be useful. Relationship Description \\(m = M()\\) \\(m\\) is a draw from the theoretical causal model \\(M\\) \\(a^m = I(m)\\) the answer under the model \\(a^m\\) is the inquiry applied to a draw from the model \\(m\\) \\(w = W()\\) the real world \\(w\\) is a draw from the true causal model \\(W\\) \\(a^w = I(w)\\) the true answer \\(a^w\\) is the inquiry applied to the real world \\(w\\) \\(d = D(w)\\) the realized dataset \\(d\\) results from the data strategy applied to the real world \\(w\\) \\(a^d = A(d)\\) the empirical answer \\(a^d\\) is the answer strategy applied to the dataset \\(d\\) The central crux of good research design is generated by the deep analogy of Models and Inquiries to Data and Answer Strategies. This analogy can guide many design decisions. The guidance that measurement procedures should be theoretically informed is an instance of the general point that data strategies should parallel theoretical models. The general preference for nonparametric estimation approaches over parametric procedures is an instance of the general point that answer strategies should parallel inquiries. \\[ M : I : : D : A \\] The DAG also clarifies the main difference between the theoretical and emprical halves of research design is the introduction of reality. When we draw from \\(M\\) to produce \\(m\\), we write \\(M() = m\\) – the function has no inputs from outside the model because it is purely theoretical. When we draw from the \\(D\\) to produce \\(d\\), we write \\(D(w) = d\\) – the dataset \\(d\\) that results from the data strategy \\(D\\) depends on the actual state of the world \\(w\\). Empirical researchers must be theorists too: our goal is to choose our theoretical model \\(M\\) such that \\(m\\) is as close to \\(w\\) as possible. Idea we want to express here: M:I::D:A, but M and D are different. How are they different? The first way is that M is theoretical and D is empirical – D() has w as an argument. The second way is that there are two “fundamental” problems facing empirical research. The first is the fundamental problem of causal inference – we can’t observe units in counterfactual states. The second is the fundmental problem of descriptive inference – we can’t observe the latent Y*, only Y as measured. References "],
["diagnosis.html", "5.2 Diagnosis", " 5.2 Diagnosis Research designs are strong when the empirical answer \\(a^d\\) generated by the design is close to the true answer \\(a^w\\). Since we can never know \\(a^w\\), we have to assess whether the distribution of \\(a^d\\) over possible realizations of the research design is close to the distribution of \\(a^m\\), the answer under the model. How can we assess how close the distributions are? The distribution of \\(a^m\\) is easy to simulate. Given a theoretical model, we can easily ask a computer to generate the distribution of the estimand. More often than not, the distribution of \\(a^M\\) is degenerate – in a fixed population, for example, most theoretical models posit that estimands like the Average Treatment Effect have just one value. The distribution of \\(a^d\\) is trickier to estimate. The actual research design will only be implemented once, so we don’t get to see the distribution of actual answers that could have eventuated from the application of D and A to the world. To solve this problem, we make a small – but extremely consequential – substitution of \\(m\\) for \\(w\\) in the DAG of a research design. Swapping \\(m\\) in for \\(w\\), we can ask the computer to simulate the distribution of \\(a^d\\) conditional on the model. If the model is wrong, the simulated distribution of \\(a^d\\) will be wrong too – as ever, garbage in, garbage out. Figure YY shows the DAG we use to simulate research designs. When we simulate designs, \\(d\\) is affected by \\(m\\) (a realization of a theoretical model), rather than by \\(w\\). This makes sense, since the computer simulations can be entirely untethered from reality. Design diagnosis is the process of simulating both \\(I(m) = a^m\\) and \\(A(d) = a^d\\) over many draws from \\(M()\\) and \\(D(m)\\), and comparing them. The specific comparisons we make are called “diagnostic statistics.” A “diagnostic statistic” is a function \\(g\\) of \\(a^m\\) and \\(a^d\\). This function could be simple, like the difference between them: \\(g(a^m, a^d) = a^d - a^m\\), or far more complex. If \\(a^m\\) and \\(a^d\\) are vector-valued (e.g., when answers provide a prediction for each observation), one might have a vector of diagnostic statistics, such as the vector of differences between each element of \\(a^m\\) and \\(a^d\\), or a scalar valued summary, such as a multidimensional distance measure. In some cases \\(a^m\\) and \\(a^d\\) might have different dimensions; for instance \\(a^d\\) may be a a pair of bounds and the diagnostic statistic is “Is \\(a^m\\) within \\(a^d\\)?” Because \\(a^m\\) and \\(a^d\\) are random variables, and any function of random variables is also a random variable, any diagnostic statisitic is also a random variable. A diagnosand is a summary of that random variable. Just like we summarize random variables with statistical functionals like the expectation and variance operators, a diagnosand is a statistical functional of the diagnostic statistic. \\[ \\phi = f(g(a^m, a^d)) \\] The \\(f()\\) is a statistical functional; statistical functionals summarize random variables. For example the expectation function \\(E[X]\\) summarizes the random variable \\(X\\) with its expectation, the mean. Diagnosands are summaries of diagnostic statics. We’ll use the Greek letter \\(\\phi\\) to describe the idea of a diagnosand in general. Let’s back up a moment to work through a concrete example of a diagnosand. Consider the diagnosand “bias.” Bias is the average difference between the estimand and the estimate. Under a model that has two potential outcomes, a treated potential outcome and an untreated potential outcome, the inquiry might be the average treatment effect (the average difference in the two potential outcomes, abbreviated as ATE). Under a single realization \\(m\\) of the model \\(M\\), the value of the ATE will be a particular number, which we call \\(a^m\\). If our data strategy is simply to collect data on those who come to be treated versus those who don’t (i.e., we do not use random assignment), and our answer strategy is difference-in-means, our answer \\(a^d\\) could be systematically different from \\(a^m\\). The diagnostic statistic is the error \\(a^d - a^m\\); this error is a random variable because each draw of \\(m\\) from \\(M\\) is slightly different. The expectation of this random variable is \\(E[a^d - a^m]\\), or the value of the bias diagnosand. Statistical power is a common diagnosand. Like bias, it is also an expectation, this time of the diagnostic statistic \\(\\mathbb{1}(p \\leq 0.05)\\), an indicator function that equals 1 if the \\(p\\)-value is below 0.05 and 0 otherwise. Power describes how frequently (under beliefs about the model) a research design would return a statistically significant result. Some diagnosands can be calculated analytically, though most require Monte Carlo computer simulation. A main purpose of the DeclareDesign software package is making the simulation step easier. In practice, it is important to diagnose a design under multiple possible \\(M\\)’s, given our fundamental uncertainty about the world \\(w\\). We do not know the precise distributions of exogenous variables or the exact functional forms of potential outcomes (e.g., we do not know the true effect size). Diagnosis, therefore, should typically involve simulating the properties of a fixed set of inquiries, data strategies, and answer strategies under multiple likely models. A \\(D\\) and \\(A\\) for a given \\(I\\) that provide good diagnosand values under multiple \\(M\\)’s can be said to be robust to multiple models. Figure 5.2: MIDA as a DAG "],
["redesign-1.html", "5.3 Redesign", " 5.3 Redesign Diagnosis is the process of learning the value of a diagnosand for a single design, often under multiple specifications of M to assess robustness to alternative models. Redesign is the process of changing parts of D and A in order to learn how the diagnosands change. The redesign process is complete when a research selects the best (or one of the best) D and A among the feasible set, as measured by the changing values of the diagnosand. For example, we can compare how the distribution of errors changes if we use a different data strategy \\(D&#39;\\): \\(P_M(a^d - a^m|D&#39;)\\) or a different answer strategy \\(A&#39;\\): \\(P_M(a^{d&#39;} - a^m|D)\\). We can also hold the data and answer strategies fixed and consider the distribution of errors under an alternative model \\(M&#39;\\): \\(P_M(a^{d} - a^{m&#39;}|D)\\). Researchers typically wish to find the optimal design for their question subject to financial and logistical constraints. Even simple designs have infinite variations, so the search space must be limited. Typically, researchers will want to find the optimal values of a small set of easily-controlled design parameters. For example, a researcher might want to find the sample size \\(N\\) and the number of treated units \\(N_t\\) that minimize a design’s bias subject to a fixed budget for the experiment in which data collection for each unit costs $25 and treating one unit costs $5. They solve the optimization problem: \\[\\begin{equation*} \\begin{aligned} &amp; \\underset{N, N_t}{\\text{argmin}} &amp; &amp; E_M(a^{d} - a^{m}|D_{N, N_t}) \\\\ &amp; \\text{s.t.} &amp; &amp; 25 * N + 5 * N_t \\leq 5000 \\end{aligned} \\end{equation*}\\] However, optimizing a design for a single diagnosand may lead to poor design choices. If we redesign by optimizing for the power diagnosand, we are likely to find a design that is highly powered but highly biased — it is targeting the wrong estimand. We can solve this by switching to the root mean-squared error as the diagnosand, which is a weighted combination of the bias and efficiency of the design. More generally, the choice of an optimal design requires the researcher to select a weighting of diagnosands. They must balance bias, efficiency, the risk of imprecise null results, the risk of getting the sign of an effect wrong, and other diagnosands given their research goals. The full evaluation of a design — declaration, diagnosis, and redesign — depends on the assessment of one or more diagnosands, and comparing the diagnoses to what could be achieved under alternative designs. "],
["example-1.html", "5.4 Example", " 5.4 Example In this section, we declare the Gulzar and Khan (n.d.) example in finer detail. The aim is to capture all of the analytically-relevant features of the design. In Chapter 2, we declared in words and in code a simplified version of the design, in order to communicate its key features. We will make this move again throughout the book: we will present simplified versions of canonical designs in the text, and also provide worked examples with the details of specific studies. Moving back and forth between the two will, we hope, enable you to both learn about the general principles and how to declare your own designs which always involve the fine details of research implementation. We first describe the design in words and then declare in code: The model for the study describes the set of units: citizens living in the two Pakistani districts the study took place in, Haripur and Abbottabad. The endogenous outcomes of interest are whether the citizen filed papers to run for office; whether the citizen was elected; and the Euclidean distance of the citizen’s preferences from average citizen preferences. The outcomes are a function in their causal model of three treatments, which emphasize either the social or personal benefits to holding public office or do not encourage anyone to run for office at all. Their model would include an expected treatment effect magnitude for each treatment and their suppositions about how correlated outcomes are within villages that they study. The inquiry is the effect of an encouragement to run for office focused on prosocial motivations for officeholding (rather than encouragements that emphasized personal benefits) on the rate of filing papers to run for office between people living in villages randomly assigned to receive. The data strategy entailed three steps: (1) randomly sampling 192 villages from among all villages in Haripur and Abbottabad districts, and using a random walk procedure to select 48 citizens in each village to participate in the experiment; (2) randomly-assigning each village with equal probability to one of three conditions (neutral, social benefits, or personal benefits); and (3) collecting administrative data on who filed papers to run and matching that back to pretreatment survey data on the 9,216 citizens. Sampling, treatment assignment, and measurement are the three common data strategy steps in an experiment; some experiments, instead, do not include a sampling step and instead assign treatments within a convenience sample. Gulzar and Khan (n.d.) have a two-step answer strategy, fitting a linear model predicting whether a citizen ran for office (the outcome) with indicators for the social benefits and the personal benefits treatments, and then calculating the difference between the two coefficients as an estimate of whether social benefits are more or less effective than the personal benefits. Their answer strategy includes presenting a table with estimated difference, a standard error clustered on village to account for village-level random assignment, and a p-value calculated using permutation inference. gulzar_khan_design &lt;- declare_population( # study is conducted in two districts in Pakistan, with 311 and 359 villages in them districts = add_level(N = 2, name = c(&quot;Haripur&quot;, &quot;Abbottabad&quot;), N_villages = c(311, 359)), # villages nested within districts villages = add_level(N = N_villages), # avg. 6500 citizens per village citizens = add_level(N = 6500) ) + # main outcome is whether a citizen filed papers to run for office # we define potential outcomes in response to being assigned to a social, personal, or neutral appeal to run declare_potential_outcomes( filed_papers ~ rbinom(N, 1, prob = 0.05 + 0.05 * (Z_appeal == &quot;social&quot;) + 0.01 * (Z_appeal == &quot;personal&quot;)), assignment_variable = Z_appeal, conditions = c(&quot;neutral&quot;, &quot;social&quot;, &quot;personal&quot;) ) + # inquiry is the difference in rates of filing papers between the social and personal appeal conditions declare_estimand(ATE = mean(filed_papers_Z_appeal_social - filed_papers_Z_appeal_personal)) + # sample 192 villages declare_sampling(clusters = villages, n = 192, sampling_variable = &quot;S_villages&quot;) + # sample 48 citizens in each village via random walk declare_sampling(strata = villages, n = 48, sampling_variable = &quot;S_citizens&quot;) + # assign villages to three arms with equal probabilities for three types of appeals to run for office declare_assignment( m_each = c(48, 72, 72), clusters = villages, conditions = c(&quot;neutral&quot;, &quot;social&quot;, &quot;personal&quot;), assignment_variable = Z_appeal ) + # recode treatment assignment for analysis into indicators for the two conditions of interest declare_step( Z_social_village = if_else(Z_appeal == &quot;social&quot;, 1, 0), Z_personal_village = if_else(Z_appeal == &quot;personal&quot;, 1, 0), handler = mutate ) + # 1. run a linear regression with condition indicators # 2. calculate the difference in effects between people in villages assigned to social appeals compared # to those assigned to personal appeals # 3. calculate robust standard errors clustered on village declare_estimator( filed_papers ~ Z_social_village + Z_personal_village, linear_hypothesis = &quot;Z_social_village - Z_personal_village = 0&quot;, term = &quot;Z_social_village - Z_personal_village = 0&quot;, clusters = villages, model = lh_robust ) References "],
["specifying-the-model.html", "Chapter 6 Specifying the model", " Chapter 6 Specifying the model Any research design – whether the research question is fundamentally causal or descriptive – implicitly relies on a causal model of the world. These models include beliefs about what variables are important and how they interrelate. Even if the model is hazy, researchers carry with them a model of the world that could in principle be expressed as a probablistic causal model. As described in the previous chapter, probabalistic causal models are described by a set of exogeneous and endogenous variables, a set of functional relationships between the variables, and a probability distribution over the exogenous variables. In order to design a strong research study, we need to introspect about our causal beliefs. We need to make the explicit the design-relevant features of the implicit causal models in our heads. This chapter is about how to go about this difficult task. We’ll make use of two different formal languages for describing causal models: DAGs and potential outcomes. DAGs are “directed acyclic graphs,” where each node on a graph is a variable and the edges that connect them represent causal effects. DAGs emphasize a mechanistic notion of causality: when the exposure variable changes, the outcome variable changes as a result. DAGs are nonparametric. This means that they do not encode all the beliefs about the full causal model. They don’t show how variables are related, just that they are related. The potential outcomes formalization emphasizes a counterfactual notion of causality. \\(Y_i(Z = 0)\\) is the outcome for unit \\(i\\) when the causal variable equal to zero and \\(Y_i(Z = 1)\\) is the outcome when it is set to one. The difference between them is the effect of the treatment on the outcome for unit \\(i\\). Since at most only one potential outcome can ever be revealed, at least one of the two potential outcomes is necessarily counterfactual. Usually, the potential outcomes function \\(Y_i()\\) has just one argument – \\(Z\\) – because it elides all other determinants of the outcome to focus only on the outcome that would occur depending on the level of the main causal variable of interest. Despite what you may have inferred from the sometimes heated disagreements between scholars who prefer one formalization to the other, DAGs and potential outcomes are compatible systems for thinking about causality. We could use only the language of causal graphs or we could use only the language of potential outcomes. We choose to use both languages because they are useful for expressing different facets of research design. We use DAGs to describe the web of causal interrelations in a concise way (writing out the potential outcomes for every relationship in the model is tedious). We use potential outcomes when we want to zoom in on particular causal relationships and to make fine distinctions between inquiries that apply to different sets of units (it’s difficult to describe effect heterogeneity with graphical models).\n"],
["defining-the-population-of-units-for-our-model.html", "6.1 Defining the population of units for our model", " 6.1 Defining the population of units for our model The first choice to make in declaring \\(M\\) is the set of units about which you wish to make inferences. This is largely determined by your inquiry (\\(I\\)). If your inquiry is about the population of Americans or Brazilians, your model should describe the characteristics of all Americans or Brazilians. If your inquiry is the sample average treatment effect, then you should describe only the units in your sample. "],
["what-variables-belong-in-the-causal-model.html", "6.2 What variables belong in the causal model?", " 6.2 What variables belong in the causal model? Ultimately our goal as researchers is to learn more and more about how the world works or in other words, to fill in more and more of a large causal model. There are an infinite number of nodes and edges in this model — from how people vote to how they save and spend money to how they find romantic partners, all of which are interrelated. However, social science has not yet filled in much of the model yet. Thankfully, in order to declare a research design and learn about it, we don’t need to specify every part of the world’s causal model. The variables that we need to specify in \\(M\\) are those we will need in the latter three elements in our research design: our inquiry \\(I\\), data strategy \\(D\\), and answer strategy \\(A\\). In order to reason about whether the data we collect will be able to provide an answer to our inquiry, we need to define all of the variables used to construct our inquiry. In descriptive research, this will mean the variables we will summarize. In causal research, this will mean the potential outcomes under different states of the world, such as treatment and control. For example, if we are studying the effects of a voter mobilization campaign on vote choice between the three candidates running in a primary election, we should define a vote choice variable and the values it takes on in two circumstances: in the presence of the voter mobilization campaign (treatment) and without it (control). The data strategy — sampling, treatment assignment, and measurement — defines many of the variables we need to specify in the model. Sampling procedures often involve stratification (e.g., sampling equal proportions of men and women), clustering (e.g., sampling all of the individuals in a household to participate in the research), or both. In the model, we need to define the variables that will be used to stratify and cluster. Similarly, treatment assignment can involve assigning treatments within blocks and cluster assignment where all units are assigned to the same status. The variables that are used to construct blocks and that will form clusters will be defined in the model. Finally, all of the variables that will be measured should also be defined in the model. When we measure latent variables imperfectly, for example sensitive questions where a true characteristic exists but respondent do not always admit it, we should define both the latent trait and measured responses. Finally, answer strategies rely on collected data to provide an answer to the inquiry, and so any variable in the collected data should be defined in the model. (Any of these variables should also be defined in the measurement strategy.) Beyond outcomes and treatment variables, we may need variables to define clusters used in clustered standard errors, to construct weights for poststratification of estimates to match population characteristics, and to visualize our data. In addition to defining these variables, we need to know how they relate to each other. We define a set of nonparametric structural causal relationships. With these variables at hand, we can define a nonparametric structural causal model and visualize the model using a directed acyclic graph. We illustrate using a DAG to describe a model with an abstract research design in which we will collect information about \\(N\\) units. We will measure a pretreatment covariate \\(X\\), assign a treatment at random \\(Z\\), and collect a posttreatment outcome \\(Y\\). We know there are other determinants of the outcome beyond \\(Z\\) and \\(X\\), but we don’t need to write down our beliefs about them because our main inquiry is the average treatment effect of \\(Z\\) on \\(Y\\). All we’ll say about those other deteriminants is that they are causally related to both \\(X\\) and \\(Y\\), but not to \\(Z\\), since \\(Z\\) will be randomly assigned by us. Following the rule that we only need to make explicit the parts of our causal model that are required for the inquiry, data strategy, and answer strategy, this bare specification is sufficient. The nonparametric structure causal model can be written like this: \\[\\begin{align*} X &amp;= f_X(U)\\\\ Y &amp;= f_Y(Z, X, U) \\end{align*}\\] This DAG encodes this model in graphical form: Possibly the most important thing that a DAG can teach us is which research questions are even answerable in a given setting by applying the “back-door criterion,” which asks whether a back-door path exists between a pair of causally-related variables. Here, the causal relationship between \\(X\\) and \\(Y\\) is confounded by the back-door path \\(X &lt;- U -&gt; Z\\). (for much more on the back-door criterion, see XXX). Since we have no information about what is in \\(U\\), we can’t learn about the effects of \\(X\\) on \\(Y\\). However, the treatment variable \\(Z\\) has no edges leading in to it, which represents the idea that \\(Z\\) is randomly assigned. There are no back door paths leading from \\(Z\\) to \\(Y\\), so we can learn about average causal effects of Z on Y. We could also draw descriptive inferences about the distributions of the observed variables like their ranges and averages or their variances and covariances. "],
["what-do-we-need-to-specify-about-these-variables.html", "6.3 What do we need to specify about these variables?", " 6.3 What do we need to specify about these variables? DAGs convey beliefs about whether whether two variables are causally related, but they do not encode beliefs about how they are related. This is no criticism of DAGs — they just don’t encode all of our causal beliefs about a system. However, in order to assess many properties of a research design we need to go further. We need to specify the scales and probability distribution of exogenous variables and the functional forms of the endogenous variables (how they relate to parent variables). This will include incorporating beliefs about effect sizes, but also correlations between variables, intra-class correlations (ICCs), and interactions. All of this to say: in order to declare and diagnose our designs, we’ll need to make the leap from nonparametric models to parametric structural causal models. This move is not without costs – each specific choice we make over and above the nonparametric model is an opportunity to be more wrong about the world! Suppose that according to our model, the effect of treatment should be larger for units with \\(X = 1\\). We can encode this belief in a design declaration. In the declare_population function, we write that \\(U\\) is normally distributed with mean 0 and standard devation 1; \\(X\\) follows a bernoulli distribution but units have a probability of \\(X\\) equalling one that depends on \\(U\\) in a particular way. In the declare_potential_outcomes function, we describes how the average effect of treatment is 0.5 when \\(X = 0\\) but 0.5 + 0.5 = 1 when \\(X = 1\\). diff_in_cates &lt;- 0.5 design &lt;- declare_population(N = 100, U = rnorm(N), X = rbinom(N, 1, prob = pnorm(0.5 * U + rnorm(N)))) + declare_potential_outcomes(Y ~ 0.5 * X + 0.5 * Z + diff_in_cates * X * Z + 0.5 * U) We could incorporate different beliefs about the causal model by changing the diff_in_cates parameter. However, notice that regardless of the value of the interaction (either zero or some other number), the DAG looks the same. So if you want to design your study for the difference-in-CATEs, then you’ll need to go beyond the DAG to write down your beliefs about the extent of heterogeneity using another tool – design declaration complements DAGs in this specific way. "],
["how-do-we-make-educated-guesses-to-fill-in-the-model.html", "6.4 How do we make educated guesses to fill in the model?", " 6.4 How do we make educated guesses to fill in the model? The content of models typically comes from two places: reading the past literature and qualitative research. Past theoretical work can guide the set of nodes that are relevant and how they are connected by edges. Past empirical work can provide further insight on the set of edges that exist (or do not). However, when past research is thin on a topic, there is no substitute for insights gained through qualitative data collection: focus groups and interviews with key informants who know aspects of the model that are hidden to the researcher; archival investigations to understand how to draw understand a causal process when the actors in it are no longer alive, or to gain insights only contained in administrative records; and immersive participant observation to see with your eyes how social actors behave. Fenno (1982) calls this “soaking and poking.” This mode of inquiry, discovery, is separate from the qualitative research designs that provide an answer to an inquiry deductively. We examine those throughout the book (particular examples are in library entry XX and YY). Instead, qualitative insights such as this, which Lieberman (2005) labels “model-building” case studies, do not aim to answer a question but rather yield a new theoretical model. Quantitative research is often seen as distinct from qualitative research, but the model building phase in both is itself qualitative. Identifying nodes and edges important to constructing \\(I\\), \\(D\\), and \\(A\\) (and thus including in \\(M\\)) typically come from two sources: reading past literature and conducting new qualitative research. Past theoretical work can guide the choice of nodes that are relevant and how they are connected by edges. Past empirical work can provide further insight on the set of edges that exist (or do not). However, when past research is thin on a topic, there is no substitute for insights gained through qualitative data collection: focus groups and interviews with key informants who know aspects of the model that are hidden to the researcher; archival investigations to understand how to draw understand a causal process when the actors in it are no longer alive, or to gain insights only contained in administrative records; and immersive participant observation to see with your eyes how social actors behave. Fenno (1982) calls this “soaking and poking.” This mode of inquiry, discovery, is separate from the qualitative research designs that provide an answer to an inquiry deductively. We examine those throughout the book (particular examples are in library entry XX and YY). Instead, qualitative insights such as this, which Lieberman (2005) labels “model-building” case studies, do not aim to answer a question but rather yield a new theoretical model. Quantitative research is often seen as distinct from qualitative research, but the model building phase in both is itself qualitative. The next step — selecting statistical distributions and their parameters to describe exogenous variables and the functional forms of endogenous variables — is often more uncomfortable. We do not know the magnitude of the effect of an intervention before we do the research or the correlation between two outcomes when those are the goal of the research. That’s why we are conducting the study! However, we are not fully in the dark in most cases and can make educated guesses about parameters like effect sizes, intraclass correlations, and correlations between variables. We can conduct meta-analyses of past, relevant studies on the same topic to identify the range of plausible effect sizes, intraclass correlations, correlations between variables, and other model parameters. Conduct such a meta-analysis might be as simple as collecting the three papers that measured similar outcomes in the past and calculating the average intraclass correlation and its range across the three. A more sophisticated but still straightforward analysis would be to calculate the precision-weighted average of effect sizes and use that as the baseline effect size in the model, but also calculate the predictive interval from a random effects meta-analysis to characterize the expected range of effect sizes across differing contexts. The key question in conduct such a meta-analysis is how to select studies that are “relevant.” There are four dimensions on which we might want to compare past studies to the current setting: similarity in the type of units, treatments, outcomes, and contexts (Cronbach and Shapiro 1982). Except in the case of pure replication studies, we are typically studying a (possibly new) treatment in a new setting, with new participants, or with new outcomes, so there will not be perfect overlap. However, the variation in effects across contexts and these other dimensions will help structure the range of our guesses specified in the model. When there are past studies that are especially close to our own, we may want to not define probability distributions that approximate the causal model of the world, but use the data from that past study directly as a stand in. To do so, instead of declaring variables and their distributions and potential outcomes, we can resample from the past data to obtain simulated alternative possible worlds. Where there are no past studies that are sufficiently similar in some dimension, we can collect new data through pilot studies. We discuss risks to relying on data from small pilot studies in planning new research in Chapter XX. In short, we should not rely on effect size estimates from small pilot studies, and only use parameters such as the range of outcome data and the standard errors of estimates which are less noisy in declaring our new design. In some cases, we can define many of the variables from early data collection in a study, e.g., from a baseline survey. Once we conduct a baseline survey, we can use the set of individuals selected for the study and their baseline characteristics to define many of the variables in our model. We can define our expectations about effect sizes and other features of endogenous variables, but rely on the correlations between exogenous variables from the baseline data. The danger here is that if we fix the characteristics of our model using baseline data, we will not consider what data could have been revealed if our sampling procedure had yielded a different set of study participants or we had conducted data collection a month later. References "],
["robustness-to-multiple-models.html", "6.5 Robustness to multiple models", " 6.5 Robustness to multiple models The most uncomfortable part of declaring \\(M\\) is choosing point estimates of its parameters such as the effect size, the mean and variance of the normal distribution that describes unknown heterogeneity. There is no need to specify a single point! Indeed, our fundamental uncertainty about the model should usually lead us to a range or even empirical distribution from past studies of each parameter. We suggest three strategies for choosing these ranges: the logical bounds of a parameter, such as choosing the range of possible effect sizes based on the largest change from the bottom of a scale to the top of a scale; the empirical distribution from past studies, either the full range of the parameter or the predictive interval from a random effects meta-analysis; or a best case-worst case bounds, based on the substantive interpretation of results in light of past results, for example ranging from an effect size of zero to the largest plausible effect size. A design that performs well in terms of power and bias under one or all of these three ranges for each parameter can be labeled “robust to multiple models.” A separate goal is assessing the performance of a research design under different models implied by alternative theories. A good design will provide probative evidence about which model is correct regardless of which model it is that aligns with the true causal model of the world. An important example is assessing the performance of a research design under a “null model” where the true effect size is zero. A good research design should report with high probability that there is insufficient evidence to reject a null effect. That same research design, under an alternative model with a large effect size, should with high probability return evidence rejecting the null hypothesis of zero effect. The example makes clear that in order to understand whether the research design is strong, we need to understand how it performs under not just multiple models, but the models implied by alternative theoretical understandings of the world. Two alternative theories of why X causes Y, through mediator M1 or mediator M2 (and not both), imply two different structural causal models. When the inquiry is which variable mediates the relationship, we need to understand how the research design performs in providing evidence for which is correct under both possibilities. "],
["fundamental-uncertainty.html", "6.6 Fundamental uncertainty", " 6.6 Fundamental uncertainty Models are models – they are abstractions and they are not the truth! The true causal structure of the world \\(W\\) generates draws from the world \\(w\\). The inquiry \\(I\\) might not even defined under \\(W\\), that is \\(I(w)\\) might be \\(NA\\). Applying the data strategy \\(D\\) to \\(w\\) might produce unexpected results. That is \\(D(w)\\) need not be anything like \\(D(m)\\). This disjuncture is unavoidable and is in large part, the whole point of doing research in the first place. We do not know \\(W\\) – that would require omniscience. We have learned parts of \\(W\\) and put them in \\(M\\) – that’s science. When research produces unexpected results, it’s an indication that something in MIDA is out of whack and it is an opportunity for learning. The next research project will amend MIDA in order to bring \\(M\\) closer to \\(W\\). "],
["defining-the-inquiry.html", "Chapter 7 Defining the inquiry", " Chapter 7 Defining the inquiry An inquiry is a summary of a theoretical model. Suppose your theory is that \\(X\\) affects \\(Y\\), but that the effect is bigger for units with higher values of a third variable C. One inquiry might be the average level of \\(Y\\). A second might be the average treatment effect of \\(X\\) on \\(Y\\). A third might be the difference in the effect of \\(X\\) on \\(Y\\) for units with high values of \\(C\\) versus units with low values. Your inquiry is your research question. Simple or complex, causal or descriptive, your inquiry is a summary feature of your theoretical model. Like models, inquiries themselves are theoretical objects. A common confusion occurs between inquiries and the output of answer strategies. If our theory posits the existence of an Average Treatment Effect, we might use an answer strategy like difference-in-means to estimate it, but the estimate is fundamentally distinct from the inquiry. Estimates are empirical, inquiries are theoretical. In general, an inquiry is a summary function \\(I\\) that operates on the model \\(M\\). When we summarize the model with the inquiry, we obtain an “answer under the model.” We formalized this as \\(I(m) = a^m\\). You can think of the difference between \\(I\\) and \\(a^M\\) as the difference between a question and its answer. \\(I\\) is the question we ask about the model and \\(a^m\\) is the answer. Alternatively, you can think of \\(I\\) as the “estimand” (that which is to be estimated) and \\(a^m\\) as the value of the estimand. In this book when we talk about inquiries, we will usually be referring to single-number summaries of models. Some common estimands are descripitive, such as the means, conditional means, correlations, partial correlations, quantiles, and truth statements about variable in the model. Others are causal, such as the average difference in one variable when a second varible is set to two different values. You can think of a single-number inquiry as the atom of research question. While most inquiries are “atomic” in this way, some inquiries are more complex than a single-number summary. For example, the best linear predictor of Y given X is a two-number summary: it is the pair of numbers (the slope and intercept) that minimizes the total squared distance between the line and each value of Y. Why stop at two-number summaries? We could imagine the best quadratic predictor of Y given X (three-number summary) or the best cubic predictor (four), and so on. We could have an inquiry that is the full conditional expectation function of Y given X, no matter how wiggly, nonlinear, and nuanced the shape of that function – it could in principle be a 1,000 number summary of the model, or much more. Complex inquiries can also be something along the lines of “should I reject \\(M\\)” as a causal model of the world? A researcher might articulate a handful of important questions about the model that all have to come out a certain way or the model itself should be rejected. These complex inquires are made up of a series of atomic inquiries – we’re interested in the sub-inquiries only insofar as they help us understand the real inquiry – is this model of the world a good one or not.\n"],
["kinds-of-inquiries.html", "7.1 Kinds of Inquiries", " 7.1 Kinds of Inquiries Atomic inquiries \\(I\\) about a model \\(M\\) fall into two basic categories, descripitive and causal. Descriptive inquiries are about how the world was, is, and will be whereas causal inquiries are about how the world would have been, would be, or would be in the future if some variables were set to different levels. You might think of the distinction in terms of a DAG. Descripitive inquiries are about the nodes of a dag whereas causal inquires are about the edges. Descriptive and causal inference have in common the difficulty of inferring unseen things from observed data. The fundamental problem of causal inference is well known. For the same reason that a unit cannot simultaneously be both treated and untreated, we can only every observe at most one potential outcome for any particular unit. The fundamental problem of descriptive inference is similar. The concepts we want to measure are latent constructs. Perfect measurement is impossible, so our measurements of the latent constructs always include some measurement error. Just like we can’t know for sure how, counterfactually, a unit would have responded if the treatment had been set to a different level, we likewise can’t know for sure whether our measurements accurately reflect the latent construct. More specifically, a descriptive inference is a conclusion about a latent variable \\(Y^*\\) on the basis of a measured variable \\(Y\\). The feature we seek to describe – our inquiry – is some summary of \\(Y^*\\) like its mean or perhaps its covariance with a second latent variable \\(X^*\\). When we do descripitive research, we draw inferences about features of the nodes of the latent causal model \\(M\\). A causal inference is a conclusion about the edge between two latent variables \\(X^*\\) and \\(Y^*\\). Even if we do an exceptional job measuring \\(X^*\\) and \\(Y^*\\) with \\(X\\) and \\(Y\\), we will still have trouble learning about edges because causal effects are quite literally unobservable. We have to infer causality on the basis of a strong research design because we can’t just see it. 7.1.1 Descriptive inquiries Descriptive inquiries are usually about latent variables since we mostly care about the true values of the variables in the models. Because of the fundamental problem of descriptive inference, measured variables are always distinct from latent variables. For the most part, we define our inquiries in terms of the true latent variables rather than in terms of their measured counterparts. Table ZZZ enumerates some common descriptive estimands. Inquiry Description \\(E_{i\\in N}(Y)\\) The average value of the variable Y \\(E[Y | X = 1]\\) A conditional expectation of Y given X = 1. \\(V[Y]\\) The variance of Y \\(Cov(X, Y)\\) The covariance of X and Y \\(BLP(Y | X)\\) The best linear predictor of Y given X \\(CEF(Y | X)\\) Conditional expectation function of Y given X Truth status of X Is X True or False 7.1.2 Causal inquiries Causal inquiries involve a comparison of at least two possible worlds. For example, an inquiry might be the causal effect of \\(X\\) on \\(Y\\) for a single unit. In order to infer that causal effect, we would need to know the value of \\(Y\\) in two worlds: one world in which \\(X\\) is set to 1 and one in which \\(X\\) is set to 0. Table ZZZ enumerates some common causal estimands. Estimand scope:What is the set of units which you want to learn the answer about? Know what ATE averages over Inquiry Description \\(E[Y_i(1) - Y_i(0)]\\) Average treatment effect (ATE) \\(E[Y_i(1) - Y_i(0) | X = 1]\\) Conditional average treatment effect (CATE) \\(E[Y_i(1) - Y_i(0) | d_i(1) \\geq d_i(0)]\\) Complier average causal effect (CACE) or local average treatment effect (LATE) \\(E[Y_i(1) - Y_i(0)]\\) PATE \\(E[Y_i(1) - Y_i(0) | S = 1]\\) SATE Generations of students have been told to excise words that connote causality from their empirical writing. “Affects” becomes “is associated with” and “impacts” becomes “moves with.” Being careful about causal language is of course very important (it’s really true that correlation does not imply causation!). But this change in language is not usually accompanied by a change in inquiry. Many times we are faced with drawing causal inferences from than ideal data – but the deficiencies of the data strategy should not lead us too far away from our inferential targets. If the inquiry is a causal inquiry, then the move from “causes” to “is correlated with” might be a good description of the actual data analysis, but it doesn’t move us closer to providing an answer to the inquiry. 7.1.3 Relationship between causal and descriptive inquiries You can think of causal inference as repeated descriptive inference: we have to describe \\(Y\\) in multiple possible worlds. Causal inquiries like the average effect of A on B in a causal model like \\(A -&gt; B &lt;- U\\) are helped enormously by good descriptive inference about the nodes \\(A\\) and \\(B\\), but the focus is on the edge between them. You typically can’t learn about the edge by doing desscripitve inference on the nodes only. If we measure \\(A\\) and \\(B\\) and find that they covary, we can’t be sure that \\(A -&gt; B\\) because it could be that \\(A &lt;- U -&gt; B\\). This problem goes by the familiar phrase that “correlation doesn’t imply causation,” which is true and is a problem that can’t be wished away. But “correlation doesn’t imply causation” also kind of misses the point. The point is that you can’t see causality because it involves counterfactuals, which are imaginary and unseen. You have to infer causality on the basis of design. "],
["how-should-you-select-inquiries.html", "7.2 How should you select Inquiries?", " 7.2 How should you select Inquiries? It’s hard to know where to start when picking a research question. We want to pick one that is interesting in its own right or one that would a facilitate real-world decision. We want to pick research questions that we can learn the answer to someday, with a lot of effort. Infeasible research questions should be abandoned as soon as possible, but of course that’s hard to do. The trouble is that it’s hard to know what research questions are feasible before you start looking into it, and it’s really hard to quit research projects once you learn they are infeasible because of the sunk cost fallacy. Sometimes, people give advice to students to follow a “theory-first” route to picking a research question. Read the literature, find an unsolved puzzle, then start choosing among the methodlogical approaches that might answer the problem. Others eschew the theory-first approach: “How on earth are you going to happen to land upon an unsolved – and yet somehow solvable – puzzle just by reading!?”. These advice-givers emphasize a method-first route. Master the technical data-gathering and analysis procedures first, then set off to find opportunies to apply them. The theory-first people then say: “how would you know an interesting theoretical question if it smacked you in the face!?” Both routes work just fine because in most any research project, scholars necessarily toggle between both modes iteratively. In order to select research questions, empiricists have to be concerned about the entire research design. We have to develop empirical strategies to provide answers to our inquiries. We have to learn a lot about how to select data and answer strategies in ways that map on to inquiries about models. So empiricists have to learn both about about models and inquiries (theory) as well as about data strategies and answer strategies (empirics). The sine qua non of a good research question (\\(I\\)) is that there is a feasible design (\\(MIDA\\)) that could answer it. That means picking a good question \\(I\\) does not just involve theory. You should study \\(M\\) to understand which \\(I\\)s are worth knowing. But you should also study \\(D\\) and \\(A\\) in order to learn how to demonstrate \\(I\\). Here’s our best advice for how to get started picking a research question: Write down the \\(M\\) and \\(I\\) of any causal model that interests you to get started thinking about selecting strong \\(D\\)s and \\(A\\)s. The goal is to learn how to map \\(I(M)\\) to \\(A(D)\\). Then return to theory to find new important inquiries, then write down a new model inquiry data strategy and answer strategy. This process is how we make progress on \\(M\\), that is, we bring \\(M\\) closer to \\(W\\), thereby making \\(M\\) truer. People who are looking in the forest for mushrooms often don’t see a mushroom for a long period of time. After a while, they acclimate. They get their ``eyes on,’’ and successful finds seem to be around every bend. In this analogy, the theory building process is going for a walk in the forest and methods training is learning to spot mushrooms – you need get your eyes on answerable research questions worth answering. "],
["example-bjorkman-and-svennson.html", "7.3 Example: Bjorkman and Svennson", " 7.3 Example: Bjorkman and Svennson [ example based on BS appendix from APSR paper ] "],
["grab-bag-of-ideas-we-dont-know-where-to-put.html", "7.4 Grab bag of ideas we don’t know where to put", " 7.4 Grab bag of ideas we don’t know where to put Study dependent estimands like Expected ATT vs realized ATT. Yes, your I depends on D, but in a weird way. Since you can write down all possible ATTs in the model, you actually can write the estimand without realizing D, you just don’t know which of the multiverse of estimands you’ve written down will actually eventuate. Since beliefs about the data strategy (it will create a class of subjects who will be treated) can sort of be hacked into the model, I wonder about how fundamental this wrinkle is. Complex counterfactuals QCA estimand. Unknown estimands: Inquiries for discovery Auxilliary Inquiries: these inquiries are not the main substantive focus of the design, but they are features of the model that have observable implications and can be checked. For models that include mediating roles for a variable, need to demonstrate that Z affects M at a minimum. Balance tables demonstrate a required assumption that Z is orthogonal to pre-treatment characteristics. A full DAG specification can enumerate the full set of conditional independencies that are implied by the model; these can be checked and verified. A good inquiry is informative about the model. - Existence of a node (descriptive mean) - Model suggests two variables should be correlated (descriptive correlation) - Arrow between a node (experiment) - CDF / PDF of a variable A bad inquiry: - does not exist - does not address a “central” part of the model - does not summarize the theory You are responsible for your inquiry. Choice of inquiry is not valueless. How do you choose an inquiry? You figure out what inquiries people are fighting about. You “enter the debate.” stuff from MODEL Potential outcomes notation is especially useful for defining inquiries. The most common causal inquiry is the Average Treatment Effect (ATE), which is written like this8: \\[ ATE \\equiv E[Y_i(1) - Y_i(0)] \\] We want to emphasize that the ATE is an average. The difference between \\(Y_i(1)\\) and \\(Y_i(0)\\) for unit \\(i\\) is an individual-level treatment effect (sometimes we’ll refer to \\(Y_i(1) - Y_i(0)\\) as \\(\\tau_i\\)). The ATE averages over all of the individual-level treatment effects in the relevant population. Some units will have a \\(tau_i\\) that is higher than the ATE, some will have a \\(\\tau_i\\) that is lower. We emphasize this because sometimes people mistakenly think that by focusing on an ATE, researchers “assume” that everyone experiences the same treatment effect. This is not true. The ATE is just a single-number summary of a possibly very heterogeneous set of responses to treatment. We can also use potential outcomes notation to define other, more complicated inquiries. Throughout the book, we’ll describe a series of them: local average treatment effects, average treatment effects on the treated, average direct effects, average indirect effects, and spillover effects, to name a few. For now, we’ll write down three new inquiries: the conditional average treatment effect (CATE) of \\(Z\\) on \\(Y\\) for units with \\(X = 1\\), the CATE among units with \\(X = 0\\), and the difference-in-CATEs: \\[ \\begin{aligned} CATE_{(X = 1)} &amp;\\equiv E[Y_i(1) - Y_i(0) | X = 1] \\\\ CATE_{(X = 0)} &amp;\\equiv E[Y_i(1) - Y_i(0) | X = 0] \\\\ Diff-in-CATEs &amp;\\equiv E[Y_i(1) - Y_i(0) | X = 1] - E[Y_i(1) - Y_i(0) | X = 0] \\end{aligned} \\] The expectation operator \\(E[]\\) is a way of describing the average of a random variable. In general, \\(E[X] = \\sum_{x \\in X} x * pr(X = x)\\) for discrete outcomes. Here we are slightly abusing the notation, since in a fixed population, \\(Y_i(1)\\) and \\(Y_i(0)\\) are not random variables. We could write the ATE as \\(\\frac{1}{N}\\sum_1^N Y_i(1) - Y_i(0)\\), or we could just imagine that we drawing one unit at random from the fixed population; the expectation operator can be defined with respect to this imaginary random variable in order to save ourselves notational headaches.↩ "],
["crafting-a-data-strategy.html", "Chapter 8 Crafting a data strategy", " Chapter 8 Crafting a data strategy The data strategy is what researchers do in the world in order to collect information about it. Depending on the design, it could include decisions about any or or all of the following: how to sample or select cases, how to assign treatments, or how to measure outcomes. These choices apply to all kinds of research. In experimental research, a large focus is given to the assignment of treatments. How many treatment conditions should there be? Should we use a simple coin flip to decide who receives treatment, or should we use a more complicated strategy like blocking? Experimenters are of course also very concerned with sampling and measurement procedures, but it is the random assignment to treatments that make experiments distinctive among research designs. Quantitative descriptive research, on the other hand, often has an inquiry like the population average of some outcome variable. Since the goal here is to draw inferences about a population on the basis of a sample, we need to pay special attention to the procedure by which units are selected into the sample. We might use a random sampling procedure in order to generate a design-based justification for generalizing from samples to population. Nonrandom sampling procedures are also possible: convenience sampling, respondent-driven sampling, and snowball sampling are all data strategies that do not include an explictly random component. Once we have selected units into the sample, we need to measure them in some way. The tools we use to measure are a critical part of the data strategy. For many social scientific studies, a main way we collect information is through surveys. A huge methodological literature on survey administration has developed to help guide researchers who have to design questionnaires. Bad survey questions yield distorted or noisy responses. They can be distored if responses are systematically biased away from the true latent target the question is designed to measure, in which case the question has low validity. They can be noisy if (hypothetically) you would obtain different answers each time you asked the same person the same question, in which case the question has low reliability. Beyond surveys, we might use administrative data to collect outcomes. The concerns about validity and reliability do not disappear once we move out of the survey environment. The information that shows up in an administrative database is itself the result of many human decisions, each of which has the possibility of increasing or decreasing the distance between the measurement and the thing to be measured. Researchers have to choose good sampling, assignment, and measurement techniques that, when combined and applied to the world, will produce information that is ready for analysis. We will discuss answer strategies – the set of analysis choices about what to do with the data once it’s collected – in the next chapter. The data and answer strategies are of course intimately interconnected. How you analyze data depends deeply on how it was collected and how you collect data depends just as deeply on how you plan to analyze it. For the moment, we are thinking through the many choices we might make as part of the data strategy, but of course they will have to be considered in concert with the answer strategy in any applied research design setting. The data strategy is a set of procedures that result in a dataset. It is important to keep these two concepts straight. If you apply data strategy \\(D\\), it produces dataset \\(d\\). The data \\(d\\) is the result of the data strategy \\(D\\). We say \\(d\\) is “the” result of \\(D\\), since when we apply the data strategy to the world, we only do so once and we obtain the data that we obtain. But when we are crafting a data strategy, we have to think about the many datasets that the data strategy could have produced. Some of the datasets might be really excellent. For example, in good datasets, we achieve good covariate balance across the treatment and control groups. Or we might draw a sample whose distribution of observable characteristics looks really similar to the population. But some of the datasets might be worse: because of the vagaries of randomization, the particular realizations of the random assignment or random sampling might more more or less balanced. We do not have to settle for data strategies that can produce worse datasets! We want to choose data strategy \\(D\\) that is likely to result in a high-quality dataset \\(d\\).\n"],
["elements-of-a-data-strategy.html", "8.1 Elements of a data strategy", " 8.1 Elements of a data strategy The data strategy has three elements: a strategy for selecting the units you will collect data about (sampling); a strategy for assigning sampled units into treatment conditions to reveal different potential outcomes (treatment assignment); and a strategy for measuring characteristics of sampled units (measurement). All studies will have a measurement strategy (otherwise nothing is measured and no data results from the data strategy), but only some will have sampling and treatment assignment procedures. A survey to measure satisfaction in a class might not sample units and instead measure all students in the class and no treatment would be assigned (we could, of course, define the sampling strategy as sampling all units). An experiment conducted in a class might have measurement and a treatment assignment strategy but no sampling. A survey of voters in a city might have a measurement strategy and a sampling strategy (random digit dialing, for example), but no treatment assignment (of course, here, we could define the treatment assignment procedure as assigning all units to a no-intervention condition). 8.1.1 Sampling Sampling strategies concern how units are selected from the population to be studied. We sample units in order to assign them to treatments, measure their characteristics, or both. Every design has a sampling strategy, but some are simple, such as “take all the units from a population.” The model and the inquiry guide choices about whom to sample and how to do it. Within the set of strategies, design diagnosis in terms of bias, power, and RMSE can guide the choice among designs that are feasible in terms of cost and logistics. There are four interrelated steps to choosing a sampling strategy. First, you define the target population that you will make sample from and important subgroups within it relevant to your inquiry. Often, the target population will not be the same as the population defined in the model. Though the model defines the population of units about which we want to make inferences, it may be impossible to obtain a list of those units or even to obtain a convenience sample of them. Studies that use Mechanical Turk face this problem: often we are interested in studying the effect of a treatment on Americans, but we have access to a convenience sample of Mechanical Turkers. The Turkers are our target population, and the population in the model is Americans. Your choice of target population should be guided by the population in the model (is it a perfect or good approximation of the population of interest). In addition to choosing the population, you choose the set of important subgroups. This choice is guided by the inquiry. If you are interested in the difference in conditional average treatment effects between two subgroups, it will be important to be able to measure and sample from those two subgroups. When the inquiry is a summary of all units in the population, there will be no important subgroups, only the whole. The second choice about a sampling strategy is determined by your budget and logistical research constraints: should you sample all units in the target population, or just some of them? If you can sample all units in the target population (such as students enrolled in Fall semester in the UCLA political science department), you typically will. Otherwise, you will study a subset. After selecting the target population, you select a method for sampling from them if you have decided to study a subsample, and the number of units overall and from important subgroups in the target population. There are three classes of sampling procedures: convenience sampling; random sampling; and purposive sampling. Convenience sampling means you obtain a set of units from the population, but how you do so is not governed by any rule. Instead, you find units in the cheapest way possible. Convenience sampling is a good choice in two circumstances: it is the only strategy that is affordable, or when sampling is ignorable. Ignorability refers to the independence of missingness and characteristics of units. For many purposes, convenience sampling is sufficient, such as conducting experiments where treatment effects are not expected to vary by characteristics of the units in the sample. In other circumstances, such as measuring a sample mean like the proportion of people who are unmarried, convenience sampling is likely to lead to badly biased estimates due to a lack of ignorability. The bias comes from the fact that the types of people you are able to sample may have different outcomes than the types of people you are unable to sample. Many types of qualitative and quantitative research involve convenience sampling. Archival research often involves a convenience sample of documents on a certain topic that exist in an archive. The question of how these documents differ from those that would be in a different archive, or how the documents available in archives differ from those that do not ever make it into the archive importantly shapes what we can learn from them (Aliza Luft paper cite). With the decline of telephone survey response rates (cite), researchers can no longer rely on random digit dialing to obtain a representative sample of people in many countries, and instead must rely on convenience samples from the internet or panels who agree to have their phone numbers in a list. Reweighting techniques in the answer strategy can, in some cases, help recover estimates for the population as a whole if sampling is ignorable conditional on variables you can measure and weight on. Random sampling addresses this problem, and breaks the dependence between whether you are sampled and unit characteristics. Individual outcomes are independent of whether a unit is sampled. [longer discussion of why] The ideal sampling strategy is random sampling from a list of all units in the population. If there is a positive probability of sampling every unit, then we can make inferences about the entire population. The easiest way to guarantee this is the case is to draw up a list of all units and to take a random sample without replacement from the list. There are many ways to draw a random sample, but two are particularly common in the social sciences: simple and stratified. Simple random sampling is sampling without replacement with a fixed sample size. Stratified random sampling takes a simple random sample of a fixed size from two or more subgroups, or strata, in the data. There are two different reasons you might select stratified random sampling over simple: your inquiry might involve comparisons between two subgroups, and you want to ensure you have a sufficient size in each subgroup; and there may be very different amounts of variability in two groups, and you want the most efficient estimate of a summary of that data so you oversample from the group with the higher variance to get a similar amount of precision from each group. Stratified sampling is a simple case of a larger class of sampling strategies, in which the probability of inclusion in the sample varies across units. In stratified sampling with two strata, the probability of inclusion is set for the two groups and is fixed within them. However, the probability can vary within strata as well. If you wish to have you sample include all age groups but upweight older people, you could make the probability of inclusion a function of age: the probability of sampling an 18 year old could be 0.01 ranging up to a probability of 0.1 for 80 year olds. When it is impossible to get a list of all units in the target population either because it does not exist or is too costly to obtain, clustered sampling is often a substitute. In cluster sampling, you obtain a list of clusters of units, randomly sample from the list of clusters, and either collect data from all units within selected clusters, or conduct a simple random sample of units within sampled cluster (this would represent a simple form of multistage sampling). The advantage is you can still obtain a random sample of units, but it is feasible to do so because you have a list of clusters (but not of all units). The disadvantage is that units within clusters often have similar outcomes, so you don’t get as much benefit from sampling more clusters as you do from sampling units from other clusters that are more different. This efficiency tradeoff – how many clusters to choose vs. how many units within clusters – can be explored in design diagnosis by comparing the RMSE of various choices. There are many other forms of multistage sampling – sample provinces then sample districts within provinces, then villages within districts, and people within villages – that fit particular aims and budgets. Purposive sampling is a catch-all term for rule-based sampling strategies that do not involve random draws but also are not purely based on convenience and cost. A common example is quota sampling. In studies where there is treatment effect heterogeneity, getting samples highly imbalanced in the characteristics by which effects vary will lead to bias for the population average treatment effect. If effects vary by gender and the study is conducted on Mechanical Turk, which skews male, there may be a problem. Quota sampling addresses the problem by ensuring through a convenience sample that fixed proportions of types of people are selected. In Mechanical Turk, we might set a quota of 50% men and 50% women. We would continue to select people in the convenience sample until we reached this threshold, by rejecting men from the sample after we reached 50%. Another common form of purposive sampling is respondent-driven sampling, which is used to sample from hard-to-reach populations such as HIV-positive needle users. RDS methods often begin with a convenience sample and then systematically obtain contacts for other units who share the same characteristic in order the build a large sample. Case selection is another term for a sampling strategy. In case study research, whether qualitative or quantitative, the way you select the (typically small) set of cases is of great importance, and considerable attention has been paid to developing purposive case selection methods. Mills (cite) method of difference is a common case selection procedure, in which cases are selected that have different outcomes and the cases are inspected to identify characteristics that differ that may have led to the divergence. Lieberman (2005) proposes a method for selecting cases for indepth study from an initial regression study: cases on the regression line, well-predicted by it, are selected to determine whether the hypothesized causal relationship took place in those cases. Off-the-line cases, that are not well-predicted, are also selected to determine what factors may be left out of the regression model that provide alternative mechanisms. In each method, the inferential goal determines how cases are selected. Research design diagnosis can also help compare the efficiency and bias of these methods to select cases. Selection on the dependent variable is purposive sampling method that has recieved substantial attention. The procedure is to select positive outcomes — units in which an event such as a revolution took place, for example — and study what led to that outcome. Geddes (XXXX) lays out the problem in this method for case study research: we cannot know whether the factors identified from investigating these cases led to the event, because we do not know whether those factors are also present in units where the outcome did not happen. If the factor is present in both, it is unlikely to have caused the positive outcome. The same problem plagues observational quantitative research. Ramsay et al. (XXXX) discuss the bias that results from studying the factors predicting suicide terrorism by only looking at observations where suicide terrorism occurred. In order to avoid these problems accidentally creeping into our research, it is important to diagnose the bias of our designs and not just the efficiency. In short, the choice of sampling strategy depends on features of the model and the inquiry, and different sampling strategies can be compared in terms of power and RMSE in design diagnosis. The model defines the population of units we are interested in making inferences about, and the target population of the sampling strategy should match that as much as possible. The model also points us to important subgroups (defined by nodes or endogenous variables) that we may wish to stratify on, depending on the variability within those subgroups. Whether we select convenience, random, or purposive sampling depends on our budget and logistical constraints as well as the efficiency (power or RMSE) of the design. If there is little bias from convenience sampling, we will often want to select it for cost reasons; if we cannot obtain a convenience sample that has the right composition, we may choose a purposive method that ensures we do. The choice between simple and stratified sampling comes down to the inquiry and to a diagnosis of the RMSE: when the inquiry involves a comparison of subgroups, we will often select stratified sampling. In either, a diagnosis of alternative designs in terms of power or RMSE will guide selection. Sampling, like all data strategies, is an intervention in the world by researchers. As a result, it may have independent causal effects on outcomes. As we will see in the next section, there is no deep difference between sampling and treatment assignment, because in each case we are randomly sampling from two potential outcomes. In treatment assignment, we are assigning units to a treatment group and a control group, and obtaining a sample of the treated potential outcome and a second random sample of the control potential outcome. In random sampling, we are assigning units to be sampled or to not be sampled. We obtain one sample of the sampled potential outcome. However, the non-sampled potential outcome exists conceptually, and may differ from the sampled potential outcome. The act of including units in the sample may change outcomes, for example if you are selected to participate in a medical trial you believe you are going to receive better care and a placebo effect changes your health condition. A research design in which you conduct an experiment on sampled units, but also unobtrusively measure outcomes in nonsampled units could be used to estimate the effect of inclusion in the sample. This design would provide a random sample of the nonsampled potential outcome. 8.1.1.1 Example: Wang et al. (2015) Xbox forecasting of elections 8.1.2 Treatment assignment Treatment assignment is closely related to sampling. A sampling strategy assigns units in a population either to be included in the sample or excluded. A simple treatment assignment strategy assigns units in the sample to one of two conditions, treatment or control. Both assign units from a group into one of two groups. The difference is only in the fact that we do not measure outcomes for nonsampled units (though we could, to learn about how nonsampled units differ from sampled units). The mechanics and statistics of sampling and simple treatment assignments are identical but for this difference. Assignment procedures may assign units into more than two treatment conditions. In a factorial design, two treatments are assigned to a set of units, and a unit can be assigned to neither treatment (this is a “pure control”), just one, the other, or both. In this case, there are four groups a unit could be assigned to. simple, complete, blocked, clustered, blocked and clustered point restricted randomization no assignment procedure at all multiple arms adaptive the inquiry and the model guide your choices of which treatments to assign and how to assign them 8.1.2.1 Example: sociology natural experiment from government or official cutoff 8.1.3 Measurement Should this be where we do the first bit of distinction between latent and observed survey experiments to measure latent characteristics more T (david McKenzie). How frequently to measure. Andy - arguing against intermediate measurement? multiple measurements of Y. make a scale the inquiry and model guide your choices of which endogenous outcomes to measure issues with rates - y^* / x^*, then rates are messed up 8.1.3.1 Example: Weaver, Prowse, and Piston (2019) References "],
["grab-bag.html", "8.2 Grab bag", " 8.2 Grab bag ethics belongs in data strategy just downloading the data. Did you offload the data strategy possibly ambiguous where the data strategy ends and the analysis strategy ends. SOMEone did parts of the datastrategy Answer strategy section distinguishes qual from quant; should the data strategy section do the same? Make the point that lots of kinds of activities are data strategies. Interviews are a complex interaction of data strategy and answer strategy make them hard to declare. Data strategy is a main locus of ethical problems. That’s not to say that ethical challenges don’t occur when we have bad models or inquiries, or when our answer strategies violate subject privacy. Data strategies change what outcomes are revealed (as in an experiment) or measured (as in a survey). The act of treating or measure can induce trauma or other bad outcomes. "],
["choosing-an-answer-strategy.html", "Chapter 9 Choosing an answer strategy", " Chapter 9 Choosing an answer strategy Answer strategies \\(A\\) are the full set of procedures you use to map the data generated by the data strategy (\\(d\\)) to the eventual estimate \\(a^d\\). Just like we can write \\(I(m) = a^m\\), we can write \\(A(d) = a^a\\). The mapping of data to estimates through the answer strategy is to all empirical research, regardless of whether the inquiry is causal or descriptive, whether the data strategy is observational or experimental, and whether the answer strategy itself is fundamentally qualitative or quantitative. A research design needs at least one answer strategy for each “atomic” inquiry, though in principle, one answer strategy could target multiple inquiries and one inquiry can be targeted by multiple answer strategies. A simple answer strategy for a randomized trial is the difference-in-means estimator. The difference-in-means answer strategy aims at the average treatment effect inquiry. Another answer strategy is the average answer to a survey question about support for a politician among 500 respondents from a random sample of the country, which answers the inquiry ``what is the average support for the politician in the country.’’ An answer strategy is a function that produces that an estimate from data. For quantitative research designs, this includes the literal function executed by the computer to analyze the data (like lm_robust in R or reg in Stata), but also includes other choices made along the way from raw data to the estimate reported in the paper. The choices include data cleaning procedures, inclusion and exclusion criteria, and the routes through the garden of forking paths researchers follow depending on how the study turns out. For some qualitative research designs, the answer strategy might not include a computer function but is nevertheless the function that maps the information gathered by the researcher to the analysis of it they give in words. It is a function in the sense that if the data were different, the analysis would be different as well. The idea that answer strategies are has implications both for how we design research but also for how we evaluate it. The full procedure includes the processes used to select final estimators, the ancillary analyses used to build confidence in the main estimates (i.e., robustness checks), and the visual and prose descriptions of the study results. When the data strategy does not go according to plan (for example, when participants do not comply with assigned treatments or respond to survey questions), the set of compensating adjustments to the data analysis are also part of the answer strategy. If the answer strategy depends on how the data turn out, we need to assess the if-then analysis procedure under different possible realizations of the data to know if the procedure is a good one. Answer strategies commonly include measures of uncertainty. When answer strategies are decisions, we can express our uncertainty about the decision as a probability it was the correct one. When answer strategies produce point estimates of parameters, we can express our uncertainty as our understanding of the sampling distribution (usually summarized by its standard deviation, which goes by the special name “standard error.”). The uncertainty measures for point estimates are often used as the basis for significance tests. Significance tests and point estimators usually refer to different inquiries. For example, if my inquiry is the ATE, my difference-in-means estimator gives me a point estimate and a standard error to characterize my uncertainty about that estiamte. The significance test regards a different inquiry from the ATE itself. The inquiry for the (null hypothesis) significance test is: is the ATE equal to zero? The possible values of \\(a^m\\) are NO and YES and the possible values of \\(a^d\\) are NO and I don't know, which leads to quite a bit of confusion about hypothesis testing. How should you choose an answer strategy? A good answer strategy is one that generates an answer \\(a^d\\) that is close to \\(a^m\\) – ideally identical. This standard is only so useful, because now we need to ask how we can keep \\(a^d\\) close to \\(a^m\\)? This question has no answer in general, because the specifics of the answer strategy depend strongly on the details of the model, the inquiry, and the data strategy. That said, inspiration for an all-purpose approach to finding a good answer stratgey comes from the “plug-in principle” (See XXX, also Aronow and Miller for a good introduction). In estimation theory, the plug-in principle reflects how you can often construct estimators from the mathematical definitions of estimands simply by replacing expectations and varaiances with sample analogues. That is, the function that defines the estimand (the inquiry) is analogous to the function that defines the estimator (the answer strategy). The plug-in principle is behind the difference-in-means estimator of the ATE. The ATE estimand is written: \\[ ATE = E[Y_i(1) - Y_i(0)] = E[Y_i(1)] - E[Y_i(0)] \\] The difference-in-means estimator just replaces the expectations with sums, where the first 1 through \\(m\\) subjects are assigned to treatment and the remainder to control. As it happens, this estimator is unbiased for the ATE estimand when the data strategy samples from \\(Y_i(1)\\) and \\(Y_i(0)\\) at random. \\[ DIM = \\frac{\\sum_1^mY_i(1)}{m} - \\frac{\\sum_{m + 1^NY_i(0)}{N-m} \\] The symmetry between the definition of the ATE and the definition of the difference-in-means operator is not an accident – it’s an instance of the plug in princple at work. The plug-in-principle points the way to how we should choose answer strategies. In order to keep \\(a^d\\) close to \\(a^m\\), we should keep \\(A(d)\\) as close to \\(I(m)\\) as is possible. "],
["example-psych-three-papers-study-with-mediation.html", "9.1 Example: psych three papers study with mediation", " 9.1 Example: psych three papers study with mediation discuss the descriptive, mediation, etc. along with meta-mida "],
["large-classes-of-answer-strategies.html", "9.2 Large Classes of Answer Strategies", " 9.2 Large Classes of Answer Strategies In this section, we describe four large classes of answer strategies. 9.2.1 Testing Tests are an elemental kind of answer strategy. Tests yield yes / no answers to specific tests. In some qualitative traditions, hoop tests, straw-in-the-wind tests, smoking-gun tests, and doubly-decisive tests are common. These tests are procedures for making analysis decisions in a structured way. In frequentist statistics, significance tests are used to make a decision whether to reject or fail to reject a particular null hypothesis. Null hypothesis significance testing has developed a (rightfully) sketchy reputation in recent years as too much weight has been put on the reject / fail to reject decision. That said, sometimes that decision is rightfully important and in such cases, null hypothesis significance testing is a great tool. [cite erin and naoki on sign tests] [cite erin on equivalence testinf] 9.2.2 Point estimation Point estimation is possibly the most common class of answer strategy in quantitative social science. Point estimators are things like the difference-in-means, ordinary least squares regression, instrumental variables regression, random forests, the LASSO, ridge regression, BART… the list goes on and on. These are the data analysis tools taught in many graduate methods courses. What’s incredible is that even though the population of estimators proliferates with each passing year, the number of kinds of inquiries they are useful for estimating stays relatively flat. When we do point estimatation, we are mainly interested in estimating averages, differences, variances, and conditional expectation functions of increasing dimensionality. There are of course many other kinds of estimands (like ratios and quantiles), but the main point is, most of the variation in how scholars conduct point estimation is in the answer strategy, not in the inquiry. (point people to Hastie and Tibshirani) 9.2.3 Bounds EV bounds trimming bounds is CI a bounding estimator? 9.2.4 Posterior distributions 9.2.5 Testing Tests are answer strategies that return TRUE or FALSE as an answer. significance testing (incl. sign test) equivalence testing straw in the wind / hoop test doubly decisive 1997 book on tests "],
["answer-strategies-as-procedures.html", "9.3 Answer Strategies as Procedures", " 9.3 Answer Strategies as Procedures Consider a randomized experiment that seeks to estimate the causal effect of a treatment. The answer strategy is not just “Logistic Regression with Covariate adjustment”. It includes every step in the process that takes the raw data, cleans and recodes it, considers 5 alternative estimators (DIM, OLS with covariate adjustment, a fancy thing your colleague suggested but you couldn’t get to converge), before finally settling on logit. Multiple estimates. Answer strategies can account how many statistical tests you are conducting. Often, when generating an answer to a single inquiry, we may construct multiple estimates that provide different types of answers of varying quality. When you present the results from many null hypothesis tests, the rate of falsely rejecting at least one of those tests even when all are true goes up, due to the multiple comparisons problem. If you plan to adjust for this problem, those adjustments are part of your answer strategy, because they will typically adjust the p-values you report and the decisions readers make with them. We may have three survey items that imperfectly estimate a latent quantity. In presenting the results, we could present three estimates from three regressions, we could adjust the three estimates using a procedure such as a family-wise error rate correction, or we could average the three items together into an index and present one estimate from one regression. Which of these three methods we select will change the properties of our answer strategy. Analysis procedures. The final estimator that goes into a paper is neither the beginning nor the end of the answer strategy. Procedures, if any, by which you explore the data and determine a final set of estimates are part of the answer strategy. Procedures for summarizing multiple estimates are one example of many. Commonly, the final estimator that is selected depended on a exploratory procedure in which multiple models are assessed, for example by comparing model fit statistics. The answer strategy of our research design is not to fit the final model — is it this multiple step if-then procedure. These procedures may be part of a prespecified analysis plan or they may be informal, so it may sometimes only be possible to declare the full design after the data is obtained. (We may find that a different analysis procedure that was not data dependent would have been preferable, if we diagnose the design after the fact.) The reason to declare the procedure rather than the final estimator is that the diagnosis of the design may differ. The procedure may be more powerful, if for example we assessed multiple sets of covariate controls and selecting the specification with the lowest standard error of the estimate. But the procedure may also exhibit poor coverage, accounting for these multiple bites at the apple. We also sometimes find that the model we planned to run to analyze the data cannot be estimated. In these cases, there is an iterative estimation procedure in which a first model is run, changes to the specification are made, and a second or third model is presented as the result. The full set of steps — a decision tree, depending on what is estimable — is the answer strategy and we can evaluate whether it is a good one not only under the realized data but under other possible realizations where the decision tree would be the same but the decisions different. In fact, there are examples of analysis procedures in most types of research, quantitative or qualitative. Many strategies for causal inference with observational data involve not only an estimation strategy but a set of falsification or placebo tests. The answer provided by these research designs depends in a crucial way on the results of these tests: if the tests fail, the design provides no definitive answer. In qualitative research, process tracing involves a set of steps, the results of which depend on information gathered in earlier steps. Many mixed methods strategies are also multi-step procedures. Nested designs involve running a quantitative analysis and then selecting cases on the basis of predictions from the regression. These designs cannot be assessed by considering a single step of the procedure in isolation. When things do not go according to plan. To compare answer strategies, you can imagine the estimators that are possible if things go well as well as if things go wrong, when there is missing data or there are outliers in variables. A good answer strategy, which might be a single estimator, or a procedure if-this-then-that, can handle both states of the world. Procedures for addressing deviations from expected analyses are part of the answer strategy. Even in the absence of a preanalysis plan, we often have a way we expect to analyze the data if things go well. When they do not — because data are missing, there is noncompliance with an intervention, or the study is suspended for example — the answers will change. These procedures determine the answer the study provides (or in some cases does not), so are part of the answer strategy. Standard operating procedures (lin and green) are documents that systematize these procedures in advance. We demonstrate the fact that the properties of procedures differ from the properties of a design with the final estimator in a simple example. We compare two possible estimation specifications, with and without covariates, to a procedure in which we run both models and report the model in our paper that has the lower p-value. The models are exactly the same, but the properties of the procedure differ from the properties of either of the two possible models. In particular, the procedure has higher power than either of the two models, but it exhibits poor coverage, which means we have a bias in our measure of uncertainty. report_lower_p_value &lt;- function(data){ fit_nocov &lt;- lm_robust(Y ~ Z, data) fit_cov &lt;- lm_robust(Y ~ Z + X, data) # select fit with lower p.value on Z if(fit_cov$p.value[2] &lt; fit_nocov$p.value[2]){ fit_selected &lt;- fit_cov } else { fit_selected &lt;- fit_nocov } fit_selected %&gt;% tidy %&gt;% filter(term == &quot;Z&quot;) } design &lt;- declare_population( N = 100, X = rbinom(N, 1, 0.5), u = rnorm(N) ) + declare_potential_outcomes(Y ~ 0.25 * Z + 10 * X + u) + declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + declare_assignment(prob = 0.5) + declare_reveal(Y, Z) + declare_estimator(Y ~ Z, model = lm_robust, label = &quot;nocov&quot;, estimand = &quot;ATE&quot;) + declare_estimator(Y ~ Z, model = lm_robust, label = &quot;cov&quot;, estimand = &quot;ATE&quot;) + declare_estimator( handler = label_estimator(report_lower_p_value), label = &quot;select-lower-p-value&quot;, estimand = &quot;ATE&quot;) diags &lt;- diagnose_design(design, sims = sims) design_label estimand_label estimator_label term bias se(bias) rmse se(rmse) power se(power) coverage se(coverage) mean_estimate se(mean_estimate) sd_estimate se(sd_estimate) mean_se se(mean_se) type_s_rate se(type_s_rate) mean_estimand se(mean_estimand) n_sims design ATE cov Z -0.2832544 0.3608655 1.0843741 0.207917 0.0 0.0000000 0.9 0.0941952 -0.0332544 0.3608655 1.1033454 0.1777175 1.0207502 0.0115592 NaN NA 0.25 0 10 design ATE nocov Z -0.2832544 0.3608655 1.0843741 0.207917 0.0 0.0000000 0.9 0.0941952 -0.0332544 0.3608655 1.1033454 0.1777175 1.0207502 0.0115592 NaN NA 0.25 0 10 design ATE select-lower-p-value Z -0.2703755 0.2457192 0.7772791 0.314109 0.1 0.1013246 0.9 0.0941952 -0.0203755 0.2457192 0.7681578 0.2744808 0.4469304 0.1235708 0 NA 0.25 0 10 "],
["robustness.html", "9.4 Robustness", " 9.4 Robustness Robustness checks are part of the answer strategy. Often, a single estimator is presented as the main analysis but then a series of alternative specifications are displayed in an appendix (such as including or excluding covariates and their interactions, different subsets of the data, or alternative statistical models). These differ from multiple estimates of a latent quantity in that the goal is not a primary analysis, but rather to support the main analysis. The purpose is to provide readers with evidence about how dependent the main results are on the specification, data subset, and statistical model used. The decision a reader makes from a paper depends not only on the main estimate but also the robustness checks. As a result, we want to assess the properties of the two together. We illustrate with a simple analysis of the correlation between two variables y1 and y2, who have a true positive correlation. y2 is also a function of an observed covariate x and measurement error. Our main analysis is a bivariate regression predicting y2 with y1. We compare this answer strategy to one in which we run that analysis, but also run a robustness check controlling for x. We do this because as the analyst we are unsure of the true DGP and wish to demonstrate to reviewer’s that our results are not dependent on the functional form we choose. bivariate_correlation_decision &lt;- function(data) { fit &lt;- lm_robust(y2 ~ y1, data) %&gt;% tidy %&gt;% filter(term == &quot;y1&quot;) tibble(decision = fit$p.value &lt;= 0.05) } interacted_correlation_decision &lt;- function(data) { fit &lt;- lm_robust(y2 ~ y1 + x, data) %&gt;% tidy %&gt;% filter(term == &quot;y1&quot;) tibble(decision = fit$p.value &lt;= 0.05) } robustness_check_decision &lt;- function(data) { main_analysis &lt;- bivariate_correlation_decision(data) robustness_check &lt;- interacted_correlation_decision(data) tibble(decision = main_analysis$decision == TRUE &amp; robustness_check$decision == TRUE) } robustness_checks_design &lt;- declare_population( N = 100, x = rnorm(N), y1 = rnorm(N), y2 = 0.15 * y1 + 0.01 * x + rnorm(N) ) + declare_estimand(y1_y2_are_related = TRUE) + declare_estimator(handler = label_estimator(bivariate_correlation_decision), label = &quot;bivariate&quot;) + declare_estimator(handler = label_estimator(robustness_check_decision), label = &quot;robustness-check&quot;) decision_diagnosis &lt;- declare_diagnosands(correct = mean(decision == estimand), keep_defaults = FALSE) diag &lt;- diagnose_design(robustness_checks_design, sims = sims, diagnosands = decision_diagnosis) We evaluate the two answer strategies in terms of the rate of correctly deciding there is a correlation between y2 and y1. In the main analysis, this means we judge there is a correlation when the p-value is below \\(0.05\\). In our robustness check answer strategy, we decide there is a correlation when both the main analysis and the robustness check return p-values below \\(0.05\\) on the coefficient on y1. We see that we are more likely to correctly judge there is a correlation in the simpler analysis strategy. This is because we added an additional criterion to our decision; both criteria, due to random noise, sometimes fail to reject the null of no correlation. Our second answer strategy is more robust in the sense that we have stronger evidence of a correlation when we run the two analyses together. But we are also less likely to decide (correctly) that there is a relationship. The robustness check is conservative. This exercise highlights that the properties of an answer strategy with secondary analyses will be different than the properties of the main analysis alone. If we planned (or conducted) robustness checks, we may wish to know how good the pair of strategies is together. Using the MIDA way of thinking about designs, we discuss in the diagnosis section another notion of the “robustness” of a design. The typical way we think of robustness checks is multiple secondary analyses conditional on the observed data to build confidence in an analysis of that fixed data. However, the motivation for these robustness checks is uncertainty about the true data generating process. By declaring a design in terms of MIDA, we can think about the robustness of a single estimator to multiple possible true data generating processes. An estimator that is robust in this sense is one that is unbiased with low uncertainty regardless of, say, the true functional form between y1 and y2. To determine whether an estimator is robust, we can redefine a set of designs with different functional forms and assess the rate of correct decisions of our robustness checks strategy under each different model. "],
["presentation-of-answer-the-answer-strategy.html", "9.5 Presentation of answer the answer strategy", " 9.5 Presentation of answer the answer strategy How results are presented in tables and figures is an important part of the answer strategy. Considerable attention has been paid to how to display data, including arguments for switching from tables to graphs as the primary way to present statistical models (Kastellec) and for visually present raw data and models together (Coppock). The reason for this attention is that what inferences readers make when reading a paper depends not only on the statistical procedures used for estimation but the medium in which they are displayed. When numerical estimates are not provided at all, and only visualizations of results, clearly aesthetic choices about which estimates are displayed and even the width of axes will determine what the reader takes away from the answer strategy. This principle applies not just to tables and visualizations; how we describe results in the text of a paper also shapes readers’ inferences from the data. Registered reports are a format for preparing scientific papers that involves prespecifying not only the form of graphs and tables but the text the author plans to write depending on the results. In short, decisions made from your results by readers are not just a function of numerical estimates but how they are presented. We explore this claim by comparing two possible graphical displays of conditional avareage treatment effects in an experiment. A common presentational format is to present the average treatment effect in one group and then the other along with confidence intervals. Inferences are made — either by the author, or by readers — as a function of whether one is significant and not the other. If that is true, the inference is that there is a difference in CATEs. An alternative is to present the estimated difference along with the two effects. The inferences can then directly be based on whether the confidence interval of the difference crosses zero. We illustrate these two visual answer strategies below: We now demonstrate that the answer strategy on the left is flawed. XXYY describe sims. "],
["grab-bag-of-things-we-dont-know-where-they-go.html", "9.6 Grab Bag of things we don’t know where they go", " 9.6 Grab Bag of things we don’t know where they go How you already choose answer strategies in current practice talk about issue of model-based vs design based, as separated from the model you assume in M. in model based you run a procedure that assumes a dgp, which may or may not be connected to the M. your data strategy should shape your answer strategy (analyse as you randomize) Flexibility / overfitting? relationship to nonparametrics claim in 2.1? If you are a bayesian, you can diagnose w.r.t. the location and stregth of your priors. Mediation Inquires: Consider \\(X -&gt; B &lt;- A\\) and \\(X -&gt; A\\). In this DAG \\(A\\) is a mediator. There are three causal inquiries: \\(X -&gt; B\\) \\(X -&gt; A\\) \\(A -&gt; B\\). If we randomize \\(X\\), we can learn about \\(X -&gt; A\\) but not \\(X -&gt; B\\), because \\(X -&gt; B\\) is confounded. We obviously don’t learn anything about \\(A -&gt; B\\) because for that we’d need to randomize \\(A\\). Mediation analysis tries to get all three for the price of one random assignment. At best we get \\(X -&gt; A\\) and the total effect of X on B, which is not just \\(X -&gt; B\\), but instead \\(X -&gt; B\\) + \\((X -&gt; A) * (A -&gt; B)\\). Mediation analysis tries to get something for nothing. "],
["diagnosis-1.html", "Chapter 10 Diagnosis", " Chapter 10 Diagnosis "],
["diagnosing-a-design.html", "10.1 Diagnosing a design", " 10.1 Diagnosing a design Research design diagnosis is the process of evaluating the properties of a research design. Since “property of a research design” is a cumbersome phrase, we made up the word “diagnosand” to refer to those properties of a research design we would like to diagnose. Many diagnosands are familiar. Power is the probability of obtaining a statistically significant result. Bias is the average deviation of estimates from the true value of the estimand. Other diagnosands are more exotic, like the Type-S error rate, which is the probability the estimate has the incorrect sign, conditional on being statistically significant (Gelman and Carlin 2014). Not every diagnosand is relevant for every study. For example, in a descriptive study whose goal is to estimate the fraction of people in France who are left-handed, statistical power is irrelevant. A hypothesis test against the null hypothesis that 0 percent of the people in France are left-handed is preposterous. We know for sure that the fraction is not zero, we just don’t know its precise value. A much more important diagnosand for this study would be RMSE (root-mean-squared-error), which is a measure of how well-measured the estimand that incorporates both bias and variance. How should you choose your diagnosands? In our experience, writing out what would make the study either a success or a failure helps tremendously. Suppose your study would be a success if it produced an estimate that is higher than 4, had a standard error of 1 or less, and yielded statistically significant evidence of treatment effect heterogeneity – write down a diagnosand that is the probability of all three of those conditions being true at the same time. If a study is a failure (in terms of not having been worth the money and effort ex post) when the confidence interval is 20 points wide or wider, then the research team should design to minimize the probability of that occurance. A brief aside on the most common diagnosand of statistical power. This diagnosand is the probability of getting a statistically significant result, which of course depends on many things about your design including, crucially, the unknown magnitude of the parameter to be estimated. You can think of statistical power as the probability of a success, where success is defined as getting a significant results. The conventional “power target” is 80% power. One could imagine redefining statistical power as “null risk,” or the probability of obtaining a null result. In these terms, the conventional power target is a 20% null risk, or a one in five chance of “failure.” Those odds aren’t great, so we recommend designing studies with lower null risk. We also think that statistical power is often over-emphasized relative to other more important diagnosands like bias and rmse. If your design is systematically biased away from zero, your statistical power will be high, but your design is nevertheless weak. "],
["estimating-diagnosands-analytically.html", "10.2 Estimating diagnosands analytically", " 10.2 Estimating diagnosands analytically Diagnosis can be be done with analytic, pencil-and-paper methods. Indeed, research design textbooks often contain many formulas for calculating power under a variety of designs. For example, Gerber and Green include the following power formula: They write: “To illustrate a power analysis, consider a completely randomized experiment where \\(N&gt;2\\) of \\(N\\) units are selected into a binary treatment. The researcher must now make assumptions about the distributions of outcomes for treatment and for control units. In this example, the researcher assumes that the control group has a normally distributed outcome with mean \\(\\mu_c\\), the treatment group has a normally distributed outcome with mean \\(\\mu_t\\), and both group’s outcomes have a standard deviation \\(\\sigma\\). The researcher must also choose \\(\\alpha\\), the desired level of statistical significance (typically 0.05). Under this scenario, there exists a simple asymptotic approximation for the power of the experiment (assuming that the significance test is two-tailed): \\[ \\beta = \\Phi \\bigg(\\frac{|\\mu_t - \\mu_c| \\sqrt{N}}{2\\sigma} - \\Phi^{-1} (1 - \\frac{\\alpha}{2}) \\bigg) \\] where \\(\\beta\\) is the statistical power of the experiment, \\(\\Phi(\\cdot)\\) is the normal cumulative distribution function (CDF), and \\(\\Phi^{-1}(\\cdot)\\) is the inverse of the normal CDF.” This power formula makes detailed assumptions about \\(M\\), \\(D\\), and \\(A\\)? Under \\(M\\), it assumes that both potential outcomes are normally distributed with group specific means and a common variance. Under \\(D\\), it assumes a particular randomization strategy (simple random assignment). Under \\(A\\), it assumes a particular hypothesis testing approach (equal variance \\(t\\)-test with \\(N - 2\\) degrees of freedom). This set of assumptions may be “close enough” in many research settings, but it can be difficult to understand the specific impacts of different beliefs about \\(M\\), \\(D\\) and \\(A\\) on the value of the diagnosand. What if instead of being normally distributed, the potential outcomes are measured in 1 - 5 Likert scales? What if the randomization procedure includes blocking? What if we include covariates in our treatment effect estimation approach? Formulas for some large sources of design variation have been derived (such as clustering), but certainly not for every design variant. Very quickly, hope for analytic design diagnosis fades. The analytic formulas are abstractions – they abstract away from design details and sometimes those design details are important. This problem is not confined to the “power” diagnosand. In randomized experiments, claims about the bias diagnosand are quite general. Many randomized designs are unbiased for the ATE, but not all. Designs that encounter noncompliance, attrition, or some forms of spillover may not be unbiased for the ATE. Even without any of those complications, cluster randomized trials with heteogeneous cluster sizes are not unbiased (joel, imai). Diagnosands depend on design details, because how you conduct your study matters for its properties. That means design diagnosis must be design-aware. Since designs are so heteogenous and can vary on so many dimensions, computer simulation is only feasible way to diagnose anything beyond the simplest ideal-type designs. "],
["estimating-diagnosands-via-simulation.html", "10.3 Estimating diagnosands via simulation", " 10.3 Estimating diagnosands via simulation Research design diagnosis usually occurs in a two-step, simulation-based procedure. First we simulate research designs over and cover, collecting “diagnostic statistics” from each run of the simulation. Second, we summarise the distribution of the diganostic statistics in order to estimate the diagnosands. So we estimate diagnosands by summarizing the distribution of diagnostic statistics – of course this raises the question: what is a diagnostic statistic? We take a draw from the model (\\(m\\)) and calculate the value of the inquiry \\(I(m) = a^m\\). We take one draw from the data strategy (\\(D(m) = d\\)), and calculates the value of the answer strategy \\(A(d) = a^d\\). A diagnostic statistic is some function of \\(a^m\\) and \\(a^d\\). A simple diagnostic statistic is “error,” or the difference between the estimate and the estimand: \\(error = a^d - a^m\\). The bias diagnosand is the expectation of the error statistic \\(E[error]\\) over all possible ways the study could have come out. Usually, we consider many diagnostic statistics at the same time. Here’s a design declaration for a two-arm trial with a balanced (50/50) design. We have 100 subjects and their responses to treatment are drawn from a normal distribution with mean 0.1 and sd 0.1. One draw of this simulation returns the following: One draw from the simulation returns the following: estimand estimate std.error conf.low conf.high p.value 0.10 0.08 0.04 0.003 0.156 0.04 Figure XXX shows the information we might obtain from a single run of the simulation. The filled point is the estimate \\(a^d\\). The open triangle is the estimand \\(a^m\\). The bell-shaped curve is our normal-approximation based estimate of the sampling distribution. The standard deviation of this estimated distribution is our estimated standard error, which expresses our uncertainty. The confidence interval around the estimate is another expression of our uncertainty: We’re not sure where \\(a^d\\) is, but if things are going according to plan, confidence intervals constructed this way will bracket \\(a^d\\) 95% of the time. From this single draw, we can’t yet estimate diagnosands, but we can estimate diagnostic statistics. The estimate was higher than the estimand in this draw, so the error is 0.10 - 0.08 = 0.02. Likewise, the squared error is (0.10 - 0.08)^2 = 0.0004. The \\(p\\)-value is 0.04, which is just barely lower than the threshold of 0.05, so “statistical significance” diagnostic statistic is equal to TRUE. The confidence interval stretches from 0.003 to 0.156, and the value of the estimand (0.10) is between those bounds, so the “covers” diagnostic statistic is equal to TRUE as well. Learning the distribution of diagnostic statistics is the main barrier to design diagnosis. If we could simply write down the distribution of diagnostic statistics, it would be a straightforward matter to summarize them in order to calculate diagnosands. But the distributions of diagnositic statistics depend on the complex of information in all four parts of a research design: M, I, D, and A. For example, the error statistic depends on both \\(a^d\\) and \\(a^m\\), so the details of each matter greatly. To calculate the distributions of the diagnostic statistics, we have to simulate designs not just once, but many many times over. The bias diagnosand is the average error over many runs of the simulation. The statistical power diagnosand is the fraction of runs in which the estimate is significant. The coverage diagnosand is the fraction of runs in which the confidence interval covers the estimand. This figure visualizes just 10 runs of the simulation (obtained with simulate_design(design)). We can see that in each run, \\(a^m\\) is a little different. This might seem counterintuitive – isn’t the estimand supposed to be a fixed number? Some estimands are fixed, others are stochastic, depending on the specifics of the model. Notice how in the design declaration, we drew the potential outcomes from a distribution rather then having them be fixed numbers. This choice incorportates some of our modeling uncertainty. The treatment effects for each unit are close to 0.1, but we’re not sure how close for each particulat unit. We can also see that some of the draws produce statistical significant estimates (the shaded areas are small and the confidence intervals don’t overlap zero), but not all. We get a sense of the true standard error by seeing how the point estimates bounce around. We get a feel for the difference between the estimates of the standard error and true standard error. Design diagnosis is the process of learning about all the ways the study might come out, not just the one way that it will. This line of code does it all in one. We simulate the design 1000 times to calculate the diagnositic satistics, then we summarise them in terms of bias, the true standard error (the standard deviation of the sampling distribution), RMSE, power, and coverage. diagnosis &lt;- diagnose_design(design, sims = 1000, diagnosands = declare_diagnosands( select = c(&quot;bias&quot;, &quot;sd_estimate&quot;, &quot;rmse&quot;, &quot;power&quot;, &quot;coverage&quot;))) diagnosis ## ## Research design diagnosis based on 100 simulations. Diagnosand estimates with bootstrapped standard errors in parentheses (100 replicates). ## ## Design Label Estimand Label Estimator Label Term N Sims Bias SD Estimate ## design ATE estimator Z 100 -0.00 0.04 ## (0.00) (0.00) ## RMSE Power Coverage ## 0.04 0.64 0.95 ## (0.00) (0.05) (0.02) "],
["interactive-element.html", "10.4 Interactive element", " 10.4 Interactive element You can have fun building up a design diagnosis run-by-run with this shiny app. Each time you run the design, it adds a new estimate to the sampling distribution so you can learn about the relationship of a single run of the design to the overall diagnosis. "],
["bias-variance-and-mean-squared-error.html", "10.5 Bias, Variance, and Mean-squared Error", " 10.5 Bias, Variance, and Mean-squared Error This section walks through the interrelationship between three important diagnosands: bias, variance, and a combination of the two: mean-squared error. Many research design decisions involve trading off bias and variance. In trade-off settings,we may need to accept higher variance in order to decrease bias. Likewise, we may need to accept a bit of bias in order to achieve lower variance. The tradeoff is captured by mean-squared error – which is the average squared distance between \\(a^d\\) and \\(a^m\\). Of course we would ideally like to have as low a mean-squared error as possible. We would like to achieve low variance and low bias simultaneously. To illustrate, consider the following three designs as represeted by three targets. The inquiry is the bullseye of the target. The data and answer strategies combine to generate a process by which arrows are shot towards the target. On the left, we have a very bad archer: even though the estimates are unbiased in the sense that they hit the bullseye “on average”, very few of the arrows are on target. In the middle, we have the Katniss Everdeen (the heroine of the Hunger Games novels who is good with a bow) of data and answer strategies: they are both on target and low variance. On the right, we have an archer who is very consisten (low variance) but biased. The mean squared error is highest on the left and lowest in the middle. The archery metaphor is common in research design textbooks because it effectively conveys the difference between variance and bias, but it does elide an important point. It really matters which target your archer is shooting at. Figure XXX shows a bizarre double-target representing two inquiries. The empirical strategy is unbiased and precise for the left inquiry, but it is clearly biased for the right inquiry. When we are describing the properties of \\(a^d\\), we have to be clear about which \\(a^a\\) they are associated with. "],
["four-large-classes-of-diagnosands.html", "10.6 Four large classes of diagnosands:", " 10.6 Four large classes of diagnosands: Type \\(M\\) error rate Type \\(I\\) error rate Type \\(D\\) error rate Type \\(A\\) error rate You can get things wrong if anything in your research design is wrong! "],
["ethics.html", "10.7 Ethics", " 10.7 Ethics There are ethical diagnosands, e.g., what is the distribution of ethical costs such as loss of autonomy. Cite Tara’s paper. Cite Lauren’s paper. Do a diagnosis of how many minutes of subject time are wasted by audit experiments, weigh that against the cost of demonstrating outright racial bias. (cite an audit study that does this calculation) "],
["redesign-2.html", "Chapter 11 Redesign", " Chapter 11 Redesign Design diagnosis is about estimating the diagnosands for a single design. Redesign is about changing design parameters to understand how diagnosands change in reponse. The purpose of redesign is to choose a strong design among the set of feasible designs – or to learn than no feasible design is worth implementing. We usually redesign with respect to the data and answer strategies. We tweak the data strategy by carying \\(N\\) to see how RMSE changes. We change the answer strategy to see how the inclusion of covariates decreases bias. While we usually redesign the empirical side, redesign can take place with respect to the theoretical side (model and inquiry) as well.9 You may have encountered some figures that implicitly do redesign. Power curves are an example of the redesign process. A power curve has sample size on the horizontal axis and statistical power on the vertical axis. From a power curve, you can learn how big a study needs to be in order to achieve a desired level of statistical power. A “minimum detectable effect” figure is similar. It has sample size on the horizontal axis too, but plots the smallest effect size for which 80% power can be achieved. These plots are useful for learning something like, “given my budget, I can only sample 400 units. At 400 units, the MDE is a 0.5 standard deviation effect. My theory says that that the effect should be smaller than that, something closer to 0.1 SDs. I should apply for more funding or study something else.” The highest level advice we have about redesign is, at the beginning of the processes at least, change one design parameter at a time. Vary the data strategy, holding the model, inquiry, and answer strategy constant. Change the answer strategy, while holding the other aspects of the design constant. There are some subtleties here. Sometimes a single design includes ncertainty about the model – for example when we draw parameter values from a random distribution. We have a parameter be drawn from a normal distribution centered at 0.3 with an SD of 0.1 because we think a resonable value for the parameter is “about three, plus or minus”. We imagine that we are “redesigning” with respect to each value of parameter, but only if the goal is to see how the changes in the model change the diagnosands. Otherwise, it’s not redesign, it’s just plain diagnosis, averaging over multiple models.↩ "],
["redesign-mountain.html", "11.1 Redesign Mountain", " 11.1 Redesign Mountain This figure shows a schematic of the redesign process. We vary design parameter 1 and design parameter 2 independently to learn the value of the diagnosand for each combination of design parameters. The redesign process can be in more than 2 dimensions of course. We suggest varying the design parameters you have control over first. "],
["redesign-to-assess-the-robustness-of-designs-to-models-gb.html", "11.2 Redesign to assess the robustness of designs to models [GB]", " 11.2 Redesign to assess the robustness of designs to models [GB] Hold inquiry constant! (read Richard Crump “Moving the Goalposts”) Hold three constant, vary one of MIDA at a time M: ICC, null model, alternative DAGs, heterogeneity I: "],
["coda.html", "Chapter 12 Coda", " Chapter 12 Coda "],
["coda-1.html", "12.1 Coda", " 12.1 Coda How specific do you need to get in your declaration? You declare at a resolution. What is the right one? Is rnorm() good enough? Hard to know! You keep zooming in, changing the resolution, until you can see clearly enough to learn about the design. "],
["part-ii-exercises.html", "Chapter 13 Part II Exercises", " Chapter 13 Part II Exercises Imai, King, and Stuart (2008) Imagine you are a reviewer on a paper that claims smoking causes a smoking addiction. The data offered in support of this claim shows that all 100 subjects who smoke are addicted to smoking and that all 100 subject who do not smoke are not addicted. is not addicted to smoking is addicted to smoking does not smoke 0 100 smokes 100 0 Do the data support the conclusion that smoking causes addiction to smoking? Now imagine you are a reviewer on a paper that claims sailing causes a sailing addiction. The data offered in support of this claim shows that all 100 subjects who sail are addicted to sailing and that all 100 subject who do not sail are not addicted. Do the data support the conclusion that sailing causes addiction to sailing? is not addicted to sailing is addicted to sailing does not sail 0 100 sails 100 0 How do your answers to (a) and (b) differ? Sketch (either in words or with a DAG) the alternative causal models for smoking and sailing that could have produced the two datasets. Propose a data and answer strategy that would distinguish between the two causal models you described in (c). The Complier Average Causal Effect (CACE) is defined as the average effect of treatment on a subset of subjects who “comply” with their treatment assignment, i.e., if assigned to treatment, they take treatment but if assigned to control, they do not take treatment. The Average Treatment Effect on the Treated (ATT) is defined as the average effect of the treatment on those subjects who were treated. These two estimands are subtley different. Declare a design and draw_estimands from it to demonstrate that these two estimands can be different. Hint: the ATT depends on the data strategy but the CACE does not. Give abstract, what can you tell me about MIDA Give a passage from one paper that is a quote of another paper, what’s MIDA. Then look up the paper and say what was missed. Did the quoted passage correctly characterize the design? References "],
["design-library.html", "Chapter 14 Design Library", " Chapter 14 Design Library This section of the book enumerates a series of common social science research designs. Each entry will include description of the design in terms of MIDA and also a declaration of the design in code. We’ll often diagnose designs over the range of values of some design parameters in order to point out especially interesting or unusual features of the design. Our goal in this section is not to provide a comprehensive accounting of all empirical research designs. It’s also not to describe any of the particular designs in exhaustive detail, because we are quite sure that in order for these designs to be useful for any practical purpose, they will need to be modified. The entries in the design library are not recipes that, if you follow the instructions, out will come high-quality research. Instead, we hope that the entries provide inspiration for how to tailor a particular class of designs – the blocked-and-clustered randomized trial or the catch-and-release design – to your own research setting. The basic structure of the design library entry will be useful, but the specifics about plausible ranges of outcomes, sample size constraints, etc, will be different in each particular setting. We’ve split up designs by Inquiry and by Data strategy. Inquires can be descriptive or causal and Data strategies can be observational or experimental. This leads to four categories of research: Observational descriptive, Experimental descriptive, Observational Causal, and Experimental causal. A third dimension along which studies can vary is whether the Answer strategy is qualitative or quantitative. If we include this dimension in our typology, we’d end up with eight broad categories of research design. We don’t see the qualitative-quantitative difference in answer strategy to be as fundamental as the differences in inquiry and data strategy, so we’ll just include both qualitative and quantitative designs in each of our four categories. Besides, social scientists always appreciate a good two-by-two: In the broadest terms, descriptive inquiries can be described as \\(f(\\mathbf{Y(Z = Realized)})\\), where \\(f()\\) is some function and \\(\\mathbf{Y(Z = Realized)}\\) is a vector of realized outcomes. That is, descriptive designs seek to summarize (using \\(f()\\)) the world as it is (as represented by \\(\\mathbf{Y(Z = Realized)}\\)). Descriptive designs can be better or worse at answering that inquiry. The quality of descriptive research designs depends on the extent of measurement, sampling, and estimation error. Causal inquiries can be described as \\(f(\\mathbf{Y(Z)})\\), where \\(Z\\) is not a realized vector of treatments, but is instead is a vector that could take on counterfactual values. A standard causal inquiry is the Average Treatment Effect, in which \\(f()\\) is the function that takes the average of the difference between two potential outcome vectors, \\(Y(Z = 1)\\) and \\(Y(Z = 0)\\). But there are many causal inquiries beyond the ATE – the thing they all have in common is that they are functions not of realized outcomes, but of potential outcomes. The quality of causal research designs depends on everything that a descriptive design depends on, but also on the understanding and quality of the mechanism that assigns units to treatment conditions. All research designs suffer from some kind of missing data problem. Rubin pointed out missing data in surveys come from people you didn’t survey or people who refused to answer. In causal inference problems, the data that are missing are the potential outcomes that were not revealed by the world. In Descriptive studies, the data that are missing are the true values of the things to be measured. Measurement error is a missing data problem too! Observational research designs are typified by researchers having no impact on the units under study. They simply record the outcomes that happened in the world and would have happened even if the study did not occur. Experimental research designs are more active – they cause some potential outcomes to be revealed but not others. In this way, researchers have an impact on the units they study. For this reason, experimental studies tend to raise more ethical questions than do observational studies. Experimenters literally change what potential outcomes become realized outcomes. Sometimes the lines between types of research become blurry. The Hawthorne effect is the name given to the idea that measuring a thing changes it. If there are Hawthorne effects, than observational research designs also change which potential outcomes are revealed. That is, if there is a difference between Y(Z = measured) and Y(Z = unmeasured), then the act of observation changes that which is observed. Passive data collection methods are sometimes preferred on these grounds. "],
["observational-designs-for-descriptive-inference.html", "Chapter 15 Observational designs for descriptive inference", " Chapter 15 Observational designs for descriptive inference section introduction "],
["random-sampling.html", "15.1 Random sampling", " 15.1 Random sampling 15.1.1 Declaration fixed_population &lt;- declare_population(N = 500, Y = sample(1:7, N, replace = TRUE))() design &lt;- declare_population(data = fixed_population) + declare_estimand(Y_bar = mean(Y)) + declare_sampling(n = 100) + declare_estimator(Y ~ 1, model = lm_robust, estimand = &quot;Y_bar&quot;) 15.1.2 Dag 15.1.3 Example 15.1.4 Simple random sampling Often we are interested in features of a population, but data on the entire population is prohibitively expensive to collect. Instead, researchers obtain data on a small fraction of the population and use measurements taken on that sample to draw inferences about the population. Imagine we seek to estimate the average political ideology of residents of the small town of Portola, California, on a left-right scale that varies from 1 (most liberal) to 7 (most conservative). We draw a simple random sample in which all residents have an equal chance of inclusion in the study. It’s a straightforward design but formally declaring it will make it easy to assess its properties. 15.1.4.1 Design Declaration Model: Even for this most basic of designs, researchers bring to bear a background model of the world. As described in Chapter 1, the three elements of a model are the signature, probability distributions over variables, and functional equations among variables. The signature here is a specification of the variable of interest, \\(Y\\), with a well defined domain (seven possible values between 1 and 7). In the code declaration below, we assume a uniform distribution over these 7 values. This choice is a speculation about the population distribution of \\(Y\\); some features of the design diagnosis will depend on the choice of distribution. The functional equations seem absent here as there is only one variable in the model. We could consider an elaboration of the model that includes three variables: the true outcome, \\(Y\\); the decision to measure the outcome, \\(M\\); and the measured outcome, \\(Y^M\\). We ignore this complication for now under the assumption that \\(Y = Y^M\\), i.e., that \\(Y\\) is measured perfectly. Finally, the model also includes information about the size of the population. Portola, California, has a population of approximately 2100 people as of 2010, so \\(N = 2100\\). Inquiry: Our inquiry is the population mean of \\(Y\\): \\(\\frac{1}{N} \\sum_1^N Y_i = \\bar{Y}\\). Data strategy: In simple random sampling, we draw a random sample without replacement of size \\(n\\), where every member of the population has an equal probability of inclusion in the sample, \\(\\frac{n}{N}\\). When \\(N\\) is very large relative to \\(n\\), units are drawn approximately independently. In this design we measure \\(Y\\) for \\(n=100\\) units in the sample; the other \\(N-n\\) units are not measured. Answer strategy: We estimate the population mean with the sample mean estimator: \\(\\widehat{\\overline{Y}} = \\frac{1}{n} \\sum_1^n Y_i\\). Even though our inquiry implies our answer should be a single number, an answer strategy typically also provides statistics that help us assess the uncertainty around that single number. To construct a 95% confidence interval around our estimate, we calculate the standard error of the sample mean, then approximate the sampling distribution of the sample mean estimator using a formula that includes a finite population correction. In particular, we approximate the estimated sampling distribution by a \\(t\\) distribution with \\(n - 1\\) degrees of freedom. In the code for our answer strategy, we spell out each step in turn. fixed_population &lt;- declare_population(N = 500, Y = sample(1:7, N, replace = TRUE))() random_sampling_design &lt;- declare_population(data = fixed_population) + declare_estimand(Ybar = mean(Y)) + declare_sampling(n = 100) + declare_estimator(Y ~ 1, model = lm_robust, estimand = &quot;Ybar&quot;) plot(random_sampling_design) 15.1.4.2 Takeaways With the design declared we can run a diagnosis and plot results from Monte Carlo simulations of the design: diagnosis &lt;- diagnose_design( design, sims = sims, bootstrap_sims = b_sims, diagnosands = diagnosands) The diagnosis indicates that under simple random sampling, the sample mean estimator of the population mean is unbiased. The graph on the left shows the sampling distribution of the estimator: it’s centered directly on the true value of the inquiry. Confidence intervals also have a sampling distribution – they change depending on the idiosyncrasies of each sample we happen to draw. The figure on the right shows that the 95% of the time the confidence intervals cover the true value of the estimand, as they should. As sample size grows, the sampling distribution of the estimator gets tighter, but the coverage of the confidence intervals stays at 95% – just the properties we would want out of our answer strategy. Things work well here it seems. In the exercises we suggest some small modifications of the design that point to conditions under which things might break down. 15.1.5 Stratified and clustered random sampling Researchers often cannot randomly sample at the individual level because it may, among other reasons, be too costly or logistically impractical. Instead, they may choose to randomly sample households, political precincts, or any group of individuals in order to draw inferences about the population. This strategy may be cheaper and simpler but may also introduce risks of less precise estimates. Say we are interested in the average party ideology in the entire state of California. Using cluster sampling, we randomly sample counties within the state, and within each selected county, randomly sample individuals to survey. Assuming enough variation in the outcome of interest, the random assignment of equal-sized clusters yields unbiased but imprecise estimates. By sampling clusters, we select groups of individuals who may share common attributes. Unlike simple random sampling, we need to take account of this intra-cluster correlation in our estimation of the standard error.10 The higher the degree of within-cluster similarity, the more variance we observe in cluster-level averages and the more imprecise are our estimates.11 We address this by considering cluster-robust standard errors in our answer strategy below. 15.1.5.1 Design Declaration Model: We specify the variable of interest \\(Y\\) (political ideology, say) as a discrete variable ranging from 1 (most liberal) to 7 (most conservative). We do not define a functional model since we are interested in the population mean of \\(Y\\). The model also includes information about the number of sampled clusters and the number of individuals per cluster. Inquiry: Our estimand is the population mean of political identification \\(Y\\). Because we employed random sampling, we can expect the value of the sample mean (\\(\\widehat{\\overline{y}}\\)) to approximate the true population parameter (\\(\\widehat{\\overline{Y}}\\)). Data strategy: Sampling follows a two-stage strategy. We first draw a random sample 30 counties in California, and in each county select 20 individuals at random. This guarantees that each county has the same probability of being included in the sample and each resident within a county the same probability of being in the sample. In this design we estimate \\(Y\\) for n = 600 respondents. Answer strategy: We estimate the population mean with the sample mean estimator: \\(\\widehat{\\overline{Y}} = \\frac{1}{n} \\sum_1^n Y_i\\), and estimate standard errors under the assumption of independent and heteroskedastic errors as well as cluster-robust standard errors to take into account correlation of errors within clusters. Below we demonstrate the the imprecision of our estimated \\(\\widehat{\\overline{Y}}\\) when we cluster standard errors and when we do not in the presence of an intracluster correlation coefficient (ICC) of 0.402. N_blocks &lt;- 1 N_clusters_in_block &lt;- 1000 N_i_in_cluster &lt;- 50 n_clusters_in_block &lt;- 30 n_i_in_cluster &lt;- 20 icc &lt;- 0.402 # M: Model fixed_pop &lt;- declare_population( block = add_level(N = N_blocks), cluster = add_level(N = N_clusters_in_block), subject = add_level(N = N_i_in_cluster, latent = draw_normal_icc(mean = 0, N = N, clusters = cluster, ICC = icc), Y = draw_ordered(x = latent, breaks = qnorm(seq(0, 1, length.out = 8))) ) )() cluster_sampling_design &lt;- declare_population(data = fixed_pop) + # I: Inquiry declare_estimand(Ybar = mean(Y)) + # D: Data Strategy declare_sampling(strata = block, clusters = cluster, n = n_clusters_in_block, sampling_variable = &quot;Cluster_Sampling_Prob&quot;) + declare_sampling(strata = cluster, n = n_i_in_cluster, sampling_variable = &quot;Within_Cluster_Sampling_Prob&quot;) + # A: Answer Strategy declare_estimator(Y ~ 1, model = lm_robust, clusters = cluster, estimand = &quot;Ybar&quot;, label = &quot;Clustered Standard Errors&quot;) 15.1.5.2 Takeaways diagnosis &lt;- diagnose_design(cluster_sampling_design, sims = sims) Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Power Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand cluster_sampling_design Ybar Clustered Standard Errors (Intercept) 500 0.01 0.25 1.00 0.95 3.97 0.25 0.25 0.00 3.97 (0.01) (0.01) (0.00) (0.01) (0.01) (0.01) (0.00) (0.00) (0.00) To appreciate the role of clustering better we also plot simulated values of our estimand with standard errors not clustered and with clustered standard errors. To do this we first add an additional estimator to the design that does not take account of clusters. new_design &lt;- cluster_sampling_design + declare_estimator(Y ~ 1, model = lm_robust, estimand = &quot;Ybar&quot;, label = &quot;Naive Standard Errors&quot;) diagnosis &lt;- diagnose_design(new_design, sims = sims) The figure above may give us the impression that our estimate with clustered standard errors is less precise, when in fact, it correctly accounts for the uncertainty surrounding our estimates. The blue lines in the graph demonstrate the estimates from simulations which contain our estimand. As our table and graphs show, the share of these simulations over the total number of simulations, also known as coverage, is (correctly) close to 95% in estimations with clustered standard errors and 54% in estimations without clustered standard errors. As expected, the mean estimate itself and the bias is the same in both specifications. 15.1.5.3 Exercises Modify the declaration to change the distribution of \\(Y\\) from being uniform to something else: perhaps imagine that more extreme ideologies are more prevalent than moderate ones. Is the sample mean estimator still unbiased? Interpret your answer. Change the sampling procedure to favor units with higher values of ideology. Is the sample mean estimator still unbiased? Interpret your answer. Modify the estimation function to use this formula for the standard error: \\(\\widehat{se} \\equiv \\frac{\\widehat\\sigma}{\\sqrt{n}}\\). This equation differs from the one used in our declaration (it ignores the total population size \\(N\\)). Check that the coverage of this new design is incorrect when \\(N=n\\). Assess how large \\(N\\) has to be for the difference between these procedures not to matter. The intra-cluster correlation coefficient (ICC) can be calculated directly and is a feature of this design.↩ In ordinary least square (OLS) models, we assume errors are independent (error terms between individual observations are uncorrelated with each other) and homoskedastic (the size of errors is homogeneous across individuals). In reality, this is often not the case with cluster sampling.↩ "],
["multilevel-regression-and-poststratification.html", "15.2 Multilevel regression and poststratification", " 15.2 Multilevel regression and poststratification 15.2.1 Declaration fixed_population &lt;- declare_population(N = 500, X = sample(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), N, replace = TRUE), Y = sample(1:7, N, replace = TRUE))() design &lt;- declare_population(data = fixed_population) + declare_estimand(Ybar = mean(Y)) + declare_sampling(strata_prob = c(0.2, 0.1, 0.3), strata = X) + declare_step(B_demeaned = (X == &quot;B&quot;) - mean(X == &quot;B&quot;), C_demeaned = (X == &quot;C&quot;) - mean(X == &quot;C&quot;), mutate) + declare_estimator(Y ~ B_demeaned + C_demeaned, term = &quot;(Intercept)&quot;, model = lm_robust, estimand = &quot;Ybar&quot;) 15.2.2 Dag 15.2.3 Example 15.2.4 Example You can use the global bib file via rmarkdown cites like this: Imai, King, and Stuart (2008) district population_size prop_white prop_black prop_asian prop_hispanic_other prop_democrat prop_republican 1 41995 0.6848196 0.2414573 0.0325039 0.0500774 0.5185611 0.2436329 2 41076 0.2573035 0.6714870 0.0072305 0.0904421 0.7334309 0.0992582 3 40878 0.3384706 0.5112775 0.0085376 0.2235677 0.6786992 0.1176531 4 41287 0.8557415 0.0381718 0.0790564 0.0357498 0.3469342 0.3956847 5 40722 0.8473061 0.0709199 0.0597957 0.0208732 0.3942886 0.3638110 6 41985 0.8783613 0.0674765 0.0123854 0.0431821 0.4042878 0.3675094 # US population delaware_population_df &lt;- fabricate( data = delaware_senate_districts_df, individuals = add_level( N = population_size, race_white = rbinom(N, 1, prob = prop_white), race_black = rbinom(N, 1, prob = prop_black), race_asian = rbinom(N, 1, prob = prop_black), race_hispanic_other = rbinom(N, 1, prob = prop_hispanic_other), pid_republican = rbinom(N, 1, prob = prop_republican), pid_democrat = rbinom(N, 1, prob = prop_democrat) ) ) %&gt;% select(-starts_with(&quot;prop_&quot;), -population_size) # population weights for MRP mrp_weights &lt;- delaware_population_df %&gt;% group_by(district, race_white, race_black, race_asian, race_hispanic_other, pid_republican, pid_democrat) %&gt;% summarize(n_cell = n()) %&gt;% group_by(district) %&gt;% mutate(proportion_cell = n_cell/sum(n_cell)) %&gt;% select(-n_cell) %&gt;% ungroup delaware_population_df &lt;- mrp_weights %&gt;% select(district, proportion_cell) %&gt;% right_join(delaware_population_df) # Lax and Philips APSR 2009 # Policies are coded dichotomously, 1 for the progay policy and 0 otherwise: Adoption (9 states allow second-parent adoption in all jurisdictions) design &lt;- declare_population( data = delaware_population_df, districts = modify_level(district_effect = rnorm(N)), individuals = modify_level( noise = rnorm(N, mean = district_effect), policy_support = rbinom(N, 1, prob = pnorm( 0.25 + 0.2 * race_white - 0.1 * race_black - 0.2 * race_hispanic_other - 0.1 * pid_democrat + 0.15 * pid_republican + noise)) ) ) + declare_estimand(handler = function(data) { data %&gt;% group_by(district) %&gt;% summarize(estimand = mean(policy_support)) %&gt;% ungroup %&gt;% mutate(estimand_label = &quot;mean_policy_support&quot;) }) + declare_sampling(n = 500) + declare_estimator(handler = tidy_estimator(function(data) { data %&gt;% group_by(district) %&gt;% summarize(estimate = mean(policy_support)) }), label = &quot;strata_means&quot;, estimand = &quot;mean_policy_support&quot;) + # this estimator owes code to https://timmastny.rbind.io/blog/multilevel-mrp-tidybayes-brms-stan/ declare_estimator(handler = tidy_estimator(function(data) { model_fit &lt;- glmer( formula = policy_support ~ race_white + race_black + race_asian + race_hispanic_other + pid_democrat + pid_republican + (1 | district), data = data, family = binomial(link = &quot;logit&quot;)) data %&gt;% mutate( support_predicted = prediction(model_fit, data = ., allow.new.levels = TRUE, type = &quot;response&quot;), support_predicted_weighted = support_predicted * proportion_cell ) %&gt;% group_by(district) %&gt;% summarize(estimate = sum(support_predicted_weighted)) }), label = &quot;mrp_mle&quot;, estimand = &quot;mean_policy_support&quot;) dat &lt;- draw_data(design) draw_estimates(design) sims &lt;- simulate_design(design, sims = 3) diag &lt;- diagnose_design(design, sims = 100, diagnosands = declare_diagnosands(select = bias), add_grouping_variables = &quot;state&quot;) This chunk is set to echo = TRUE and eval = do_diagnosis simulations_pilot &lt;- simulate_design(design, sims = sims) Right after you do simulations, you want to save the simulations rds. Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. kable(head(simulations_pilot)) design_label sim_ID estimator_label term estimate std.error statistic p.value conf.low conf.high df outcome design 1 estimator Z 0.9624445 0.1705951 5.641688 0.0000002 0.6234399 1.301449 88.32465 Y design 2 estimator Z 0.9532166 0.2124610 4.486548 0.0000200 0.5315217 1.374911 96.66334 Y design 3 estimator Z 0.9674071 0.2137695 4.525468 0.0000170 0.5431863 1.391628 97.96114 Y design 4 estimator Z 1.0545908 0.1942247 5.429747 0.0000004 0.6690878 1.440094 96.58766 Y design 5 estimator Z 1.0708566 0.1897958 5.642153 0.0000002 0.6942127 1.447501 97.99051 Y design 6 estimator Z 0.7197320 0.1964379 3.663916 0.0004076 0.3297870 1.109677 95.63827 Y References "],
["inference-about-unobserved-variables.html", "15.3 Inference about unobserved variables", " 15.3 Inference about unobserved variables 15.3.1 Declaration design &lt;- declare_population(N = 100, Y_star = rnorm(N)) + declare_estimand(Y_bar = mean(Y_star)) + declare_measurement(Y_1 = 0.1 * Y_star + rnorm(N, sd = 0.25), Y_2 = Y_star + rnorm(N, sd = 0.25), Y_3 = 1 + 0.5 * Y_star + rnorm(N, sd = 0.25), Y_idx = (Y_1 + Y_2 + Y_3) / 3) + declare_estimator(Y_idx ~ 1, model = lm_robust, estimand = &quot;Y_bar&quot;) 15.3.2 Dag 15.3.3 Example "],
["structural-estimation.html", "15.4 Structural estimation", " 15.4 Structural estimation 15.4.1 Declaration 15.4.2 Dag 15.4.3 Example "],
["experimental-designs-for-descriptive-inference.html", "Chapter 16 Experimental designs for descriptive inference", " Chapter 16 Experimental designs for descriptive inference Why would we ever need to do an experiment to do descriptive inference? Suppose we want to understand the causal model \\(M\\) of a violin. In particular, we have a descriptive inquiry \\(I\\) about the pitch of the highest string, the E string. We want to know if the E string is in tune. Call the latent pitch of the string \\(Y^*\\). No matter how hard we listen to the string, we can’t hear \\(Y^*\\) – it is latent. As part of a data strategy \\(D\\), we could measure the pitch by \\(P\\) plucking it: \\(Y^* -&gt; Y &lt;- P\\). This is descriptive research about the causal model \\(M\\), because the DAG of the violin includes four string nodes which each cause pitch nodes; we’d like to know a descripitive fact about the pitch nodes (what frequency they are virbaring at.) This could also be a causal inquiry: the untreated potential outcome is the pitch of the unplucked string, as defined by the frequency of vibrartion – while strings are never perfectly still, we can call the untreated potential outcome \\(Y_i(0) = 0hz\\). The treated potential outcome is the vibrations when the string is plucked \\(Y_i(1) = 650hz\\). The causal effect of plucking the string is \\(Y_i(1) - Y_i(0) = 650 - 0 = 650\\). This is a sense in which causal inference and descriptive inference are the same. Whether framed as a desriptive inquiry or a casual inquiry, we arrive at an answer of 650 hertz. Violinists reading this will know that that means the E string is flat and will need to be tuned up to \\(659.3hz\\) (if using equal temperment). "],
["audit-experiments.html", "16.1 Audit experiments", " 16.1 Audit experiments 16.1.1 Declaration design &lt;- declare_population(N = 100, U = rnorm(N)) + declare_potential_outcomes(R ~ if_else(Z + U &gt; 0.5, 1, 0), conditions = list(Z = c(0, 1))) + declare_potential_outcomes(Q ~ if_else(R == 1, Z + U, NA_real_), conditions = list(Z = c(0, 1), R = c(0, 1))) + declare_estimand(ATE_R = mean(R_Z_1 - R_Z_0)) + declare_estimand(CATE_ar = mean(Q_Z_1_R_1 - Q_Z_0_R_1), subset = (R_Z_1 == 1 &amp; R_Z_0 == 0)) + declare_assignment(prob = 0.5) + declare_reveal(R, Z) + declare_reveal(Q, c(Z, R)) + declare_estimator(R ~ Z, estimand = &quot;ATE_R&quot;, label = &quot;ATE_R&quot;) + declare_estimator(Q ~ Z, subset = (R == 1), estimand = &quot;CATE_ar&quot;, label = &quot;CATE_ar&quot;) 16.1.2 Dag 16.1.3 Example A basic requirement of a good research design is that the question it seeks to answer does in fact have an answer, at least under plausible models of the world. In our framework, this means that an inquiry \\(I\\) must have an associated answer \\(a^M\\), which refers to the answer under the model. Interestingly, we sometimes might not be conscious that the questions we ask do not have answers. Fortunately, when we ask a computer to answer such a question, it complains. How could a question not have an answer? Answerless questions can arise when inquiries depend on variables that do not exist or are undefined for some units. In other words, when there is a mismatch between the model and the inquiry, we’re asking a question about something that doesn’t exist. Consider an audit experiment (see Audit Experiment Design) that seeks to assess the effects of an email from a Latino name (versus a White name) on whether and how well election officials respond to requests for information. For example, do they use a positive or negative tone. These questions seem reasonable enough. The problem, however, is that if there are officials who don’t send responses, tone is undefined. More subtly, if there is an official that does send an email but would not have sent it in a different treatment condition, then tone is undefined for one of their potential outcomes. 16.1.4 Design Declaration Model: The model has two outcome variables, \\(R_i\\) and \\(Y_i\\). \\(R_i\\) stands for “response” and is equal to 1 if a response is sent, and 0 otherwise. \\(Y_i\\) is the tone of the response and is normally distributed when it is defined. \\(Z_i\\) is the treatment and equals 1 if the email is sent using a Latino name and 0 otherwise. The table below shows the potential outcomes for four possible types of subjects, depending on the potential outcomes of \\(R_i\\). A types always respond regardless of treatment and D types never respond, regardless of treatment. B types respond if and only if they are treated, whereas C types respond if and only if they are not treated. The table also includes columns for the potential outcomes of \\(Y_i\\), showing which potential outcome subjects would express depending on their type. The key thing to note is that for the B, C, and D types, the effect of treatment on \\(Y_i\\) is undefined because messages never sent have no tone. The last (and very important) feature of our model is that the outcomes \\(Y_i\\) are possibly correlated with subject type. Even though both \\(E[Y_i(1) | \\text{Type} = A]\\) and \\(E[Y_i(1) | \\text{Type} = B]\\) exist, there’s no reason to expect that they are the same. In the design we assume a distribution of types with 40% A, 5% B, 10% C, and 45% D. Causal Types Type \\(R_i(0)\\) \\(R_i(1)\\) \\(Y_i(0)\\) \\(Y_i(1)\\) A 1 1 \\(Y_i(0)\\) \\(Y_i(1)\\) B 0 1 NA \\(Y_i(1)\\) C 1 0 \\(Y_i(0)\\) NA D 0 0 NA NA Inquiry: We have two inquiries. The first is straightforward: \\(E[R_i(1) - R_i(0)]\\) is the Average Treatment Effect on response. The second inquiry is the undefined inquiry that does not have an answer: \\(E[Y_i(1) - Y_i(0)]\\). We will also consider a third inquiry, which is defined: \\(E[Y_i(1) - Y_i(0) | \\mathrm{Type} = A]\\), which is the average effect of treatment on tone among \\(A\\) types. Data strategy: The data strategy will be to use complete random assignment to assign 250 of 500 units to treatment. Answer strategy: We’ll try to answer all three inquiries with the difference-in-means estimator, but as the diagnosis will reveal, this strategy works well for some inquiries but not others. # Model ------------------------------------------------------------------- population &lt;- declare_population( N = 500, type = sample(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;), size = N, replace = TRUE, prob = c(.40, .05, .10, .45))) potential_outcomes &lt;- declare_potential_outcomes( R_Z_0 = type %in% c(&quot;A&quot;, &quot;C&quot;), R_Z_1 = type %in% c(&quot;A&quot;, &quot;B&quot;), Y_Z_0 = ifelse(R_Z_0, rnorm(n = sum(R_Z_0), mean = .1*(type == &quot;A&quot;) - 2*(type == &quot;C&quot;)), NA), Y_Z_1 = ifelse(R_Z_1, rnorm(n = sum(R_Z_1), mean = .2*(type == &quot;A&quot;) + 2*(type == &quot;B&quot;)), NA) ) # Inquiry ----------------------------------------------------------------- estimand_1 &lt;- declare_estimand(ATE_R = mean(R_Z_1 - R_Z_0)) estimand_2 &lt;- declare_estimand(ATE_Y = mean(Y_Z_1 - Y_Z_0)) estimand_3 &lt;- declare_estimand( ATE_Y_for_As = mean(Y_Z_1[type == &quot;A&quot;] - Y_Z_0[type == &quot;A&quot;])) # Data Strategy ----------------------------------------------------------- assignment &lt;- declare_assignment(m = 250) # Answer Strategy --------------------------------------------------------- estimator_1 &lt;- declare_estimator(R ~ Z, estimand = estimand_1, label = &quot;ATE_R&quot;) estimator_2 &lt;- declare_estimator(Y ~ Z, estimand = estimand_2, label = &quot;ATE_Y&quot;) estimator_3 &lt;- declare_estimator(Y ~ Z, estimand = estimand_3, label = &quot;ATE_YA&quot;) # Design ------------------------------------------------------------------ design &lt;- population + potential_outcomes + assignment + estimand_1 + estimand_2 + estimand_3 + declare_reveal(outcome_variables = c(&quot;R&quot;, &quot;Y&quot;)) + estimator_1 + estimator_2 + estimator_3 16.1.5 Takeaways We now diagnose the design: diagnosis &lt;- diagnose_design(design, sims = sims) Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Power Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand design ATE_R ATE_R Z 100 0.00 0.04 0.20 0.98 -0.05 0.04 0.04 0.00 -0.05 (0.00) (0.00) (0.04) (0.01) (0.00) (0.00) (0.00) (0.00) (0.00) design ATE_Y ATE_Y Z 100 NA NA 0.91 NA 0.54 0.19 0.15 NA NA NA NA (0.03) NA (0.02) (0.01) (0.00) NA NA design ATE_Y_for_As ATE_YA Z 100 0.31 0.34 0.91 0.52 0.54 0.19 0.15 0.04 0.22 (0.01) (0.01) (0.03) (0.04) (0.02) (0.01) (0.00) (0.02) (0.01) We learn three things from the design diagnosis. First, as expected, our experiment is unbiased for the average treatment effect on response. Next, we see that our second inquiry, as well as our diagnostics for it, are undefined. The diagnosis tells us that our definition of potential outcomes produces a definition problem for the estimand. Note that the diagnosands that are defined, including power, depend only on the answer strategy and not on the estimand. Finally, our third estimand – the average effects for the \\(A\\) types – is defined but our estimates are biased. The reason for this is that we cannot tell from the data which types are the \\(A\\) types: we are not conditioning on the correct subset. Indeed, we are unable to condition on the correct subset. If a subject responds in the treatment group, we don’t know if she is an \\(A\\) or a \\(B\\) type; in the control group, we can’t tell if a responder is an \\(A\\) or a \\(C\\) type. Our difference-in-means estimator of the ATE on \\(Y\\) among \\(A\\)s will be off whenever \\(A\\)s have different outcomes from \\(B\\)s and \\(C\\)s. In some cases, the problem might be resolved by changing the inquiry. Closely related estimands can often be defined, perhaps by redefining \\(Y\\) (e.g., emails never sent have a tone of zero). Some redefinitions of the problem, as in the one we examine above, require estimating effects for unobserved subgroups which is a difficult challenge. 16.1.6 Applications This kind of problem is surprisingly common. Here are three more distinct instances of the problem: \\(Y\\) is the decision to vote Democrat (\\(Y=1\\)) or Republican (\\(Y=0\\)), \\(R\\) is the decision to turn out to vote and \\(Z\\) is a campaign message. The decision to vote may depend on treatment but if subjects do not vote then \\(Y\\) is undefined. \\(Y\\) is the weight of infants, \\(R\\) is whether a child is born and \\(Z\\) is a maternal health intervention. Fertility may depend on treatment but the weight of unborn (possibly never conceived) babies is not defined. \\(Y\\) is the charity to whom contributions are made during fundraising and \\(R\\) is whether anything is contributed and \\(Z\\) is an encouragement to contribute. The identity of beneficiaries is not defined if there are no contributions. All of these problem exhibit a form of post treatment bias (see section Post treatment bias) but the issue goes beyond picking the right estimator. Our problem here is conceptual: the effect of treatment on the outcome just doesn’t exist for some subjects. 16.1.7 Exercises The amount of bias on the third estimand depends on both the distribution of types and the correlation of types with the potential outcomes of Y. Modify the declaration so that the estimator of the effect on Y is unbiased, changing only the distribution of types. Repeat the exercise, changing only the correlation of type with the potential outcomes of \\(Y\\). Try approaching the problem by redefining the inquiry, seeking to assess the effect of treatment on the share of responses with positive tone. "],
["experiments-for-sensitive-questions.html", "16.2 Experiments for sensitive questions", " 16.2 Experiments for sensitive questions 16.2.1 Declaration design &lt;- declare_population( N = 100, U = rnorm(N), Y_star = rbinom(N, size = 1, prob = 0.3), S = case_when(Y_star == 0 ~ 0L, Y_star == 1 ~ rbinom(N, size = 1, prob = 0.2)), X = rbinom(N, size = 3, prob = 0.5) ) + declare_estimand(proportion = mean(Y_star)) + declare_measurement(Y_direct = Y_star - S) + declare_potential_outcomes(Y_list ~ Y_star * Z + X) + declare_assignment(prob = 0.5) + declare_estimator(Y_direct ~ 1, model = lm_robust, estimand = &quot;proportion&quot;, label = &quot;direct&quot;) + declare_estimator(Y_list ~ Z, estimand = &quot;proportion&quot;, label = &quot;list&quot;) 16.2.2 Dag 16.2.3 Example setup: a descriptive estimand, the proportion holding sensitive characteristic; two experimental designs to recover it, list experiments and randomized response if identification assumptions are violated (focus on ceiling/floor), estimates of ATE still unbiased but not for the descriptive estimand compare design where the ceiling/floor categories are minimized through Glynn (2013) design advice to use negatively-correlated items and a high prevalence and a low prevalence item both designs exhibit bias-variance tradeoff (more control of variance with RR) 16.2.4 List experiments Sometimes, subjects might not tell the truth when asked about certain attitudes or behaviors. Responses may be affected by sensitivity bias, or the tendency of survey subjects to dissemble for fear of negative repercussions if some reference group learns their true response (Blair, Coppock, and Moor 2018). In such cases, standard survey estimates based on direct questions will be biased. One class of solutions to this problem is to obscure individual responses, providing protection from social or legal pressures. When we obscure responses systematically through an experiment, we can often still identify average quantities of interest. One such design is the list experiment (introduced by Miller (1984)), which asks respondents for the count of the number of `yes’ responses to a series of questions including the sensitive item, rather than for a yes or no answer on the sensitive item itself. List experiments give subjects cover by aggregating their answer to the sensitive item with responses to other questions. During the 2016 Presidential Election in the U.S., some observers were concerned that pre-election estimates of support for Donald Trump might have been downward biased by “Shy Trump Supporters” – survey respondents who supported Trump in their hearts, but were embarrassed to admit it to pollsters. To assess this possibility, Coppock (2017) obtained estimates of Trump support that were free of social desirability bias using a list experiment. Subjects in the control and treatment groups were asked: “Here is a list of [three/four] things that some people would do and some people would not. Please tell me HOW MANY of them you would do. We do not want to know which ones of these you would do, just how many. Here are the [three/four] things:” Control Treatment If it were up for a vote, I would vote to raise the minimum wage to 15 dollars an hour If it were up for a vote, I would vote to raise the minimum wage to 15 dollars an hour If it were up for a vote, I would vote to repeal the Affordable Care Act, also known as Obamacare If it were up for a vote, I would vote to repeal the Affordable Care Act, also known as Obamacare If it were up for a vote, I would vote to ban assault weapons If it were up for a vote, I would vote to ban assault weapons If the 2016 presidential election were being held today and the candidates were Hillary Clinton (Democrat) and Donald Trump (Republican), I would vote for Donald Trump. The treatment group averaged 1.843 items while the control group averaged 1.548 items, for a difference-in-means estimate 0.296. We will show this is an unbiased estimator for the average treatment effect between being asked to respond to the treated list and the control list (invoking the usual assumptions of randomized experiments, including SUTVA). But our estimand is the proportion of people who support Donald Trump. For the difference-in-means to be an unbiased estimator of the proportion of respondents who would say “yes” to the sensitive item, we invoke two additional assumptions: no design effects and no “liars” (see Imai 2011). The first highlights the fact that we need a good estimate of the average control item count from the control group (in this example 1.843). We use that to net out the control item count from responses to the treated group (what is left is the sensitive item proportion). When respondents provide a different control item count in the treated group than in the control group, for example because they evaluate items relatively and the inclusion of the sensitive item changes their answers (see Flavin and Keane 2011), the design breaks down. The no liars assumption says that respondents provide truthful answers to the sensitive item within the count. The justification for the assumption is that the plausible cover of being asked within a count makes it possible for respondents to answer truthfully. The estimate is, under these assumptions, free from sensitivity bias, but it’s also much higher variance. The 95% confidence interval for the list experiment estimate is nearly 14 percentage points wide, whereas the the 95% confidence interval for the (possibly biased!) direct question asked of the same sample is closer to 4 percentage points. The choice between list experiments and direct question is therefore a bias-variance tradeoff. List experiments may have less bias, but they are higher variance. Direct questions may be biased, but they have less variance. 16.2.4.1 Declaration Model: Our model includes subjects’ true support for Donald Trump and whether or not they are “shy”. These two variables combine to determine how subjects will respond when asked directly about Trump support. The potential outcomes model combines three types of information to determine how subjects will respond to the list experiment: their responses to the three nonsensitive control items, their true support for Trump, and whether they are assigned to see the treatment or the control list. Notice that our definition of the potential outcomes embeds the no liars and no design effects assumptions required for the list experiment design. We also have a global parameter that reflects our expectations about the proportion of Trump supporters who are shy. It’s set at 6%, which is large enough to make a difference for polling, but not so large as to be implausible. Inquiry: Our estimand is the proportion of voters who actually plan to vote for Trump. Data strategy: First we sample 500 respondents from the U.S. population at random, then we randomly assign 250 of the 500 to treatment and the remainder to control. In the survey, we ask subjects both the direct question and the list experiment question. Answer strategy: We estimate the proportion of truthful Trump voters in two ways. First, we take the mean of answers to the direct question. Second, we take the difference in means in the responses to the list experiment question. # Model ------------------------------------------------------------------- proportion_shy &lt;- .06 list_design &lt;- # Model declare_population( N = 5000, # true trump vote (unobservable) truthful_trump_vote = draw_binary(.45, N), # shy voter (unobservable) shy = draw_binary(proportion_shy, N), # direct question response (1 if Trump supporter and not shy, 0 otherwise) Y_direct = if_else(truthful_trump_vote == 1 &amp; shy == 0, 1, 0), # nonsensitive list experiment items raise_minimum_wage = draw_binary(.8, N), repeal_obamacare = draw_binary(.6, N), ban_assault_weapons = draw_binary(.5, N) ) + declare_potential_outcomes( Y_list_Z_0 = raise_minimum_wage + repeal_obamacare + ban_assault_weapons, Y_list_Z_1 = Y_list_Z_0 + truthful_trump_vote ) + # Inquiry declare_estimand(proportion_truthful_trump_vote = mean(truthful_trump_vote), ATE = mean(Y_list_Z_1 - Y_list_Z_0)) + # Data Strategy declare_sampling(n = 500) + declare_assignment(prob = .5) + declare_reveal(Y_list) + # Answer Strategy declare_estimator( Y_direct ~ 1, model = lm_robust, term = &quot;(Intercept)&quot;, estimand = &quot;proportion_truthful_trump_vote&quot;, label = &quot;direct&quot;) + declare_estimator( Y_list ~ Z, model = difference_in_means, estimand = c(&quot;proportion_truthful_trump_vote&quot;, &quot;ATE&quot;), label = &quot;list&quot;) simulations_list &lt;- simulate_design(list_design, sims = sims) The plot shows the sampling distribution of the direct and list experiment estimators. The sampling distribution of the direct question is tight but biased; the list experiment (if the requisite assumptions hold) is unbiased, but higher variance. The choice between these two estimators of the prevalence rate depends on which – bias or variance – is more important in a particular setting. See Blair, Coppock, and Moor (2018) for an extended discussion of how the choice of research design depends deeply on the purpose of the project. 16.2.4.2 Violations of identifying assumptions In our model, the definition of the treated potential outcome, Y_list_Z_1 = Y_list_Z_0 + truthful_trump_vote, bakes in the no design effects and no liars assumptions. The first component is the control item count Y_list_Z_0, which ensures the respondent’s count of control items is the same in both groups. The second is the true trump vote, which assumes no liars. What do we learn from this experimental design if these assumptions do not hold? We examine the case of “ceiling effects,” in which respondents whose control item count is the maximum (in the example, they would vote yes to all three control items) withhold their true support for Trump in the treatment group. We thus redefine the treated potential outcome to be a function of the original count, but those who would respond 4 (all control items plus Trump support) instead respond 3. These are the “liars.” list_design_ceiling &lt;- replace_step( list_design, step = 2, new_step = declare_potential_outcomes( Y_list_Z_0 = raise_minimum_wage + repeal_obamacare + ban_assault_weapons, Y_list_Z_1_no_liars = Y_list_Z_0 + truthful_trump_vote, Y_list_Z_1 = ifelse(Y_list_Z_1_no_liars == 4, 3, Y_list_Z_1_no_liars)) ) diagnosis_list_ceiling &lt;- diagnose_design(list_design_ceiling, sims = sims, bootstrap_sims = b_sims) estimator_label estimand_label bias rmse list ATE -0.022 0.091 direct proportion_truthful_trump_vote -0.026 0.031 list proportion_truthful_trump_vote -0.127 0.153 We see that the list experiment is still an unbiased estimator for the average difference in responses to the treatment list and the shorter control list (ATE). But under ceiling effects, it is no longer an unbiased estimator for the proportion of truthful Trump vote. Indeed, it is more unbiased than the direct question. The divergence illustrates a common feature of experimental designs for descriptive inference: the average treatment effect can be estimated without bias under SUTVA and randomization with these designs, but additional assumptions are required in order to add an interpretation of the ATE as the descriptive quantity of interest. The burden is on the researcher to demonstrate the credibility of these additional assumptions. The experimental design alone is not sufficient justification. 16.2.4.3 Addressing potential assumption violations by design Researchers may bolster the assumptions to identify the descriptive estimand through changes to the data strategy or the answer strategy. Changes to the data strategy for the list experiment aim to reduce the risk of design effects and violations of the no liars assumptions. For example, when there is a risk of ceiling effects, Glynn (2013) proposes selecting control items that are inversely correlated. With three items, if two of the items are perfectly negatively correlated (i.e., if you say “yes” to one item you say “no” to the other), then the control item count will always be below the maximum of three when ceiling effects bite. We illustrate this design change by replacing the population declaration for the design with ceiling effects. This change is in the population, but is really a part of the data strategy because it involves the choice of measurement tool (which control items the researcher selects to ask respondents). We see that the design is stil lunbiased for the ATE and now is unbiased for the proportion of truthful Trump vote. This is because there are no longer ceiling effects, which represented a violation of the no design effects assumption required to interpret the ATE as the proportion of truthful Trump vote. Changes the answer strategy have been proposed to address both the no design effects and no liars assumption. Blair and Imai (2012) propose a statistical test for the design effects assumption; if it does not pass, they suggest not analyzing the list experiment data (i.e., this is a procedure that makes up an answer strategy). Scholars have also identified improvements to the answer strategy to address violations of no liars: Blair and Imai (2012) provides a model that adjusts for ceiling and floor effects and Li (2019) provides a bounds approach that relaxes the assumption. 16.2.5 Randomized response technique library(rr) rr_forced_known &lt;- function(data) { fit &lt;- try(rrreg(Y_forced_known ~ 1, data = data, p = 2/3, p0 = 1/6, p1 = 1/6, design = &quot;forced-known&quot;)) pred &lt;- try(as.data.frame(predict(fit, avg = TRUE, quasi.bayes = TRUE))) if(class(fit) != &quot;try-error&quot; &amp; class(pred) != &quot;try-error&quot;) { names(pred) &lt;- c(&quot;estimate&quot;, &quot;std.error&quot;, &quot;conf.low&quot;, &quot;conf.high&quot;) pred$p.value &lt;- with(pred, 2 * pnorm(-abs(estimate / std.error))) } else { pred &lt;- data.frame(estimate = NA, std.error = NA, conf.low = NA, conf.high = NA, p.value = NA, error = TRUE) } pred } rr_mirrored &lt;- function(data) { fit &lt;- try(rrreg(Y_mirrored ~ 1, data = data, p = 2/3, design = &quot;mirrored&quot;)) pred &lt;- try(as.data.frame(predict(fit, avg = TRUE, quasi.bayes = TRUE))) if(class(fit) != &quot;try-error&quot; &amp; class(pred) != &quot;try-error&quot;) { names(pred) &lt;- c(&quot;estimate&quot;, &quot;std.error&quot;, &quot;conf.low&quot;, &quot;conf.high&quot;) pred$p.value &lt;- with(pred, 2 * pnorm(-abs(estimate / std.error))) } else { pred &lt;- data.frame(estimate = NA, std.error = NA, conf.low = NA, conf.high = NA, p.value = NA, error = TRUE) } pred } proportion_shy &lt;- .06 rr_design &lt;- declare_population( N = 100, # true trump vote (unobservable) truthful_trump_vote = draw_binary(.45, N), # shy voter (unobservable) shy = draw_binary(proportion_shy, N), # Direct question response (1 if Trump supporter and not shy, 0 otherwise) Y_direct = as.numeric(truthful_trump_vote == 1 &amp; shy == 0)) + declare_estimand(sensitive_item_proportion = mean(truthful_trump_vote)) + declare_potential_outcomes(Y_forced_known ~ (dice == 1) * 0 + (dice %in% 2:5) * truthful_trump_vote + (dice == 6) * 1, conditions = 1:6, assignment_variable = &quot;dice&quot;) + declare_potential_outcomes(Y_mirrored ~ (coin == &quot;heads&quot;) * truthful_trump_vote + (coin == &quot;tails&quot;) * (1 - truthful_trump_vote), conditions = c(&quot;heads&quot;, &quot;tails&quot;), assignment_variable = &quot;coin&quot;) + declare_assignment(prob_each = rep(1/6, 6), conditions = 1:6, assignment_variable = &quot;dice&quot;) + declare_assignment(prob_each = c(2/3, 1/3), conditions = c(&quot;heads&quot;, &quot;tails&quot;), assignment_variable = &quot;coin&quot;) + declare_reveal(Y_forced_known, dice) + declare_reveal(Y_mirrored, coin) + declare_estimator(handler = tidy_estimator(rr_forced_known), label = &quot;forced_known&quot;, estimand = &quot;sensitive_item_proportion&quot;) + declare_estimator(handler = tidy_estimator(rr_mirrored), label = &quot;mirrored&quot;, estimand = &quot;sensitive_item_proportion&quot;) + declare_estimator(Y_direct ~ 1, model = lm_robust, term = &quot;(Intercept)&quot;, label = &quot;direct&quot;, estimand = &quot;sensitive_item_proportion&quot;) rr_design &lt;- set_diagnosands(rr_design, diagnosands = declare_diagnosands(select = c(mean_estimate, bias, rmse, power))) rr_diagnosis &lt;- diagnose_design(rr_design, sims = sims, bootstrap_sims = b_sims) kable(reshape_diagnosis(rr_diagnosis)) Design Label Estimand Label Estimator Label Term N Sims Mean Estimate Bias RMSE Power rr_design sensitive_item_proportion direct (Intercept) 10 0.43 -0.02 0.03 1.00 (0.01) (0.00) (0.00) (0.00) rr_design sensitive_item_proportion forced_known NA 10 0.45 -0.00 0.02 1.00 (0.02) (0.01) (0.00) (0.00) rr_design sensitive_item_proportion mirrored NA 10 0.42 -0.03 0.09 0.90 (0.04) (0.03) (0.02) (0.10) 16.2.5.1 Bias-variance tradeoff rr_designs &lt;- redesign(rr_design, proportion_shy = seq(from = 0, to = 0.5, by = 0.05), N = seq(from = 500, to = 5000, by = 500)) rr_tradeoff_diagnosis &lt;- diagnose_design(rr_designs, sims = sims, bootstrap_sims = b_sims) 16.2.6 References References "],
["conjoint-experimetns.html", "16.3 Conjoint experimetns", " 16.3 Conjoint experimetns Conjoint survey experiments have become hugely popular in political science and beyond for studying multidimensional choice. In popular “forced-choice” design variant, subjects are presented with a choice task: a pair of profiles (of candidates, of immigrants, of policies) and are asked to make a binary choice between them. When you’re designing a conjoint, you have to make (at least) three choices: The number of attributes The number of levels within each attribute The number of choice tasks ask subjects to rate. The right number of attributes is governed by the “masking/satisficing” tradeoff. If you don’t include an important attribute (like partisanship in a candidate choice experiment), you’re worried that subjects will partially infer partisanship from other attributes (like race or gender). If so, partisanship is “masked”, and the estimates for the effects of race or gender will be biased by this “omitted variable.” But if you add too many attributes in order to avoid masking, you may induce “satisficing” among subjects, whereby they only take in a little bit of information, enough to make a “good enough” choice among the candidates. The right number of levels is governed by your sample size. If an attribute has three levels, it’s like you’re conducting a three-arm trial, so you’ll want to have enough subjects for each arm. The more levels, the lower the power. The right number of choice tasks depends on your survey budget. You can always add more pairs of profiles and the only cost is the opportunity cost of asking a different question of the survey that may serve some higher scientific purpose. If you’re worried that respondents will get bored with the task, you can always throw out profile pairs that come later in the survey. (???) suggest that you can ask many pairs without much loss of data quality. 16.3.1 Declaration # applies the function to each pair Y_function &lt;- function(data) { data %&gt;% group_by(pair) %&gt;% mutate(Y = if_else(E == max(E), 1, 0)) %&gt;% ungroup } design &lt;- declare_population( subject = add_level(N = 500), pair = add_level(N = 4), candidate = add_level(N = 2, U = runif(N)) ) + declare_assignment(assignment_variable = &quot;A1&quot;) + declare_assignment(assignment_variable = &quot;A2&quot;, conditions = c(&quot;young&quot;, &quot;middle&quot;, &quot;old&quot;)) + declare_assignment(assignment_variable = &quot;A3&quot;) + declare_step( E = 0.05 * A1 + 0.04 * (A2 == &quot;middle&quot;) + 0.08 * (A2 == &quot;old&quot;) + 0.02 * A3 + U, handler = fabricate) + declare_measurement(handler = Y_function) + declare_estimator(Y ~ A1 + A2 + A3, model = lm_robust, term = TRUE) 16.3.2 Dag 16.3.3 Example "],
["behavioral-games.html", "16.4 Behavioral games", " 16.4 Behavioral games 16.4.1 Declaration design &lt;- declare_population( games = add_level(N = 100), players = add_level( N = 2, prosociality = runif(N), fairness = prosociality, cutoff = pmax(prosociality - 0.25, 0) ) ) + declare_estimand(mean_fairness = mean(fairness), mean_cutoff = mean(cutoff)) + declare_assignment(blocks = games, conditions = c(&quot;proposer&quot;, &quot;responder&quot;), assignment_variable = &quot;role&quot;) + declare_step(id_cols = games, names_from = role, values_from = c(prosociality, fairness, cutoff), handler = pivot_wider) + declare_measurement(proposal = fairness_proposer * 0.5, response = if_else(proposal &gt;= cutoff_responder, 1, 0)) + declare_estimator(proposal ~ 1, model = lm_robust, estimand = &quot;mean_fairness&quot;, label = &quot;mean_fairness&quot;) + declare_estimator(response ~ 1, model = lm_robust, estimand = &quot;mean_cutoff&quot;, label = &quot;mean_cutoff&quot;) 16.4.2 Dag 16.4.3 Example "],
["observational-designs-for-causal-inference.html", "Chapter 17 Observational designs for causal inference", " Chapter 17 Observational designs for causal inference section introduction "],
["selection-on-observables.html", "17.1 Selection on observables", " 17.1 Selection on observables 17.1.1 Declaration design &lt;- declare_population(N = 100, X_1 = rnorm(N), X_2 = rnorm(N), Z = if_else(X_1 + X_2 &gt; 0, 1, 0), U = rnorm(N)) + declare_potential_outcomes(Y ~ Z + X_1 + X_2 + U) + declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + declare_reveal() + declare_estimator(Y ~ X_1 + X_2, model = lm_robust, estimand = &quot;LATE&quot;) 17.1.2 Dag 17.1.3 Example (matching and regression etc.) 17.1.4 Classic Confounding We want to know the effect of Z on Y, but it’s confounded by X DIM is biased, OLS is unbiased because we happen to get the functional forms right enough. Figure 17.1: DAG with one observed confounder design_1 &lt;- declare_population(N = 100, U_z = rnorm(N), U_x = rnorm(N), U_y = rnorm(N), X = U_x) + declare_potential_outcomes(Y ~ 0.5*Z + X + U_y) + declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + declare_assignment(prob_unit = pnorm(U_z + U_x), simple = TRUE) + declare_estimator(Y ~ Z, estimand = &quot;ATE&quot;, label = &quot;DIM&quot;) + declare_estimator(Y ~ Z + X, model = lm, estimand = &quot;ATE&quot;, label = &quot;OLS&quot;) dx_1 &lt;- diagnose_design(design_1, sims = sims, bootstrap_sims = b_sims) dx_1 ## ## Research design diagnosis based on 100 simulations. Diagnosand estimates with bootstrapped standard errors in parentheses (20 replicates). ## ## Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Power ## design_1 ATE DIM Z 100 0.94 0.96 1.00 ## (0.02) (0.02) (0.00) ## design_1 ATE OLS Z 100 0.01 0.23 0.55 ## (0.02) (0.01) (0.04) ## Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand ## 0.06 1.44 0.24 0.27 0.00 0.50 ## (0.02) (0.02) (0.02) (0.00) (0.00) (0.00) ## 0.95 0.51 0.23 0.23 0.00 0.50 ## (0.02) (0.02) (0.01) (0.00) (0.00) (0.00) 17.1.5 What if the functional form is wrong? Oh no, the functional form is wrong, so even though we’re controlling for all confounders, there’s still bias. Solution: matching might do a better job since it’s sort of a “nonparametric” form of covariate control. design_2 &lt;- declare_population(N = 100, U_z = rnorm(N), U_x = rnorm(N), U_y = rnorm(N), X = U_x) + declare_potential_outcomes(Y ~ 0.5*Z + X + X^2 + U_y) + declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + declare_assignment(prob_unit = pnorm(U_z + U_x + U_x^2), simple = TRUE) + declare_estimator(Y ~ Z, estimand = &quot;ATE&quot;, label = &quot;DIM&quot;) + declare_estimator(Y ~ Z + X, model = lm, estimand = &quot;ATE&quot;, label = &quot;OLS&quot;) dx_2 &lt;- diagnose_design(design_2, sims = sims, bootstrap_sims = b_sims) dx_2 ## ## Research design diagnosis based on 100 simulations. Diagnosand estimates with bootstrapped standard errors in parentheses (20 replicates). ## ## Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Power ## design_2 ATE DIM Z 100 1.32 1.36 1.00 ## (0.03) (0.03) (0.00) ## design_2 ATE OLS Z 100 0.87 0.93 0.95 ## (0.03) (0.03) (0.02) ## Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand ## 0.01 1.82 0.34 0.33 0.00 0.50 ## (0.01) (0.03) (0.02) (0.00) (0.00) (0.00) ## 0.24 1.37 0.33 0.35 0.00 0.50 ## (0.04) (0.03) (0.02) (0.00) (0.00) (0.00) 17.1.6 What if you have unobserved confounding? Figure 17.2: DAG with unobserved confounding design_3 &lt;- declare_population(N = 100, U_z = rnorm(N), U_x = rnorm(N), U_y = correlate(rnorm, given = U_z, rho = 0.9), X = U_x) + declare_potential_outcomes(Y ~ 0.5*Z + X + U_y) + declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + declare_assignment(prob_unit = pnorm(U_z + U_x), simple = TRUE) + declare_estimator(Y ~ Z, estimand = &quot;ATE&quot;, label = &quot;DIM&quot;) + declare_estimator(Y ~ Z + X, model = lm, estimand = &quot;ATE&quot;, label = &quot;OLS&quot;) dx_3 &lt;- diagnose_design(design_3, sims = sims, bootstrap_sims = b_sims) dx_3 ## ## Research design diagnosis based on 100 simulations. Diagnosand estimates with bootstrapped standard errors in parentheses (20 replicates). ## ## Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Power ## design_3 ATE DIM Z 100 1.72 1.74 1.00 ## (0.02) (0.02) (0.00) ## design_3 ATE OLS Z 100 1.02 1.04 1.00 ## (0.02) (0.02) (0.00) ## Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand ## 0.00 2.22 0.21 0.22 0.00 0.50 ## (0.00) (0.02) (0.01) (0.00) (0.00) (0.00) ## 0.00 1.52 0.17 0.19 0.00 0.50 ## (0.00) (0.02) (0.01) (0.00) (0.00) (0.00) 17.1.7 What if the observed covariate is post-treatment? Figure 17.3: DAG with one observed mediator design_4 &lt;- declare_population(N = 100, U_z = rnorm(N), U_m = rnorm(N), U_y = rnorm(N)) + declare_potential_outcomes(M ~ 0.5*Z + U_m) + declare_potential_outcomes(Y ~ 0.5*Z + (0.5*Z + U_m) + U_y) + declare_assignment(prob_unit = pnorm(U_z), simple = TRUE) + declare_reveal(c(M, Y), Z) + declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + declare_estimator(Y ~ Z, estimand = &quot;ATE&quot;, label = &quot;DIM&quot;) + declare_estimator(Y ~ Z + M, model = lm, estimand = &quot;ATE&quot;, label = &quot;OLS&quot;) dx_4 &lt;- diagnose_design(design_4, sims = sims, bootstrap_sims = b_sims) dx_4 ## ## Research design diagnosis based on 100 simulations. Diagnosand estimates with bootstrapped standard errors in parentheses (20 replicates). ## ## Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Power ## design_4 ATE DIM Z 100 -0.01 0.26 0.93 ## (0.02) (0.02) (0.03) ## design_4 ATE OLS Z 100 -0.49 0.52 0.64 ## (0.02) (0.02) (0.05) ## Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand ## 0.96 0.99 0.26 0.28 0.00 1.00 ## (0.02) (0.02) (0.02) (0.00) (0.00) (0.00) ## 0.32 0.51 0.18 0.21 0.00 1.00 ## (0.05) (0.02) (0.01) (0.00) (0.00) (0.00) "],
["instrumental-variables.html", "17.2 Instrumental variables", " 17.2 Instrumental variables 17.2.1 Declaration design &lt;- declare_population(N = 100, U = rnorm(N)) + declare_potential_outcomes(D ~ if_else(Z + U &gt; 0, 1, 0), assignment_variables = Z) + declare_potential_outcomes(Y ~ 0.1 * D + 0.25 + U, assignment_variables = D) + declare_estimand(late = mean(Y_D_1[D_Z_1 == 1 &amp; D_Z_0 == 0] - Y_D_0[D_Z_1 == 1 &amp; D_Z_0 == 0])) + declare_assignment(prob = 0.5) + declare_reveal(D, Z) + declare_reveal(Y, D) + declare_estimator(Y ~ D | Z, model = iv_robust, estimand = &quot;LATE&quot;) explain the IV estimator is the ratio of \\(ITT_y^\\widehat / ITT_d^\\widehat\\) identical to noncompliance in an experiment 17.2.2 Dag 17.2.3 Redesign excludability violations if you do, go \\(ITT_d\\) and \\(ITT_y\\) 17.2.4 Suggested readings Applications - Mo, Cecilia Hyunjung, Katherine Conn, Georgia Anderson-Nilsson. 2019. “Can National Service Activism Activate Women’s Political Ambition? Evidence from Teach For America.” Politics, Groups, and Identities 7(4): 864-877. Methodological literature - Angrist and Pischke ch. 4 - Gerber and Green ch. 5 and ch. 6 17.2.5 Example "],
["difference-in-differences.html", "17.3 Difference-in-differences", " 17.3 Difference-in-differences 17.3.1 Declaration design &lt;- declare_population( unit = add_level(N = 2, X = rnorm(N, sd = 0.5)), period = add_level(N = 2, nest = FALSE), unit_period = cross_levels(by = join(unit, period), U = rnorm(N, sd = 0.01)) ) + declare_potential_outcomes(Y ~ X + 0.5 * as.numeric(period) + Z + U) + declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0), subset = period == 2) + declare_step(Z = X == max(X), handler = mutate) + declare_reveal(Y = if_else(Z == 0 | period == 1, Y_Z_0, Y_Z_1), handler = mutate) + declare_estimator(Y ~ Z * period, model = lm_robust, se_type = &quot;none&quot;) 17.3.2 Dag 17.3.3 Example 17.3.4 Two-period two-group setting Show that comparison of T and C in period 2 is biased and comparison of T between period 1 and 2 is biased, but DiD unbiased in presence of confounding in treatment assignment (unit with higher unit shock is always treated) and time trends N_units &lt;- 2 N_time_periods &lt;- 2 two_period_two_group_design &lt;- declare_population( units = add_level(N = N_units, unit_shock = rnorm(N, sd = 0.5)), periods = add_level(N = N_time_periods, nest = FALSE, time = (1:N_time_periods) - N_time_periods + 1), unit_period = cross_levels(by = join(units, periods), unit_time_shock = rnorm(N, sd = 0.01)) ) + # internal note: the unbiasedness obtains whether or not there is a unit-time shock declare_potential_outcomes( Y_Z_0 = unit_shock + 0.5 * time + unit_time_shock, # common pretreatment trend Y_Z_1 = Y_Z_0 + 1) + declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0), subset = time == 1) + declare_assignment(Z = unit_shock == max(unit_shock), handler = mutate) + declare_reveal( Y = case_when(Z == 0 | time &lt; 1 ~ Y_Z_0, TRUE ~ Y_Z_1), handler = mutate) + declare_estimator(estimate = (mean(Y[Z == 1 &amp; time == 1]) - mean(Y[Z == 0 &amp; time == 1])) - (mean(Y[Z == 1 &amp; time == 0]) - mean(Y[Z == 0 &amp; time == 0])), estimator_label = &quot;DiD&quot;, handler = summarize, label = &quot;DiD&quot;) + declare_estimator(estimate = mean(Y[Z == 1 &amp; time == 1]) - mean(Y[Z == 1 &amp; time == 0]), estimator_label = &quot;Diff&quot;, handler = summarize, label = &quot;Over-Time&quot;) + declare_estimator(estimate = mean(Y[Z == 1 &amp; time == 1]) - mean(Y[Z == 0 &amp; time == 1]), estimator_label = &quot;DiM&quot;, handler = summarize, label = &quot;DiM&quot;) diagnosis_two_period_two_group &lt;- diagnose_design( two_period_two_group_design, diagnosands = declare_diagnosands(select = bias), sims = sims, bootstrap_sims = b_sims) kable(get_diagnosands(diagnosis_two_period_two_group)) design_label estimand_label estimator_label bias se(bias) n_sims two_period_design ATE DiD -0.0421836 0.0671367 1000 two_period_design ATE Diff 0.4619315 0.0487573 1000 two_period_design ATE DiM 1.1511461 0.0547887 1000 17.3.5 Parallel trends assumption Introduce assumption and visual test # add an additional pretreatment time period in order to visually test for parallel pre-trends three_period_two_group_design &lt;- redesign(two_period_two_group_design, N_time_periods = 3) draw_data(three_period_two_group_design) %&gt;% group_by(Z, time) %&gt;% summarize(Y = mean(Y)) %&gt;% mutate(Z_color = factor(Z, levels = c(FALSE, TRUE), labels = c(&quot;Untreated&quot;, &quot;Treated&quot;))) %&gt;% ggplot(aes(time, Y, color = Z_color)) + geom_line() + scale_color_discrete(&quot;&quot;) + scale_x_discrete(&quot;Time&quot;, limits = c(-1, 0, 1)) Formal test (DID on T = -1 and T = 0 periods, i.e. a year backward from the DiD) There is a result that shows that the two-step procedure of the parallel trends assumption then DID if test passes that shows poor coverage of SEs in final DID (https://arxiv.org/abs/1804.01208). Cite here. 17.3.6 Multi-period design Switch to regression context with 20 periods, 100 units and show same results hold with two-way FE (controlling for one period before T is insufficient to remove bias) N_units &lt;- 20 N_time_periods &lt;- 20 multi_period_design &lt;- declare_population( units = add_level(N = N_units, unit_shock = rnorm(N), unit_treated = 1*(unit_shock &gt; median(unit_shock)), unit_treatment_start = sample(2:(N_time_periods - 1) - N_time_periods + 1, N, replace = TRUE)), periods = add_level(N = N_time_periods, nest = FALSE, time = (1:N_time_periods) - N_time_periods + 1), unit_period = cross_levels(by = join(units, periods), noise = rnorm(N), pretreatment = 1*(time &lt; unit_treatment_start)) ) + declare_potential_outcomes( Y_Z_0 = unit_shock + 0.5 * time + noise, # common pretreatment trend Y_Z_1 = Y_Z_0 + 0.2) + declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0), subset = time == 1) + declare_assignment(Z = 1*(unit_treated &amp; pretreatment == FALSE), handler = fabricate) + declare_reveal(Y, Z) + declare_estimator(Y ~ Z + time, fixed_effects = ~ units + periods, model = lm_robust, label = &quot;twoway-fe&quot;, estimand = &quot;ATE&quot;) diagnosis_multi_period_multi_group &lt;- diagnose_design(multi_period_design, diagnosands = declare_diagnosands(select = bias), sims = sims, bootstrap_sims = b_sims) kable(get_diagnosands(diagnosis_multi_period_multi_group)) design_label estimand_label estimator_label term bias se(bias) n_sims multi_period_design ATE twoway-fe Z -0.0005784 0.0066689 1000 Show that in case where some units switch back and forth between T and C during panel there is bias (point to Imai and Kim appear with weighted FE estimator to fix this) "],
["regression-discontinuity.html", "17.4 Regression Discontinuity", " 17.4 Regression Discontinuity 17.4.1 Declaration cutoff &lt;- 0.5 control &lt;- function(X) { as.vector(poly(X, 4, raw = TRUE) %*% c(.7, -.8, .5, 1))} treatment &lt;- function(X) { as.vector(poly(X, 4, raw = TRUE) %*% c(0, -1.5, .5, .8)) + .15} design &lt;- declare_population( N = 1000, U = rnorm(N, 0, 0.1), X = runif(N, 0, 1) + U - cutoff, Z = 1 * (X &gt; 0) ) + declare_potential_outcomes(Y ~ Z * treatment(X) + (1 - Z) * control(X) + U) + declare_estimand(LATE = treatment(0) - control(0)) + declare_reveal(Y, Z) + declare_estimator(Y ~ poly(X, 4) * Z, model = lm_robust, estimand = &quot;LATE&quot;) 17.4.2 Dag 17.4.3 Example Regression discontinuity designs exploit substantive knowledge that treatment is assigned in a particular way: everyone above a threshold is assigned to treatment and everyone below it is not. Even though researchers do not control the assignment, substantive knowledge about the threshold serves as a basis for a strong identification claim. Thistlewhite and Campbell introduced the regression discontinuity design in the 1960s to study the impact of scholarships on academic success. Their insight was that students with a test score just above a scholarship cutoff were plausibly comparable to students whose scores were just below the cutoff, so any differences in future academic success could be attributed to the scholarship itself. Regression discontinuity designs identify a local average treatment effect: the average effect of treatment exactly at the cutoff. The main trouble with the design is that there is vanishingly little data exactly at the cutoff, so any answer strategy needs to use data that is some distance away from the cutoff. The further away from the cutoff we move, the larger the threat of bias. We’ll consider an application of the regression discontinuity design that examines party incumbency advantage – the effect of a party winning an election on its vote margin in the next election. 17.4.4 Design Declaration Model: Regression discontinuity designs have four components: A running variable, a cutoff, a treatment variable, and an outcome. The cutoff determines which units are treated depending on the value of the running variable. In our example, the running variable \\(X\\) is the Democratic party’s margin of victory at time \\(t-1\\); and the treatment, \\(Z\\), is whether the Democratic party won the election in time \\(t-1\\). The outcome, \\(Y\\), is the Democratic vote margin at time \\(t\\). We’ll consider a population of 1,000 of these pairs of elections. A major assumption required for regression discontinuity is that the conditional expectation functions for both treatment and control potential outcomes are continuous at the cutoff.12 To satisfy this assumption, we specify two smooth conditional expectation functions, one for each potential outcome. The figure plots \\(Y\\) (the Democratic vote margin at time \\(t\\)) against \\(X\\) (the margin at time \\(t-1\\)). We’ve also plotted the true conditional expectation functions for the treated and control potential outcomes. The solid lines correspond to the observed data and the dashed lines correspond to the unobserved data. cutoff &lt;- .5 control &lt;- function(X) { as.vector(poly(X, 4, raw = TRUE) %*% c(.7, -.8, .5, 1))} treatment &lt;- function(X) { as.vector(poly(X, 4, raw = TRUE) %*% c(0, -1.5, .5, .8)) + .15} rd_design &lt;- # Model ------------------------------------------------------------------- declare_population( N = 1000, X = runif(N, 0, 1) - cutoff, noise = rnorm(N, 0, .1), Z = 1 * (X &gt; 0) ) + declare_potential_outcomes(Y ~ Z * treatment(X) + (1 - Z) * control(X) + noise) + # Inquiry ----------------------------------------------------------------- declare_estimand(LATE = treatment(0) - control(0)) + # Data Strategy ----------------------------------------------------------------- declare_reveal(Y, Z) + # Answer Strategy --------------------------------------------------------- declare_estimator(formula = Y ~ poly(X, 4) * Z, model = lm_robust, estimand = &quot;LATE&quot;) Inquiry: Our estimand is the effect of a Democratic win in an election on the Democratic vote margin of the next election, when the Democratic vote margin of the first election is zero. Formally, it is the difference in the conditional expectation functions of the control and treatment potential outcomes when the running variable is exactly zero. The black vertical line in the plot shows this difference. Data strategy: We collect data on the Democratic vote share at time \\(t-1\\) and time \\(t\\) for all 1,000 pairs of elections. There is no sampling or random assignment. Answer strategy: We will approximate the treated and untreated conditional expectation functions to the left and right of the cutoff using a flexible regression specification estimated via OLS. In particular, we fit each regression using a fourth-order polynomial. Much of the literature on regression discontinuity designs focuses on the tradeoffs among answer strategies, with many analysts recommending against higher-order polynomial regression specifications. We use one here to highlight how well such an answer strategy does when it matches the functional form in the model. We discuss alternative estimators in the exercises. rd_diagnosis &lt;- diagnose_design(rd_design, sims = sims, bootstrap_sims = b_sims) Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. summary(rd_diagnosis) ## ## Research design diagnosis based on 100 simulations. Diagnosand estimates with bootstrapped standard errors in parentheses (20 replicates). ## ## Design Label Estimand Label Estimator Label Term N Sims Bias RMSE ## rd_design LATE estimator poly(X, 4)1 100 2.08 24.82 ## (2.36) (1.50) ## Power Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand ## 0.06 0.95 2.23 24.85 26.52 0.33 0.15 ## (0.02) (0.02) (2.36) (1.54) (0.28) (0.21) (0.00) 17.4.5 Takeaways We highlight three takeaways. First, the power of this design is very low: with 1,000 units we do not achieve even 10% statistical power. However, our estimates of the uncertainty are not too wide: the coverage probability indicates that our confidence intervals indeed contain the estimand 95% of the time as they should. Our answer strategy is highly uncertain because the fourth-order polynomial specification in regression model gives weights to the data that greatly increase the variance of the estimator (Gelman and Imbens (2017)). In the exercises we explore alternative answer strategies that perform better. Second, the design is biased because polynomial approximations of the average effect at exactly the point of the threshold will be inaccurate in small samples (Sekhon and Titiunik (2017)), especially as units farther away from the cutoff are incorporated into the answer strategy. We know that the estimated bias is not due to simulation error by examining the bootstrapped standard error of the bias estimates. Finally, from the figure, we can see how poorly the average effect at the threshold approximates the average effect for all units. The average treatment effect among the treated (to the right of the threshold in the figure) is negative, whereas at the threshold it is positive. This clarifies that the estimand of the regression discontinuity design, the difference at the cutoff, is only relevant for a small – and possibly empty – set of units very close to the cutoff. 17.4.6 Further Reading Since its rediscovery by social scientists in the late 1990s, the regression discontinuity design has been widely used to study diverse causal effects such as: prison on recidivism (Mitchell et al. (2017)); China’s one child policy on human capital (Qin, Zhuang, and Yang (2017)); eligibility for World Bank loans on political liberalization (Carnegie and Samii (2017)); and anti-discrimination laws on minority employment (Hahn, Todd, and Van der Klaauw (1999)). We’ve discussed a “sharp” regression discontinuity design in which all units above the threshold were treated and all units below were untreated. In fuzzy regression discontinuity designs, some units above the cutoff remain untreated or some units below take treatment. This setting is analogous to experiments that experience noncompliance and may require instrumental variables approaches to the answer strategy (see Compliance is a Potential Outcome). Geographic regression discontinuity designs use distance to a border as the running variable: units on one side of the border are treated and units on the other are untreated. Keele and Titiunik (2016) use such a design to study whether voters are more likely to turn out when they have the opportunity to vote directly on legislation on so-called ballot initiatives. A complication of this design is how to measure distance to the border in two dimensions. 17.4.7 Exercises Gelman and Imbens (2017) point out that higher order polynomial regression specifications lead to extreme regression weights. One approach to obtaining better estimates is to select a bandwidth, \\(h\\), around the cutoff, and run a linear regression. Declare a sampling procedure that subsets the data to a bandwidth around the threshold, as well as a first order linear regression specification, and analyze how the power, bias, RMSE, and coverage of the design vary as a function of the bandwidth. The rdrobust estimator in the rdrobust package implements a local polynomial estimator that automatically selects a bandwidth for the RD analysis and bias-corrected confidence intervals. Declare another estimator using the rdrobust function and add it to the design. How does the coverage and bias of this estimator compare to the regression approaches declared above? Reduce the number of polynomial terms of the the treatment() and control() functions and assess how the bias of the design changes as the potential outcomes become increasingly linear as a function of the running variable. Redefine the population function so that units with higher potential outcome are more likely to locate just above the cutoff than below it. Assess whether and how this affects the bias of the design. References "],
["bayesian-process-tracing.html", "17.5 Bayesian process-tracing", " 17.5 Bayesian process-tracing Process-tracing is a qualitative method that uses evidence from in-depth interviews and written records to test causal theories. Process-tracing designs often focus on “causes-of-effects” inquiries (e.g., did the presence of a strong middle class cause a revolution?), rather than on “effects-of-causes” inquiries (e.g., what is the average effect of a strong middle class on the probability of a revolution happening?) Goertz and Mahoney (2012). Causes-of-effects inquiries imply a hypothesis – “the strong middle class caused the revolution,” say. One widely-promoted answer strategy suggests evaluating whether such hypotheses are correct given the presence or absence of different “clues” found in the archives or interviews [Collier, Brady, and Seawright (2004); Mahoney (2012); Bennett and Checkel (2015); fairfield2013going]. Van Evera (1997) categorizes different kinds of clues according to whether one would only believe the hypothesis if one observed the clue (necessity) and whether observing the clue would suffice to infer the hypothesis was correct (sufficiency).13 Of course, it is rare to have such certainty—we typically attach varying degrees of belief to statements of truth. Bayesian process-tracing uses probability theory to form a posterior belief about a hypothesis, given our beliefs about whether we would observe different pieces of evidence if we are right or wrong~. Extending Van Evera (1997), “hoop tests”\" are clues that are nearly certain to be seen if the hypothesis is true, but likely either way, “smoking-guns” are unlikely to be seen in general but are extremely unlikely if a hypothesis is false, “straws-in-the-wind” are more likely when the hypothesis is true but still somewhat likely when it is not, and ``doubly-decisive’’ clues are very likely to be seen if a hypothesis is true and very unlikely if it is false. 17.5.1 Declaration \\(M\\) Model: We posit a population of 195 cases, each of which does or does not exhibit the presence of an outcome, \\(Y \\in \\{0,1\\}\\). For the sake of illustration, we will suppose that \\(Y\\) represents the presence or absence of a civil war. Each case also exhibits the presence or absence of a potential cause, \\(Z \\in \\{0,1\\}\\). For example, we might suppose that \\(Z\\) represents the presence or absence of natural resources. At random, 30% of the cases get \\(Z=1\\). We also assume researchers observe a clue, \\(C\\), that is informative for whether \\(Z\\) does or does not have a causal relationship with \\(Y\\). The potential outcomes of \\(Y\\) depend both on whether the cause, \\(Z\\), is present in a case and what “type” of causal relation the case exhibits. Conceptually, there are exactly four distinct types of causal relations. First, the presence of \\(Z\\) might cause \\(Y\\): if \\(Z = 0\\), then \\(Y = 0\\) and if \\(Z = 1\\) then \\(Y = 1\\). In other words, civil wars happen in such cases because the country has natural resources. Second, the absence of \\(Z\\) might cause \\(Y\\): if \\(Z = 0\\) then \\(Y = 1\\) and if \\(Z = 1\\) then \\(Y = 0\\). In such cases, civil war breaks out because the country does not have natural resources, and would not break out if the country had natural resources. Finally, \\(Y\\) might be present irrespective of \\(Z\\) or \\(Y\\) might be absent irrespective of \\(Z\\). Continuing our analogy, such countries would have had civil war or peace, irrespective of whether they also had natural resources (i.e., because war is related to some other causal process). We specify a model in which civil war is governed by causal pathway 1 (\\(Z\\) causes \\(Y\\)) in roughly 20% of cases, by pathway 2 (\\(\\neg Z\\) causes \\(Y\\)) in only 10% of cases, by pathway 3 (\\(Y\\) irrespective of \\(z\\)) in 20% of countries, and by pathway 4 (\\(\\neg Y\\) irrespective of \\(Z\\)) in half of all countries. There are also potential outcomes for the clue, \\(C\\). These depend on the case’s causal type. Specifically, the clue appears with .25 probability if the case is one in which \\(Z\\) causes \\(Y\\), and with probability .005 if it is one where \\(Y\\) occurs regardless. The clue does not appear in the other types of cases. Crucially, while the outcome, cause, and clue are observable to the researcher, the type is not. \\(I\\) Inquiry: We wish to know the answer to a sample-specific “cause of effects” question: given the specific case we sampled, what is the probability that \\(Z\\) caused \\(Y\\)? More formally, we want to know \\(\\Pr(Y_i(Z_i=0)=0| Z_i=1, Y_i(Z_i=1)=1)\\)—that is, what are the chances that \\(Y\\) would have been 0 if \\(Z\\) were 0 for a unit \\(i\\) for which \\(Z\\) was 1 and \\(Y\\) was 1. This is equivalent to asking the probability that the case is of type 1. The inquiry thus takes the value 0 or 1 depending on the type of case. \\(D\\) Data Strategy: The fundamental problem that the researcher faces is that of observational equivalence: different causal types can cause the same data patterns. The issue is mitigated by following a controversial sampling strategy: selecting on \\(Y\\). By selecting at random one case in which both \\(Z\\) and \\(Y\\) are present, the researcher can narrow her uncertainty to two candidate types: the second and fourth causal types are incapable of producing the data \\(Z = 1, Y = 1\\). It cannot be that natural resources were the cause of peace (type 2), or that peace would have happened irrespective of natural resources (type 4), in a country that had a civil war and natural resources. \\(A\\) Answer Strategy: The researcher uses Bayes’ rule to update about the probability that \\(Z\\) caused \\(Y\\) given \\(C\\). types &lt;- c(&#39;Z_caused_Y&#39;, &#39;Z_caused_not_Y&#39;, &#39;always_Y&#39;, &#39;always_not_Y&#39;) design &lt;- declare_population(N = 195, Z = draw_binary(prob = .3, N = N), type = sample(x = types, size = N, replace = TRUE, prob = c(.2, .1, .2, .5))) + declare_potential_outcomes( Y ~ Z * (type == &quot;Z_caused_Y&quot;) + (1 - Z) * (type == &quot;Z_caused_not_Y&quot;) + (type == &quot;always_Y&quot;), conditions = list(Z = c(0, 1), type = types)) + declare_potential_outcomes( pr_C_1 ~ Z * (.25 * (type == &quot;Z_caused_Y&quot;) + .005 * (type == &quot;always_Y&quot;)), conditions = list(Z = c(0, 1), type = types)) + declare_reveal(c(Y, pr_C_1), c(Z, type)) + declare_measurement(C = draw_binary(prob = pr_C_1)) + declare_sampling(handler = function(data) data %&gt;% filter(Z==1 &amp; Y==1) %&gt;% sample_n(size = 1)) + declare_estimand(did_Z_cause_Y = type == &#39;Z_caused_Y&#39;) + declare_estimator( pr_type_Z_caused_Y = .5, pr_C_1_type_Z_caused_Y = .25, pr_C_1_type_always_Y = .005, pr_C_type_Z_caused_Y = C * pr_C_1_type_Z_caused_Y + (1 - C) * (1 - pr_C_1_type_Z_caused_Y), pr_C_type_always_Y = C * pr_C_1_type_always_Y + (1 - C) * (1 - pr_C_1_type_always_Y), posterior = pr_type_Z_caused_Y * pr_C_type_Z_caused_Y / (pr_type_Z_caused_Y * pr_C_type_Z_caused_Y + pr_C_type_always_Y * (1 - pr_type_Z_caused_Y)), estimator_label = &quot;Smoking Gun&quot;, estimand_label = &quot;did_Z_cause_Y&quot;, handler = summarize) 17.5.2 Dag 17.5.3 Exercises Inspect the DAG. How would removing the arrow pointing from type to C affect the inferences a researcher could draw? Run draw_data(design). Interpret the value of the type variable. Look at the values of Z and Y. Explain which types could produce these values and why. The variable pr_C_1_Z_1_type_Z_caused_Y indicates “The potential outcome of the clue probability, given the case is the type in which Z causes Y,” while pr_C_1_Z_1_type_always_Y gives the corresponding potential outcome for cases whose causal type is one in which Y always happens regardless of Z. Why do these potential outcomes make the clue a smoking gun? Explain why the potential outcome Y_Z_0_type_always_Y takes the value 1 whereas the potential outcome Y_Z_0_type_Z_caused_Y takes the value 0. Using the code below, diagnose the design and interpret the diagnosands. diagnose_design(design, diagnosands = declare_diagnosands( bias = mean(posterior - estimand), rmse = sqrt(mean((posterior - estimand) ^ 2)), mean_estimand = mean(estimand), mean_posterior = mean(posterior), keep_defaults = FALSE), sims = 1000) Look at the estimator declaration. What prior beliefs do pr_C_1_type_Z_caused_Y and pr_C_1_type_always_Y represent? How would setting these beliefs to the same value affect the design, and why? What is the implication for clue selection in Bayesian Process Tracing designs? Declare a new_design in which you modify the prior belief that the case belongs to the first causal type by changing pr_type_Z_caused_Y to a different value (say, .7). Diagnose new_design. How does the bias parameter change, and why? Keeping the modification you just made, modify the code for new_design further so that the clue potential outcomes function is pr_C_1 ~ Z * (.9999 * (type == \"Z_caused_Y\") + .0001 * (type == \"always_Y\" and the clue priors are pr_C_1_type_Z_caused_Y = .9999 and pr_C_1_type_always_Y = .0001. What kind of clue does the researcher have now? Diagnose new_design. Why do these changes to the clue reduce bias? 17.5.4 Online Appendix Applied Example Point here is to explore cases where you look at multiple sources of evidence In many parts of the world, people rely on non-state institutions to construct social order. Sometimes, those institutions persist for a long time [e.g., mourides], whereas sometimes they break down. Why do non-state institutions fail? Some scholars of African societies think that traditional institutions declined in the 1960s and 1970s due to the forceful efforts of post-independence leaders who saw these alternative authorities as a threat to the young state. Other scholars such as Ensminger (1990) point instead to the internal political economy of rural societies, and emphasize the role of economic interests in the decline of traditional institutions. Ensminger (1990) is an economic anthropologist who does a bunch of cool work with the Orma in Kenya, a nomadic pastoralist group. She points out that, whereas the Orma were able to avoid a tragedy of the commons by policing access to scarce water resources by competing somali pastoralists throughout the 1960s and 1970s, by the 1980s the power of the council of elders was weakened, as evidenced by frequent defection of individual orma who sold their water to somalis, thus hurting the interests of the group. Her study presents a case of causal process tracing in which the scholar presents evidence from rich fieldwork to support the inference that some effect was produced by a specific cause. It is useful to formalize as it provides lessons about what kinds of clues qualitative researchers might seek in order to maximize the probative value of their answer strategy. The key pieces are the outcome (breakdown of council of elders – measured through defections by orma selling their water to somalis), its potential cause (economic diversification – measured as a move away from cattle-based pastoralism towards sendentary economic activity – teaching, shops – by some Orma), and the pieces of evidence or “clues” that support the notion that the relationship between the outcome and the cause is causal. We set up an imaginary study inspired by Ensminger (1990) The researcher seeks to test the claim that economic diversification leads to the breakdown of informal institutions. They look for a group in Kenya that has experienced diversification and whose institutions of managing the commons have failed. [think of cool empirical implications]. They want to test this claim against the idea that informal institutions failed due to a deliberate attempt by state authorities to supplant traditional leaders. Their answer strategy diverges from the one in the book insofar as they seek not one but two clues to test their prior explanation. This time the researcher needs to think not only about the typology of their clues in terms of Van Evera stuff, but also the joint probability distribution of the clues. We make use of the joint_prob function from the book R package, which calculates the joint probability distribution of two correlated events, given their marginal probabilities and correlation Researcher imposes monotonicity (no Z_caused_not_Y types) – economic diversity either has no or a negative effect on institutions Note: no variation in outcome. All informal institutions have declined, we just want to know if economic diversification caused it. A quant study would provide no leverage. Note: joint probabilities of observing clues depend only on type, not (as before) on type and Z types &lt;- c(&#39;Z_caused_Y&#39;, &#39;Z_caused_not_Y&#39;, &#39;always_Y&#39;, &#39;always_not_Y&#39;) design &lt;- declare_population(N = 20, Z = draw_binary(prob = .5, N = N), type = sample(x = types, size = N, replace = TRUE, prob = c(.5, 0, .5, 0))) + declare_potential_outcomes( Y ~ Z * (type == &quot;Z_caused_Y&quot;) + (1 - Z) * (type == &quot;Z_caused_not_Y&quot;) + (type == &quot;always_Y&quot;), conditions = list(Z = c(0, 1), type = types)) + declare_potential_outcomes( pr_C1C2_00 ~ (joint_prob(.75,.3,0,&quot;00&quot;) * (type == &quot;Z_caused_Y&quot;) + joint_prob(.25,.005,0,&quot;00&quot;) * (type == &quot;always_Y&quot;)), conditions = list(Z = c(0, 1), type = types)) + declare_potential_outcomes( pr_C1C2_01 ~ (joint_prob(.75,.3,0,&quot;01&quot;) * (type == &quot;Z_caused_Y&quot;) + joint_prob(.25,.005,0,&quot;01&quot;) * (type == &quot;always_Y&quot;)), conditions = list(Z = c(0, 1), type = types)) + declare_potential_outcomes( pr_C1C2_10 ~ (joint_prob(.75,.3,0,&quot;10&quot;) * (type == &quot;Z_caused_Y&quot;) + joint_prob(.25,.005,0,&quot;10&quot;) * (type == &quot;always_Y&quot;)), conditions = list(Z = c(0, 1), type = types)) + declare_potential_outcomes( pr_C1C2_11 ~ (joint_prob(.75,.3,0,&quot;11&quot;) * (type == &quot;Z_caused_Y&quot;) + joint_prob(.25,.005,0,&quot;11&quot;) * (type == &quot;always_Y&quot;)), conditions = list(Z = c(0, 1), type = types)) + declare_reveal(c(Y, pr_C1C2_00,pr_C1C2_01,pr_C1C2_10,pr_C1C2_11), c(Z, type)) + declare_assignment(blocks = ID, block_prob_each = cbind(pr_C1C2_00,pr_C1C2_01,pr_C1C2_10,pr_C1C2_11), conditions = c(&quot;00&quot;,&quot;01&quot;,&quot;10&quot;,&quot;11&quot;), assignment_variable = &quot;C1C2&quot;) + declare_sampling(handler = function(data) data %&gt;% filter(Z==1 &amp; Y==1) %&gt;% sample_n(size = 1)) + declare_estimand(did_Z_cause_Y = type == &#39;Z_caused_Y&#39;) + declare_measurement( C1 = ifelse(C1C2 == &quot;10&quot; | C1C2 == &quot;11&quot;, 1, 0), C2 = ifelse(C1C2 == &quot;01&quot; | C1C2 == &quot;11&quot;, 1, 0), handler = fabricate) + declare_estimator( pr_type_Z_caused_Y = .5, pr_C_1_type_Z_caused_Y = .75, pr_C_1_type_always_Y = .25, C = C1, pr_C_type_Z_caused_Y = C * pr_C_1_type_Z_caused_Y + (1 - C) * (1 - pr_C_1_type_Z_caused_Y), pr_C_type_always_Y = C * pr_C_1_type_always_Y + (1 - C) * (1 - pr_C_1_type_always_Y), posterior = pr_type_Z_caused_Y * pr_C_type_Z_caused_Y / (pr_type_Z_caused_Y * pr_C_type_Z_caused_Y + pr_C_type_always_Y * (1 - pr_type_Z_caused_Y)), label = &quot;Straw in the Wind&quot;, estimand_label = &quot;did_Z_cause_Y&quot;, handler = summarize) + declare_estimator( pr_type_Z_caused_Y = .5, pr_C_1_type_Z_caused_Y = .30, pr_C_1_type_always_Y = .005, C = C2, pr_C_type_Z_caused_Y = C * pr_C_1_type_Z_caused_Y + (1 - C) * (1 - pr_C_1_type_Z_caused_Y), pr_C_type_always_Y = C * pr_C_1_type_always_Y + (1 - C) * (1 - pr_C_1_type_always_Y), posterior = pr_type_Z_caused_Y * pr_C_type_Z_caused_Y / (pr_type_Z_caused_Y * pr_C_type_Z_caused_Y + pr_C_type_always_Y * (1 - pr_type_Z_caused_Y)), label = &quot;Smoking Gun&quot;, estimand_label = &quot;did_Z_cause_Y&quot;, handler = summarize) + declare_estimator( pr_type_Z_caused_Y = .5, pr_C1_1_type_Z_caused_Y = .30, pr_C1_1_type_always_Y = .005, pr_C2_1_type_Z_caused_Y = .75, pr_C2_1_type_always_Y = .25, rho = 0, pr_C_type_Z_caused_Y = joint_prob(pr_C1_1_type_Z_caused_Y, pr_C2_1_type_Z_caused_Y, rho, which_prob = C1C2), pr_C_type_always_Y = joint_prob(pr_C1_1_type_always_Y, pr_C2_1_type_always_Y, rho, which_prob = C1C2), posterior = pr_type_Z_caused_Y * pr_C_type_Z_caused_Y / (pr_type_Z_caused_Y * pr_C_type_Z_caused_Y + pr_C_type_always_Y * (1 - pr_type_Z_caused_Y)), label = &quot;Joint Updating&quot;, estimand_label = &quot;did_Z_cause_Y&quot;, handler = summarize) # diagnose_design(design, # diagnosands = declare_diagnosands( # bias = mean(posterior - estimand), # rmse = sqrt(mean((posterior - estimand) ^ 2)), # mean_estimand = mean(estimand), # mean_posterior = mean(posterior), # keep_defaults = FALSE), # sims = 500) # design_sims &lt;- simulate_design(design, sims = 500) 17.5.5 Dag dag &lt;- dagify(C1 ~ type, C2 ~ type, Y ~ Z + type) nodes &lt;- tibble( name = c( &quot;type&quot;,&quot;Z&quot;,&quot;C1&quot;, &quot;C2&quot;, &quot;Y&quot;), label = name, annotation = c( &quot;**Type**&lt;br&gt;Causal relationship&lt;br&gt;between economic diversification&lt;br&gt;and breakdown of institutions&quot;, &quot;**Cause**&lt;br&gt;Economic Diversification&quot;, &quot;**Clue 1**&lt;br&gt;Smoking gun interview evidence&quot;, &quot;**Clue 2**&lt;br&gt;Straw-in-the-wind archival evidence&quot;, &quot;**Outcome**&lt;br&gt;Breakdown of traditional&lt;br&gt;institutions&quot;), x = c(1, 1, 5, 5, 5), y = c(1.5,3.5,2,1.5,3.5), nudge_direction = c(&quot;S&quot;, &quot;N&quot;, &quot;N&quot;, &quot;S&quot;,&quot;N&quot;), answer_strategy = &quot;uncontrolled&quot; ) ggdd_df &lt;- make_dag_df(dag, nodes, design) base_dag_plot %+% ggdd_df References "],
["synthetic-controls.html", "17.6 Synthetic controls", " 17.6 Synthetic controls 17.6.1 Declaration design &lt;- declare_population( unit = add_level(N = 10, units = 1:N, X = rnorm(N, sd = 0.5)), period = add_level(N = 3, time = 1:N, nest = FALSE), unit_period = cross_levels(by = join(unit, period), U = rnorm(N)) ) + declare_potential_outcomes(Y ~ X + 0.5 * as.numeric(period) + Z + U) + declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0), subset = period == 3) + declare_step(handler = mutate, Z = unit == &quot;01&quot;) + declare_reveal(Y = if_else(Z == 0 | period &lt; 3, Y_Z_0, Y_Z_1), handler = mutate) + declare_step(predictors = &quot;X&quot;, time.predictors.prior = 1:2, dependent = &quot;Y&quot;, unit.variable = &quot;units&quot;, time.variable = &quot;time&quot;, treatment.identifier = 1, controls.identifier = 2:10, handler = synth_weights_tidy) + declare_estimator(Y ~ Z, subset = time &gt;= 3, weights = synth_weights, model = lm_robust, label = &quot;synth&quot;) 17.6.2 Dag # Simulation -------------------------------------------------------------- # simulations &lt;- simulate_design(design, sims = 100) # ggplot(simulations, aes(estimate)) + geom_histogram() # Synth plot -------------------------------------------------------------- # data &lt;- draw_data(design) # summary_df &lt;- data %&gt;% # group_by(Z, time) %&gt;% # summarize(Y = weighted.mean(Y, w = synth_weights)) # ggplot(summary_df, aes(x = time, y = Y, color = Z)) + # geom_line(size = 2, alpha = 0.5) + # geom_line(data = data, aes(x = time, y = Y, group = units), color = &quot;black&quot;, alpha = 0.3) + # geom_point(data = data, aes(x = time, y = Y, size = synth_weights^2), alpha = 0.3) + # geom_vline(xintercept = 2.5) dag &lt;- dagify(Y ~ X + period + Z + U, Z ~ X) nodes &lt;- tibble( name = c(&quot;U&quot;, &quot;X&quot;, &quot;period&quot;, &quot;Z&quot;, &quot;Y&quot;), label = c(&quot;U&quot;, &quot;X&quot;, &quot;T&quot;, &quot;Z&quot;, &quot;Y&quot;), annotation = c( &quot;**Unknown heterogeneity**&quot;, &quot;**Unit effect**&quot;, &quot;**Time period**&quot;, &quot;**Treatment assignment**&quot;, &quot;**Outcome variable**&quot; ), x = c(5, 1, 1, 3, 5), y = c(1.5,3.5, 1, 2.5, 2.5), nudge_direction = c(&quot;S&quot;, &quot;N&quot;, &quot;S&quot;, &quot;S&quot;,&quot;N&quot;), answer_strategy = &quot;uncontrolled&quot;) ggdd_df &lt;- make_dag_df(dag, nodes, design) base_dag_plot %+% ggdd_df 17.6.3 Example Modeled after the example here: https://www.mitpressjournals.org/doi/abs/10.1162/REST_a_00429?casa_token=o-zWqCima50AAAAA:yiEERZfdhAUoHV0-xBYNjgdljvgfRXrriR8foG7X8nHSUAMFrLcw2vWY8e9pHzmRT24MMAIv9hvKpQ Did the 2007 Legal Arizona Workers Act Reduce the State’s Unauthorized Immigrant Population? Sarah Bohn, Magnus Lofstrom, and Steven Raphael The Review of Economics and Statistics 2014 96:2, 258-269 Abstract: We test for an effect of Arizona’s 2007 Legal Arizona Workers Act (LAWA) on the proportion of the state’s population characterized as noncitizen Hispanic. We use the synthetic control method to select a group of states against which Arizona’s population trends can be compared. We document a notable and statistically significant reduction in the proportion of the Hispanic noncitizen population in Arizona. The decline observed matches the timing of LAWA’s implementation, deviates from the time series for the synthetic control group, and stands out relative to the distribution of placebo estimates for other states in the nation. Outline: (1) how does synth work? - declaration: set up states with time trends and levels that are both correlated with a type and following the linear model assumed by SCM - try three estimators: (1) difference-in-difference; (2) single difference in treated period; and (3) difference in treated period weighted by Synth weights. - show that synth works under its assumptions; plot of time series of treat and synthetic control; plot of the time series from all units to illustrate which are picked (sorted by weights) (2) what are synth’s assumptions? - linear model; treated unit is in convex hull of control units’ pretreatment time series (3) how to diagnose when you are outside the convex hull - declaration outside the convex hull and use the Abadie diagnostic demonstrating a poor match. (possibly explore power of this diagnostic) - show that synth is biased in this setting. augsynth is not. # tidy function that takes data and just adds the synthetic control weights to it synth_weights_tidy &lt;- function(data) { dataprep.out &lt;- dataprep( foo = data, predictors = &quot;prop_non_hispanic_below_hs&quot;, predictors.op = &quot;mean&quot;, time.predictors.prior = 1998:2006, dependent = &quot;prop_non_hispanic_below_hs&quot;, unit.variable = &quot;state_number&quot;, time.variable = &quot;year&quot;, treatment.identifier = 4, controls.identifier = c(1:3, 5:50), # states without Arizona time.optimize.ssr = 1998:2006, time.plot = 1998:2009) capture.output(fit &lt;- synth(data.prep.obj = dataprep.out)) tab &lt;- synth.tab(dataprep.res = dataprep.out, synth.res = fit) data %&gt;% left_join(tab$tab.w %&gt;% mutate(synth_weights = w.weights) %&gt;% dplyr::select(synth_weights, unit.numbers), by = c(&quot;state_number&quot; = &quot;unit.numbers&quot;)) %&gt;% mutate(synth_weights = replace(synth_weights, state_number == 4, 1)) } augsynth_tidy &lt;- function(data) { fit &lt;- augsynth(prop_non_hispanic_below_hs ~ legal_worker_act, state, year, t_int = 2007, data = data) res &lt;- summary(fit)$att %&gt;% filter(Time == 2007) %&gt;% select(Estimate, Std.Error) names(res) &lt;- c(&quot;estimate&quot;, &quot;std.error&quot;) res$p.value &lt;- 2 * pt(-abs(res$estimate/res$std.error), df = nrow(data) - 15) res$conf.low &lt;- res$estimate - 1.96 * res$std.error res$conf.high &lt;- res$estimate + 1.96 * res$std.error res } # note need to clean up the range of the data, currently over 1 design &lt;- declare_population( states = add_level( N = 50, state = state.abb, state_number = as.numeric(as.factor(state)), state_shock = runif(N, -.15, .15), border_state = state %in% c(&quot;AZ&quot;, &quot;CA&quot;, &quot;NM&quot;, &quot;TX&quot;), state_shock = ifelse(border_state, .2, state_shock) ), years = add_level( N = 12, nest = FALSE, year = 1998:2009, post_treatment_period = year &gt;= 2007, year_shock = runif(N, -.025, .025), year_trend = year - 1998 ), obs = cross_levels( by = join(states, years), # treatment indicator: legal_worker_act = if_else(post_treatment_period == TRUE &amp; state == &quot;AZ&quot;, 1, 0), state_year_shock = runif(N, -.025, .025), prop_non_hispanic_below_hs_baseline = 0.4 + state_shock + year_shock + (.01 + .05 * border_state) * year_trend + state_year_shock ) ) + declare_potential_outcomes( prop_non_hispanic_below_hs ~ prop_non_hispanic_below_hs_baseline + 0.25 * legal_worker_act, assignment_variable = legal_worker_act) + declare_estimand( ATE_AZ = mean(prop_non_hispanic_below_hs_legal_worker_act_1 - prop_non_hispanic_below_hs_legal_worker_act_0), subset = legal_worker_act == TRUE) + declare_reveal(prop_non_hispanic_below_hs, legal_worker_act) + declare_step(handler = synth_weights_tidy) + declare_estimator( prop_non_hispanic_below_hs ~ legal_worker_act, subset = year &gt;= 2007, weights = synth_weights, model = lm_robust, label = &quot;synth&quot;) + declare_estimator( prop_non_hispanic_below_hs ~ legal_worker_act, subset = year &gt;= 2007, model = lm_robust, label = &quot;unweighted&quot;) + declare_estimator( prop_non_hispanic_below_hs ~ I(state == &quot;AZ&quot;) + post_treatment_period + legal_worker_act, term = &quot;legal_worker_act&quot;, model = lm_robust, label = &quot;unweighted_did&quot;) + declare_estimator(handler = tidy_estimator(augsynth_tidy), label = &quot;augsynth&quot;) state_data &lt;- draw_data(design) state_data %&gt;% dplyr::select(state, synth_weights) %&gt;% distinct %&gt;% arrange(-synth_weights) %&gt;% head ## state synth_weights ## 1 AZ 1.000 ## 2 NM 0.990 ## 3 TX 0.007 ## 4 CA 0.001 ## 5 AL 0.000 ## 6 AK 0.000 state_data %&gt;% ggplot() + geom_line(aes(year, prop_non_hispanic_below_hs)) + facet_wrap(~ state) state_data %&gt;% mutate(treatment_state = factor(state == &quot;AZ&quot;, levels = c(FALSE, TRUE), labels = c(&quot;Synthethic Control&quot;, &quot;Arizona&quot;))) %&gt;% group_by(treatment_state, year) %&gt;% summarize(prop_non_hispanic_below_hs = weighted.mean(prop_non_hispanic_below_hs, w = synth_weights)) %&gt;% ggplot(aes(x = year, y = prop_non_hispanic_below_hs, color = treatment_state)) + geom_line() + geom_vline(xintercept = 2007) + scale_x_continuous(breaks = scales::pretty_breaks()) + annotate(&quot;text&quot;, x = 2006.7, y = 1.7, label = &quot;Law Introduced in 2007&quot;, hjust = &quot;right&quot;, family = &quot;Palatino&quot;) + labs(color = &quot;&quot;) + xlab(&quot;&quot;) + ylab(&quot;Proportion Non-Hispanic Below H.S. Education&quot;) + dd_theme() simulations &lt;- simulate_design(design, sims = sims) Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. synth_diagnosands &lt;- declare_diagnosands(select = c(&quot;bias&quot;, &quot;rmse&quot;, &quot;coverage&quot;)) diagnosis &lt;- diagnose_design(simulations, diagnosands = synth_diagnosands, bootstrap_sims = b_sims) kable(reshape_diagnosis(diagnosis)) Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Coverage design ATE_AZ augsynth NA 1000 0.00 0.02 0.66 (0.00) (0.00) (0.01) design ATE_AZ synth legal_worker_act 1000 0.01 0.02 1.00 (0.00) (0.00) (0.00) design ATE_AZ unweighted legal_worker_act 1000 0.66 0.66 0.00 (0.00) (0.00) (0.00) design ATE_AZ unweighted_did legal_worker_act 1000 0.28 0.28 0.00 (0.00) (0.00) (0.00) we see that Synth outperforms either method 17.6.4 When there are not good controls, standard synth will get the wrong answer # declaration outside the convex hull design_outside_hull &lt;- replace_step( design, step = 2, new_step = declare_potential_outcomes( prop_non_hispanic_below_hs ~ prop_non_hispanic_below_hs_baseline + 0.25 * legal_worker_act + 0.2 * (state == &quot;AZ&quot;), assignment_variable = legal_worker_act)) state_data_outside_hull &lt;- draw_data(design_outside_hull) simulations_outside_hull &lt;- simulate_design(design_outside_hull, sims = sims) Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. diagnosis_outside_hull &lt;- diagnose_design(simulations_outside_hull, diagnosands = synth_diagnosands, bootstrap_sims = b_sims) kable(reshape_diagnosis(diagnosis_outside_hull)) Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Coverage design_outside_hull ATE_AZ augsynth NA 1000 -0.01 0.03 0.75 (0.00) (0.00) (0.01) design_outside_hull ATE_AZ synth legal_worker_act 1000 0.20 0.20 0.00 (0.00) (0.00) (0.00) design_outside_hull ATE_AZ unweighted legal_worker_act 1000 0.86 0.86 0.00 (0.00) (0.00) (0.00) design_outside_hull ATE_AZ unweighted_did legal_worker_act 1000 0.28 0.28 0.00 (0.00) (0.00) (0.00) # plot the synthetic control constructed in this way (it usually picks just texas and is highly biased) state_data_outside_hull %&gt;% mutate(treatment_state = factor(state == &quot;AZ&quot;, levels = c(FALSE, TRUE), labels = c(&quot;Synthethic Control&quot;, &quot;Arizona&quot;))) %&gt;% group_by(treatment_state, year) %&gt;% summarize(prop_non_hispanic_below_hs = weighted.mean(prop_non_hispanic_below_hs, w = synth_weights)) %&gt;% ggplot(aes(x = year, y = prop_non_hispanic_below_hs, color = treatment_state)) + geom_line() + geom_vline(xintercept = 2007) + scale_x_continuous(breaks = scales::pretty_breaks()) + annotate(&quot;text&quot;, x = 2006.7, y = 1.7, label = &quot;Law Introduced in 2007&quot;, hjust = &quot;right&quot;, family = &quot;Palatino&quot;) + labs(color = &quot;&quot;) + xlab(&quot;&quot;) + ylab(&quot;Proportion Non-Hispanic Below H.S. Education&quot;) + dd_theme() 17.6.5 References "],
["experimental-designs-for-causal-inference.html", "Chapter 18 Experimental designs for causal inference", " Chapter 18 Experimental designs for causal inference Section introduction "],
["two-arm-trials.html", "18.1 Two arm trials", " 18.1 Two arm trials 18.1.1 Declaration design &lt;- declare_population(N = 100, U = rnorm(N)) + declare_potential_outcomes(Y ~ Z + U) + declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + declare_assignment(prob = 0.5) + declare_estimator(Y ~ Z, estimand = &quot;ATE&quot;) 18.1.2 Dag 18.1.3 Example You can use the global bib file via rmarkdown cites like this: Imai, King, and Stuart (2008) design &lt;- declare_population(N = 100, u = rnorm(N)) + declare_potential_outcomes(Y ~ Z + u) + declare_assignment(prob = 0.5) + declare_reveal(Y, Z) + declare_estimator(Y ~ Z, model = difference_in_means) This chunk is set to echo = TRUE and eval = do_diagnosis simulations_pilot &lt;- simulate_design(design, sims = sims) Right after you do simulations, you want to save the simulations rds. Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. kable(head(simulations_pilot)) design_label sim_ID estimator_label term estimate std.error statistic p.value conf.low conf.high df outcome design 1 estimator Z 0.9624445 0.1705951 5.641688 0.0000002 0.6234399 1.301449 88.32465 Y design 2 estimator Z 0.9532166 0.2124610 4.486548 0.0000200 0.5315217 1.374911 96.66334 Y design 3 estimator Z 0.9674071 0.2137695 4.525468 0.0000170 0.5431863 1.391628 97.96114 Y design 4 estimator Z 1.0545908 0.1942247 5.429747 0.0000004 0.6690878 1.440094 96.58766 Y design 5 estimator Z 1.0708566 0.1897958 5.642153 0.0000002 0.6942127 1.447501 97.99051 Y design 6 estimator Z 0.7197320 0.1964379 3.663916 0.0004076 0.3297870 1.109677 95.63827 Y References "],
["blocked-trials.html", "18.2 Blocked trials", " 18.2 Blocked trials 18.2.1 Declaration design &lt;- declare_population(N = 100, X = rbinom(N, 1, 0.3), U = rnorm(N)) + declare_potential_outcomes(Y ~ Z + X + U) + declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + declare_assignment(blocks = X, block_prob = c(0.1, 0.5)) + declare_estimator(Y ~ Z, estimand = &quot;ATE&quot;, label = &quot;Naive DIM&quot;) + declare_estimator(Y ~ Z, blocks = X, estimand = &quot;ATE&quot;, label = &quot;Blocked DIM&quot;) 18.2.2 Dag 18.2.3 Example 18.2.4 Two-arm trials and designs with blocking and clustering design &lt;- declare_population( villages = add_level(N = 100, X = rbinom(N, 1, 0.3), Q = rnorm(N)), people = add_level(N = 5, U = rnorm(N)) ) + declare_potential_outcomes(Y ~ Z * X + U + Q) + declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + declare_assignment(clusters = villages, blocks = X, block_prob = c(0.1, 0.5)) + declare_estimator( Y ~ Z, model = difference_in_means, estimand = &quot;ATE&quot;, label = &quot;Naive DIM&quot;) + declare_estimator( Y ~ Z, clusters = villages, blocks = X, model = difference_in_means, estimand = &quot;ATE&quot;, label = &quot;Blocked DIM&quot; ) + declare_estimator( Y ~ Z, clusters = villages, fixed_effects = X, model = lm_robust, estimand = &quot;ATE&quot;, label = &quot;Naive FE&quot; ) "],
["multiarm-designs.html", "18.3 Multiarm Designs", " 18.3 Multiarm Designs 18.3.1 Declaration 18.3.2 Dag 18.3.3 Example You can use the global bib file via rmarkdown cites like this: Imai, King, and Stuart (2008) design &lt;- declare_population(N = 100, u = rnorm(N)) + declare_potential_outcomes(Y ~ Z + u) + declare_assignment(prob = 0.5) + declare_reveal(Y, Z) + declare_estimator(Y ~ Z, model = difference_in_means) This chunk is set to echo = TRUE and eval = do_diagnosis simulations_pilot &lt;- simulate_design(design, sims = sims) Right after you do simulations, you want to save the simulations rds. Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. kable(head(simulations_pilot)) design_label sim_ID estimator_label term estimate std.error statistic p.value conf.low conf.high df outcome design 1 estimator Z 0.9624445 0.1705951 5.641688 0.0000002 0.6234399 1.301449 88.32465 Y design 2 estimator Z 0.9532166 0.2124610 4.486548 0.0000200 0.5315217 1.374911 96.66334 Y design 3 estimator Z 0.9674071 0.2137695 4.525468 0.0000170 0.5431863 1.391628 97.96114 Y design 4 estimator Z 1.0545908 0.1942247 5.429747 0.0000004 0.6690878 1.440094 96.58766 Y design 5 estimator Z 1.0708566 0.1897958 5.642153 0.0000002 0.6942127 1.447501 97.99051 Y design 6 estimator Z 0.7197320 0.1964379 3.663916 0.0004076 0.3297870 1.109677 95.63827 Y References "],
["encouragement-designs.html", "18.4 Encouragement designs", " 18.4 Encouragement designs 18.4.1 Declaration direct_effect_of_encouragement &lt;- 0.0 proportion_defiers &lt;- 0.0 design &lt;- declare_population( N = 100, type = sample( x = c(&quot;Always-Taker&quot;, &quot;Never-Taker&quot;, &quot;Complier&quot;, &quot;Defier&quot;), prob = c(0.1, 0.1, 0.8, 0.0), size = N, replace = TRUE ), U = rnorm(N) ) + declare_potential_outcomes( D ~ case_when( Z == 1 &amp; type %in% c(&quot;Always-Taker&quot;, &quot;Complier&quot;) ~ 1, Z == 1 &amp; type %in% c(&quot;Never-Taker&quot;, &quot;Defier&quot;) ~ 0, Z == 0 &amp; type %in% c(&quot;Never-Taker&quot;, &quot;Complier&quot;) ~ 0, Z == 0 &amp; type %in% c(&quot;Always-Taker&quot;, &quot;Defier&quot;) ~ 1 ) ) + declare_potential_outcomes( Y ~ 0.5 * (type == &quot;Complier&quot;) * D + 0.25 * (type == &quot;Always-Taker&quot;) * D + 0.75 * (type == &quot;Defier&quot;) * D + # Building in NO excludability violation 0 * Z + U, assignment_variables = c(&quot;D&quot;, &quot;Z&quot;) ) + declare_estimand(CACE = mean(Y_D_1_Z_1 - Y_D_0_Z_0), subset = type == &quot;Complier&quot;) + declare_assignment(prob = 0.5) + declare_reveal(D, assignment_variable = &quot;Z&quot;) + declare_reveal(Y, assignment_variables = c(&quot;D&quot;, &quot;Z&quot;)) + declare_estimator(Y ~ D | Z, model = iv_robust, estimand = &quot;CACE&quot;) 18.4.2 Dag 18.4.3 Example Idea for this one would be to show how violations of no defiers and excludability lead to bias. types &lt;- c(&quot;Always-Taker&quot;, &quot;Never-Taker&quot;, &quot;Complier&quot;, &quot;Defier&quot;) direct_effect_of_encouragement &lt;- 0.0 proportion_defiers &lt;- 0.0 design &lt;- declare_population( N = 500, type = sample( types, N, replace = TRUE, prob = c(0.1, 0.1, 0.8 - proportion_defiers, proportion_defiers) ), noise = rnorm(N) ) + declare_potential_outcomes( D ~ case_when( Z == 0 &amp; type %in% c(&quot;Never-Taker&quot;, &quot;Complier&quot;) ~ 0, Z == 1 &amp; type %in% c(&quot;Never-Taker&quot;, &quot;Defier&quot;) ~ 0, Z == 0 &amp; type %in% c(&quot;Always-Taker&quot;, &quot;Defier&quot;) ~ 1, Z == 1 &amp; type %in% c(&quot;Always-Taker&quot;, &quot;Complier&quot;) ~ 1 ) ) + declare_potential_outcomes( Y ~ 0.5 * (type == &quot;Complier&quot;) * D + 0.25 * (type == &quot;Always-Taker&quot;) * D + 0.75 * (type == &quot;Defier&quot;) * D + direct_effect_of_encouragement * Z + noise, assignment_variables = c(&quot;D&quot;, &quot;Z&quot;) ) + declare_estimand(CACE = mean((Y_D_1_Z_1 + Y_D_1_Z_0) / 2 - (Y_D_0_Z_1 + Y_D_0_Z_0) / 2), subset = type == &quot;Complier&quot;) + declare_assignment(prob = 0.5) + declare_reveal(D, assignment_variable = &quot;Z&quot;) + declare_reveal(Y, assignment_variables = c(&quot;D&quot;, &quot;Z&quot;)) + declare_estimator(Y ~ D | Z, model = iv_robust, estimand = &quot;CACE&quot;) designs &lt;- redesign( design, proportion_defiers = seq(0, 0.3, length.out = 5), direct_effect_of_encouragement = seq(0, 0.3, length.out = 5) ) simulations &lt;- simulate_design(designs, sims = sims) gg_df &lt;- simulations %&gt;% group_by(proportion_defiers, direct_effect_of_encouragement) %&gt;% summarize(bias = mean(estimate - estimand)) ggplot(gg_df, aes( proportion_defiers, bias, group = direct_effect_of_encouragement, color = direct_effect_of_encouragement )) + geom_point() + geom_line() 18.4.4 References "],
["stepped-wedge-designs.html", "18.5 Stepped wedge designs", " 18.5 Stepped wedge designs 18.5.1 Declaration design &lt;- declare_population( unit = add_level(N = 8, X = rnorm(N)), period = add_level(N = 3, time = as.numeric(period), nest = FALSE), obs = cross_levels(by = join(unit, period), U = rnorm(N)) ) + declare_potential_outcomes(Y ~ X + U + Z * time) + declare_assignment(clusters = unit, conditions = 1:4, assignment_variable = &quot;wave&quot;) + declare_assignment(Z = as.numeric(time &gt;= wave), ipw = 1 / (Z * 2/8 + (1 - Z) * (1 - 2/8)), handler = fabricate) + declare_reveal(Y, Z) + declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + declare_estimator(Y ~ Z, model = lm_robust, estimand = &quot;ATE&quot;, label = &quot;1: Wave 2 only&quot;, subset = period == 2) + declare_estimator(Y ~ Z, model = lm_robust, estimand = &quot;ATE&quot;, label = &quot;2: Weighted, clustered SW&quot;, weights = ipw, clusters = unit) + declare_estimator(Y ~ Z, model = lm_robust, estimand = &quot;ATE&quot;, label = &quot;3: Unweighted, unclustered SW&quot;) 18.5.2 Dag 18.5.3 Example In a stepped wedge design, individuals are randomly assigned to enter into treatment in different stages, and at each stage their outcomes are remeasured. The first panel of Figure X illustrates where the study gets its name: here, two randomly selected individuals (or clusters) enter the treatment at before each of the three waves of measurement, while two individuals are randomly selected to remain in control. Such designs have a remarkable feature: because who enters the treatment group at any given stage is random, the data collected in each measurement wave can (almost) be treated like a fresh experiment. Often, stepped wedge designs are vaunted for their policy appeal: stepped wedges are a good option for rigorous causal inference when it’s not fair or feasible to prevent some people from getting a treatment. Yet, that same goal can be achieved in the context of any experiment with one wave of measurement: just treat the control once you’ve conducted the final measurement. The real advantage of the stepped wedge is not its ethical appeal: it’s the ability to squeeze more power out of a small sample. Stepped-wedge designs are more complicated than they at first appear. Below, we explain why: this design can be thought of as a block-randomized experiment with heterogeneous probabilities and a high risk of spillovers. Show SW is better-powered but both coefficient and SE will be biased if not analyzed correctly. Show how allocating more sample in earlier waves better here due to high variance in treatment POs. Show how spillovers are very plausible and can be overcome. Show extreme case in which no power gain from SW (when u_it = 0, ICC = 1): this is the intuition behind SW – power gains derive from inter-temporal variation 18.5.4 Design Declaration Model: our design has eight units randomized and remeasured at three time points. Units’ untreated potential outcomes consist of an unit- and a unit-period-specific shock. Their treated potential increases relative to their control potential outcome by a rate of 1 each period (see second panel of Figure X). Inquiry: Since any unit at any given point in time reveals one of only two potential outcomes, we can define our estimand simply as the average treatment effect. As panel 2 shows, the ATE averaged over the three periods is (1 + 2 + 3) / 3 = 2. An important assumption that enables us to define the estimand in this way is that units reveal the same treated or untreated potential outcome, irrespective of what happened in the previous period. This amounts to a no spillovers assumption, which may be implausible in practice. We return to this below. Data strategy: our assignment strategy works in two stages. We first cluster-assign unit-periods to a wave in which they will be treated. One-quarter of the units are not treated in any wave (pure control): p_00 = 2/8. The remaining three quarters are distributed evenly among the three waves of treatment: p_W1 &lt;- p_W2 &lt;- p_W3 &lt;- 2/8. In a second phase, the wave assignment is converted into a simple indicator, Z, for whether the unit is treated in a given period or not. There is an important but subtle difference between the wave and Z variables [explain Z is joint prob] Answer strategy: [describe: we look at an approach that treats it like blocks. What are inverse propensities weights? Simply put, they correspond to the number of observations that a sampled observation “represents.” We also compare to the power of a design that only involves one wave of measurement, e.g. at wave 2 where units are treated 5050.] p_00 &lt;- p_W1 &lt;- p_W2 &lt;- p_W3 &lt;- 2/8 design &lt;- declare_population( t = add_level(N = 3, trend = as.numeric(t), # u_t = rnorm(N), p = c(p_W1, p_W1 + p_W2, p_W1 + p_W2 + p_W3)), i = add_level(N = 8, u_i = rnorm(N), nest = FALSE), obs = cross_levels(by = join(t, i), u_it = rnorm(N))) + declare_potential_outcomes(Y_Z_0 = u_i + u_it, Y_Z_1 = u_i + u_it + trend) + declare_assignment(clusters = i, conditions = 1:4, prob_each = c(p_W1, p_W2, p_W3, p_00), assignment_variable = &quot;wave&quot;) + declare_step(Z = as.numeric(t &gt;= wave), ip = 1 / (Z * p + (1 - Z) * (1 - p)), handler = fabricate) + declare_reveal(Y, Z) + declare_estimand(ate = mean(Y_Z_1 - Y_Z_0)) + declare_estimator(Y ~ Z, model = lm_robust, label = &quot;1: Wave 2 only&quot;, subset = t == 2) + declare_estimator(Y ~ Z, model = lm_robust, label = &quot;2: Weighted, clustered SW&quot;, weights = ip, clusters = i) + declare_estimator(Y ~ Z, model = lm_robust, label = &quot;3: Unweighted, unclustered SW&quot;) # Diagnose design diagnosis &lt;- diagnose_design(design) Estimator Label Bias Power Coverage Mean Estimate SD Estimate Mean Se 1: Wave 2 only 0.06 0.38 0.92 2.06 1.04 0.95 (0.04) (0.02) (0.01) (0.04) (0.03) (0.01) 2: Weighted, clustered SW 0.04 0.53 0.96 2.04 0.79 0.77 (0.03) (0.02) (0.01) (0.03) (0.03) (0.01) 3: Unweighted, unclustered SW 0.36 0.94 0.86 2.36 0.68 0.59 (0.03) (0.01) (0.02) (0.03) (0.02) (0.00) Comparing 1 and 2, stepped wedge gives a big improvement in power, is unbiased, and gets the coverage correct. Clearly better than just doing a cross-section at wave 2. Estimator 3 shows two pitfalls, however, that can lead us to overestimate benefits of SW. First, it is biased: [explain how treated POs are more commonly observed in periods when they’re higher, we need to weight for this. How weights come about: There is one way that a unit can be observed in a treated state in wave 1: they are assigned to W1 with probability p_W1. There are two ways in which a unit can be treated in wave 2: they are assigned in W1 with p_W1 or in W2 with p_W2. Because being assigned in W1 or in W2 are exclusive and independent events, we can get the prob of being treated in any period by wave 2 by summing the probs.] Second, standard errors are wrong. Units should be treated as clusters (draw analogue to two-stage random assignment in saturation design?) A natural question is how power changes as more or fewer units are assigned. Here, we consider variations on the stepped wedge design declared above, in which we hold constant at 3 the number of units assigned to the pure control, and shift an increasing proportion of the assignment to later waves. designs &lt;- list( a = redesign(design, p_00 = 3/8, p_W1 = 3/8, p_W2 = 1/8, p_W3 = 1/8), b = redesign(design, p_00 = 3/8, p_W1 = 2/8, p_W2 = 2/8, p_W3 = 1/8), c = redesign(design, p_00 = 3/8, p_W1 = 2/8, p_W2 = 1/8, p_W3 = 2/8), d = redesign(design, p_00 = 3/8, p_W1 = 1/8, p_W2 = 3/8, p_W3 = 1/8), e = redesign(design, p_00 = 3/8, p_W1 = 1/8, p_W2 = 2/8, p_W3 = 2/8), f = redesign(design, p_00 = 3/8, p_W1 = 1/8, p_W2 = 1/8, p_W3 = 3/8) ) # Diagnose design diagnoses &lt;- diagnose_designs(designs) We see a monotonic decrease in the power as more of the sample is treated later. This occurs because treating units later means observing less of the potential outcomes. The potential outcomes are more variant, so we’re better off when we observe more of them [I think!] 18.5.5 Spillovers An important assumption is that the potential outcomes revealed are not a function of what wave the unit entered into treatment. In other words, there are no Show how, in case of such spillovers, you can just treat the effects as different, but then you lose power gains (and possibly change estimand) 18.5.6 References "],
["randomized-saturation-design.html", "18.6 Randomized Saturation Design", " 18.6 Randomized Saturation Design 18.6.1 Declaration design &lt;- declare_population( group = add_level(N = 10, X = rnorm(N)), unit = add_level(N = 100, U = rnorm(N)) ) + declare_assignment(clusters = group, conditions = c(&quot;low&quot;, &quot;high&quot;), assignment_variable = S) + declare_step(S_prob = case_when(S == &quot;low&quot; ~ 0.25, S == &quot;high&quot; ~ 0.75), mutate) + declare_assignment(blocks = group, prob_unit = S_prob) + declare_potential_outcomes( Y ~ Z + (S == &quot;high&quot;) + Z*(S == &quot;high&quot;) + X + U, conditions = list(Z = c(0, 1), S = c(&quot;low&quot;, &quot;high&quot;))) + declare_estimand(ATE_saturation = mean(Y_Z_0_S_high - Y_Z_0_S_low), ate_no_spill = mean(Y_Z_1_S_low - Y_Z_0_S_low)) + declare_reveal(Y, c(Z, S)) + declare_estimator(Y ~ Z + S, model = lm_robust, term = c(&quot;Z&quot;, &quot;Shigh&quot;), estimand = c(&quot;ATE_saturation&quot;, &quot;ate_no_spill&quot;), label = &quot;main effect&quot;) 18.6.2 Dag 18.6.3 Example Randomized saturation designs (Baird et al. (2018)) offer researchers a way to estimate the diffusion of intervention effects within some geographic or social network. Most approaches work by first cluster-assigning non-overlapping groups of individuals to treatment saturations, then block-assigning individuals to treatment in the proportions determined by the saturations. Asunka et al. (2019), for example, wanted to know if the presence of election monitors at ballot stations would displace violence and fraud to other ballot stations. They randomized constituencies to low, medium, and high levels of saturation, and then randomized ballot stations to have election monitoring or not in low, medium, or high concentrations, depending on the randomized saturation. In the original study, the authors did not include a zero-saturation condition. Here, we declare a simplified version of their design in which a zero-saturation condition is included. Main points to develop: Randomized saturation is great when you get the model right. Though, show how IPW reduces the power to detect main effect, especially if there’s no spillover. Randomized saturation assumes a model that may be wrong. In particular, spillovers are restricted to containers. But this might not be correct. 18.6.4 Design Declaration Model: Potential outcomes are defined in terms of S—the saturation—and Z—whether or not a ballot station is treated. We model spillovers in two ways. In the first, the amount of spillover that affects a unit is determined by how many other units in its network are treated. In the second, the amount of spillover a unit receives is determined by whether that unit’s geographic neighbor is treated, irrespective of whether they share a network. Inquiry: We want to know the effect of having high and medium levels of saturation versus low saturation in the control: \\(E[Y_i(Z_i = 0, S_i = \\text{high})-Y_i(Z_i = 0, S_i = \\text{low})]\\) and \\(E[Y_i(Z_i = 0, S_i = \\text{medium})-Y_i(Z_i = 0, S_i = \\text{low})]\\). We also want to know the “direct effect”–e.g. what happens to those directly treated if we disregard spillovers. Here it is defined over potential outcomes that the experiment does not reveal, since no one is treated in low-saturation constituencies: \\(E[Y_i(Z_i = 1, S_i = \\text{low})-Y_i(Z_i = 0, S_i = \\text{low})]\\). Data strategy: We assign entire groups of individual ballot stations to one of three saturations: low (0%), medium (50%), and high (75%). We then randomize individuals within groups to treatment or control in the proportions dictated by the saturation. Thus, the saturation is cluster-randomized, whereas treatment is block-randomized. Answer strategy: We weight each individual by the inverse of the probability that they find themselves in the condition they’re in. To estimate spillovers, we run one regression comparing high and one regression comparing medium to low saturation control units. To estimate the direct effect, we run a regression of the outcome on the treatment indicatior on the full sample, controlling for saturation. N_individuals &lt;- 60 N_groups &lt;- 15 G_per_saturation &lt;- c(5,5,5) design &lt;- declare_population(N = N_individuals, X = 1:N, U = rnorm(N), G = ntile(X, N_groups)) + declare_assignment(assignment_variable = &quot;S&quot;, clusters = G, conditions = c(&quot;low&quot;,&quot;med&quot;,&quot;high&quot;), m_each = G_per_saturation) + declare_assignment(prob = 0, blocks = G, assignment_variable = &quot;Z_S_low&quot;) + declare_assignment(prob = .5, blocks = G, assignment_variable = &quot;Z_S_med&quot;) + declare_assignment(prob = .75, blocks = G, assignment_variable = &quot;Z_S_high&quot;) + declare_step( spillover_low = ave(Z_S_low, G, FUN = sum) * .1, spillover_med = ave(Z_S_med, G, FUN = sum) * .1, spillover_high = ave(Z_S_high, G, FUN = sum) * .1, handler = fabricate, label = &quot;spillover&quot;) + declare_potential_outcomes( Y ~ Z * -.20 + U + spillover_low * (S == &quot;low&quot;) + spillover_med * (S == &quot;med&quot;) + spillover_high * (S == &quot;high&quot;), conditions = list(Z = c(0,1), S = c(&quot;low&quot;,&quot;med&quot;,&quot;high&quot;))) + declare_estimand(high = mean(Y_Z_0_S_high - Y_Z_0_S_low), med = mean(Y_Z_0_S_med - Y_Z_0_S_low), ate_no_spill = mean(Y_Z_1_S_low - Y_Z_0_S_low)) + declare_reveal(Z,S) + declare_step( w = 1 / (S_cond_prob * (Z_S_low_cond_prob * (S == &quot;low&quot;) + Z_S_med_cond_prob * (S == &quot;med&quot;) + Z_S_high_cond_prob * (S == &quot;high&quot;))), handler = fabricate) + declare_reveal(Y,c(Z, S)) + declare_estimator(model = lm_robust, formula = Y ~ S, subset = Z == 0 &amp; S %in% c(&quot;high&quot;,&quot;low&quot;), estimand = &quot;high&quot;, weights = w, label = &quot;high vs low&quot;) + declare_estimator(model = lm_robust, formula = Y ~ S, subset = Z == 0 &amp; S %in% c(&quot;med&quot;,&quot;low&quot;), weights = w, estimand = &quot;med&quot;, label = &quot;med vs low&quot;) + declare_estimator(model = lm_robust, formula = Y ~ Z + S, term = &quot;Z&quot;, weights = w, estimand = &quot;ate_no_spill&quot;, label = &quot;main effect&quot;) Here’s what our hypothetical country looks like: draw_data(design) %&gt;% ggplot(aes(x = 1, y = X, color = as.factor(G))) + geom_point() + scale_color_discrete(&quot;Ballot station&quot;) + scale_y_continuous(&quot;Latitude&quot;) + scale_x_continuous(&quot;Longitude&quot;) + geom_hline(yintercept = seq(1,N_individuals,by = N_individuals / N_groups) - .5) Let’s diagnose diagnosis &lt;- diagnose_design(design, sims = sims) Our diagnosis shows this design does a pretty great job, under this model of spillovers: diagnosis %&gt;% reshape_diagnosis() %&gt;% kable() Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Power Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand design ate_no_spill main effect Z 500 0.01 0.35 0.10 0.93 -0.19 0.35 0.33 0.08 -0.20 (0.01) (0.01) (0.01) (0.01) (0.01) (0.01) (0.00) (0.04) (0.00) design high high vs low Shigh 500 -0.05 0.51 0.12 0.90 0.25 0.50 0.48 0.13 0.30 (0.02) (0.02) (0.02) (0.01) (0.02) (0.02) (0.01) (0.05) (0.00) design med med vs low Smed 500 0.00 0.39 0.09 0.93 0.20 0.39 0.38 0.04 0.20 (0.02) (0.01) (0.01) (0.01) (0.02) (0.01) (0.00) (0.03) (0.00) It’s particularly nice, since we’re able to estimate the direct effect (whose constitutive POs we never observe) by partialling out spillovers. Show here: power tradeoffs for main effects versus spillovers, in terms of proportion of sample allocated to the “low” versus other conditions and also in terms of IPW (equivalent sample size with everyone in the .5 condition) Now, we consider a model of spillovers in which fraud is displaced latitudinally, from one neighbor to the next. Say, because there are roads traveling north and fraudsters disregard boundaries (in reality, they are unlikely to do so). distal_design &lt;- replace_step(design = design, step = &quot;spillover&quot;, new_step = declare_step(next_neighbor = c(N,1:(N-1)), spillover_low = Z_S_low[next_neighbor], spillover_med = Z_S_med[next_neighbor], spillover_high = Z_S_high[next_neighbor], handler = fabricate) ) distal_diagnosis &lt;- diagnose_design(distal_design, sims = sims) When there are next-neighbor spillovers that ignore boundaries, the estimator is biased again distal_diagnosis %&gt;% reshape_diagnosis() %&gt;% kable() Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Power Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand distal_design ate_no_spill main effect Z 500 -0.25 0.43 0.23 0.88 -0.45 0.36 0.36 0.00 -0.20 (0.02) (0.01) (0.02) (0.01) (0.02) (0.01) (0.00) (0.00) (0.00) distal_design high high vs low Shigh 500 0.20 0.54 0.48 0.91 0.95 0.50 0.48 0.00 0.75 (0.02) (0.02) (0.02) (0.01) (0.02) (0.02) (0.01) (0.00) (0.00) distal_design med med vs low Smed 500 0.15 0.42 0.33 0.92 0.65 0.40 0.41 0.00 0.50 (0.02) (0.01) (0.02) (0.01) (0.02) (0.01) (0.00) (0.00) (0.00) References "],
["multi-study-designs.html", "Chapter 19 Multi-study designs", " Chapter 19 Multi-study designs section introduction "],
["papers-with-multiple-studies.html", "19.1 Papers with multiple studies", " 19.1 Papers with multiple studies In many research projects, we seek to evaluate multiple observable implications for a single theory. [Examples: Psychology; APSR articles that have an experiment plus observational work; replication efforts.] In such cases, a single piece of evidence does not constitute sufficient evidence to validate the theory as a whole. Rather, we believe in the theory when multiple pieces of evidence support it. Conventions around what constitutes a convincing pattern of evidence vary. Some researchers will not believe a theory unless each piece of evidence in support of it is statistically significant. Less stringent approaches simply seek evidence “consistent” with the theory, such as the observation that all effects are signed in the predicted direction. Here, we declare an \\(N\\)-study design, and examine the consequences of these two different approaches to evaluating a theory in light of multiple studies. We show that, under multiple observable implications generated by the same process, conditioning on significance can lead one strongly astray. Generally speaking, looking at the sign of effects is more probative because it is much less prone to false negatives. With small numbers of studies, however, the risks of false positives are high. 19.1.1 Design Declaration Model: We declare \\(N\\) populations, all of which are governed by the same data-generating process: X is exogenous and standard normally-distributed, M is standard-normally distributed and correlated with X by rho, and Y is a function of the main effect of X as well as the interaction between X and M, with the size and sign of the direct and interactive effects determined by tau and gamma, respectively. Inquiry: We want to know, in a global sense, if our theory is “right.” Here, that means that the effect of X on Y is positive and increasing with M, and that X affects M. When tau is positive, our theory is correct. When it is zero or negative, our theory is incorrect. Data strategy: We conduct and collect independent datasets on \\(N\\) datasets of size n. In the example below, we conduct three studies, assuming we only observe X and Y in the first, only M and X in the second, and Y, M, and X, in the third. Answer strategy: Using linear regression, we estimate the bivariate correlation between X and Y in study 1, the bivariate correlation between X and M in study 2, and the interaction between X and M on Y in study 3. n1 &lt;- 100 n2 &lt;- 100 n3 &lt;- 100 rho &lt;- .5 gamma &lt;- tau &lt;- .2 generate_study_sample &lt;- function(n, rho, tau, gamma, data){ fabricate(N = n, X = rnorm(N), M = rnorm(N, X * rho, sqrt(1 - rho^2)), U = rnorm(N), Y = tau * X + gamma * M * X + U) } three_study_design &lt;- # Study 1 -- Bivariate correlation between X and Y declare_population(n = n1, tau = tau, gamma = gamma, rho = rho, handler = generate_study_sample) + declare_estimator(Y ~ X, term = &quot;X&quot;, model = lm_robust, label = &quot;Study 1&quot;) + # Study 2 -- Bivariate correlation between M and X declare_population(n = n2, tau = tau, gamma = gamma, rho = rho, handler = generate_study_sample) + declare_estimator(M ~ X, term = &quot;X&quot;, model = lm_robust, label = &quot;Study 2&quot;) + # Study 3 -- Interaction in X and M declare_population(n = n3, tau = tau, gamma = gamma, rho = rho, handler = generate_study_sample) + declare_estimator(Y ~ X + M + X:M, term = &quot;X:M&quot;, model = lm_robust, label = &quot;Study 3&quot;) 19.1.2 DAG ggdd_df &lt;- make_dag_df(dag, nodes, design2) base_dag_plot %+% ggdd_df ggdd_df &lt;- make_dag_df(dag, nodes, design3) base_dag_plot %+% ggdd_df 19.1.3 Takeaways Let us compare the performance of the “all significant” versus “all signed” approaches to theory confirmation when the theory is “correct” (tau and gamma positive), versus when it is “incorrect” (both parameters zero). In the first approach, a theory is deemed “supported” by the evidence when all effects are significant. In the second, the theory is supported by the evidence when the signs of all effects are positive. # Simulate design simulations &lt;- simulate_design(three_study_design) # Simulate null design null_three_study_design &lt;- redesign(three_study_design, tau = 0, gamma = 0, rho = 0) null_simulations &lt;- simulate_design(null_three_study_design) In the first three rows of the table, the theory is correct in that tau, gamma, and rho are positive, in the second three rows it is incorrect because both the main effect and interaction are zero. The “power” column tells us the proportion of simulations in which the effect is significant at the \\(\\alpha = .05\\) level, the “all significant” column tells us the proportion of simulations in which all of the studies have significant effects, the “positive” column tells us how often the study found a positively signed result, while the “all positive column” tells us the proportion of simulations where all studies had a positively signed result. tau estimator_label gamma power all_significant positive all_positive 0.2 Study 1 0.2 0.406 0.256 0.968 0.938 0.2 Study 2 0.2 0.998 0.256 1.000 0.938 0.2 Study 3 0.2 0.616 0.256 0.966 0.938 0.0 Study 1 0.0 0.058 0.000 0.500 0.114 0.0 Study 2 0.0 0.052 0.000 0.506 0.114 0.0 Study 3 0.0 0.060 0.000 0.530 0.114 Notice that, because the studies are independent, the probability that all are significant is equal to the product of their power: Pr(all studies significant) = Pr(Study 1 significant) \\(\\times\\)…\\(\\times\\) Pr(Study \\(N\\) significant). Thus, if you only believe a theory if the studies conducted to test it yield significant results, and those studies are all powered at the conventionally accepted level of 80%, you erroneously reject the theory with a probability of \\(.8^N\\). If you do three, conventionally well-powered, randomized studies each shooting at the right quantity of interest, then in almost half of the cases where you are right, you will think you are wrong. Furthermore, notice how detrimental the addition of a small study can be by this metric, even if it gets at an important mechanism. As soon as you condition your inference about the theory being correct on the significance of all the observable implications, a low-powered test can sharply increase the risk of false rejection. What can we say about the risk of false positives? The power of the individual studies is where it should be: for a stated error rate of $= $5%, the studies are every so slightly anti-conservative. However, the “all significant” desideratum creates a rejection rate that is too high. Some of these problems, though not all, are alleviated when we disregard significance and just look at signs. When the theory is right, there is a very good chance that all of the effects we estimate are positive: we surmise the theory is correct roughly 94% of the time when it actually is. When the theory is not correct in the sense that the true effects are zero, random error means that they are positive half the time and negative the other half. Consequently, the probability of erroneously accepting the theory based on the sign of effects when the true underlying effects are zero is equal to \\(.5^N\\). Here, that means we erroneously infer we are right at a relatively high rate of 12% of simulations. In the design above, the rates at which we rejected or accepted theories seemed to depend on the number of studies we considered. In the graph below, we look at twenty-nine different \\(N\\)-study designs, all of which seek to confirm a theory by replicating evidence for it \\(N\\) times. The first design is made of two studies, each independtly evaluating the hypothesis that \\(Y\\) is positively correlated with \\(X\\). Again, we consider how conditioning an inference about the theory on whether all results are significant or all results are positive affects error rates. Again, we see that significance is not a probative way to look for observable theoretical implications. As soon as there are more than four studies, there is virtually no chance of confirming even a true theory by this metric. We see a sort of reverse multiple-comparisons problem: increasing the number of tests makes false rejections increasingly more likely when we are interested in the joint probability of all the tests saying the same thing. Unless the number of studies is small, whether all produce significant results essentially yields no information about whether the theory is correct. By contrast, looking at signs only can be highly probative. In this application, the optimal number of studies is about eight. At that point, there is virtually no chance of erroneously inferring that the theory is correct when the effects are zero, but when the theory is correct there is a good chance (almost 75%) that all of the available evidence will be signed accordingly. As the number of studies increases, so too does the probability of discordant results, and using the unanimity of signs to judge whether the theory is correct becomes increasingly unwise. "],
["multi-site-studies.html", "19.2 Multi-site studies", " 19.2 Multi-site studies "],
["the-metaketa-design.html", "19.3 The Metaketa design", " 19.3 The Metaketa design Study Number Place What Works Design Implemented Success? 1 A \\(Y_A ~ Z_A\\) \\(MIDA_A\\) Yes 2 B \\(Y_B ~ Z_B\\) \\(MIDA_B\\) Yes 3 C \\(Y_C ~ Z_C\\) \\(MIDA_C\\) Yes 4 D \\(Y_D ~ Z_D\\) \\(MIDA_D\\) Yes 5 E \\(Y_E ~ Z_E\\) \\(MIDA_E\\) Yes The worry is that \\(I_A\\) and \\(I_B\\) etc are fundamentally different inquiries about different dags [faceted figure of 5 different DAGs with nodes in different languages roman, greek, numeric, windings]. The purpose of theory is to tell us when the \\(I\\)’s across context are different, but that is HARD and that is how research has gone for YEARS. The metaketa design says WAIT – LET’S agree on a \\(MIDA_{MK}\\) that we will implement in all 5 places, mutatis mutandis. That is, everyone “localizes” their design because things work slightly differently in different places, but if we try hard, we can implement the same “important” treatment on the same “important” outcome, even though the details (like the language of the treatment materials, the specifics of the measurement strategies.) will differ from place to place. Study Number Place What Works Design Implemented Success? 1 A \\(Y_A ~ Z_A\\) \\(\\delta_A * MIDA_{MK} - (1 - \\delta_A) * MIDA_A\\) ? 2 B \\(Y_B ~ Z_B\\) \\(\\delta_B * MIDA_{MK} - (1 - \\delta_B) * MIDA_B\\) ? 3 C \\(Y_C ~ Z_C\\) \\(\\delta_C * MIDA_{MK} - (1 - \\delta_C) * MIDA_C\\) ? 4 D \\(Y_D ~ Z_D\\) \\(\\delta_D * MIDA_{MK} - (1 - \\delta_D) * MIDA_D\\) ? 5 E \\(Y_E ~ Z_E\\) \\(\\delta_E * MIDA_{MK} - (1 - \\delta_E) * MIDA_E\\) ? Coordinating trials like the metaketa design means causing teams to implement designs that they otherwise would not have. Collaboration can bring many benefits and shared struggle, but fundamentally, it pulls all the designs towards the common \\(MIDA_{MK}\\) design. The strength of the pull is given by the \\(\\delta\\) parameters. Will the metaketa design be a success? Only if (1) the \\(MIDA_{MK}\\) design is a strong one and it is correct about how \\(Y\\) and \\(Z\\) relate. If the \\(MIDA_{MK}\\) design is poor, however, then all the studies will be a failure. Instead of learning \\(I_A\\) in place \\(A\\) and learning \\(I_B\\) in place \\(B\\), we learn nothing anywhere. The trouble with the standard approach (separate teams investigating phenomena that later theory tries to label as instances of the same thing) is that theory is hard and we can never really be sure if \\(I_A\\) means the same thing for \\(M_A\\) as \\(I_B\\) means for \\(M_B\\). The metaketa design tries to solve this theoretical problem on the front end – let’s agree on what the \\(M\\) we all have in mind is (with local variations), then run a study. This approach works well if the \\(M\\) we all agree on is approximately correct, and it works worse than the standard approach if we’re wrong. 19.3.1 Unbiased estimates of out-of-sample sites in presence of heterogeneous effects starting point is fixed budget and you’re thinking about two possible designs: (1) a single large study in one context or (2) a set of five studies in five different contexts with the same intervention and outcome measures When there are heterogeneous effects, you can get good predictions out of sample even when average effects differ substantially (and you do better with multiple sites when sites in the the population have different proportions of subject types that are correlated with het fx) Two notable features of the design: - there must be het fx for this to work (otherwise our estimates get biased toward zero because of overfitting to the het variables) - we have to have information about the covariate in the population and the sample (here we used the proportion of people in each het type) Findings: - these two strategies are both unbiased - the design with five sites has half the RMSE of the one-site design. this is because of the variation in the proportions of types across sites. - interestingly there is poor coverage (anti-conservative) when you use the single site design (when you have contextual variation as well, i.e. effect differs across sites for reasons not captured by the het fx, coverage is off for all designs. will keep this point out, seems like too much and you don’t need contextual effects to get different effects across sites, those come from different proportions of types) meta_re_estimator &lt;- function(data){ site_estimates_df &lt;- data %&gt;% group_by(site) %&gt;% do(tidy(lm_robust(Y ~ Z, data = .))) %&gt;% filter(term == &quot;Z&quot;) %&gt;% ungroup meta_fit &lt;- rma(estimate, std.error, data = site_estimates_df, method = &quot;REML&quot;) with(meta_fit, tibble( estimate = as.vector(beta), std.error = se, p.value = pval, conf.low = ci.lb, conf.high = ci.ub)) } post_strat_estimator &lt;- function(data, pr_types_population) { if(length(unique(data$site)) &gt; 1) { fit &lt;- lm_robust(Y ~ Z*as.factor(subject_type) + as.factor(site), data = data) tidy(fit) } else { fit &lt;- lm_robust(Y ~ Z*as.factor(subject_type), data = data) } alpha &lt;- .05 lh_fit &lt;- try({ linearHypothesis( fit, hypothesis.matrix = paste(paste(paste(pr_types_population[91:100][-1], &quot;*&quot;, matchCoefs(fit, &quot;Z&quot;), sep = &quot;&quot;), collapse = &quot; + &quot;), &quot; = 0&quot;), level = 1 - alpha) }) if(!inherits(lh_fit, &quot;try-error&quot;)) { tibble(estimate = drop(attr(lh_fit, &quot;value&quot;)), std.error = sqrt(diag(attr(lh_fit, &quot;vcov&quot;))), df = fit$df.residual, statistic = estimate / std.error, p.value = 2 * pt(abs(statistic), df, lower.tail = FALSE), conf.low = estimate + std.error * qt(alpha / 2, df), conf.high = estimate + std.error * qt(1 - alpha / 2, df)) } else { tibble(error = TRUE) } } # need to have biased sampling to get bias here # two kinds of populations, one in which the study type determines the subject types and you select on study type # a second kind where study type determines study shock # in second type if you adjust for subject type then you will be able to unbiased recover global multi_site_designer &lt;- function( N_sites = 10, n_study_sites = 5, n_subjects_per_site = 1000, feasible_effect = 0, subject_type_effects = seq(from = -0.1, to = 0.1, length.out = 10), pr_types = c( # rows are sites, columns are types 0.005, 0.005, 0.09, 0.15, 0.25, 0.1, 0, 0.1, 0.15, 0.15, 0.1, 0.15, 0.15, 0.15, 0.25, 0.005, 0, 0.1, 0.09, 0.005, 0.15, 0.15, 0.15, 0.005, 0.005, 0, 0.25, 0.09, 0.1, 0.1, 0, 0.15, 0.005, 0.09, 0.005, 0.15, 0.25, 0.1, 0.1, 0.15, 0.005, 0.1, 0.09, 0.25, 0.15, 0.15, 0.005, 0, 0.1, 0.15, 0.005, 0.15, 0.25, 0.1, 0, 0.1, 0.005, 0.15, 0.09, 0.15, 0.15, 0.15, 0.005, 0.25, 0.1, 0.15, 0.09, 0.005, 0.1, 0, 0.25, 0.1, 0.15, 0, 0.005, 0.15, 0.15, 0.1, 0.005, 0.09, 0.005, 0.1, 0.1, 0.15, 0, 0.25, 0.15, 0.09, 0.005, 0.15, 0.005, 0.09, 0.15, 0.1, 0, 0.1, 0.15, 0.005, 0.25, 0.15) ) { declare_population( site = add_level(N = N_sites, feasible_site = sample(c(rep(1, 8), rep(0, 2)), N, replace = FALSE)), subject_types = add_level( N = 10, subject_type = 1:10, subject_type_effect = subject_type_effects, type_proportion = pr_types, N_subjects = ceiling(2500 * type_proportion) ), subjects = add_level(N = N_subjects, noise = rnorm(N)) ) + declare_potential_outcomes(Y ~ Z * (0.1 + subject_type_effect + feasible_effect * feasible_site) + noise) + declare_estimand(ATE_feasible = mean(Y_Z_1 - Y_Z_0), subset = feasible_site == FALSE) + # true effect for feasible sites declare_sampling(clusters = site, strata = feasible_site, strata_n = c(0, n_study_sites)) + declare_sampling(strata = site, n = n_subjects_per_site) + declare_assignment(blocks = site, prob = 0.5) + declare_estimand(study_site_ATE = mean(Y_Z_1 - Y_Z_0)) + declare_estimator(handler = tidy_estimator(post_strat_estimator), pr_types_population = pr_types, label = &quot;post-strat&quot;) } single_site_large_design &lt;- multi_site_designer(n_study_sites = 1, n_subjects_per_site = 2500) small_study_five_sites &lt;- multi_site_designer(n_study_sites = 5, n_subjects_per_site = 500) simulations_small_large &lt;- simulate_design(single_site_large_design, small_study_five_sites, sims = sims) diagnosis_small_large &lt;- diagnose_design(simulations_small_large %&gt;% filter(!is.na(estimate) &amp; !is.na(std.error) &amp; !is.na(statistic) &amp; !is.na(p.value) &amp; !is.na(conf.low) &amp; !is.na(conf.high)), bootstrap_sims = b_sims) kable(get_diagnosands(diagnosis_small_large)) design_label estimand_label estimator_label bias se(bias) rmse se(rmse) power se(power) coverage se(coverage) mean_estimate se(mean_estimate) sd_estimate se(sd_estimate) mean_se se(mean_se) type_s_rate se(type_s_rate) mean_estimand se(mean_estimand) n_sims single_site_large_design ATE_feasible post-strat 0.0031475 0.0119653 0.3629044 0.0129728 0.1025126 0.009576 0.9356784 0.0077135 0.1040090 0.0119606 0.3633932 0.0130575 0.3182932 0.0065550 0.1666667 0.0371110 0.1008615 0.0003499 995 single_site_large_design study_site_ATE post-strat 0.0036542 0.0119708 0.3636432 0.0131462 0.1025126 0.009576 0.9356784 0.0076497 0.1040090 0.0119606 0.3633932 0.0130575 0.3182932 0.0065550 0.1666667 0.0371110 0.1003548 0.0005471 995 small_study_five_sites ATE_feasible post-strat 0.0156817 0.0058686 0.1857540 0.0064505 0.1419940 0.010929 0.9385700 0.0076310 0.1160789 0.0058907 0.1857092 0.0065791 0.1689084 0.0021306 0.0212766 0.0124541 0.1003972 0.0003763 993 small_study_five_sites study_site_ATE post-strat 0.0151117 0.0058942 0.1864589 0.0065325 0.1419940 0.010929 0.9395770 0.0076094 0.1160789 0.0058907 0.1857092 0.0065791 0.1689084 0.0021306 0.0212766 0.0124541 0.1009671 0.0001825 993 19.3.2 Bayesian estimation can improve estimates of effects for sampled sites you can improve site-level effect estimates by analyzing with simple Bayesian model because of its shrinkage property, even when the Bayesian model is wrong about distribution of effects in population this is the point from the blog post; I will modify the above design so it can also make this point, switching between the normal distribution and uniform distribution for the fx distribution stan_model &lt;- &quot; data { int&lt;lower=0&gt; J; // number of sites real y[J]; // estimated effects real&lt;lower=0&gt; sigma[J]; // s.e. of effect estimates } parameters { real mu; real&lt;lower=0&gt; tau; real eta[J]; } transformed parameters { real theta[J]; real tau_sq = tau^2; for (j in 1:J) theta[j] = mu + tau * eta[j]; } model { target += normal_lpdf(eta | 0, 1); target += normal_lpdf(y | theta, sigma); } &quot; stan_re_estimator &lt;- function(data) { site_estimates_df &lt;- data %&gt;% group_by(site) %&gt;% do(tidy(lm_robust(Y ~ Z, data = .))) %&gt;% filter(term == &quot;Z&quot;) %&gt;% ungroup J &lt;- nrow(site_estimates_df) df &lt;- list(J = J, y = site_estimates_df$estimate, sigma = site_estimates_df$std.error) fit &lt;- stan(model_code = stan_model, data = site_estimates_df) fit_sm &lt;- summary(fit)$summary data.frame(estimate = fit_sm[,1][c(&quot;mu&quot;, &quot;tau&quot;, &quot;theta[1]&quot;, &quot;prob_pos&quot;)]) } bayes_estimator &lt;- declare_estimator(handler = stan_re_estimator) 19.3.3 when things break down: confounded sampling none of these designs work when you’re trying to make predictions for sites that are systematically different, i.e. are not in the same population as the sampling frame the design was set up to include several sites where researchers could not feasibly set up experiments. in the original design, effects do not depend on whether sites are feasible for the experiment. when effects do vary, there are systematic differences for those target sites. those differences might come from three sources: mean effect size differs in places that are sampled vs not sampled; individual-level het fx sizes that systematically differ in places that are sampled to study vs others; covariate profiles that do not exist in sites outside the sampling frame. I introduce effects in the first way and show there is substantial bias. small_study_five_sites_feasible_effects &lt;- multi_site_designer(n_study_sites = 5, n_subjects_per_site = 500, feasible_effect = -0.25) simulations_feasible_effects &lt;- simulate_design(small_study_five_sites_feasible_effects, sims = sims) diagnosis_feasible_effects &lt;- diagnose_design(simulations_feasible_effects %&gt;% filter(!is.na(estimate) &amp; !is.na(std.error) &amp; !is.na(statistic) &amp; !is.na(p.value) &amp; !is.na(conf.low) &amp; !is.na(conf.high)), bootstrap_sims = b_sims) kable(get_diagnosands(diagnosis_feasible_effects)) design_label estimand_label estimator_label bias se(bias) rmse se(rmse) power se(power) coverage se(coverage) mean_estimate se(mean_estimate) sd_estimate se(sd_estimate) mean_se se(mean_se) type_s_rate se(type_s_rate) mean_estimand se(mean_estimand) n_sims small_study_five_sites_feasible_effects ATE_feasible post-strat -0.0203957 0.0053320 0.1680045 0.0051913 0.0804829 0.008917 0.9557344 0.0064813 0.0803788 0.0053339 0.1667939 0.0052236 0.163637 0.00196 0.075 0.0315973 0.1007745 0.0003584 994 small_study_five_sites_feasible_effects study_site_ATE post-strat 0.2299906 0.0053397 0.2842747 0.0048958 0.0804829 0.008917 0.6549296 0.0155612 0.0803788 0.0053339 0.1667939 0.0052236 0.163637 0.00196 0.925 0.0315973 -0.1496118 0.0002008 994 Other points I decided to abandon to keep this simple: - tradeoff: context-specific interventions and comparability of intervention effects - tradeoff: comparability and fidelity to context in outcome measurement "],
["part-iii-exercises.html", "Chapter 20 Part III Exercises", " Chapter 20 Part III Exercises Measuring sensitive traits with direct questions may be biased due to sensitivity bias: subjects may perceive (rightly or wrongly) that someone such as the enumerator, their neighbors, or the authorities will impose costs on them if they provide a dispreferred answer. In sensitive settings, direct questions provide a comparatively precise answer, but they are biased. One alternative to direct questions is the list experiment (see section XXX). Under their assumptions, list experiments are unbiased, but they can be quite imprecise. Thus, the choice between list experiments and direct questions includes a bias-variance tradeoff. Following the formulas given in _____, the variance of the direct question estimator is the following, where \\(\\pi^*\\) is the true prevalence rate and \\(\\delta\\) is sensitivity bias. \\[ \\frac{1}{N - 1} \\bigg\\{ \\pi^* (1 - \\pi^*) + \\delta (1 - \\delta) - 2(\\delta - \\pi^*\\delta) \\bigg\\} \\] The variance of the list experiment is given by this expression, where \\(\\mathbb{V}(Y_i(0)\\) is the variance of the control item count and \\(\\mathrm{cov}(Y_i(0), D_i^*)\\) is the covariance of the control item count with the densitive trait. \\[ \\frac{1}{N-1} \\bigg\\{ \\pi^*(1-\\pi^*) + 4 \\mathbb{V}(Y_i(0)) + 4 \\mathrm{cov}(Y_i(0), D_i^*) \\bigg\\} \\] Our goal is to compare the direct question and list experiment designs with respect to the RMSE diagnosand. Recall that RMSE equals the square root of variance plus bias squared: \\(RMSE = \\sqrt{Variance + Bias^2}\\). Assume the following design parameters: \\(\\delta = 0.10\\), \\(\\pi^* = 0.50\\), \\(\\mathbb{V}(Y_i(0) = 0.075\\), \\(\\mathrm{cov}(Y_i(0), D_i^*) = 0.025\\). What is the RMSE of the direct question when \\(N\\) = 100? What is the RMSE of the list experiment when \\(N\\) = 100? Make a figure with \\(N\\) on the horizontal axis and RMSE on the vertical axis. Plot the RMSE for both designs over a range of sample sizes from 100 to 2000. Hint: you’ll need to write a function for each design that takes \\(N\\) as an input and returns RMSE. You can get started by filling out this starter function: direct_rmse &lt;- function(N){ # stuff goes here} How large does the sample size need to be before the list experiment is preferred to the direct question on RMSE grounds? Comment on how your answer to (d) would change if \\(\\delta\\) were equal to 0.2? What are the implications for the choice between list experiments and direct questions? "],
["research-design-lifecycle.html", "Chapter 21 Research Design Lifecycle", " Chapter 21 Research Design Lifecycle Empirical results are what we think we now know as a result of conducting a study; research design is why we think we know it. The set of reasons why a study means what we think it means – the research design itself – is of central importance throughout the entirety of the research design lifecycle, beginning with the research idea in its most embryonic form as a flash of inspiration, through the many phases of implementation, to the actual writing and publishing of the piece, and beyond, to the integration of the acquired knowledge into our collective scientific understanding of the world. At each stage of this process, your research design – your specification of \\(M\\), \\(I\\), \\(D\\), and \\(A\\) – shapes your own choices as well as how others will learn from your work. This part of the book works through each of the many discrete stages of the lifecycle. While it is necessarily presented in a linear fashion, the stages are all deeply entwined by their common connection to MIDA. For example, we show how many disputes among scholars about the proper interpretation come down to differing understandings of some part of M, I, D, or A – if your preanalysis plan is sufficiently precise about your beliefs about these features of your design, then the disputes can be specified more precisely, the better to resolve them. Not every research project will explicitly feature all of these stages. For example, prosepctive research designs like experiments and surveys often included pilot studies to learn about important unknown features of \\(M\\) before implementing the full studies. Retrospective studies, like textual analyses of speeches delivered to Parliament, might not. "],
["planning.html", "Chapter 22 Planning", " Chapter 22 Planning provide your MIDA Pre-analysis plans (PAPs) are used by researchers to register a set of analyses before they learn about outcomes. PAPs clarify which choices were made before observing outcomes and which were made afterwards. Ironically, the set of decisions that should be specified in a PAP is itself remarkably unclear. PAP templates and checklists are proliferating, and the number of items they suggest range from nine to sixty. PAPs themselves are becoming longer and more detailed, with some in the American Economic Association (AEA) and Evidence in Governance and Politics (EGAP) study registries reaching hundreds of pages, as researchers seek to be ever more comprehensive. For all that, it is hard to assess whether these detailed plans actually contain key analytically-relevant details. Lot of debate about the value of PAPs (cite cite cite). Summary. Our take: these are helpful tools for researchers to plan research better; and they have a single interpretation that is useful, clarifying what the researcher was thinking at each stage. Their information signal is a function of the distance from the start of post-intervention outcome data collection. But they are useful at any stage to clarify what decisions were made when. MIDA provides a roadmap: a PAP centered around MIDA will describe the researcher’s beliefs about the world, research questions, interventions and measurement, and analysis plans informed by these other elements. The PAP can include a research design diagnosis to communicate the properties of the design as understood before seeing the results of the study, highlighting the diagnosands the researcher believed were important. – involving partners – exante vs expost power calculations – frontloading research design decisions (get more specific, identify problems) – what happens when things go wrong (not just ideal plan, but plan for how you deal with problems; SOPs) – attrition, noncompliance, study failure, missing covariate data, etc. – the properties of your design depend not only on the ideal plan but the procedure – registered reports: part of your answer strategy is how you present and writup results. registered reports are a version of preanalysis plans that include the full results-free writeup. virtue is you prespecify the whole answer strategy! you can do if-then guides to writing up. – shouldn’t just be about analysis. prespecify what you will do in D during implementation if things go wrong, this informs what we can learn. if you drop some units before random assignment (or after), will you rerandomize, add units, change sampling, etc.? this informs what we learn about the design (often expanding what we can learn by saying - if we try to treat this type of unit we won’t be able to) Benefits – Declaration can reduce ambiguities in the design. For example, a PAP that says the researchers will “use OLS with clustered standard errors” leaves out which covariates will be included and how they will be coded, whether any interactions will be estimated, and which of several common formulae will be used to calculate standard errors. By contrast, an answer strategy declared in code makes each of these choices explicit. – Design declaration can minimize the risk of fishing critiques, since the number of small decisions about modeling and constructing tables and figures that take place after data collection is substantially reduced given the ability to simulate data beforehand. – Design declaration may minimize the need for deviations. Simulating the design may reveal aspects of the design researchers were unaware of — variables they did not (yet) plan to collect, assumptions they did not know they needed to make, or planned design strategies were infeasible. The improvements come before implementation not afterwards. – Fairer replication, fairer reanalysis. Replicators lose a degree of freedom in determining what an author might have intended by a given analysis. Moreover, ex ante design diagnosis communicates the assumptions under which they thought the design was a good one before they ran the study. This makes it clearer whether a re-analysis involves an improvement in estimation strategies given a researcher’s assumptions or instead requires deviations from a researcher’s assumptions in order to to be justified. – Easier design comparison. If a design is declared at the preanalysis plan phase, then it enables direct comparison with the design as implemented in the final write-up. Side-by-side comparison of the code neatly clarifies which design choices were made ex-ante and ex-post. Side-by-side comparison of the performance of a planned and implemented designs clarify under what conditions deviations from plans are defensible improvements. The design declaration itself is a compact description of the study you want to conduct The diagnosis (and redesign process) shows why you picked the design you did. the Mock analysis section should run the actual tables and figures you plan to create using a draw of the design. ethics (summarize and cite Jay’s ideas here: http://www.jasonlyall.com/wp-content/uploads/2020/08/PreregisterYourEthics.pdf) ethical outcomes are potential outcomes, so we need to think about them ex ante not just on the basis of revealed outcomes SOPs 22.0.1 countering objections: Time-consuming. Yes but in our experience we only pay this cost for failed studies. Fur successful studies, nearly all work that goes in to the pap pays off in terms of higher qualtity design, literal words we already wrote, and written-in-advance analysis code. Won’t stop determined cheaters. Yes – remember that the goal is not to prevent fraud, it’s to help researchers improve their designs. Science depends on trust. Replication is better (Coffman and Niederle (2015)). These are complements, not substitutes. I can’t preregister what I will do because I don’t know what I will find. That’s fine too, just write down how you will go about “finding” things so we (and YOU) can understand your own process. 22.0.2 Other thoughts when is the right moment to write a pap? 22.0.3 citations on paps Casey, Glennerster, and Miguel (2012): early entry Olken (2015): halfway skeptical Green and Lin (2016): Standard operating procedures Christensen and Miguel (2018): review Coffman and Niederle (2015): a skeptical take (prefer replications). Humphreys, Sierra, and Windt (2013): nonbinding Miguel et al. (2014): distinguishes between disclosure and PAP Ofosu and Posner (2020): apparently paps hinder publication? References "],
["ethical-review.html", "Chapter 23 Ethical Review", " Chapter 23 Ethical Review As members of society, we are all enjoined to act ethically in all areas of our lives, and the specific domain of conducting research is no different in this respect. Laws specifically govern many areas of research activity – we don’t have much to say about these legal questions except that researchers should obey the local laws. Similarly, we don’t have much to say about the governing role of institutional review boards (or IRBs). IRBs apply rules to research activity; the application of these rules is quite varied from institution to institution, and this variation can appear quite arbitrary to the individual researcher seeking IRB approval. Obtaining IRB approval is an absolute necessity for most empirical projects (even when the approval is nothing more than an affirmation that the research project is exempt from IRB oversight!). That said, the approbation of the IRB has very little to do with the ethical status of a research projects. Many unethical studies could pass IRB review and many ethical studies can run into frustrating IRB headaches. For a fascinating history of how we came to have the modern IRB, we recommend “Ethical Imperialism,” which describes how procedures that were originally designed for medical trials were applied, mostly without modification, to social scientific research studies. Instead, we conceive of the effect of research activities on the well-being of research staff, participants, and perhaps even future consumers of the research product as additional Inquiries that researchers have an ethical obligation to learn about. We discussed in the data strategy section (section ZZZ) how different data strategies may be more or less ethically encumbered. A common concern is that measurement imposes an ethical cost on subjects by wasting their time. Subjects’ time is a valuable resource – they often donate willingly to the scientific enterprise by taking a survey or similar. Sometimes their generosity is repaid with financial compensation for their time. Sometimes subjects are unknowing participants in a research study because obtaining informed consent would so distort their behavior as to hinder the ability of the researchers to study it. We discussed in the diagnosis section how expected ethical downsides can be formalized as diagnosands so that researchers can choose the best design among the “ethical” studies; or if designs can’t be discretized into ethical and non ethical, researchers can explicitly consider how much cost they are willing to impose on subjects to learn about the world. Every researcher should consider the ethical implications of the data and answer strategies for research staff and participants. Some research projects should not be conducted – the vast majority of those projects are typically weeded out well before the ethical review stage. It’s important to communicate your design choices so that they can specifically evaluate your ethical judgements. It’s difficult for others to understand (a) your design and (b) what the ethical tradeoffs truly entail. Declared designs help immensely because they focus attention on what part, exactly, people are responding to. Is it the measurement they object to? Is it the random assignment? papers to cite here: the A/B illusion (Michelle Mayer); Cite APSA guidelines; preregistering ethics guidelines (Lyall paper); report on ethics in paper (Lauren Young’s paper); Mac’s paper; Belmont report Further readings. – history: belmont report. tuskegee experiment. stanford prison experiment. – laws and IRBs: revised common rule, paper on IRBs – guidelines from econ, poli sci, soc, psych. – ethics of experiments: macartan’s paper, dawn teele’s chapter; A/B illusion; – ethics of outcome data collection: https://link.springer.com/epdf/10.1007/s11133-020-09458-9?sharing_token=ZXNJXewvrZN0ouaL02wFDPe4RwlQNchNByi7wbcMAY4-EQXsmvNo8IbQcOueStzgc77hZU2onyyln_mVDwcUpdOR-pwRaPn3QvTGa3Im4FtNdYCpNVCfjs7pObv1rfyp-y7KtLBchLUMAHPMkKVeCSMwqZH6G5ldDK9hxy4NRoQ%3D – new ideas: jay lyall paper; lauren young paper "],
["partners.html", "Chapter 24 Partners", " Chapter 24 Partners Partnering with organizations in research — cooperating to intervene in the world or to measure outcomes — is increasingly common in the social sciences. Organizations can learn about how to achieve their (private) goals better by partnering with social scientists to study current programs and policies. In some cases these private goals overlap with public goods (e.g., a government learning how to expand access to healthcare), in others they are largely private (e.g., Uber learning about how to improve its revenues). Social scientists’ have their own goals: to produce knowledge and publish it. Understanding the private goals of the partner and of the researcher is essential to selecting a research design amenable to both parties. MIDA can help in doing this, by formalizing tradeoffs between the two sets of goals. One common divergence between partner and researcher goals is that partner organizations often do want to learn, but their primary mission is focused on doing. Learning how to do their work better is a goal, but it may be in the short term secondary to trying to doing the work. Donors and other constituents expect to see outputs rather than outcomes, often understandably because it is simpler to monitor the outputs of an organization (trainings held, doors knocked on) than outcomes (quality services provided, votes changed). Because of this at least short-term divergence in goals, a central tradeoff in partnering with organizations is a learning-doing tradeoff. Both researchers and organizations wish to “do” (produce outputs) and “learn” (learn how to better translate outputs into outcomes), but because of the difference in goals the weight they place on these two activities may differ. This tradeoff is commonly referred to as the exploration-exploitation tradeoff (cite), and motivates a large literature on adaptive trials that aim to directly optimize doing but in the most effective way. Research design diagnosis can help navigate the learning-doing tradeoff. One instance of the tradeoff is that the proportion of units who receive a treatment (e.g., a medicine) represents the rate of “doing” and also affects the amount of learning in that in the extreme if all units are treated there can (typically) be no learning about the effect of the treatment. The tradeoff here is represented in a graph of the RMSE vs. the proportion treated. In the absence of partners, researchers might simply ignore the proportion treated axis and select the proportion with the lowest RMSE. With a partner organization, the researcher might use this graph in conversation with the partner to jointly select the design that has the lowest RMSE that has a sufficiently high proportion treated to meet the partner’s needs to satisfy its stakeholders. Choosing the proportion treated is one example of integrating partner constraints into research designs to generate feasible designs. A second common problem is that there are a set of units that must be treated, for ethical or political reasons (e.g., the home district of a government partner must receive the treatment), or that must not be treated. If these constraints are discovered after treatment assignment, they lead to noncompliance, which may substantially complicate analysis of the experiment and even prevent providing an answer to the original inquiry. Considerable thought has been given to avoiding this type of noncompliance. Gerber and Green recommend, before randomizing treatment, exploring possible treatment assignments with the partner organization and using this exercise to elicit the set of units that must or cannot be treated. King et al. (cite) describe a ``politically-robust’’ design, which uses pair-matched block randomization and when any unit is dropped due to political constraints the pair is dropped from the study14 Design diagnosis can help in these circumstances by providing a mechanism to specify possible patterns of noncompliance and diagnosing alternative designs to mitigate the negative consequences. In addition, they can be used to communicate with partners about the consequences of noncompliance for the research to help make better decisions together about how to avoid noncompliance once treatment is randomized and the research is in progress. 24.0.1 Scattered thoughts Partnerships and ethics should be in here Research for Impact (Levine (2020)) When to walk away Advice: there’s usually a boss that gives you the green light, then staff people who actually help you. Our advice is to ensure that at least some meetings are with both the boss and the staff people so that the staff people know that the boss cares about the project and are therefore motivated to help you. A crucial person at the partner organization is the person who knows where the data are and how you can access them. Many people at partner organizations do not have this power, but you must be frequent touch. In some cases, some member of the research staff can actually embed wiht the partner organization to ensure all the good things. Involve partners in PAP writing. when should partners be coauthors on the paper? References "],
["funding.html", "Chapter 25 Funding", " Chapter 25 Funding In Section XX we discuss the diagnosands related to cost. In any design, there will be a tradeoffs between diagnosands about the value of the research (in terms of RMSE or average movemenent from priors to posteriors for example) as well as costs. Cost are potential outcomes, so may vary across possible implementations of a study (e.g., when the number of treated units vary and treatment is costly), and thus diagnosing the average or maximum cost may be as important as understanding the benefits. To relax the budget constraint, researchers apply for funding. Funding applicants wish to obtain as large a grant as possible for their design, but have difficulty credibly communicating the quality of their design given the subjectivity of the exercise. Funders, on the flip side, wish to get the most value for money in the set of proposals they decide to fund, and have difficulty assessing the quality of proposed research. MIDA and design declaration provide a tool for speaking in a common language that can be more easily verified by both sides about what design is being proposed and what its value is to knowledge under a set of assumptions that can be interrogated by funders. Funding applications often aim to communicate what research design is being proposed; why learning answers from the design would be useful, important, or interesting to scholars, the public, policymakers, or another audience; how the research design provides credible answers to the question; that the researcher is capable of executing the design; and that there is value-for-money in the design and the answers it provides. A new section of funding applications that would aid in communicating about each of these questions is declaring the MIDA of the design and presenting a diagnosis of the design. In addition to common diagnosands such as bias and efficiency, two special diagnosands may be valuable: cost and, related, value-for-money. Cost can be included for each design variant as a function of design features such as sample size, the number of treated units, and the duration of survey interviews. The cost may vary by these parameters and may vary across possible designs when, for example, the number of treated units is a random number. Simulating the design across possible realizations of the design, thus, provides a distribution of costs as a function of choices the researcher makes. Value for money is a diagnosand that is a function of cost and also the amount that is learned from the design. RMSE might be one value criterion, another would be the average difference between priors and posteriors under a Bayesian answer strategy (a direct measure of learning). In some cases, funders request applicants to provide multiple options and multiple price points or to make clear how a design could be altered such that it could be funded at a lower (or higher) level. Redesigning a design with differing sample sizes would communicate how the researcher conceptualizes these options, but also provide the funder with an understanding of tradeoffs between RMSE and cost in these design variants. Applicants could use redesign to justify the high cost of their request and to ask for additional funding. Ex ante power analyses, required by an increasing number of funders, illustrate the crux of the misaligned incentives between applicants and funders. A power analysis can demonstrate almost any design is “sufficiently powered” by changing expected effect sizes and noise. – funders often request power analysis, but these are typically described in words and thus the assumptions behind them cannot be interrogated fully. (a) not in code, so not specific; (b) user power calculators that are wrong (cite paper); (c) do not provide the details funders need to verify whether they agree with the assumptions. – for funders, providing MIDA declared in code allows them to change the parameters of the design and test how the properties of the design change with beliefs about the world in M or data strategy parameters in D such as sample size, rather than having to rely on claims by applicants – often funders require regular reporting on changes in plan – MIDA and design declaration provides a way to communicate (a) what those changes are and (b) how they change the values of diagnosands. – value for money as a diagnosand (cost of each design as a function of design parameters) – allows funders to compare on a common scale (the same set of diagnosands) funding proposals – often trying to evaluate “quality” but hard to do that with narrative proposals 25.0.1 References no idea, but arnold has a document on how to prepare proposals for them. "],
["piloting.html", "Chapter 26 Piloting", " Chapter 26 Piloting – can’t learn causal effect – what can you learn? about Y0, about M, about se – bring in blog post The designs and results of past studies are important guides for selecting M, I, D, and A. Our understanding of the nodes and edges in the causal graph of M, expected effect sizes, the distribution of outcomes, feasible randomization schemes, and many other features are directly selected from past research or chosen based on a literature review of the distribution over past studies. However, researchers face a problem in being guided by past research: the research context and our inquiries often differ in at least subtle ways from any past study. Even when we are replicating a past study, we are collecting data in a different time period and if effects vary over time then aspects of M may differ from the original study. To deal with this, we often run pilot studies. These take many forms: focus groups to learn about features of M or to learn how to ask survey questions; small-scale tests of measurement tools to verify our data collection technology works; up to mini studies with the planned design but on a smaller scale. Pilot studies are constrained by our time and by money. If we were not constrained, we would run the full study and learn what is wrong with our design and then run a corrected design for the main study. Since we cannot due to our constraints, we run either smaller mini studies or test out only a subset of the elements of our planned design. This places us in a bind: we are running a design smaller or less complete than the study we imagine conducting, and so the properties of the pilot design will not measure up. MIDA provides a framework for thinking about two aspects of piloting: what can be learned from a pilot? And what are the properties of the joint research design of pilot plus main study? Diagnosing the pilot study on its own provides stark insights, which amount to: we cannot provide answers to the inquiry in the main study, and should not try to do so. There are also aspects of the logistics of research that within time and financial constraints we simply cannot learn until we run the main study. Science is imperfect, and also iterative, but these mistakes or suboptimal design choices also often lead to discoveries. – how does it help to diagnose the design together? the properties of the main study change when we do a pilot. This is because if we run the pilot study, we are doing so to make decisions about how to run the main study, and so our design of the main study and thus its results may depend on the results (and design) of the pilot study. In this section, we illustrate several general principles that flow from diagnosing pilot studies. Purposes of pilot studies: Existence proofs: – is there variation in Y – is there variation in X – what are nodes in M – what are feasible D’s, what are feasible treatments / can you implement the treatment (existence proof) Harder questions requiring bigger sample sizes: – what is the distribution of X (helps select stratification proportions etc.) – what is the standard deviation of Y0 26.0.0.1 Assessing a pilot design declare pilot itself and diagnose just as if it were the main study if you can’t learn the answer, don’t make any decisions based on it 26.0.0.2 Assessing a sequenced design if you are making decisions about MIDA for main study based on pilot, diagnose the procedure of two studies, think about POs of pilot 26.0.0.3 Pilots and baselines Designs can be reassessed after baselines and before treatment assignment – so some of the questions you might do a pilot for can just be answered in a baseline 26.0.0.4 BLOG material Data collection is expensive, and we often only get one bite at the apple. In response, we often conduct an inexpensive (and small) pilot test to help better design the study. Pilot studies have many virtues, including practicing the logistics of data collection and improving measurement tools. But using pilots to get noisy estimates in order to determine sample sizes for scale up comes with risks. Pilot studies are often used to get a guess of the average effect size, which is then plugged into power calculators when designing the full study. The procedure is: Conduct a small pilot study (say, N = 50) Obtain an estimate of the effect size (this is noisy, but better than nothing!) Conduct a power analysis for a larger study (say, N = 500) on the basis of the estimated effect size in the pilot We show in this post that this procedure turns out to be dangerous: at common true effect sizes found in the social sciences, you are at risk of selecting an underpowered design based on the noisy effect estimate in your pilot study. A different procedure has better properties: Conduct a small pilot study (say, N = 50) Obtain an estimate of the standard deviation of the outcome variable (again, this is a noisy estimate but better than nothing!) Estimate the minimum detectable effect (MDE) for a larger study (say, N = 500), using the estimated standard deviation We show what happens in each procedure, using DeclareDesign. In each case, we’ll think about a decision the researcher wants to make based on the pilot: should I move forward with my planned study, or should I go back to the drawing board? We’ll rely on power to make that decision in the first procedure and the MDE in the second procedure. [omitting code] For each true effect size, the simulations will give us a distribution of estimated effects that a researcher might use as a basis for power analysis. For example, for a true effect size of 0 the researcher might still estimate an effect of 0.10, and so conduct their power analysis assuming that the true effect is 0.10. For each true effect, we can thus construct a distribution of power estimates a researcher might obtain from estimated effects. Since we know the true power for the true underlying effect, we can compare the distribution of post-hoc power estimates to the true power one would estimate if one knew the true effect size. What did we find? In the plot, we show our guesses for the power of the main study based on our pilot effect size estimates. At high true effect sizes (top row), we do pretty well. Most of our guesses are above 80% power, leading us to the correct decision that the study is powered. Indeed we often underestimate our power in these cases meaning that we run larger studies than we need to. However, at low true effect sizes (bottom row) we show we are equally likely to find that the design is in fact powered as underpowered. We are equally likely to guess the power of the design is 90% as 10%. There is a good chance that we will falsely infer that our design is well powered just because we happened to get a high estimate from a noisy pilot. 26.0.1 How about estimating the standard deviation of the outcome? Now, let’s look at the second approach. Here, instead of using our pilot study to estimate the effect size for a power calculation, we estimate the standard deviation of the outcome and use this to calculate the main study’s minimum detectable effect. The decision we want to make is: is this MDE small enough to be able to rule out substantively important effects? We calculate the minimum detectable effect size using the approximation from (Gelman and Hill 2006, pg. 441), 2.8 times the estimated standard error. We estimate the standard error using Equation 3.6 from (???). In summary, pilot studies can be valuable in planning research for many reasons, but power calculations based on noisy effect size estimates can be misleading. A better approach is to use the pilot to learn about the distribution of outcome variables. The variability of the outcome variable can then be plugged into MDE formulas or even power calculations with, say, the smallest effect size of political, economic, or social importance. In the same spirit, pilot studies could also be used to learn the strength of the correlation between pre-treatment covariates and the outcome variable. With this knowledge in hand, researchers can develop their expectations about how much precision there is to be gained from covariate control or blocking. References "],
["implementation.html", "Chapter 27 Implementation", " Chapter 27 Implementation outline: – use your MIDA to figure out how to implement – redesign as you go when you haven’t gotten specific enough – redesign when new features of M come up – redesign when you can’t do the D you planned (and make sure A fits it) – consider the whole design – ex ante declared, then changed – and what could have happened, which may be known to you ex ante (there could or could not be attrition) or unknown (you learn during implementation that there are some kinds of units that won’t comply, so you need to think not just about which ones did or did not comply but which ones could have). the whole process is a function of your interventions in the world (treatment or measurement), so write down the whole process and potential outcomes to understand what you can and cannot learn. – use your MIDA to help you make logistical choices, help it prevent you from making bad decisions and use as tool to communicate with partners and implementers about why you do or do not wnat to make different changes (or BETTER evaluate tradeoffs in those decisions) idea bin: – MIDA is a roadmap for how to implement the study – when there is a part not specified, redesign to specify and diagnose again – lots of choices you make after you start, because it was not clear what decisions would have to be made ex ante: how to make these choices? (redesign and diagnose!). – often randomization procedure will have to take into account specifities of the number of units (odd numbers), blocks with differing numbers of units, cluster of varying sizes, etc. that require revising D – unexpected constraints come in that affect D, and may require changes to D but also A. redesign. – how to make decisions about unexpected changes when things going wrong? (even if you don’t have a PAP). often want to create multiple variants of the design incorporating what went wrong. common example: unexpected noncompliance or unexpectedly high rates of attrition. may want to change analysis plan, and register it, so can use redesign to develop that new plan. but also may discover diagnosands are not good enough, so may want to change data strategy midstream to mitigate these problems. – what to do when you run out of money or feasibility of sample size or other aspects of D becomes clear. redesign mountain subject to new cost constraints. – make go-no go decisions about whether to continue – how to know if your inquiry is no longer answerable – updating your PAP – assessing what you can learn based on the implementation challenges: these are potential outcomes, i.e. could be affected by treatment, so are often informative about what we can learn about the original research question – often need to convince partners to not change plans – MIDA can be a tool for helping assess tradeoffs in learning and doing – Projects that succeed have direct researcher invovlement. Outsourcing too much of the design can lead to big troubles. 27.0.1 Citations Failure (Karlan and Appel (2018)) References "],
["reconciliation.html", "Chapter 28 Reconciliation", " Chapter 28 Reconciliation Inevitably, the research design as implemented will differ in some way from the research design as planned. When they diverge, misleading readers to believe that the design as implemented was the design as planned all along is an error. We should be transparent about how the design changed over the course of the research process – first of all, it’s the honest thing to do, and second, those changes condition how we interpret the research. Suppose the original design described a three-arm trial: one control and two treatments, but the design as implemented drops all subjects assigned to the second treatment. Sometimes this is an entirely appropriate and reasonable design modification: perhaps it turns out that due to an implementation failure, the second treatment was simply not delivered. Other times, these modification is less benign – perhaps the estimate of the effect of the second treatment does not achieve statistical significance, so the author simply omits it from the analysis. For this reason, we urge researchers to explicitly reconcile the design as planned with the design as implemented. Having a publicly-posted preanalysis plan can make the reconciliation process especially credible – we know for sure what the planned design was because the preanalysis plan describes it (in greater or lesser detail) pre-implementation. However, a preanalysis plan is not a prerequisite for engaging in reconciliation. The scientific enterprise is built in large measure on trust – we are ready to believe researchers who say, here is the design I though I would implement but due to unanticipated developements, here is the design I ended up implementing. In some cases, reconciliation will lead to additional learning beyond what can be inferred from the final design itself. When some units could refused to be included in the study sample or some units refused measurement, we learn that important features about those units. Understanding sample exclusions, noncompliance, and attrition not only may inform future research design planning choices, but contribute substantively to our understanding of the social setting. A policy implemented in the same way the study would likely also not be able to work in the units who refused to participate, and future research could examine why or how to convince them of the policy’s benefits. What belongs in a reconciliation? At a minimum, we need a full description of the planned design, a full description of the implemented design, and a list of the differences. This can be made explicit through the declaration of both design in computer code, then “diffing” the two design objects. In declare design: design1 &lt;- declare_population(N = 100, u = rnorm(N)) + declare_potential_outcomes(Y ~ Z + u) + declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + declare_sampling(n = 75) + declare_assignment(m = 50) + declare_reveal(Y, Z) + declare_estimator(Y ~ Z, estimand = &quot;ATE&quot;) design2 &lt;- declare_population(N = 200, u = rnorm(N)) + declare_potential_outcomes(Y ~ 0.5*Z + u) + declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + declare_sampling(n = 100) + declare_assignment(m = 25) + declare_reveal(Y, Z) + declare_estimator(Y ~ Z, model = lm_robust, estimand = &quot;ATE&quot;) compare_designs(design1, design2) compare_design_code(design1, design2) compare_design_summaries(design1, design2) compare_design_data(design1, design2) compare_design_estimates(design1, design2) compare_design_estimands(design1, design2) 28.0.1 Scattered thoughts Ofosu and Posner (2019) References "],
["writing.html", "Chapter 29 Writing", " Chapter 29 Writing 29.0.1 Writing a paper basic outline of an empirical paper is: intro, theory, design, results, discussion. Introduction: Mini MIDA Theory: M, I, guesses about \\(a^M\\) Design: D, A; diagnosis under M and I. Results: \\(a^D\\) Discussion: what we learn about \\(a^M\\) and \\(M\\) from \\(a^D\\) Comments: 1. please keep kosher. Theory section material belongs in the theory section, not in the design or results section, etc. in discussion sections, often researchers talk about about future research. That’s because some inquiries are not addressed by the current MIDA, so in light of \\(a^D\\), new MIDAs can be proposed. Theory sections are for a couple of things. first they are about \\(M\\) (what are the relevant features of the model necessary to describe \\(I\\)). Second, they summarise what the previous literature has to say about \\(I\\). In particular what we think \\(a^M\\) might be. often this is just a sign. The literature review section should be aware of the MIDAs that were used to generate past \\(a^D\\)s. If they are poor (e.g., prone to bias), they should get less weight. Also should not selectively review in the sense that contrary views should also included, conditional on good MIDAs. A literature review is a “summary function” in the sense that it is a qualitative meta-analysis. That summary function should be purged of researcher bias (of course hard to know for sure) Little something here on “primacy claims” Elaborations of the basic structure. Sometimes multi-paper. They should at a minimum have the same M. Sometimes a paper is multiple ways of getting at the same I. sometimes it’s different Is. This is where we talk about the psych 3 paper diesgn: study 1, correlation between X and Y. Study 2 is randomize X. Study 3 is pretend to learn mechanisms by which X affects Y. something here about “mixing methods” If they all have different Is, then you can’t make up for the deficiencies of one method with the strengths of the others. If they have the same Is, then perhaps we “triangulate” ? Discussion section stuff. This is where we form “posteriors”. That is, we interpret \\(a^M\\) in light of beliefs about \\(M\\). We also update about \\(M\\). for example, you might learn that an edge you (or someone) thought existed doesn’t exist. Heterogeneity teaches you about “functional form”. You might posit the existence of unmeasured but nevertheless real nodes. This might be the place where you propose new MIDAs that help you to learn about your updated M. "],
["publication.html", "Chapter 30 Publication", " Chapter 30 Publication MIDA is a tool for communicating the quality of a design to reviewers and, later, readers 30.0.1 Review process Comments can come in about all four aspects of the research design. About M: please cite work that is about other edges and nodes in M. About I: the reviewers disagree that your I is “important”; they want you to write about a different I. About D: they think the sample is poor, the randomziation scheme is lame, the measurement strategy is insufficient. About A: they want logit. The goal is to respond to reviewer comments in a way that does not compromise the essential strength of your MIDA. Do not let the review process make your paper worse. You must understand how the reviewer’s comments change MIDA – do they make it stonger? if so, then adopt. Are they irrelevant to MIDA? if so, adopt. Do they actively hurt MIDA? if so, use diagnosis to demonstrate that the proposed changes would harm the research design. Idea: redesign after review, so you have a design that reflects the original design modified by reviewer requests and use diagnosis to demonstrate its (still) high quality to readers who may be skeptical of changes made during the review process that were not originally planned. 30.0.2 Publication bias Here we look at risks from publication bias and illustrate two distinct types of upwards bias that arise from a “significance filter.” A journal for publishing null results might help, but the results in there are also likely to be biased, downwards. Two distinct problems arise if only significant results are published: The results of published studies will be biased towards larger magnitudes. The published studies will be unrepresentative of the distribution of true effects in the relevant population of studies. These two problems are quite distinct. The first problem is more familiar: conditional on any true effect size, larger estimates have an easier time passing the statistical significance filter, so the distribution of published results will be biased upwards because it will be missing all of the smaller estimates. The second problem is more subtle. If different studies seek to measure effects that are of different size, conditioning on statistical significance means that we are more likely to learn from places that have large effects than from places that have small effects. The significance filter means that our answers to any particular question will be biased and it means that the set of questions we see answers to will be biased as well. The Journal of Significant Results is a poor guide to the true distribution of causal effects. What about a Journal of Null Results? Such a journal would condition acceptance on failing to achieve statistical significance. The set of articles published in such a journal would also be biased. Looking first at the Journal of Significant Results, we see the familiar problem: the average estimate is biased away from the true value of the estimand. This problem is greatly helped by increasing the sample size. But we can also see the second problem – the distribution of estimands (the true effects under study) is also biased towards larger effects, this problem is also allayed, though less dramatically, by larger sample sizes. The Journal of Null Results suffers from a parallel problem, only in reverse. Now estimands are smaller than is typical in the population and, on average, estimates are biased down relative to these estimands. Strikingly, the bias in estimand selection is worse at the larger sample size (though downwards bias within the set of published studies is smaller). Now, we agree that proactively publishing null results may help when considering entire research literatures as a whole, and for this reason alone a Journal of Null Results is probably a good thing. But, better would be to not do any conditioning at all. The Journal of Interesting Designs would condition only on the question being interesting and the design being appropriate to answering the question. We see that the distribution of estimates and estimands are both centered on the correct average value. Idea of results-blind review Idea of registered reports "],
["archiving.html", "Chapter 31 Archiving", " Chapter 31 Archiving One of the biggest successes in the push for greater research transparency has been changing norms surrounding the sharing of data and analysis code after studies have been published. It has been become de rigeur at many journals to post these materials at publicly-available respositories like the OSF or dataverse. This development is undoubtedly a good thing. In older manuscripts, sometimes data or analyses are described as being “available upon request” but of course such requests are sometimes ignored. Furthermore, a century from now, study authors will no longer be with us even if they wanted to respond to such requests. Public respositories have a much better chance of preserving study information for the future. What is in the typical replication archive. First, the data \\(d\\) itself. Sometimes this is the raw data, sometimes it is only the “cleaned” data that is actually called by analysis scripts. Where ethicially possible, we think it is probably better to post as much of the raw data as possible alongside a cleaning script that ingests the raw data and outputs the cleaned data. Cleaning scripts might be considered a part of the data strategy \\(D\\) in the sense that they complete the measurement procedures laid out in \\(D\\). They might also be considered part of the answer strategy in the sense that they apply an interpretation to the data provided by the world. We do not take a hard stance on whether data cleaning procedures rightly belong in \\(D\\) or \\(A\\) since the choice likely varies from study to study. Replication archives also include \\(A\\), the set of fucntions applied to \\(d\\) that produce \\(a^D\\). It is vitally important that the actual analysis code is archived because the natural-language descriptions of \\(A\\) that are typically given in papers are imprecise. As a small example, many articles describe their answer strategies as “ordinary least squares” but do not fully describe the set of covariates used or what flavor of standard errors were estimated. The actual analysis code makes \\(A\\) explicit. While typical replication archives include \\(d\\) and \\(A\\), we think that future replication archives should also include a design declaration that fully describes \\(M\\), \\(I\\), \\(D\\), and \\(A\\) – that is, we should archive designs, not just data and analysis code. This should be done in code and words. In addition, a diagnosis should be included, demonstrating the properties as understood by the author and also indicating the diagnosands that the author considered in judging the quality of the design. See also: alex’s paper on active maintenance. Graphic: file structure for a replication archive for the example introduced in planning with MIDA in it. Example is archive at OSF: https://osf.io/4vuqh "],
["reanalysis.html", "Chapter 32 Reanalysis", " Chapter 32 Reanalysis The crucial thing about a reanalysis is that \\(d\\) is fixed. You can bring new \\(M\\)’s \\(I\\)’s and \\(A\\)s to bear, but the revealed data is fixed. how do we update from the reanalysis research design (the original design plus the reanalysis of d)? – the design is the research design from before with two sets of estimates from two different A’s – need an aggregation function (decisionmaking function) that converts the two sets of results into a decision or posterior A is constrained: it must follow from the D of the study, which cannot be changed because the study was run already. what can be learned from reanalysis? (1) confirm there were not errors (consider changing A only) (2) reassess what is known about the same inquiry, using new information about the world (change M, change A to suit new M) (3) learn something new from the data about another node or edge or a different summary about the same ones (change I and possibly A to match it; possibly M if a node was missing; possibly add data) (4) assess “robustness” of findings - point to discussion of this in answer strategy (or move it here) (change A) (5) update M based on new research and assess what d can tell us from this study (change M and possibly I, possibly A to fit changed M and I) how can we assess the properties of a reanalysis? diagnose changed MIDA. important to not condition on d, the design includes the actual D, and we need to consider what results d’ we would get from the reanalysis under different realizations of D. there are now two A’s, so need to specify a decision function about how to integrate the two findings. this could be throw away the old a, or combine them in some way. if it’s a “robustness” to alternative A, then you may want to combine not throw out for example. it’s crucial to specify how you do that, that’s part of the answer strategy. 32.0.1 Grab bag Clemens (2017) on taxonomy of these kinds of efforts if you’re going to use d to learn about a different M for a different I, you need to understand their D References "],
["replication.html", "Chapter 33 Replication", " Chapter 33 Replication After your study is completed, it may one day be replicated. Replication differs from reanalysis in that a replication study involves the specification of a new MIDA and collection of new data. As discussed in the previous, a reanalysis may re-specify parts of the research design, but always re-uses the original data \\(d\\) in some way. So-called “exact” replications hold key features of I, D, and A fixed, but draw a new dataset \\(d_{new}\\) from \\(D()\\) and apply the same \\(A\\) to the new \\(d\\) in order to produce a fresh answer \\(a_{new}^D\\). Replications are said to “succeed” when \\(a_{old}^D\\) and \\(a_{new}^D\\) are similar and to “fail” when they are not. Dichotomizing replication attempts into successes and failures is usually not that helpful, and it would be better to simply characterize how similar \\(a_{old}^D\\) and \\(a_{new}^D\\) are. Of course, exact replication is impossible: at least some elements of M have changed between the first study and the replication. Specifying how they might have changed, e.g., how outcomes vary with time, will help judge differences observed between \\(a_{old}^D\\) and \\(a_{new}^D\\). Statistical noise will also play a role. Replication studies benefit enormously from the knowledge gains produced by the original studies. For example, we learn a large amount about \\(M\\) and the likely value of \\(a^M\\) from the original study. The \\(M\\) of the replication study can and should incorportate this new information. For example, if we learn from the original study that \\(a^M\\) is positive but it might be small, the replication study could respond by changing \\(D\\) in order to increase the sample size. This is the intuition behind the “small telescopes” paper, which offers a rule of thumb for how much larger replication studies should be than original studies. We think this rule of thumb is useful, though design diagnosis and redesign is an even better way to learn about how to change the design of the replication study in light of the original study. When designing original studies, you should anticipate that someday your work will be replicated. This improves your ex ante incentives. To the extent that you want future replication studies to arrive a similar answers to the original study you produce (i.e., you want their \\(a_{new}^D\\) to match your \\(a_{old}^D\\) as closely as possible), you will want to choose designs that bring \\(a_{old}^D\\) as close to \\(a^M\\) as possible, under the presupposition that faithful replicators will also design their studies in such a way that \\(a_{new}^D\\) will also be close to \\(a^M\\). Replication studies necessarily differ from original studies – it is literally impossible to reproduce the exact conditions of the original study in the same way it’s impossible to step in the same river twice. Another way of putting that same statment is that \\(D_{new}\\) is necessarily different from \\(D_{old}\\). Theory (i.e., beliefs about \\(M\\)) is the tool we use to say that \\(D_{old}\\) is similar enough to \\(D_{new}\\) to consititute a close enough replication study. As a concrete example, many survey experimental replications involve using the exact same experimental stimuli but changing the study sample, e.g., from a nationally representative sample to a convenience sample. So-called “conceptual” replications alter both \\(M\\) and \\(D\\), but keep \\(I\\) and \\(A\\) as similar as possible. That is, a conceptual replication tries to ascertain whether a relationship in one context (\\(I(M_{old})\\)) also holds in a new context (\\(I(M_{new}\\)). The trouble and promise of conceptual replications lies in the success of the designer at holding \\(I\\) constant. Too often, a conceptual replication fails because in changing \\(M\\), too much changes about \\(I\\) such that too much changes about the “concept” under replication. There should be a summary function for how to interpret the difference between \\(a_{old}^D\\) and \\(a_{new}^D\\). This may be take the new one and throw out the old if MIDA was poor in the first. It may be taking the average. It may be a precision-weighted average. Specifying this function ex ante may be useful, to avoid the choice of summary depending on the results of the replication. This summary function will be reflected in A and in the discussion section of the replication paper. "],
["resolving-disputes.html", "Chapter 34 Resolving Disputes", " Chapter 34 Resolving Disputes The problem: Acrimonious debates arise; Hard to interpret contribution of replication and reanalysis; First main task (accumulation of knowledge) damaged. The solution: Some of this comes down to basic disagreements we can’t resolve. But some of it comes down to a lack of principles guiding how design decisions are made and how the results they produce should be interpreted. And a lack of procedures for understanding the consequences of decisions. Principles for making design choices, tailored to the distinct challenges posed to reanalysis and to replication Some changes are justifiable / encouraged, some things are not justifiable / discouraged, conditions for justification are clarified Procedures for putting principles into practice Declaration and diagnosis through DD Current practice for replication is to exactly replicate data strategy and analysis strategy in new or same context. This is not needed! Standard should be: best answer to same inquiry in new or same context! But how can we justify changes to D and A that give a “better” answer to same inquiry? Replacing them with alternative practices justified by design simulation 1. M always changes! (you have more information on tau or sd(tau)) 2. Home ground dominance: Change A or D-and-A if A’ &gt; A under M 3. Robustness to alternative models: Change A or D-and-A if A’ ≥ A under M AND A’ &gt; A under M’ E.g. change from simple to complete RA 4. Model plausibility:If A’ &lt; A under M AND A’ &gt; A under M’, then change to A’ or D-and-A IFF M’ is more plausible than M E.g. switching to balanced design if you believe variances equal across treatment groups 5. Undefined inquiries. Change I to I’ if I is undefined under M If I is defined under M: You can’t change to I’, You can’t change D to D’ if that means I unidentifiable. Disorganized thoughts: - Changes to D include both interventions (sampling and randomization), as well as the inclusion of different / new datasets on the same model (this is common in econ reanalyses at state-level). The collection of “different” data through a change in question wording also fits into this. Need to think about a good typology of data strategies. - There’s often a broader research question that’s being answered, and when I changes sometimes both are answering the same broader question. But focuses debate on whether that claim is true that I and I’ answer the same broader I. - In replication can you use data from study 1 to assess the plausibility of M? - When does changing outcomes change the inquiry? Example: you used z-scores in your original analysis in order to measure an effect on five different measures of some latent construct. I show that taking a simple average has better properties (e.g., statistical power), and use this instead of z-score. Have I changed estimand? If so, are there any instances of “recoding” or even “rewording” of outcome measures that we would be OK with, insofar as they get better answers to the inquiry without changing the inquiry? One way of looking at this: inquiry is in reference to summary of a latent variable, which stays constant, but D changes which is different measurement of the latent variable Point to keep in mind from this: D change might be in sampling/treat assignment or measurement Key thing we are saying here: there are two dimensions of change with measurement. (1) are you changing estimands because the latent construct is changing implicitly; (2) are you changing to a better/worse measurement of the same latent construct. 34.0.1 Example policing Knox, Lowe, and Mummolo (2020) (https://www.cambridge.org/core/journals/american-political-science-review/article/administrative-records-mask-racially-biased-policing/66BC0F9998543868BB20F241796B79B8) study the statistical biases that accompany estimates of racial bias in police use of force when presence in the dataset (being stopped by police) is conditioned on an outcome that is a downstream consequence of race. They show the estimate is not identified unless additional modelling assumptions are brought to bear. Gaebler et al. (2020) (https://5harad.com/papers/post-treatment-bias.pdf) study the same question and make such modeling assumptions (subset ignorability, definition 2). In a twitter thread (https://twitter.com/jonmummolo/status/1275790509647241222?s=20), Mummolo shows the three DAGs that are compatible with subset ignorability. We agree with Mummolo that these DAGs assume away causal paths that are very plausible. This document provides a design declaration for this setting and shows how estimates of the controlled direct effect (effect of race on force among the stopped) are biased unless those paths are set to zero by assumption. Design Declaration There are four variables: (D: minority, M: stop, U: suspicion (unobserved), Y: force) and five paths: D_M = 1 # effect of minority on stop U_M = 1 # effect of suspicion on stop D_Y = 1 # effect of minority on force U_Y = 1 # effect of suspicion on force M_Y = 1 # effect of stop on force This basic design allows all five paths. design_1 &lt;- declare_population(N = 1000, D = rbinom(N, size = 1, prob = 0.5), U = rnorm(N)) + declare_potential_outcomes(M ~ rbinom(N, size = 1, prob = pnorm(D_M * D + U_M * U)), assignment_variable = &quot;D&quot;) + declare_reveal(M, D) + declare_potential_outcomes(Y ~ rnorm(N, D_Y * D + M_Y * M + U_Y * U), conditions = list(D = c(0, 1), M = c(0, 1))) + declare_reveal(outcome_variables = &quot;Y&quot;, assignment_variables = c(&quot;D&quot;, &quot;M&quot;)) + declare_estimand(CDE = mean(Y_D_1_M_1 - Y_D_0_M_1)) + declare_estimator(Y ~ D, subset = M == 1, estimand = &quot;CDE&quot;) We redesign the design 3 times, removing one path at a time, then simulate all four designs. # no effect of D on M design_2 &lt;- redesign(design_1, D_M = 0) # no effect of U on M design_3 &lt;- redesign(design_1, U_M = 0) # no effect of U on Y design_4 &lt;- redesign(design_1, U_Y = 0) This chunk is set to echo = TRUE and eval = do_diagnosis simulations &lt;- simulate_designs(design_1, design_2, design_3, design_4, sims = sims) Right after you do simulations, you want to save the simulations rds. This plot confirms that unless one of those implausible assumptions hold, estimates of the CDE are biased. "],
["synthesis.html", "Chapter 35 Synthesis", " Chapter 35 Synthesis One of the last, if not the last, stage of the lifecyle of a research design is its eventual incorportation in to our common scientific understanding of the world. Research findings about specific Is – specific \\(a^D\\)s need to be synthesized into our broader scientific understanding. Research synthesis takes two basic forms. The first is meta-analysis, in which a series of \\(a^D\\)s are analyzed together in order to better understand features of the distribution of answers obtained in the literature. Traditional meta-analysis typically focuses on the average of k answers: \\(a_1^D\\),\\(a_2^D\\),…\\(a_k^D\\). Studies can be averaged together in many ways that are better and worse. Sometimes the answers are averaged together according to their precision – a precision weighted average of estimates from many studies is equivalent to fixed-effects meta analysis. Sometimes studies are “averaged” by counting up how many of the estimates are positive and significant, how many are negative and significant, and how many are null. This is the typical averaging approach taken in a literature review. Regardless of the averaging approach, the goal of this kind of synthesis is to learn as much as possible about a particular \\(I\\) by drawing on evidence from many studies. A second kind of synthesis is an attempt to bring together many \\(a^D\\), each of which targets a different Inquiry about a common Model. This is the kind of synthesis that takes place across an entire research literature. Different scholars focus on different nodes and edges of the common model, so a synthesis needs to incorporate the diverse sources of evidence. How can you best anticipate how your research findings will be synthesized? For the first kind of synthesis – meta analysis – you must be cognizant of keeping a commonly understood \\(I\\) in mind. You want to select inquiries not for their novelty, but because of their commonly-understood importance. We want many studies on the effects of having women versus men elected officials on public goods because we want to understand this particular \\(I\\) in great detail and specificity. While the specifics of the models \\(M\\) might differ from study to study, the fact that the \\(I\\)s are all similar enough to be synthesized allows for a specific kind of knowledge accumulation. For the second kind of synthesis – literature-wide progress on a full causal model – even greater care is required. Specific studies cannot make up bespoke models \\(M\\) but instead must understand how the specific \\(M\\) adopted in the study is a specical case of some master \\(M\\) that is in principle agreed to by a wider research community. The nonstop, neverending proliferation of study-specific theories is a threat to this kind of knowledge accumulation. (Cite cyrus on causal empiricism, that psych paper on crazy proliferation of theories). A research synthesis is a “meta MIDA” M: A master Model that subsumes portions of the sub Ms? I: This is a summary of all of the Is across the stueis. D: This is the inclusion / exclusion criteria. Transformations of the study data. standardization. (sampling, measurement.) A: things like Random effects, fixed effects, Quimpo. \\(I_1 \\approx I_2 \\approx I_3\\) Not \\(a^M_1 \\approx a^M_2 \\approx a^M_3\\) Meta-analysis can be used not just to guess about effects out-of-sample but also to re-evaluate effects in sample: https://declaredesign.org/blog/2018-12-11-meta-analysis.html\n"],
["grab-bag-2.html", "35.1 grab bag", " 35.1 grab bag – systematic reviews are sign and significance, meta-analysis are point estimates "],
["part-iv-exercises.html", "Chapter 36 Part IV Exercises", " Chapter 36 Part IV Exercises "],
["epilogue.html", "Chapter 37 Epilogue", " Chapter 37 Epilogue paragraph 1: revolutionary changes in each social science discipline to improve credibility of science that have changed incentives for researchers paragraph 2: lots of smart people trying to respond to them in their own fields paragraph 3: MIDA is a way of structuring your thinking about what your design is that can help respond to these changed incentives – and communicating that you are doing so "],
["glossary.html", "Chapter 38 Glossary", " Chapter 38 Glossary 38.0.1 Notation \\(M\\) \\(I\\) \\(D\\) \\(A\\) \\(m\\) \\(a^M\\) \\(d\\) \\(a^D\\) \\(w\\) 38.0.2 Terms Model Inquiry Data Strategy Answer Strategy estimand estimator estimate DAG Diagnosis Redesign "],
["references-5.html", "References", " References "]
]
