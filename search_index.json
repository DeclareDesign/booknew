[
["index.html", "Declaring and Diagnosing Research Designs Chapter 1 Preface", " Declaring and Diagnosing Research Designs Graeme Blair, Jasper Cooper, Alexander Coppock, and Macartan Humphreys Chapter 1 Preface With this book, we hope to promote a new, comprehensive way of thinking about research designs in the social sciences. We hope this way of thinking will make research designs more transparent and more robust. But we also hope it will make research design easier, easier to produce good designs, but also easier to share designs and build off of the designs that others have developed. The core idea is to start think of a design as an object that can be interrogated. The design encodes your beliefs about the world, it describes your questions, and it lays out how you go about answering those questions, in terms both of what data you use and how you use it. A key idea is that all of these features can be provided in code and if done right the information provided is enough to be able to simulate a run of the design and assess its properties. For a researcher, being able to simulate a design puts you in a powerful position as you can then start assessing the conditions under which a design perfoms well or badly. For a reader, a complete declaration lets you quickly examine the analytic core of a design. As you work through designs in this book we hope you will develop a nose for quickly parsing what goes in the model, what are the kinds of inquiries you should expect to see, what are the data strategies, what are analysis strategies, and how these all link together. What this book will not do derive estimators no analytic results no optimality guarantees GIGO "],
["part-a-introduction-to-design-declaration-and-diagnosis.html", "Chapter 2 Part A: Introduction to Design Declaration and Diagnosis", " Chapter 2 Part A: Introduction to Design Declaration and Diagnosis This section is only in words, not in code (until the software primer) Preamble Explain MIDA Diagnosands and diagnosis Software primer "],
["part-b-a-guide-to-declaration-and-diagnosis-in-practice.html", "Chapter 3 Part B: A Guide to Declaration and Diagnosis in Practice", " Chapter 3 Part B: A Guide to Declaration and Diagnosis in Practice What is your research question? Topics vs. questions; how to come up with a question - lit review, hunch, data Do you want to answer a causal or descriptive question? Types of questions: COE vs EOC for causal; questions about one of the modes of the distribution of a variable What is your inquiry if you don’t know the question in advance? (Discovery) Is the answer to your question known? How would you know? Meta-analysis, systematic review, disagreement in literature Is your question important? (Change decisions; change answer in a literature) "],
["how-to-select-m.html", "3.1 How to select M", " 3.1 How to select M Introduce DAGs and potential outcomes What is your DAG? How do you write a DAG? What must be in it? What will be missing from it (heterogeneity!)? What parts of background variables must be specified, and how can you specify them (existing data, substantive bounds on variables) Isn’t defining the effect size for potential outcomes begging the question? Pilot studies Spillovers are potential outcomes Compliance is a potential outcome Attrition is a potential outcome Measurements are potential outcomes "],
["how-to-select-i.html", "3.2 How to select I", " 3.2 How to select I When would a null finding be interesting? Is the answer defined? What is the set of units which you want to learn the answer about? (since we can’t learn about an individual unit) Know what ATE averages over Know what is local in a LATE Know when SATEs are informative for PATEs Choice of inquiry is not valueless Make your models vulnerable (something like falsification) Relative value of killing bad ideas vs. giving life to good ones "],
["how-to-select-d.html", "3.3 How to select D", " 3.3 How to select D Need to figure out how to say, how do you go about selecting D Reducing variance via sampling and assignment procedures There is no causation without manipulation Studying many questions can be costly "],
["how-to-select-a.html", "3.4 How to select A", " 3.4 How to select A Analyze as you randomize when you can (not just for experiments!) Reducing variance via estimation choices Answer strategy procedures (if this then that). Deal with fishing. The role of robustness checks PAPs as answer strategies (writing, reconciliation) "],
["part-c-diagnosis.html", "Chapter 4 Part C: Diagnosis", " Chapter 4 Part C: Diagnosis Definition and practical details of Monte Carlo and diagnosands (and discussion of formulae) How to select diagnosands "],
["what-should-you-diagnose-your-design-against.html", "4.1 What should you diagnose your design against?", " 4.1 What should you diagnose your design against? Hold inquiry constant! Compare performance of the design according to choices the researcher makes D: Sample size, fraction randomly assigned, parameters of the randomized response A: Possible estimators, priors(???) And against the researcher’s beliefs M: ICC, null model, alternative DAGs, heterogeneity Conduct subject to constraints: budget, ethical, logistical "],
["how-to-diagnose-iterate-and-improve-diagnosis-as-a-process.html", "4.2 How to diagnose, iterate, and improve (diagnosis as a process)", " 4.2 How to diagnose, iterate, and improve (diagnosis as a process) "],
["part-d-design-library.html", "Chapter 5 Part D: Design Library ", " Chapter 5 Part D: Design Library "],
["observational-designs-for-descriptive-inference.html", "5.1 Observational designs for descriptive inference", " 5.1 Observational designs for descriptive inference Simple random sampling Stratified clustered random sampling Measuring latent variables Topic analysis Respondent driven sampling Multilevel regression and poststratification Meta-analysis "],
["experimental-designs-for-descriptive-inference.html", "5.2 Experimental designs for descriptive inference", " 5.2 Experimental designs for descriptive inference Audit experiments Experiments for sensitive questions Conjoint experiments Experimental games "],
["experimental-designs-for-causal-inference.html", "5.3 Experimental designs for causal inference", " 5.3 Experimental designs for causal inference Multiarm designs Experiments with blocks and clusters Experiments where blocks and clusters are also sampled from populations of blocks/clusters Factorial designs Encouragement designs Stepped-wedge designs Crossover designs Parallel design for mediation effects Partial population design for spillover analysis Selective trials Adaptive trials Multi-site designs (could be descriptive too; hard part is I) "],
["observational-designs-for-causal-inference.html", "5.4 Observational designs for causal inference", " 5.4 Observational designs for causal inference Difference in differences Matching Regression discontinuity designs Synthetic control Process tracing Nested designs Qualitative comparative analysis Cross national time series "],
["part-e-design-criticism.html", "Chapter 6 Part E: Design Criticism", " Chapter 6 Part E: Design Criticism Peers: Better scholarly critique Funders: Evaluating and supporting research "],
["ideas-we-dont-know-if-to-cut-or-integrate-or-what.html", "Chapter 7 IDEAs we don’t know if to cut or integrate or what", " Chapter 7 IDEAs we don’t know if to cut or integrate or what How to designing multi-study projects and research agendas Publication bias (journal of null results) Knowledge accumulation Post-hoc learning about designs "],
["what-is-a-design.html", "Chapter 8 What is a Design?", " Chapter 8 What is a Design? We make heavy use of a fairly general framework for describing the core elements of a research design. The basic idea is a research design is a specification of a problem and a strategy to answer it. We build on two influential frameworks. King, Keohane, and Verba (1994) enumerate four components of a research design: a theory, a research question, data, and an approach to using the data. Geddes (2003) articulates the links between theory formation, research question formulation, case selection and coding strategies, and strategies for case comparison and inference. In both cases the set of steps are closely aligned to those in the framework we propose. In the exposition, we also employ elements from Pearl (2009) approach to structural modeling, which provides a syntax for mapping design inputs to design outputs as well as the potential outcomes framework as presented, for example, in Imbens and Rubin (2015), which many political scientists use to clarify their inferential targets. We characterize the design problem at a high level of generality with the central focus being on the relationship between questions and answer strategies. References "],
["elements-of-a-research-design.html", "8.1 Elements of a Research Design", " 8.1 Elements of a Research Design The specification of a problem requires a description of the world and the question to be asked about the world as described. The answering requires a description of what information is used and how conclusions are reached given the information. At its most basic we think of a research design, \\(\\Delta\\), as including four elements \\(&lt;M,I,D,A&gt;\\): A , \\(M\\), of how the world works. In general following Pearl’s definition of a probabilistic causal model we will assume that a model contains three core elements. First, a specification of the variables \\(X\\) about which research is being conducted. This includes endogenous and exogenous variables (\\(V\\) and \\(U\\) respectively) and the ranges of these variables. In the formal literature this is sometimes called the of a model (Halpern 2000). Second, a specification of how each endogenous variable depends on other variables (the functional relations'' or, as in @Imbens2015,potential outcomes’’), \\(F\\). Third, a probability distribution over exogenous variables, \\(P(U)\\). An , \\(I\\), about the distribution of variables, \\(X\\), perhaps given interventions on some variables. Using Pearl’s notation we can distinguish between questions that ask about the conditional values of variables, such as \\(\\Pr(X_1 | X_2 =1)\\) and questions that ask about values that would arise under interventions: \\(\\Pr(X_1 | do(X_2 = 1))\\). We let \\(a^M\\) denote the answer to \\(I\\) . Conditional on the model, \\(a^M\\) is the value of the estimand, the quantity that the researcher wants to learn about. A strategy, \\(D\\), generates data \\(d\\) on \\(X\\). Data \\(d\\) arises, under model \\(M\\) with probability \\(P_M(d|D)\\). The data strategy includes sampling strategies and assignment strategies, which we denote with \\(P_S\\) and \\(P_Z\\) respectively. Measurement techniques are also a part of data strategies and can be thought of as a selection of observable variables that carry information about unobservable variables. An answer strategy, \\(A\\), that generates answer \\(a^A\\) using data \\(d\\). A key feature of this bare specification is that if \\(M\\), \\(D\\), and \\(A\\) are sufficiently well described, the answer to question \\(I\\) has a distribution \\(P_M(a^A|D)\\). Moreover, one can construct a distribution of comparisons of this answer to the correct answer, under \\(M\\), for example by assessing \\(P_M(a^M-a^A|D)\\). One can also compare this to results under different data or analysis strategies, \\(P_M(a^M-a^A|D&#39;)\\) and \\(P_M(a^M-a^{A&#39;}|D)\\), and to answers generated under alternative models, \\(P_M(a^{M&#39;}-a^{A}|D)\\), as long as these possess signatures that are consistent with inquiries and answer strategies. MIDA captures the analysis-relevant features of a design, but it does not describe substantive elements, such as how theories are derived or interventions are implemented. Yet many other aspects of a design that are not explicitly labeled in these features enter into this framework if they are analytically relevant. For example, logistical details of data collection such as the duration of time between a treatment being administered and endline data collection enter into the model if the longer time until data collection affects subject recall of the treatment. However, information in {} is typically insufficient to assess those substantive elements, an important and separate part of assessing the quality of a research study. References "],
["using-diagnosands-to-evaluate-a-design.html", "8.2 Using diagnosands to evaluate a design", " 8.2 Using diagnosands to evaluate a design The ability to calculate distributions of answers, given a model, opens multiple avenues for assessment and critique. How good is the answer you expect to get from a given strategy? Would you do better, given some desideratum, with a different data strategy? With a different analysis strategy? How good is the strategy if the model is wrong in some way or another? To allow for this kind of diagnosis of a design, we introduce two further concepts, both functions of research designs. These are quantities that a researcher or a third party could calculate with respect to a design. A {} is a summary statistic generated from a “run” of a design—that is, the results given a possible realization of variables, given the model and data strategy. A diagnostic statistic may or may not depend on the model as well as realized data. For example the statistic: \\(e=\\) “difference between the estimated and the actual average treatment effect” depends on the model (since the ATE depends on the model’s assumptions about potential outcomes). The statistic \\(s = \\mathbb{1}(p \\leq 0.05)\\), interpreted as “the result is considered statistically significant at the 5% level”,’’ does not depend on the model but it does presuppose an answer strategy that reports a \\(p\\) value. Diagnostic statistics are governed by probability distributions that arise because both the model and the data generation, given the model, may be stochastic. A Diagnosand is a summary of the distribution of a diagnostic statistic. For example, (expected) in the estimated treatment effect is \\(\\mathbb{E}(e)\\) and statistical is \\(\\mathbb{E}(s)\\). To illustrate, consider the following design. A model M specifies three variables \\(X\\), \\(Y\\) and \\(Z\\) (all defined on the reals). These form the signature. In additional we assume functional relationships between them that allow for the possibility of confounding (for example, \\(Y = bX + Z + \\epsilon_Y; X = Z+ \\epsilon_X\\), with \\(Z, \\epsilon_X, \\epsilon_Z\\) distributed standard normal). The inquiry \\(I\\) is ``what would be the average effect of a unit increase in \\(X\\) on \\(Y\\) in the population?’’ Note that this question depends on the signature of the model, but not the functional equations of the model (the answer provided by the model does of course depend on the functional equations). Consider now a data strategy, \\(D\\), in which data is gathered on \\(X\\) and \\(Y\\) for \\(n\\) randomly selected units. An answer \\(a^A\\), is then generated using ordinary least squares as the answer strategy, \\(A\\). We have specified all the components of MIDA. We now ask: How strong is this research design? One way to answer this question is with respect to the diagnosand “expected error.” Here the model’s functional equations provide an answer, \\(a^M\\) to the inquiry (for any draw of \\(\\beta\\)), and so the distribution of the expected error, given the model, \\(a^A-a^M\\), can be calculated. In this example the expected performance of the design may be poor, as measured by this diagnosand, because the data and analysis strategy do not handle the confounding described by the model. In comparison, better performance may be achieved through an alternative data strategy (e.g., where \\(D&#39;\\) randomly assigned \\(X\\) to \\(n\\) units before recording \\(X\\) and \\(Y\\)) or an alternative analysis strategy (e.g., \\(A&#39;\\) conditions on \\(Z\\)). These design evaluations depend on the model, and so one might reasonably ask how performance would look were the model different (for example if the underlying process involved nonlinearities). In all cases, the evaluation of a design depends on the assessment of a diagnosand, and comparing the diagnoses to what could be achieved under alternative designs. In section X we discuss possible choices of diagnosands and operate a set of these "],
["what-is-a-complete-design-declaration.html", "8.3 What is a Complete Design Declaration?", " 8.3 What is a Complete Design Declaration? A declaration of a research design that is in some sense complete is required in order to implement it, communicate its essential features, and to assess its properties. Yet existing definitions make clear that there is no single conception of a complete research design: the Consolidated Standards of Reporting Trials (CONSORT) Statement widely used in medicine includes 22 features and other proposals range from nine to 60 components. We propose a conditional notion of completeness: we say a design is ``diagnosand-complete’’ for a given diagnosand if that diagnosand can be calculated from the declared design. Thus a design that is diagnosand complete for one diagnosand may not be for another. Consider for example the diagnosand statistical power. Power is the probability that a p-value is lower than a critical value. Thus, power-completeness requires that the answer strategy return a p value. It does not, however, require a well-defined estimand. In contrast, Bias- or RMSE-completeness does not require a hypothesis test, but does require the specification of an estimand. Diagnosand-completeness is a desirable property to the extent that it means a diagnosand can be calculated. How useful this is depends however on how useful the diagnosand is for decision making. Thus evaluating completeness should focus first on whether diagnosands for which completeness holds are indeed useful ones. This usefulness depends in part on whether the information on which diagnoses are made is believable. A design may be bias-complete for instance under the assumptions of a particular spillover structure, for example. Readers may disagree with these assumptions but there are still gains from the declaration as the grounds for claims for unbiasedness are clear and the effects of deviations from model assumptions can be assessed. In practice, different research communities set different standards for what constitutes sufficient information to make such conjectures about the world plausible. "],
["designs-as-concatenations-of-steps.html", "8.4 Designs as concatenations of steps", " 8.4 Designs as concatenations of steps "],
["designs-as-strategies.html", "8.5 Designs as strategies", " 8.5 Designs as strategies "],
["model-inquiry-data-strategy-answer-strategy.html", "Chapter 9 Model-Inquiry-Data-Strategy-Answer Strategy", " Chapter 9 Model-Inquiry-Data-Strategy-Answer Strategy beginning of chapter on MIDA "],
["model-steps.html", "9.1 Model steps", " 9.1 Model steps 9.1.1 Simple data 9.1.2 Hierarchical data 9.1.3 Time series data 9.1.4 Panel data "],
["inquiry-steps.html", "9.2 Inquiry steps", " 9.2 Inquiry steps 9.2.1 Potential outcomes metadata 9.2.2 Potential outcomes functions Creating metadata is either implausible or insufficient for designs with contunuous outcome data or with complex spillovers. "],
["data-steps.html", "9.3 Data steps", " 9.3 Data steps 9.3.1 Simple random sampling 9.3.2 Stratified, clustered random sampling 9.3.3 Arbitrary sampling 9.3.4 Simple assignments 9.3.5 Block clustered assignments 9.3.6 Randomization workflow "],
["answer-steps.html", "9.4 Answer steps", " 9.4 Answer steps "],
["putting-it-all-together.html", "9.5 Putting it all together", " 9.5 Putting it all together "],
["diagnsosis.html", "9.6 Diagnsosis", " 9.6 Diagnsosis 9.6.1 Standard diagnosands 9.6.2 Custom diagnosands 9.6.3 Is a diagnosand part of a design? "],
["design-library.html", "Chapter 10 Design Library", " Chapter 10 Design Library This section of the book ennumerates a series of common and not-so-common social science research design. Each entry will include description of the design in terms of MIDA and also a declaration of the design in code. We’ll often diagnose designs over the range of values of some design parameters in order to point out especially interesting or unusual features of the design. Our goal in this section is not to provide a comprehensive accounting of all empirical research designs. It’s also not to describe any of the particular designs in exhaustive detail, because we are quite sure that in order for these designs to be useful for any practical purpose, they will need to be modified. The entries in the design library are not recipies that, if you follow the instructions, out will come high-quality research. Instead, we hope that the entries provide inspiration for how to tailor a particular class of designs – the blocked-and-clustered randomized trial or the catch-and-release design – to your own research setting. The basic structure of the design library entry will be useful, but the specifics about plausible ranges of outcomes, sample size constraints, etc, will be different in each particular setting. We’ve split up designs by Inquiry and by Data strategy. Inquires can be descripitve or causal and Data strategies can be observational or experimental. This leads to four categories of research: Observational descriptive, Experimental descriptive, Observational Causal, and Experimental causal. A third dimension along which studies can vary is whether the Answer strategy is qualitative or quantitative. If we include this dimension in our typology, we’d end up with eight broad categories of research design. We don’t see the qualitative-quantitative difference in answer strategy to be as fundamental as the differences in inquiry and data strategy, so we’ll just include both qualitative and quantitative designs in each of our four categories. Besides, social scientists always appreciate a good two-by-two: In the broadest terms, descriptive inquiries can be described as \\(f(\\mathbf{Y(Z = Realized)})\\), where \\(f()\\) is some function and \\(\\mathbf{Y(Z = Realized)}\\) is a vector of realized outcomes. That is, descriptive designs seek to summarize (using \\(f()\\)) the world as it is (as represented by \\(\\mathbf{Y(Z = Realized)}\\)). Descripitive designs can be better or worse at answering that inquiry. The quality of descriptive research designs depends on the extent of measurement, sampling, and estimation error. Causal inquiries can be described as \\(f(\\mathbf{Y(Z)})\\), where \\(Z\\) is not a realized vector of treatments, but is instead is a vector that could take on counterfactual values. A standard causal inquiry is the Average Treatment Effect, in which \\(f()\\) is the function that takes the average of the difference between two potential outcome vectors, \\(Y(Z = 1)\\) and \\(Y(Z = 0)\\). But there are many causal inquiries beyond the ATE – the thing they all have in common is that they are functions not of realized outcomes, but of potential outcomes. The quality of causal reseach designs depends on everything that a descriptive design depends on, but also on the understanding and quality of the mechanism that assigns units to treatment conditions. All research designs suffer from some kind of missing data problem. Rubin pointed out missing data in surveys come from people you didn’t survey or people who refused to answer. In causal inference problems, the data that are missing are the potential outcomes that were not revealed by the world. In Descriptive studies, the data that are missing are the true values of the things to be measured. Measurement error is a missing data problem too! Observational research designs are typified by reseachers having no impact on the units under study. They simply record the outcomes that happened in the world and would have happened even if the study did not occur. Experimental research designs are more active – they cause some potential outcomes to be revealed but not others. In this way, researchers have an impact on the units they study. For this reason, experimental studies tend to raise more ethical questions than do observational studies. Experimenters literally change what potential outcomes become realized outcomes. Sometimes the lines between types of research become blurry. The Hawthorne effect is the name given to the idea that measuring a thing changes it. If there are hawthorne effects, than observational research designs also change which potential outcomes are revealed. That is, if there is a difference between Y(Z = measured) and Y(Z = unmeasured), then the act of observation changes that which is observed. Passive data collection methods are sometimes preferred on these grounds. "],
["observational-designs-for-descriptive-inference-1.html", "Chapter 11 Observational Designs for Descriptive Inference", " Chapter 11 Observational Designs for Descriptive Inference blah blah about this section of designs "],
["simple-random-sampling-1.html", "11.1 Simple Random Sampling", " 11.1 Simple Random Sampling Often we are interested in features of a population, but data on the entire population is prohibitively expensive to collect. Instead, researchers obtain data on a small fraction of the population and use measurements taken on that sample to draw inferences about the population. Imagine we seek to estimate the average political ideology of residents of the small town of Portola, California, on a left-right scale that varies from 1 (most liberal) to 7 (most conservative). We draw a simple random sample in which all residents have an equal chance of inclusion in the study. It’s a straightforward design but formally declaring it will make it easy to assess its properties. 11.1.1 Design Declaration Model: Even for this most basic of designs, researchers bring to bear a background model of the world. As described in Chapter 1, the three elements of a model are the signature, probability distributions over variables, and functional equations among variables. The signature here is a specification of the variable of interest, \\(Y\\), with a well defined domain (seven possible values between 1 and 7). In the code declaration below, we assume a uniform distribution over these 7 values. This choice is a speculation about the population distribution of \\(Y\\); some features of the design diagnosis will depend on the choice of distribution. The functional equations seem absent here as there is only one variable in the model. We could consider an elaboration of the model that includes three variables: the true outcome, \\(Y\\); the decision to measure the outcome, \\(M\\); and the measured outcome, \\(Y^M\\). We ignore this complication for now under the assumption that \\(Y = Y^M\\), i.e., that \\(Y\\) is measured perfectly. Finally, the model also includes information about the size of the population. Portola, California, has a population of approximately 2100 people as of 2010, so \\(N = 2100\\). Inquiry: Our inquiry is the population mean of \\(Y\\): \\(\\frac{1}{N} \\sum_1^N Y_i = \\bar{Y}\\). Data strategy: In simple random sampling, we draw a random sample without replacement of size \\(n\\), where every member of the population has an equal probability of inclusion in the sample, \\(\\frac{n}{N}\\). When \\(N\\) is very large relative to \\(n\\), units are drawn approximately independently. In this design we measure \\(Y\\) for \\(n=100\\) units in the sample; the other \\(N-n\\) units are not measured. Answer strategy: We estimate the population mean with the sample mean estimator: \\(\\widehat{\\overline{Y}} = \\frac{1}{n} \\sum_1^n Y_i\\). Even though our inquiry implies our answer should be a single number, an answer strategy typically also provides statistics that help us assess the uncertainty around that single number. To construct a 95% confidence interval around our estimate, we calculate the standard error of the sample mean, then approximate the sampling distribution of the sample mean estimator using a formula that includes a finite population correction. In particular, we approximate the estimated sampling distribution by a \\(t\\) distribution with \\(n - 1\\) degrees of freedom. In the code for our answer strategy, we spell out each step in turn. # Model ------------------------------------------------------------------- N &lt;- 2100 fixed_population &lt;- declare_population(N = N, Y = sample(1:7, N, replace = TRUE))() population &lt;- declare_population(data = fixed_population) # Inquiry ----------------------------------------------------------------- estimand &lt;- declare_estimand(Ybar = mean(Y)) # Data Strategy ----------------------------------------------------------- n &lt;- 100 sampling &lt;- declare_sampling(n = n) # Answer Strategy --------------------------------------------------------- estimator &lt;- declare_estimator(Y ~ 1, model = lm_robust, estimand = estimand, label = &quot;Sample Mean Estimator&quot;) # Design ------------------------------------------------------------------ design &lt;- population + estimand + sampling + estimator diagnosands &lt;- declare_diagnosands(select = c(bias, coverage, mean_estimate, sd_estimate)) 11.1.2 Takeaways With the design declared we can run a diagnosis and plot results from Monte Carlo simulations of the design: diagnosis &lt;- diagnose_design( design, sims = sims, bootstrap_sims = b_sims, diagnosands = diagnosands) The diagnosis indicates that under simple random sampling, the sample mean estimator of the population mean is unbiased. The graph on the left shows the sampling distribution of the estimator: it’s centered directly on the true value of the inquiry. Confidence intervals also have a sampling distribution – they change depending on the idiosyncrasies of each sample we happen to draw. The figure on the right shows that the 95% of the time the confidence intervals cover the true value of the estimand, as they should. As sample size grows, the sampling distribution of the estimator gets tighter, but the coverage of the confidence intervals stays at 95% – just the properties we would want out of our answer strategy. Things work well here it seems. In the exercises we suggest some small modifications of the design that point to conditions under which things might break down. 11.1.3 Exercises Modify the declaration to change the distribution of \\(Y\\) from being uniform to something else: perhaps imagine that more extreme ideologies are more prevalent than moderate ones. Is the sample mean estimator still unbiased? Interpret your answer. Change the sampling procedure to favor units with higher values of ideology. Is the sample mean estimator still unbiased? Interpret your answer. Modify the estimation function to use this formula for the standard error: \\(\\widehat{se} \\equiv \\frac{\\widehat\\sigma}{\\sqrt{n}}\\). This equation differs from the one used in our declaration (it ignores the total population size \\(N\\)). Check that the coverage of this new design is incorrect when \\(N=n\\). Assess how large \\(N\\) has to be for the difference between these procedures not to matter. 11.1.4 Design Inspector 11.1.5 References "],
["stratified-clustered-random-sampling-1.html", "11.2 Stratified clustered random sampling", " 11.2 Stratified clustered random sampling Researchers often cannot randomly sample at the individual level because it may, among other reasons, be too costly or logistically impractical. Instead, they may choose to randomly sample households, political precincts, or any group of individuals in order to draw inferences about the population. This strategy may be cheaper and simpler but may also introduce risks of less precise estimates. Say we are interested in the average party ideology in the entire state of California. Using cluster sampling, we randomly sample counties within the state, and within each selected county, randomly sample individuals to survey. Assuming enough variation in the outcome of interest, the random assignment of equal-sized clusters yields unbiased but imprecise estimates. By sampling clusters, we select groups of individuals who may share common attributes. Unlike simple random sampling, we need to take account of this intra-cluster correlation in our estimation of the standard error.1 The higher the degree of within-cluster similarity, the more variance we observe in cluster-level averages and the more imprecise are our estimates.2 We address this by considering cluster-robust standard errors in our answer strategy below. 11.2.1 Design Declaration Model: We specify the variable of interest \\(Y\\) (political ideology, say) as a discrete variable ranging from 1 (most liberal) to 7 (most conservative). We do not define a functional model since we are interested in the population mean of \\(Y\\). The model also includes information about the number of sampled clusters and the number of individuals per cluster. Inquiry: Our estimand is the population mean of political identification \\(Y\\). Because we employed random sampling, we can expect the value of the sample mean (\\(\\widehat{\\overline{y}}\\)) to approximate the true population parameter (\\(\\widehat{\\overline{Y}}\\)). Data strategy: Sampling follows a two-stage strategy. We first draw a random sample 30 counties in California, and in each county select 20 individuals at random. This guarantees that each county has the same probability of being included in the sample and each resident within a county the same probability of being in the sample. In this design we estimate \\(Y\\) for n = 600 respondents. Answer strategy: We estimate the population mean with the sample mean estimator: \\(\\widehat{\\overline{Y}} = \\frac{1}{n} \\sum_1^n Y_i\\), and estimate standard errors under the assumption of independent and heteroskedastic errors as well as cluster-robust standard errors to take into account correlation of errors within clusters. Below we demonstrate the the imprecision of our estimated \\(\\widehat{\\overline{Y}}\\) when we cluster standard errors and when we do not in the presence of an intracluster correlation coefficient (ICC) of 0.402. N_blocks &lt;- 1 N_clusters_in_block &lt;- 1000 N_i_in_cluster &lt;- 50 n_clusters_in_block &lt;- 30 n_i_in_cluster &lt;- 20 icc &lt;- 0.402 # M: Model fixed_pop &lt;- declare_population( block = add_level(N = N_blocks), cluster = add_level(N = N_clusters_in_block), subject = add_level(N = N_i_in_cluster, latent = draw_normal_icc(mean = 0, N = N, clusters = cluster, ICC = icc), Y = draw_ordered(x = latent, breaks = qnorm(seq(0, 1, length.out = 8))) ) )() cluster_sampling_design &lt;- declare_population(data = fixed_pop) + # I: Inquiry declare_estimand(Ybar = mean(Y)) + # D: Data Strategy declare_sampling(strata = block, clusters = cluster, n = n_clusters_in_block, sampling_variable = &quot;Cluster_Sampling_Prob&quot;) + declare_sampling(strata = cluster, n = n_i_in_cluster, sampling_variable = &quot;Within_Cluster_Sampling_Prob&quot;) + # A: Answer Strategy declare_estimator(Y ~ 1, model = lm_robust, clusters = cluster, estimand = &quot;Ybar&quot;, label = &quot;Clustered Standard Errors&quot;) 11.2.2 Takeaways diagnosis &lt;- diagnose_design(cluster_sampling_design, sims = sims) Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Power Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand cluster_sampling_design Ybar Clustered Standard Errors (Intercept) 500 0.01 0.25 1.00 0.95 3.97 0.25 0.25 0.00 3.97 (0.01) (0.01) (0.00) (0.01) (0.01) (0.01) (0.00) (0.00) (0.00) To appreciate the role of clustering better we also plot simulated values of our estimand with standard errors not clustered and with clustered standard errors. To do this we first add an additional estimator to the design that does not take account of clusters. new_design &lt;- cluster_sampling_design + declare_estimator(Y ~ 1, model = lm_robust, estimand = &quot;Ybar&quot;, label = &quot;Naive Standard Errors&quot;) diagnosis &lt;- diagnose_design(new_design, sims = sims) The figure above may give us the impression that our estimate with clustered standard errors is less precise, when in fact, it correctly accounts for the uncertainty surrounding our estimates. The blue lines in the graph demonstrate the estimates from simulations which contain our estimand. As our table and graphs show, the share of these simulations over the total number of simulations, also known as coverage, is (correctly) close to 95% in estimations with clustered standard errors and 54% in estimations without clustered standard errors. As expected, the mean estimate itself and the bias is the same in both specifications. 11.2.3 References The intra-cluster correlation coefficient (ICC) can be calculated directly and is a feature of this design.↩ In ordinary least square (OLS) models, we assume errors are independent (error terms between individual observations are uncorrelated with each other) and homoskedastic (the size of errors is homogeneous across individuals). In reality, this is often not the case with cluster sampling.↩ "],
["respondent-driven-sampling.html", "11.3 Respondent driven sampling", " 11.3 Respondent driven sampling "],
["sample-re-sample.html", "11.4 Sample re-sample", " 11.4 Sample re-sample "],
["measuring-latent-variables.html", "11.5 Measuring latent variables", " 11.5 Measuring latent variables "],
["topic-analysis.html", "11.6 Topic analysis", " 11.6 Topic analysis lda_tidy &lt;- function(data, k, matrix) { tdm &lt;- data %&gt;% mutate(word = as.character(word)) %&gt;% group_by(document, word) %&gt;% count %&gt;% ungroup %&gt;% cast_dtm(document, word, n) fit &lt;- LDA(x = tdm, k = k) tidy(fit, matrix = matrix) %&gt;% group_by(topic) %&gt;% top_n(10, beta) %&gt;% arrange(-beta) %&gt;% mutate(word_num = 1:n()) %&gt;% ungroup %&gt;% arrange(topic, -beta) %&gt;% dplyr::select(-beta) %&gt;% spread(word_num, term) %&gt;% unite(top_10_words, `1`:`10`) %&gt;% spread(topic, top_10_words) %&gt;% rename(topic_1_top_10_words = `1`, topic_2_top_10_words = `2`) } laws &lt;- gutenberg_download(c(2, 17150)) %&gt;% group_by(gutenberg_id) %&gt;% slice(-c(1:10)) %&gt;% ungroup %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words) topic_probs &lt;- laws %&gt;% group_by(gutenberg_id) %&gt;% mutate(topic_n = n()) %&gt;% group_by(gutenberg_id, word) %&gt;% summarize(topic_n = unique(topic_n), word_count = n(), word_prob = word_count / topic_n) %&gt;% ungroup topic_probs &lt;- topic_probs %&gt;% expand(gutenberg_id, word) %&gt;% left_join(topic_probs) %&gt;% dplyr::select(gutenberg_id, word, word_prob) %&gt;% spread(gutenberg_id, word_prob) %&gt;% rename(pr_topic_1 = `2`, pr_topic_2 = `17150`) %&gt;% mutate(pr_topic_1 = replace_na(pr_topic_1, 0), pr_topic_2 = replace_na(pr_topic_2, 0)) corpus &lt;- topic_probs %&gt;% pull(word) topic_1_prob &lt;- topic_probs %&gt;% pull(pr_topic_1) topic_2_prob &lt;- topic_probs %&gt;% pull(pr_topic_2) N_documents &lt;- 50 N_topics &lt;- 2 design &lt;- declare_population( document = add_level( N = 200, # document length is drawn from a Poisson distribution N_words_per_document = rpois(N, lambda = 500), # draw topic proportion for each document from Dirichlet; save pr(topic1) theta_topic2 = rdirichlet(N, alpha = rep(1, N_topics))[, 2] ), words = add_level( N = N_words_per_document, # draw topic (1 or 2) based on theta probabilities for the document topic = simple_ra(N, prob = theta_topic2, conditions = 1:2), word = case_when( topic == 1 ~ simple_ra(N, prob_each = topic_1_prob, conditions = corpus), topic == 2 ~ simple_ra(N, prob_each = topic_2_prob, conditions = corpus) ) ) ) + declare_estimator( handler = tidy_estimator(lda_tidy), k = 2, matrix = &quot;beta&quot;, label = &quot;lda-topics&quot;) dat &lt;- draw_data(design) document1 &lt;- dat %&gt;% dplyr::filter(document == &quot;001&quot;) %&gt;% pull(word) %&gt;% paste0(collapse = &quot; &quot;) document2 &lt;- dat %&gt;% dplyr::filter(document == &quot;002&quot;) %&gt;% pull(word) %&gt;% paste0(collapse = &quot; &quot;) kable(cbind(document1, document2), col.names = c(&quot;Document 1&quot;, &quot;Document 2&quot;)) Document 1 Document 2 breach time wife enjoy department purchased _memoires 173 deed god subject borne section see money section property witnesses tome probable 278 law dies account morrison 139 cultivation unreclaimed enjoy time children silver confronted cultivated field 197 constable business fixed time naval 275 machine father harboured portion quarrel fines lessee time lost mina life eyes annulment 227 disease section owner wine built possessed owner storage death migration respecting 179 monument boatman lifetime death owner seized accused child invalidated 144 respecting produce judge father’s reimbursement released sickness corn children poor marriage father’s breach house trial exceed sons scarcely bare horn district common ascertained time congress 141 1789 indictment 123 clark corn economical accused 275 feature brought establishment rear bonds 171 bronze unusual votary favor cuts house share see witness property process sixth committed mina gentleman carried plant deluge bring free husband 111 35 brought 2.5 res husband resigns offense 148 jury mother’s 131 arising son press money passenger peaceably ravaged common fees 177 womb law free constitution life district retained life war se crops brawlers heap judge time local crop sentence husband militia deprived death section marriage seduced money regulated garden section arising morgan harvester common see constitution 148 house purchase field god accidental fines thereof 226 35 138 destroyed stolen daughter seed harboured gate hands passed section robbery carrying reserved impartial house dispute 218 master caused agreed 126 impression limited oxen public gentle acquired compensation also section militia granted jury bring penal branding gentleman’s ripe months held maintenance january january 248 eye daughter land agent december unreasonable enumeration life half freedom charges rights sheep bail 127 125 default 164 owner punishments section 118 146 bridegroom impartial dowry sons cut 1791 boatman speech loss loss rules 11 concubine king’s gentleman woman woman return enjoy loss cultivation children marry conqueror 175 impartial 119 criminal lose security 1789 public owner constitution votary process offense 57 constitution greatly garden temple money respecting slave peaceably service 28 price hoe jilting corn worry city witnesses 201 consent handicraft 207 slave account house property lost gur infamous 15 reaper lines months share house causing memorandum death remission 168 1791 sell translation assistance 45 orchard land portion 44 assign shortly deprived liability leased hire judgement forces see silver time palace law 225 bore pay trial grievances wife iii arising construed illegal 17 owe neat granted marriage 9 law morgan twenty witnesses actual naval maidservant lost speech amendments enjoy soldier settlement god ratified price 170 oil house merchant respecting common translation wife blows marriage home 42 mothers oath house children owner intelligible 55 ten preserved also field 129 48 unusual slave hired ox limb seated thy jury scandal 210 rights duties branding house runnel inheritance regulated wagon capital custom woman account hand house 16 widow 10 ten merchant remarkable complaint write gardener bill thereof wife retained 277 security foreign powers cultivated womb stands government section section leave law defense doctor fault injury pay limb militia indelible children bought slave seized wine mother delegated 175 ox rights _memoires brander 207 sheep king 152 bail daughter return 193 borne lost laws woman 40 produce prosecutions heavy mina people criminal master 170 supported 17 cultivator free agent mother release seated merchant implements 215 death miscarriage bill deed hands 190 prohibiting law person private describing ox unreasonable seized people common consent leave mother bail liberty lease public suits oath reserved controversy law time constitution quartered public united operations seized jury militia favor capital wife due united effects militia actual redress father’s verily nature owner 215 disparage common people delegated religion rights united crime redress arising law prescribed law disparage papers iv time manner issue time committed prescribed 127 witnesses required united accounts exercise government district service relationships infamous deny original criminal excessive unreasonable corn property field security compensation enjoy speech enjoy hand militia actual militia security house searched retained danger 128 common jeopardy explorations public speedy congress crime property exceed jury service drowning art iv controversy thereof wishes marriages dollars security deprived time slave construed jury ten badly establishment ii prohibited prohibited slave constitution witnesses violated delegated united trial bill ix law assemble criminal witnesses arising presentment unusual exercise thereof exceed militia woman 25 persons ratified gentleman ii time builder witness actual redress crime unreasonable process people ten law law trial constitution prohibiting freedom land law ox life 250 recaptured 137 defense danger jury accused seizures time mina criminal persons 180 houses security ten grievances mina excessive impartial wife exercise offense rules criminal confronted held defense divorced congress obtaining security seized people freedom people united bill powers held counsel service abridging united witnesses section bronze excessive affirmation petition establishment abridging supported free grand husband time ilu ii land peaceably construed dates persons militia rights trial bear fines affirmation infringed war affirmation militia prescribed nature war accused obtaining amendments 112 sentence life owner single delegated people dollars wife defense arms witnesses owner compensation actual concubine rights husband required witness person rights printed oath iii suits freedom suitor government time witnesses crime child bail forces answer powers arising houses person indictment 147 25 enjoy court liberty establishment bail borne militia deprived committed favor previously punishments free life forces law jury prosecutions code compensation poor 234 male constitution respecting redress danger house imposed peace people seizures inflicted public unreasonable district required viii 242 sought section person obtaining slave 223 money ii religion ur united jury government warrants grand deprived united witness government seizures law danger court exercise examined field public deed exceed retained dollars subject 25 property unusual owner committed people service people preserved disparage soldier examined common shepherd militia violated enjoy people ratified common petition bail constitution war people actual limb thereof amendments necessarily disparage persons disparage reserved naval committed servant housebreaking freighted reserved common prohibiting cruel 116 1791 grounds owner _per nature levy speech previously retained freedom cruel property establishment compelled people speech 25 twenty delegated people regulated religion redress searched rules prescribed property jury grievances naval owner common violated affirmation free offense sheep trial 141 person life land free disparage controversy infringed counsel defense nature service crime property people congress deny quartered person impartial jury law forces press people effects arms # diagnosis compare_word_lists &lt;- function(list1, list2){ list1 &lt;- str_split(list1, &quot;_&quot;) list2 &lt;- str_split(list2, &quot;_&quot;)[[1]] sapply(list1, function(x) sum(x %in% list2) == length(list2)) } topic_1_words &lt;- topic_probs %&gt;% top_n(3, pr_topic_1) %&gt;% arrange(-pr_topic_1) %&gt;% slice(1:3) %&gt;% pull(word) %&gt;% paste0(collapse = &quot;_&quot;) topic_2_words &lt;- topic_probs %&gt;% top_n(3, pr_topic_2) %&gt;% arrange(-pr_topic_2) %&gt;% slice(1:3) %&gt;% pull(word) %&gt;% paste0(collapse = &quot;_&quot;) diag &lt;- declare_diagnosands(in_topic_1 = mean(compare_word_lists(topic_1_top_10_words, topic_1_words) | compare_word_lists(topic_2_top_10_words, topic_1_words)), in_topic_2 = mean(compare_word_lists(topic_1_top_10_words, topic_2_words) | compare_word_lists(topic_2_top_10_words, topic_2_words)), in_either_topic = mean(compare_word_lists(topic_1_top_10_words, topic_1_words) | compare_word_lists(topic_2_top_10_words, topic_1_words) | compare_word_lists(topic_1_top_10_words, topic_2_words) | compare_word_lists(topic_2_top_10_words, topic_2_words )), in_both_topic = mean( (compare_word_lists(topic_1_top_10_words, topic_1_words) | compare_word_lists(topic_1_top_10_words, topic_2_words)) &amp; compare_word_lists(topic_2_top_10_words, topic_1_words) | compare_word_lists(topic_2_top_10_words, topic_2_words )), keep_defaults = FALSE) diagnosis &lt;- diagnose_design(design, diagnosands = diag, sims = 10, bootstrap_sims = 10) kable(reshape_diagnosis(diagnosis)) Design Label Estimator Label N Sims In Topic 1 In Topic 2 In Either Topic In Both Topic design lda-topics 10 0.90 0.60 0.90 0.70 (0.10) (0.12) (0.10) (0.14) 11.6.1 References "],
["multilevel-regression-and-poststratification.html", "11.7 Multilevel regression and poststratification", " 11.7 Multilevel regression and poststratification "],
["experimental-designs-for-descriptive-inference-1.html", "Chapter 12 Experimental Designs for Descriptive Inference", " Chapter 12 Experimental Designs for Descriptive Inference "],
["audit-experiments.html", "12.1 Audit experiments", " 12.1 Audit experiments "],
["experiments-for-sensitive-questions.html", "12.2 Experiments for sensitive questions", " 12.2 Experiments for sensitive questions 12.2.1 List experiments Sometimes, subjects might not tell the truth when asked about certain attitudes or behaviors. Responses may be affected by sensitivity bias, or the tendancy of survey subjects to dissemble for fear of negative repercussions if some reference group learns their true response (Blair, Coppock, and Moor 2018). In such cases, standard survey estimates based on direct questions will be biased. One class of solutions to this problem is to obscure individual responses, providing protection from social or legal pressures. When we obscure responses systematically through an experiment, we can often still identify average quantities of interest. One such design is the list experiment (introduced by Miller (1984)), which asks respondents for the count of the number of `yes’ responses to a series of questions including the sensitive item, rather than for a yes or no answer on the sensitive item itself. List experiments give subjects cover by aggregating their answer to the sensitive item with responses to other questions. During the 2016 Presidential Election in the U.S., some observers were concerned that pre-election estimates of support for Donald Trump might have been downwardly biased by “Shy Trump Supporters” – survey respondents who supported Trump in their hearts, but were embarrased to admit it to pollsters. To assess this possibility, Coppock (2017) obtained estimates of Trump support that were free of social desirability bias using a list experiment. Subjects in the control and treatment groups were asked: “Here is a list of [three/four] things that some people would do and some people would not. Please tell me HOW MANY of them you would do. We do not want to know which ones of these you would do, just how many. Here are the [three/four] things:” Control Treatment If it were up for a vote, I would vote to raise the minimum wage to 15 dollars an hour If it were up for a vote, I would vote to raise the minimum wage to 15 dollars an hour If it were up for a vote, I would vote to repeal the Affordable Care Act, also known as Obamacare If it were up for a vote, I would vote to repeal the Affordable Care Act, also known as Obamacare If it were up for a vote, I would vote to ban assault weapons If it were up for a vote, I would vote to ban assault weapons If the 2016 presidential election were being held today and the candidates were Hillary Clinton (Democrat) and Donald Trump (Republican), I would vote for Donald Trump. The treatment group averaged 1.843 items while the control group averaged 1.548 items, for a difference-in-means estimate of support for Donald Trump of 29.6% (note that this estimate is representative of US adults and not of US adults who would actually vote). The trouble with this estimate is that, while it’s plausibly free from social desirability bias, it’s also much higher variance. The 95% confidence interval for the list experiment estimate is nearly 14 percentage points wide, whereas the the 95% confidence interval for the (possibly biased!) direct question asked of the same sample is closer to 4 percentage points. The choice between list experiments and direct question is therefore a bias-variance tradeoff. List experiments may have less bias, but they are higher variance. Direct questions may be biased, but they have less variance. 12.2.1.1 Declaration Model: Our model includes subjects’ true support for Donald Trump and whether or not they are “shy”. These two variables combine to determine how subjects will respond when asked directly about Trump support. The potential outcomes model combines three types of information to determine how subjects will respond to the list experiment: their responses to the three nonsensitive control items, their true support for Trump, and whether they are assigned to see the treatment or the control list. Notice that our definition of the potential outcomes embeds the “No Liars” and “No Design Effects” assumptions required for the list experiment design (see Blair and Imai 2012 for more on these assumptions). We also have a global parameter that reflects our expectations about the proportion of Trump supporters who are shy. It’s set at 6%, which is large enough to make a difference for polling, but not so large as to be implausible. Inquiry: Our estimand is the proportion of voters who actually plan to vote for Trump. Data strategy: First we sample 500 respondents from the U.S. population at random, then we randomly assign 250 of the 500 to treatment and the remainder to control. In the survey, we ask subjects both the direct question and the list experiment question. Answer strategy: We estimate the proportion of truthful Trump voters in two ways. First, we take the mean of answers to the direct question. Second, we take the difference in means in the responses to the list experiment question. # Model ------------------------------------------------------------------- proportion_shy &lt;- .06 list_design &lt;- # Model declare_population( N = 5000, # true trump vote (unobservable) truthful_trump_vote = draw_binary(.45, N), # shy voter (unobservable) shy = draw_binary(proportion_shy, N), # Direct question response (1 if Trump supporter and not shy, 0 otherwise) Y_direct = as.numeric(truthful_trump_vote == 1 &amp; shy == 0), # Nonsensitive list experiment items raise_minimum_wage = draw_binary(.8, N), repeal_obamacare = draw_binary(.6, N), ban_assault_weapons = draw_binary(.5, N) ) + declare_potential_outcomes( Y_list_Z_0 = raise_minimum_wage + repeal_obamacare + ban_assault_weapons, Y_list_Z_1 = Y_list_Z_0 + truthful_trump_vote ) + # Inquiry declare_estimand(proportion_truthful_trump_vote = mean(truthful_trump_vote)) + # Data Strategy declare_sampling(n = 500) + declare_assignment(prob = .5) + declare_reveal(Y_list) + # Answer Strategy declare_estimator( Y_direct ~ 1, model = lm_robust, term = &quot;(Intercept)&quot;, estimand = &quot;proportion_truthful_trump_vote&quot;, label = &quot;direct&quot; ) + declare_estimator( Y_list ~ Z, model = difference_in_means, estimand = &quot;proportion_truthful_trump_vote&quot;, label = &quot;list&quot;) simulations_list &lt;- simulate_design(list_design, sims = sims) The plot shows the sampling distribution of the direct and list experimetn estimators. The sampling distribution of the direct question is tight but biased; the list experiment (if the requisite assumptions hold) is unbiased, but higher variance. The choice between these two estimators of the prevalence rate depends on which – bias or variance – is more important in a particular setting. See Blair, Coppock, and Moor (2018) for an extended discussion of how the choice of research design depends deeply on the purpose of the project. 12.2.2 Randomized response technique 12.2.2.1 Declaration library(rr) rr_forced_known_tidy &lt;- function(data) { fit &lt;- rrreg(Y_forced_known ~ 1, data = data, p = 2/3, p0 = 1/6, p1 = 1/6, design = &quot;forced-known&quot;) pred &lt;- as.data.frame(predict(fit, avg = TRUE, quasi.bayes = TRUE)) names(pred) &lt;- c(&quot;estimate&quot;, &quot;std.error&quot;, &quot;conf.low&quot;, &quot;conf.high&quot;) pred$p.value &lt;- with(pred, 2 * pnorm(-abs(estimate / std.error))) pred } rr_mirrored_tidy &lt;- function(data) { fit &lt;- rrreg(Y_mirrored ~ 1, data = data, p = 2/3, design = &quot;mirrored&quot;) pred &lt;- as.data.frame(predict(fit, avg = TRUE, quasi.bayes = TRUE)) names(pred) &lt;- c(&quot;estimate&quot;, &quot;std.error&quot;, &quot;conf.low&quot;, &quot;conf.high&quot;) pred$p.value &lt;- with(pred, 2 * pnorm(-abs(estimate / std.error))) pred } proportion_shy &lt;- .06 rr_design &lt;- declare_population( N = 100, # true trump vote (unobservable) truthful_trump_vote = draw_binary(.45, N), # shy voter (unobservable) shy = draw_binary(proportion_shy, N), # Direct question response (1 if Trump supporter and not shy, 0 otherwise) Y_direct = as.numeric(truthful_trump_vote == 1 &amp; shy == 0)) + declare_estimand(sensitive_item_proportion = mean(truthful_trump_vote)) + declare_potential_outcomes(Y_forced_known ~ (dice == 1) * 0 + (dice %in% 2:5) * truthful_trump_vote + (dice == 6) * 1, conditions = 1:6, assignment_variable = &quot;dice&quot;) + declare_potential_outcomes(Y_mirrored ~ (coin == &quot;heads&quot;) * truthful_trump_vote + (coin == &quot;tails&quot;) * (1 - truthful_trump_vote), conditions = c(&quot;heads&quot;, &quot;tails&quot;), assignment_variable = &quot;coin&quot;) + declare_assignment(prob_each = rep(1/6, 6), conditions = 1:6, assignment_variable = &quot;dice&quot;) + declare_assignment(prob_each = c(2/3, 1/3), conditions = c(&quot;heads&quot;, &quot;tails&quot;), assignment_variable = &quot;coin&quot;) + declare_reveal(Y_forced_known, dice) + declare_reveal(Y_mirrored, coin) + declare_estimator(handler = tidy_estimator(rr_forced_known_tidy), label = &quot;forced_known&quot;, estimand = &quot;sensitive_item_proportion&quot;) + declare_estimator(handler = tidy_estimator(rr_mirrored_tidy), label = &quot;mirrored&quot;, estimand = &quot;sensitive_item_proportion&quot;) + declare_estimator(Y_direct ~ 1, model = lm_robust, term = &quot;(Intercept)&quot;, label = &quot;direct&quot;, estimand = &quot;sensitive_item_proportion&quot;) rr_design &lt;- set_diagnosands(rr_design, diagnosands = declare_diagnosands(select = c(mean_estimate, bias, rmse, power))) 12.2.3 References References "],
["conjoint-experiments.html", "12.3 Conjoint experiments", " 12.3 Conjoint experiments "],
["experimental-games.html", "12.4 Experimental games", " 12.4 Experimental games "],
["observational-designs-for-causal-inference-1.html", "Chapter 13 Observational Designs for causal inference", " Chapter 13 Observational Designs for causal inference "],
["difference-in-differences.html", "13.1 Difference in differences", " 13.1 Difference in differences "],
["matching.html", "13.2 Matching", " 13.2 Matching "],
["regression-discontinuity.html", "13.3 Regression Discontinuity", " 13.3 Regression Discontinuity Regression discontinuity designs exploit substantive knowledge that treatment is assigned in a particular way: everyone above a threshold is assigned to treatment and everyone below it is not. Even though researchers do not control the assignment, substantive knowledge about the threshold serves as a basis for a strong identification claim. Thistlewhite and Campbell introduced the regression discontinuity design in the 1960s to study the impact of scholarships on academic success. Their insight was that students with a test score just above a scholarship cutoff were plausibly comparable to students whose scores were just below the cutoff, so any differences in future academic success could be attributed to the scholarship itself. Regression discontinuity designs identify a local average treatment effect: the average effect of treatment exactly at the cutoff. The main trouble with the design is that there is vanishingly little data exactly at the cutoff, so any answer strategy needs to use data that is some distance away from the cutoff. The further away from the cutoff we move, the larger the threat of bias. We’ll consider an application of the regression discontinuity design that examines party incumbency advantage – the effect of a party winning an election on its vote margin in the next election. 13.3.1 Design Declaration Model: Regression discontinuity designs have four components: A running variable, a cutoff, a treatment variable, and an outcome. The cutoff determines which units are treated depending on the value of the running variable. In our example, the running variable \\(X\\) is the Democratic party’s margin of victory at time \\(t-1\\); and the treatment, \\(Z\\), is whether the Democratic party won the election in time \\(t-1\\). The outcome, \\(Y\\), is the Democratic vote margin at time \\(t\\). We’ll consider a population of 1,000 of these pairs of elections. A major assumption required for regression discontinuity is that the conditional expectation functions for both treatment and control potential outcomes are continuous at the cutoff.3 To satisfy this assumption, we specify two smooth conditional expectation functions, one for each potential outcome. The figure plots \\(Y\\) (the Democratic vote margin at time \\(t\\)) against \\(X\\) (the margin at time \\(t-1\\)). We’ve also plotted the true conditional expectation functions for the treated and control potential outcomes. The solid lines correspond to the observed data and the dashed lines correspond to the unobserved data. Inquiry: Our estimand is the effect of a Democratic win in an election on the Democratic vote margin of the next election, when the Democratic vote margin of the first election is zero. Formally, it is the difference in the conditional expectation functions of the control and treatment potential outcomes when the running variable is exactly zero. The black vertical line in the plot shows this difference. Data strategy: We collect data on the Democratic vote share at time \\(t-1\\) and time \\(t\\) for all 1,000 pairs of elections. There is no sampling or random assignment. Answer strategy: We will approximate the treated and untreated conditional expectation functions to the left and right of the cutoff using a flexible regression specification estimated via OLS. In particular, we fit each regression using a fourth-order polynomial. Much of the literature on regression discontinuity designs focuses on the tradeoffs among answer strategies, with many analysts recommending against higher-order polynomial regression specifications. We use one here to highlight how well such an answer strategy does when it matches the functional form in the model. We discuss alternative estimators in the exercises. # Model ------------------------------------------------------------------- cutoff &lt;- 0.5 control &lt;- function(X) { as.vector(poly(X, 4, raw = T) %*% c(.7, -.8, .5, 1))} treatment &lt;- function(X) { as.vector(poly(X, 4, raw = T) %*% c(0, -1.5, .5, .8)) + .15} population &lt;- declare_population( N = 1000, X = runif(N,0,1) - cutoff, noise = rnorm(N,0,.1), Z = 1 * (X &gt; 0) ) potential_outcomes &lt;- declare_potential_outcomes( Y_Z_0 = control(X) + noise, Y_Z_1 = treatment(X) + noise) # Inquiry ----------------------------------------------------------------- estimand &lt;- declare_estimand(LATE = treatment(0) - control(0)) # Answer Strategy --------------------------------------------------------- estimator &lt;- declare_estimator( formula = Y ~ poly(X, 4) * Z, model = lm_robust, estimand = estimand) # Design ------------------------------------------------------------------ design &lt;- population + potential_outcomes + estimand + reveal_outcomes + estimator 13.3.2 Takeaways diagnosis &lt;- diagnose_design(rd_design, sims = sims, bootstrap_sims = b_sims) We highlight three takeaways. First, the power of this design is very low: with 1,000 units we do not achieve even 10% statistical power. However, our estimates of the uncertainty are not too wide: the coverage probability indicates that our confidence intervals indeed contain the estimand 95% of the time as they should. Our answer strategy is highly uncertain because the fourth-order polynomial specification in regression model gives weights to the data that greatly increase the variance of the estimator (Gelman and Imbens (2017)). In the exercises we explore alternative answer strategies that perform better. Second, the design is biased because polynomial approximations of the average effect at exactly the point of the threshold will be inaccurate in small samples (Sekhon and Titiunik (2017)), especially as units farther away from the cutoff are incorporated into the answer strategy. We know that the estimated bias is not due to simulation error by examining the bootstrapped standard error of the bias estimates. Finally, from the figure, we can see how poorly the average effect at the threshold approximates the average effect for all units. The average treatment effect among the treated (to the right of the threshold in the figure) is negative, whereas at the threshold it is positive. This clarifies that the estimand of the regression discontinuity design, the difference at the cutoff, is only relevant for a small – and possibly empty – set of units very close to the cutoff. 13.3.3 Further Reading Since its rediscovery by social scientists in the late 1990s, the regression discontinuity design has been widely used to study diverse causal effects such as: prison on recidivism (Mitchell et al. (2017)); China’s one child policy on human capital (Qin, Zhuang, and Yang (2017)); eligibility for World Bank loans on political liberalization (Carnegie and Samii (2017)); and anti-discrimination laws on minority employment (Hahn, Todd, and Van der Klaauw (1999)). We’ve discussed a “sharp” regression discontinuity design in which all units above the threshold were treated and all units below were untreated. In fuzzy regression discontinuity designs, some units above the cutoff remain untreated or some units below take treatment. This setting is analogous to experiments that experience noncompliance and may require instrumental variables approaches to the answer strategy (see Compliance is a Potential Outcome). Geographic regression discontinuity designs use distance to a border as the running variable: units on one side of the border are treated and units on the other are untreated. Keele and Titiunik (2016) use such a design to study whether voters are more likely to turn out when they have the opportunity to vote directly on legislation on so-called ballot initiatives. A complication of this design is how to measure distance to the border in two dimensions. 13.3.4 Exercises Gelman and Imbens (2017) point out that higher order polynomial regression specifications lead to extreme regression weights. One approach to obtaining better estimates is to select a bandwidth, \\(h\\), around the cutoff, and run a linear regression. Declare a sampling procedure that subsets the data to a bandwidth around the threshold, as well as a first order linear regression specification, and analyze how the power, bias, RMSE, and coverage of the design vary as a function of the bandwidth. The rdrobust estimator in the rdrobust package implements a local polynomial estimator that automatically selects a bandwidth for the RD analysis and bias-corrected confidence intervals. Declare another estimator using the rdrobust function and add it to the design. How does the coverage and bias of this estimator compare to the regression approaches declared above? Reduce the number of polynomial terms of the the treatment() and control() functions and assess how the bias of the design changes as the potential outcomes become increasingly linear as a function of the running variable. Redefine the population function so that units with higher potential outcome are more likely to locate just above the cutoff than below it. Assess whether and how this affects the bias of the design. 13.3.5 References References "],
["synthetic-controls.html", "13.4 Synthetic controls", " 13.4 Synthetic controls Modeled after the example here: https://www.mitpressjournals.org/doi/abs/10.1162/REST_a_00429?casa_token=o-zWqCima50AAAAA:yiEERZfdhAUoHV0-xBYNjgdljvgfRXrriR8foG7X8nHSUAMFrLcw2vWY8e9pHzmRT24MMAIv9hvKpQ Did the 2007 Legal Arizona Workers Act Reduce the State’s Unauthorized Immigrant Population? Sarah Bohn, Magnus Lofstrom, and Steven Raphael The Review of Economics and Statistics 2014 96:2, 258-269 set up states with time trends and levels that are correlated with a type try three estimators: (1) difference-in-difference; (2) difference in treated period; and (3) difference in treated period weighted by Synth weights # tidy function that takes data and just adds the synthetic control weights to it synth_weights_tidy &lt;- function(data) { dataprep.out &lt;- dataprep( foo = data, predictors = &quot;prop_non_hispanic_below_hs&quot;, predictors.op = &quot;mean&quot;, time.predictors.prior = 1998:2006, dependent = &quot;prop_non_hispanic_below_hs&quot;, unit.variable = &quot;state_number&quot;, time.variable = &quot;year&quot;, treatment.identifier = 4, controls.identifier = c(1:3, 5:50), # states without Arizona time.optimize.ssr = 1998:2006, time.plot = 1998:2009) capture.output(fit &lt;- synth(data.prep.obj = dataprep.out)) tab &lt;- synth.tab(dataprep.res = dataprep.out, synth.res = fit) data %&gt;% left_join(tab$tab.w %&gt;% mutate(synth_weights = w.weights) %&gt;% dplyr::select(synth_weights, unit.numbers), by = c(&quot;state_number&quot; = &quot;unit.numbers&quot;)) %&gt;% mutate(synth_weights = replace(synth_weights, state_number == 4, 1)) } # note need to clean up the range of the data, currently over 1 design &lt;- declare_population( states = add_level( N = 50, state = state.abb, state_number = as.numeric(as.factor(state)), state_shock = runif(N, -.15, .15), border_state = state %in% c(&quot;AZ&quot;, &quot;CA&quot;, &quot;NM&quot;, &quot;TX&quot;), state_shock = ifelse(border_state, .2, state_shock) ), years = add_level( N = 12, nest = FALSE, year = 1998:2009, post_treatment_period = year &gt;= 2007, year_shock = runif(N, -.01, .01), year_trend = year - 1998 ), obs = cross_levels( by = join(states, years), legal_worker_act = 1*(post_treatment_period == TRUE &amp; state == &quot;AZ&quot;), state_year_shock = runif(N, -.01, .01), prop_non_hispanic_below_hs_baseline = 0.4 + state_shock + year_shock + (.01 + .05 * border_state) * year_trend + state_year_shock ) ) + declare_potential_outcomes( prop_non_hispanic_below_hs ~ prop_non_hispanic_below_hs_baseline + 0.25 * legal_worker_act, assignment_variable = legal_worker_act) + declare_estimand( ATE_AZ = mean(prop_non_hispanic_below_hs_legal_worker_act_1 - prop_non_hispanic_below_hs_legal_worker_act_0), subset = legal_worker_act == TRUE) + declare_reveal(prop_non_hispanic_below_hs, legal_worker_act) + declare_step(handler = synth_weights_tidy) + declare_estimator(prop_non_hispanic_below_hs ~ legal_worker_act, subset = year &gt;= 2007, weights = synth_weights, model = lm_robust, label = &quot;synth&quot;) + declare_estimator(prop_non_hispanic_below_hs ~ legal_worker_act, subset = year &gt;= 2007, model = lm_robust, label = &quot;unweighted&quot;) + declare_estimator(prop_non_hispanic_below_hs ~ I(state == &quot;AZ&quot;) + post_treatment_period + legal_worker_act, term = &quot;legal_worker_act&quot;, model = lm_robust, label = &quot;unweighted_did&quot;) state_data &lt;- draw_data(design) state_data %&gt;% dplyr::select(state, synth_weights) %&gt;% distinct %&gt;% arrange(-synth_weights) %&gt;% head ## state synth_weights ## 1 AZ 1.000 ## 2 TX 0.965 ## 3 NM 0.018 ## 4 CA 0.011 ## 5 AL 0.000 ## 6 AK 0.000 state_data %&gt;% ggplot() + geom_line(aes(year, prop_non_hispanic_below_hs)) + facet_wrap(~ state) This chunk is set to echo = TRUE and eval = do_diagnosis simulations &lt;- simulate_design(design, sims = sims) Right after you do simulations, you want to save the simulations rds. Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. synth_diagnosands &lt;- declare_diagnosands(select = &quot;bias&quot;) diagnosis &lt;- diagnose_design(simulations, diagnosands = synth_diagnosands, bootstrap_sims = b_sims) reshape_diagnosis(diagnosis) ## Design Label Estimand Label Estimator Label Term N Sims ## 1 design ATE_AZ synth legal_worker_act 100 ## 2 ## 3 design ATE_AZ unweighted legal_worker_act 100 ## 4 ## 5 design ATE_AZ unweighted_did legal_worker_act 100 ## 6 ## Bias ## 1 0.00 ## 2 (0.00) ## 3 0.66 ## 4 (0.00) ## 5 0.28 ## 6 (0.00) we see that Synth outperforms either method 13.4.1 References "],
["cross-national-time-series.html", "13.5 Cross national time series", " 13.5 Cross national time series "],
["qualtitative-comparative-analysis.html", "13.6 Qualtitative Comparative Analysis", " 13.6 Qualtitative Comparative Analysis "],
["process-tracing.html", "13.7 Process tracing", " 13.7 Process tracing "],
["experimental-designs-for-causal-inference-1.html", "Chapter 14 Experimental Designs for Causal Inference", " Chapter 14 Experimental Designs for Causal Inference "],
["multiarm-designs.html", "14.1 Multiarm designs", " 14.1 Multiarm designs "],
["experiments-with-blocks-and-clusters.html", "14.2 Experiments with blocks and clusters", " 14.2 Experiments with blocks and clusters "],
["factorial-designs.html", "14.3 Factorial designs", " 14.3 Factorial designs "],
["encouragement-designs.html", "14.4 Encouragement designs", " 14.4 Encouragement designs Idea for this one would be to show how violations of no defiers and excludability lead to bias. types &lt;- c(&quot;Always-Taker&quot;, &quot;Never-Taker&quot;, &quot;Complier&quot;, &quot;Defier&quot;) direct_effect_of_encouragement &lt;- 0.0 proportion_defiers &lt;- 0.0 design &lt;- declare_population( N = 500, type = sample( types, N, replace = TRUE, prob = c(0.1, 0.1, 0.8 - proportion_defiers, proportion_defiers) ), noise = rnorm(N) ) + declare_potential_outcomes( D ~ case_when( Z == 0 &amp; type %in% c(&quot;Never-Taker&quot;, &quot;Complier&quot;) ~ 0, Z == 1 &amp; type %in% c(&quot;Never-Taker&quot;, &quot;Defier&quot;) ~ 0, Z == 0 &amp; type %in% c(&quot;Always-Taker&quot;, &quot;Defier&quot;) ~ 1, Z == 1 &amp; type %in% c(&quot;Always-Taker&quot;, &quot;Complier&quot;) ~ 1 ) ) + declare_potential_outcomes( Y ~ 0.5 * (type == &quot;Complier&quot;) * D + 0.25 * (type == &quot;Always-Taker&quot;) * D + 0.75 * (type == &quot;Defier&quot;) * D + direct_effect_of_encouragement * Z + noise, assignment_variables = c(&quot;D&quot;, &quot;Z&quot;) ) + declare_estimand(CACE = mean((Y_D_1_Z_1 + Y_D_1_Z_0) / 2 - (Y_D_0_Z_1 + Y_D_0_Z_0) / 2), subset = type == &quot;Complier&quot;) + declare_assignment(prob = 0.5) + declare_reveal(D, assignment_variable = &quot;Z&quot;) + declare_reveal(Y, assignment_variables = c(&quot;D&quot;, &quot;Z&quot;)) + declare_estimator(Y ~ D | Z, model = iv_robust, estimand = &quot;CACE&quot;) designs &lt;- redesign( design, proportion_defiers = seq(0, 0.3, length.out = 5), direct_effect_of_encouragement = seq(0, 0.3, length.out = 5) ) simulations &lt;- simulate_design(designs, sims = sims) gg_df &lt;- simulations %&gt;% group_by(proportion_defiers, direct_effect_of_encouragement) %&gt;% summarize(bias = mean(estimate - estimand)) ggplot(gg_df, aes( proportion_defiers, bias, group = direct_effect_of_encouragement, color = direct_effect_of_encouragement )) + geom_point() + geom_line() 14.4.1 References "],
["stepped-wedge-designs.html", "14.5 Stepped wedge designs", " 14.5 Stepped wedge designs "],
["crossover-designs.html", "14.6 Crossover designs", " 14.6 Crossover designs "],
["parallel-design-for-mediation-effects.html", "14.7 Parallel design for mediation effects", " 14.7 Parallel design for mediation effects "],
["partial-population-design-for-spillover-analysis.html", "14.8 Partial population design for spillover analysis", " 14.8 Partial population design for spillover analysis "],
["selective-trials.html", "14.9 Selective trials", " 14.9 Selective trials "],
["adaptive-trials.html", "14.10 Adaptive trials", " 14.10 Adaptive trials "],
["selective-trials-1.html", "14.11 Selective trials", " 14.11 Selective trials The treatment in some randomized controlled trials is a benefit, opportunity, or other good. For example, researchers might allocate job training (cite), health products (cite, cite), or even cash, no strings attached. Experiments in which the treatment is such a benefit raise thorny ethical problems because random allocation by definition does not distribute the benefit to the most deserving. A class of experimental designs called patient preference trials (cite) or selective trials (Chassang et al. 2012) aim to mitigate these ethical concerns by preferentially treating subjects who want the treatment more with higher probability. In addition to ethical concerns, selective trials may also shed light on the “external validity” of the study. In a standard experimental design, subjects are offered or not offered the treatment and the main target of inference is the average treatment effect. However, the average treatment effect might not be the most politically, economically, or socially relevant estimand. Imagine that in the “real world,” certain types of people are more likely to select into treatment – possibly because they are more deserving, but also possibly because they are more willing to pay or are otherwise advantaged. The “externally valid” estimand might be the effect of treatment among those who, in the real world, would end up taking the treatment. As we will see, estimating this estimand doesn’t require a selective trial, but the design of selective trials does highlight this imporant characteristic of subjects. By using a selective trial, we can to try to solve two problems at once. First, we want to meet the ethical challenge of preferentially treating subjects who want, deserve, or need the treatment more than others. Second, we want to increase the external validity of the study by estimating effects among people who – outside of the experimental context – would be more likely to take treatment. 14.11.1 Design Declaration Model: Our model needs to allow subjects to differ according to their utility from obtaining treatment. That is, the causal effect of the treatment on subject happiness varies from person to person. We also need to allow subjects to vary according to their probability of selecting into taking the treatment in the “real world.” We also imagine that subjects’ utility from treatment and their probability of selecting into the treatment are positively but not perfectly correlated. In the background of our model is the normative position that it is ethically preferable to give treatment to people who have the highest utility of treatment. This value has to be weighed against the value of the research study itself. This number is hard to known, so for simplicity, we imagine that the study is more valuable, the more precise its treatment effect estimates. Our model also has to allow the effects of treatment on the main outcome of interest to vary from subject to subject and that this heterogeneity has to be correlated with the probability of selecting into the treatment. Without such treatment effect heterogeneity, there would be no external validity concern. Inquiry: We have two inquiries: the Average Treatment Effect for all subjects (the ATE) and the conditional average treatment effect (CATE) among those subjects who would take treatment in the real world. Data strategy: The two most important features of the data strategy are the measurement of subject preferences and the random assignment procedure. Measuring subject preferences may or may not be straightforward. In some settings, one might imagine giving subjects a pre-treatment survey that asks them to report how much they want the treatment, or which of the many treatment options they would most prefer, or the intensity of their preferences. The measurement of these preferences may suffer from various forms of measuremnet error. TEPPEI and CO are concerned that these measures might be biased by social desirability bias. Chassang et al. (2012) describe an incentive-compatible mechanism for eliciting subjects willingness-to-pay from the good, which provides some measure of the relative amount that subjects “want” the treatment. We ideally want measures of two latent variables: subject utility and subject probability of selecting in to the treatment in the real world. We assume that we have accomplished both measurement tasks well, though we fully grant that in any particular research scenario, the measurement of preferences has to be taken very seriously. The second important feature of the data strategy is the random assignment procedure. The standard approach is to use simple random assignment, i.e., we assign subjects to treatment with a constant probability \\(p\\). Alternatively, we could assign subjects proportionally to their utility from treatment. Answer strategy: Our answer strategy has to account for the fact that units can be assigned to treatment with different probabilities, so we’ll employ inverse probability weighting. We’ll just two estimators, one that targets the ATE and another that targets the CATE. We also want to estimate the total amount of utility from treatment that subjects received. strategy_switch &lt;- 0 design &lt;- # Model ------------------------------------------------------------------- declare_population( N = 100, # Measure subject preferences utility_from_treatment = runif(N), # Measure willingness-to-pay, determine who would take treatment in the &quot;real world&quot; willingness_to_pay = correlate(given = utility_from_treatment, draw_handler = runif, rho = 0.75), real_world = as.numeric(willingness_to_pay &gt;= 0.5), # Idiosyncractic variation noise = rnorm(N) ) + # treatment effects are correlated with willingness to pay declare_potential_outcomes(Y ~ 0.5 * Z + 0.2 * willingness_to_pay * Z + willingness_to_pay + noise) + # Inquiry ----------------------------------------------------------------- declare_estimands( # Maximum possible subject utility, subject to budget constraint MPU = sum(utility_from_treatment[which(utility_from_treatment &gt;= quantile(utility_from_treatment, probs = 0.5))] ), ATE = mean(Y_Z_1 - Y_Z_0), CATE = mean(Y_Z_1[real_world == 1] - Y_Z_0[real_world == 1]) ) + # Data Strategy ----------------------------------------------------------- # first, figure out assignment probabilities. If switch = 0, do 50/50; otherwise proportional to utility declare_step( assignment_prob = 0.5 * (1 - strategy_switch) + utility_from_treatment * strategy_switch, handler = fabricate ) + # second, actually assign treatments declare_assignment( prob = assignment_prob, simple = TRUE, assignment_variable = &quot;Z&quot; ) + declare_reveal(Y, Z) + # Answer Strategy --------------------------------------------------------- declare_estimator( handler = tidy_estimator(function(data) { with(data, data.frame(estimate = sum(utility_from_treatment * Z))) }), estimand = &quot;MPU&quot;, label = &quot;total utility&quot; ) + declare_estimator( Y ~ Z, model = lm_robust, weights = 1 / (assignment_prob * Z + (1 - assignment_prob) * (1 - Z)), estimand = &quot;ATE&quot;, label = &quot;ATE&quot; ) + declare_estimator( Y ~ Z, model = lm_robust, weights = 1 / (assignment_prob * Z + (1 - assignment_prob) * (1 - Z)), estimand = &quot;CATE&quot;, label = &quot;CATE&quot;, subset = real_world == 1 ) standard_trial &lt;- redesign(design, strategy_switch = 0) selective_trial &lt;- redesign(design, strategy_switch = 1) diagnosis &lt;- diagnose_design(standard_trial, selective_trial, sims = sims, bootstrap_sims = b_sims) The diagnosis shows that under both designs, we obtain unbiased estimates of both the ATE and the CATE. The power for both these estimands is higher under the standard design and lower under the selective trial. If design choices are made without any accomodation for subject prefereces, the standard design strictly domindates the selective trial. However, it’s clear that the average total subject utility is higher under the selective trial than the standard design. It is up to researchers in each subject domain to determine the relative weights to give to subject utility and statistical power. In our view, giving zero weight to either of these diagnosands would be bad, but we don’t know what the right weights are. reshape_diagnosis(diagnosis)[,c(&quot;Design Label&quot;, &quot;Estimand Label&quot;, &quot;Mean Estimand&quot;, &quot;Mean Estimate&quot;, &quot;Bias&quot;, &quot;Power&quot;)] ## Design Label Estimand Label Mean Estimand Mean Estimate Bias ## 1 standard_trial ATE 0.60 0.58 -0.02 ## 2 (0.00) (0.01) (0.01) ## 3 standard_trial CATE 0.65 0.63 -0.02 ## 4 (0.00) (0.01) (0.01) ## 5 standard_trial MPU 37.41 25.09 -12.33 ## 6 (0.07) (0.14) (0.13) ## 7 selective_trial ATE 0.60 0.62 0.02 ## 8 (0.00) (0.01) (0.01) ## 9 selective_trial CATE 0.65 0.69 0.04 ## 10 (0.00) (0.02) (0.02) ## 11 selective_trial MPU 37.48 33.43 -4.05 ## 12 (0.07) (0.17) (0.12) ## Power ## 1 0.78 ## 2 (0.02) ## 3 0.57 ## 4 (0.02) ## 5 NA ## 6 NA ## 7 0.63 ## 8 (0.02) ## 9 0.51 ## 10 (0.02) ## 11 NA ## 12 NA 14.11.2 References References "],
["model-principles.html", "Chapter 15 Model principles", " Chapter 15 Model principles "],
["generate-null-models.html", "15.1 Generate null models", " 15.1 Generate null models "],
["generality-makes-models-vulnerable.html", "15.2 Generality makes models vulnerable", " 15.2 Generality makes models vulnerable "],
["specify-which-variables-are-manipulable.html", "15.3 Specify which variables are manipulable", " 15.3 Specify which variables are manipulable "],
["assign-over-vectors.html", "15.4 Assign over vectors", " 15.4 Assign over vectors "],
["inquiry-principles.html", "Chapter 16 Inquiry principles", " Chapter 16 Inquiry principles "],
["some-designs-have-badly-posed-questions-but-design-diagnosis-can-alert-you-to-the-problem.html", "16.1 Some designs have badly posed questions but design diagnosis can alert you to the problem", " 16.1 Some designs have badly posed questions but design diagnosis can alert you to the problem An obvious requirement of a good research design is that the question it seeks to answer does in fact have an answer, at least under plausible models of the world. Interestingly, we can sometimes get quite far along a research path without being conscious that the questions we ask do not have answers and the answers we get are answering different questions. Fortunately, computers complain when they are asked to answer badly posted questions. How could a question not have an answer? Answerless questions can arise when inquiries depend on variables that do not exist or are undefined for some units. Consider an audit experiment that seeks to assess the effects of an email from a Latino name (versus a White name) on whether and how well election officials respond to requests for information. For example, do they use a positive or negative tone. These questions seem reasonable enough. The problem, however, is that if there are officials who don’t send responses, tone is undefined. More subtly, if there is an official that does send an email but would not have sent it in a different treatment condition, then tone is undefined for one of their potential outcomes. 16.1.1 A design with sometimes undefined outcomes Here are the key parts to the design: Model: The model has two outcome variables, \\(R_i\\) and \\(Y_i\\). \\(R_i\\) stands for “response” and is equal to 1 if a response is sent, and 0 otherwise. \\(Y_i\\) is the tone of the response and is normally distributed when it is defined. \\(Z_i\\) is the treatment and equals 1 if the email is sent using a Latino name and 0 otherwise. The table below shows the potential outcomes for four possible types of subjects, depending on the potential outcomes of \\(R_i\\). A types respond if and only if they are not treated B types respond if and only if they are treated C types never respond, regardless of treatment D types always respond regardless of treatment The table also includes columns for the potential outcomes of \\(Y_i\\), showing which potential outcome subjects would express depending on their type. The key thing to note is that for the A, B, and C types, the effect of treatment on \\(Y_i\\) is undefined because messages never sent have no tone. The last (and very important) feature of our model is that the outcomes \\(Y_i\\) are possibly correlated with subject type. Even though both \\(E[Y_i(1) | \\text{Type} = D]\\) and \\(E[Y_i(1) | \\text{Type} = B]\\) exist, there’s no reason to expect that they are the same. Causal Types Type \\(R_i(0)\\) \\(R_i(1)\\) \\(E(Y_i(0))\\) \\(E(Y_i(1))\\) A 1 0 -1 NA B 0 1 NA -1 C 0 0 NA NA D 1 1 0 1 Inquiry: We have two inquiries. The first is straightforward: \\(E[R_i(1) - R_i(0)]\\) is the Average Treatment Effect on response. The second inquiry is the undefined inquiry that does not have an answer: \\(E[Y_i(1) - Y_i(0)]\\). We will also consider a third inquiry, which is defined: \\(E[Y_i(1) - Y_i(0) | \\mathrm{Type} = D]\\), which is the average effect of treatment on tone among \\(D\\) types. Data strategy: The data strategy will be to use complete random assignment to assign 250 of 500 units to treatment. Answer strategy: We’ll try to answer all three inquiries with the difference-in-means estimator. This design can be declared formally like this: # Model ------------------------------------------------------------------- population &lt;- declare_population( N = 500, noise = rnorm(N), type = sample(1:4, N, replace = TRUE, prob = c(0, 1/3, 1/3, 1/3)), A = type == 1, B = type == 2, C = type == 3, D = type == 4) pos &lt;- declare_potential_outcomes( R_Z_0 = A|D, # A and D types report in the control condition R_Z_1 = B|D, # B and D types report in the treatment condition Y_Z_0 = ifelse(R_Z_0, 0*D - 1*A + noise, NA), Y_Z_1 = ifelse(R_Z_1, 1*D - 1*B + noise, NA)) # Inquiry ----------------------------------------------------------------- estimands &lt;- declare_estimand( ATE_R = mean(R_Z_1 - R_Z_0), ATE_Y = mean(Y_Z_1 - Y_Z_0), ATE_Y_for_Ds = mean(Y_Z_1[D] - Y_Z_0[D])) # Data Strategy ----------------------------------------------------------- assignment &lt;- declare_assignment() reveal &lt;- declare_reveal(outcome_variables = c(&quot;R&quot;, &quot;Y&quot;)) # Answer Strategy --------------------------------------------------------- est1 &lt;- declare_estimator(R ~ Z, estimand = &quot;ATE_R&quot;, label = &quot;DIM_R&quot;) est2 &lt;- declare_estimator(Y ~ Z, estimand = list(&quot;ATE_Y&quot;, &quot;ATE_Y_for_Ds&quot;), label = &quot;DIM_Y&quot;) # Design ------------------------------------------------------------------ design &lt;- population + pos + estimands + assignment + reveal + est1 + est2 16.1.2 What we find from diagnosis Here is a diagnosis of this design: diagnosis &lt;- diagnose_design(design, sims = sims, bootstrap_sims = b_sims) Design Label Estimator Label Term N Sims Bias RMSE Power Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand design DIM_R Z 1000 0.00 0.04 1.00 0.98 0.33 0.04 0.04 0.00 0.33 (0.00) (0.00) (0.00) (0.00) (0.00) (0.00) (0.00) (0.00) (0.00) design DIM_Y Z 1000 NA NA 0.05 NA 0.00 0.15 0.16 NA NA NA NA (0.01) NA (0.00) (0.00) (0.00) NA NA design DIM_Y Z 1000 -1.00 1.01 0.05 0.00 0.00 0.15 0.16 0.57 1.00 (0.00) (0.00) (0.01) (0.00) (0.00) (0.00) (0.00) (0.07) (0.00) We learn three things from the design diagnosis. First, as expected, our experiment is unbiased for the average treatment effect on response. Second, our second inquiry, as well as our diagnostics for it, are undefined. The diagnosis tells us that our definition of potential outcomes produces a definition problem for the estimand. Note that some diagnosands are defined, including power. These are diagnosands that depend only on the answer strategy and not on the estimand. The third estimand – the average effects for the \\(D\\) types – is defined but our estimates are biased. The reason for this is that we cannot tell from the data which types are the \\(D\\) types: we are not conditioning on the correct subset. Indeed, we are unable to condition on the correct subset. If a subject responds in the treatment group, we don’t know if she is an \\(B\\) or a \\(D\\) type; in the control group, we can’t tell if a responder is an \\(A\\) or a \\(D\\) type. Our difference-in-means estimator of the ATE on \\(Y\\) among \\(D\\)s will be off whenever \\(D\\)s have different outcomes from \\(A\\)s and \\(B\\)s. There are some solutions to this problem. In some cases, the problem might be resolved by changing the inquiry. Closely related estimands can often be defined, perhaps by redefining \\(Y\\) (e.g., emails never sent have a tone of zero). Some redefinitions of the problem, as in the one we examine above, require estimating effects for unobserved subgroups which is a difficult challenge. 16.1.3 Other instances of this problem This kind of problem is surprisingly common. Here are three more instances of the problem: \\(Y\\) is the decision to vote Democrat (\\(Y=1\\)) or Republican (\\(Y=0\\)), \\(R\\) is the decision to turn out to vote and \\(Z\\) is a campaign message. The decision to vote may depend on treatment but if subjects do not vote then \\(Y\\) is undefined. \\(Y\\) is the weight of infants, \\(R\\) is whether a child is born and \\(Z\\) is a maternal health intervention. Fertility may depend on treatment but the weight of unborn (possibly never conceived) babies is not defined. \\(Y\\) is the charity to whom contributions are made during fundraising and \\(R\\) is whether anything is contributed and \\(Z\\) is an encouragement to contribute. The identity of beneficiaries is not defined if there are no contributions. All of these problem exhibit a form of post treatment bias (see section Post treatment bias) but the issue goes beyond picking the right estimator. Our problem here is conceptual: the effect of treatment on the outcome just doesn’t exist for some subjects. 16.1.4 Keep thinking Some puzzles: The amount of bias on the third estimand depends on both the distribution of types and the correlation of types with the potential outcomes of Y. Modify the declaration so that the estimator of the effect on Y is unbiased, changing only the distribution of types. Repeat the exercise, changing only the relation between the types and the potential outcomes of \\(Y\\). Try approaching the problem by redefining the inquiry, seeking to assess the effect of treatment on the share of responses with positive tone. "],
["there-is-no-causation-without-manipulation.html", "16.2 There is no causation without manipulation", " 16.2 There is no causation without manipulation "],
["know-what-ate-averages-over.html", "16.3 Know what ATE averages over", " 16.3 Know what ATE averages over "],
["know-what-is-local-in-a-late.html", "16.4 Know what is local in a LATE", " 16.4 Know what is local in a LATE "],
["know-when-sates-are-informative-for-pates.html", "16.5 Know when SATEs are informative for PATEs", " 16.5 Know when SATEs are informative for PATEs "],
["inquiry-strategies-before-inquiries-are-defined.html", "16.6 Inquiry strategies before inquiries are defined", " 16.6 Inquiry strategies before inquiries are defined "],
["data-principles.html", "Chapter 17 Data Principles", " Chapter 17 Data Principles "],
["why-systematic-data-collection-is-not-random-sampling.html", "17.1 Why systematic data collection is not random sampling", " 17.1 Why systematic data collection is not random sampling "],
["keep-treatment-and-control-groups-parallel.html", "17.2 Keep treatment and control groups parallel", " 17.2 Keep treatment and control groups parallel "],
["how-blocking-improves-precision.html", "17.3 How blocking improves precision", " 17.3 How blocking improves precision "],
["how-clusters-reduce-precision.html", "17.4 How clusters reduce precision", " 17.4 How clusters reduce precision "],
["answer-strategies-matter-for-power.html", "17.5 Answer strategies matter for power", " 17.5 Answer strategies matter for power "],
["allocating-treatments-at-extremes-can-improve-precision.html", "17.6 Allocating treatments at extremes can improve precision", " 17.6 Allocating treatments at extremes can improve precision "],
["gains-from-multiple-measurements.html", "17.7 Gains from multiple measurements", " 17.7 Gains from multiple measurements "],
["gains-from-continuous-measures.html", "17.8 Gains from continuous measures", " 17.8 Gains from continuous measures "],
["factorial-designs-can-be-more-powerful-than-multiarm-designs.html", "17.9 Factorial designs can be more powerful than multiarm designs", " 17.9 Factorial designs can be more powerful than multiarm designs "],
["allocating-treatments-at-extremes-can-improve-precision-1.html", "17.10 Allocating treatments at extremes can improve precision", " 17.10 Allocating treatments at extremes can improve precision "],
["balance-tests-tests-the-null-of-randomization-not-the-null-of-balance.html", "17.11 Balance tests tests the null of randomization, not the null of balance", " 17.11 Balance tests tests the null of randomization, not the null of balance "],
["assess-imbalance-on-substantive-rather-than-statistical-grounds.html", "17.12 Assess imbalance on substantive rather than statistical grounds", " 17.12 Assess imbalance on substantive rather than statistical grounds "],
["the-dangers-of-the-big-stick.html", "17.13 The dangers of the big stick", " 17.13 The dangers of the big stick "],
["answer-principles.html", "Chapter 18 Answer principles", " Chapter 18 Answer principles "],
["use-controls-for-prognostic-pretreatment-covariates.html", "18.1 Use controls for prognostic pretreatment covariates", " 18.1 Use controls for prognostic pretreatment covariates "],
["conditioning-on-post-treatment-variables-can-introduce-bias.html", "18.2 Conditioning on post treatment variables can introduce bias", " 18.2 Conditioning on post treatment variables can introduce bias "],
["even-pretreatment-controls-can-introduce-bias-in-observational-analysis.html", "18.3 Even pretreatment controls can introduce bias in observational analysis", " 18.3 Even pretreatment controls can introduce bias in observational analysis "],
["heterogeneous-treatment-propensities-cannot-be-ignored.html", "18.4 Heterogeneous treatment propensities cannot be ignored", " 18.4 Heterogeneous treatment propensities cannot be ignored "],
["the-source-of-uncertainty-matters-for-inference.html", "18.5 The source of uncertainty matters for inference", " 18.5 The source of uncertainty matters for inference "],
["neyman-variance-is-conservative-for-sample-variance.html", "18.6 Neyman variance is conservative for sample variance", " 18.6 Neyman variance is conservative for sample variance "],
["cluster-standard-errors-at-the-level-of-treatment-assignment-for-the-sate.html", "18.7 Cluster standard errors at the level of treatment assignment for the SATE", " 18.7 Cluster standard errors at the level of treatment assignment for the SATE "],
["test-selection-should-take-account-of-both-validity-and-power.html", "18.8 Test selection should take account of both validity and power", " 18.8 Test selection should take account of both validity and power "],
["randomization-does-not-justify-the-t-distribution.html", "18.9 Randomization does not justify the t-distribution", " 18.9 Randomization does not justify the t-distribution "],
["you-can-check-whether-confidence-intervals-are-correct-given-the-model.html", "18.10 You can check whether confidence intervals are correct, given the model", " 18.10 You can check whether confidence intervals are correct, given the model "],
["no-evidence-of-an-effect-is-not-evidence-of-no-effect-except-for-bayesians.html", "18.11 No evidence of an effect is not evidence of no effect, except for Bayesians", " 18.11 No evidence of an effect is not evidence of no effect, except for Bayesians "],
["cross-mida-principles.html", "Chapter 19 Cross MIDA principles", " Chapter 19 Cross MIDA principles "],
["as-ye-randomize-so-shall-ye-analyze.html", "19.1 As ye randomize so shall ye analyze", " 19.1 As ye randomize so shall ye analyze "],
["selective-reporting-introduces-bias.html", "19.2 Selective reporting introduces bias", " 19.2 Selective reporting introduces bias "],
["there-is-often-a-bias-variance-tradeoff.html", "19.3 There is often a bias-variance tradeoff", " 19.3 There is often a bias-variance tradeoff "],
["spillovers-are-a-design-challenge.html", "19.4 Spillovers are a design challenge", " 19.4 Spillovers are a design challenge "],
["identify-primary-questions.html", "19.5 Identify primary questions", " 19.5 Identify primary questions "],
["check-your-models.html", "19.6 Check your models", " 19.6 Check your models "],
["workflows.html", "Chapter 20 Workflows", " Chapter 20 Workflows "],
["design-development.html", "20.1 Design development", " 20.1 Design development "],
["design-registration-and-reconciliation.html", "20.2 Design Registration and Reconciliation", " 20.2 Design Registration and Reconciliation "],
["critique.html", "Chapter 21 Critique", " Chapter 21 Critique "],
["new-principles-for-replication-and-critique.html", "21.1 New principles for replication and critique", " 21.1 New principles for replication and critique "],
["polemics.html", "21.2 Polemics", " 21.2 Polemics "],
["community.html", "Chapter 22 Community", " Chapter 22 Community "],
["evaluating-and-supporting-research.html", "22.1 Evaluating and supporting research", " 22.1 Evaluating and supporting research "],
["a-library-of-designs.html", "22.2 A library of designs", " 22.2 A library of designs "],
["ajr-example.html", "22.3 AJR example", " 22.3 AJR example "],
["references-8.html", "References", " References "]
]
