[
["index.html", "Research Design: Declare, Diagnose, Redesign Welcome", " Research Design: Declare, Diagnose, Redesign Graeme Blair, Jasper Cooper, Alexander Coppock, and Macartan Humphreys Welcome Draft book manuscript. Comments welcomed. Please treat with caution. © 2019 Graeme Blair, Jasper Cooper, Alexander Coppock, and Macartan Humphreys "],
["preamble.html", "Chapter 1 Preamble", " Chapter 1 Preamble With this book, we hope to promote a new, comprehensive way of thinking about research designs in the social sciences. We hope this way of thinking will make research designs more transparent and more robust. But we also hope it will make research design easier, easier to produce good designs, but also easier to share designs and build off of the designs that others have developed. The core idea is to start think of a design as an object that can be interrogated. The design encodes your beliefs about the world, it describes your questions, and it lays out how you go about answering those questions, in terms both of what data you use and how you use it. A key idea is that all of these features can be provided in code and if done right the information provided is enough to be able to simulate a run of the design and assess its properties. For a researcher, being able to simulate a design puts you in a powerful position as you can then start assessing the conditions under which a design perfoms well or badly. For a reader, a complete declaration lets you quickly examine the analytic core of a design. As you work through designs in this book we hope you will develop a nose for quickly parsing what goes in the model, what are the kinds of inquiries you should expect to see, what are the data strategies, what are analysis strategies, and how these all link together. "],
["improving-research-designs.html", "Chapter 2 Improving research designs", " Chapter 2 Improving research designs As empirical social scientists, we routinely face two research design problems. First, we need to select high-quality designs, given financial and practical constraints. Second, we need to communicate those designs to readers and reviewers. To select strong designs, we often rely on rules of thumb, simple power calculators, or principles from the methodological literature that typically address one component of a design while assuming optimal conditions for others. These relatively informal practices can result in the selection of suboptimal designs, or worse, designs that are simply too weak to deliver useful answers. To convince others of the quality of our designs, we often defend them with references to previous studies that used similar approaches, with power analyses that may rely on assumptions unknown even to ourselves, or with ad hoc simulation code. In cases of dispute over the merits of different approaches, disagreements sometimes fall back on first principles or epistemological debates rather than on demonstrations of the conditions under which one approach does better than another. In this paper we describe an approach to address these problems. We introduce a framework — MIDA — that asks researchers to specify information about their background model (M), their inquiry (I), their data strategy (D), and their answer strategy (A). We then introduce the notion of “diagnosands,” or quantitative summaries of design properties. Familiar diagnosands include statistical power, the bias of an estimator with respect to an estimand, or the coverage probability of a procedure for generating confidence intervals. Many aspects of design quality can be assessed through design diagnosis, but many cannot. For instance the contribution to an academic literature, relevance to a policy decision, and impact on public debate are unlikely to be quantifiable ex ante. Using this framework, researchers can declare a research design as a computer code object and then diagnose its statistical properties on the basis of this declaration. We emphasize that the term “declare” does not imply a public declaration or even necessarily a declaration before research takes place. A researcher may declare the features of designs in our framework for their own understanding and declaring designs may be useful before or after the research is implemented. Researchers can declare and diagnose their designs with the companion software for this paper, DeclareDesign, but the principles of design declaration and diagnosis do not depend on any particular software implementation. The formal characterization and diagnosis of designs before implementation can serve many purposes. First, researchers can learn about and improve their inferential strategies. Done at this stage, diagnosis of a design and alternatives can help a researcher select from a range of designs, conditional upon beliefs about the world. Later, a researcher may include design declaration and diagnosis as part of a preanalysis plan or in a funding request. At this stage, the full specification of a design serves a communication function and enables third parties to understand a design and an author’s intentions. Even if declared ex-post, formal declaration still has benefits. The characterization can help readers understand the properties of a research project, facilitate transparent replication, and can help guide future (re-)analysis of the study data. Formally declaring research designs as objects in the manner we describe here brings, we hope, four benefits. It can facilitate the diagnosis of designs in terms of their ability to answer the questions we want answered under specified conditions; it can assist in the improvement of research designs through comparison with alternatives; it can enhance research transparency by making design choices explicit; and it can provide strategies to assist principled replication and reanalysis of published research. "],
["what-is-a-research-design.html", "2.1 What is a research design?", " 2.1 What is a research design? We present a general description of a research design as the specification of a problem and a strategy to answer it. We build on two influential research design frameworks. (King, Keohane, and Verba 1994, 13) enumerate four components of a research design: a theory, a research question, data, and an approach to using the data. Geddes (2003) articulates the links between theory formation, research question formulation, case selection and coding strategies, and strategies for case comparison and inference. In both cases, the set of components are closely aligned to those in the framework we propose. We characterize the design problem at a high level of generality with the central focus being on the relationship between questions and answer strategies. A declaration of a research design comprises four components: a causal model of the world, which defines a set of units we want to learn about, a set of variables that represent characteristics of the units, and how those variables interact; a research inquiry, which is a function of variables defined in the model; a data strategy to intervene in the world to learn an answer to the inquiry, including interventions that are implemented but also measurement that is collected; and an answer strategy that defines how we construct an answer to the inquiry from the data that results from implementing our data strategy. The four together, which we refer to as MIDA, represent both your suppositions about how the world works and the choices you make as the researcher to intervene in and learn about the world. The model defines a set of units, people or neighborhoods or social groups, that we wish to study. Often, this set of units is a large population of units we cannot afford to take measurements of, but we can nevertheless define and make inferences about through sampling and studying a subset of its units. The model then includes a set of baseline characteristics that describe each unit and the probability distributions of each characteristic (i.e., are heights normally distributed, or is there skew that comes from stunting in infants). Finally, the model includes a set of endogenous outcome variables that may be functions of exogenous (pretreatment) characteristics and the effects of interventions. Each endogenous outcome variable has a function that defines the variables that affect what values it takes on. When an outcome depends on an intervention, it will be a potential outcome, where we can define what value the outcome would take on if a unit received the treatment and what outcome that unit would take on if it did not receive the treatment. Typically, endogenous outcome variables are random variables, either because it is a function of an exogenous baseline variable for which we defined a probability distributions or because whether a unit is assigned to treatment or control is randomly assigned as part of the data strategy. In an experiment in two districts in Pakistan, Gulzar and Khan (n.d.) study how to motivate citizens to run for political office for the first time. Thus, their set of units is all citizens in Haripur and Abbottabad, about whom they consider baseline policy preferences, in order to characterize how different the policy preferences of citizens are from those eventually elected. Their endogenous outcomes of interest are whether the citizen filed papers to run for office; was elected; and the Euclidean distance from average citizen preferences. The outcomes are a function in their causal model of three treatments, which emphasize either the social or personal benefits to holding public office or do not encourage anyone to run for office at all. Their model would include an expected treatment effect magnitude for each treatment and their suppositions about how correlated outcomes are within villages that they study. Defining the model can feel like an odd exercise. Since researchers presumably want to learn about the model, declaring it in advance may seem to beg the question. Yet declaring a model is often unavoidable when declaring reserach designs. In practice, doing so is already familiar to any researcher who has calculated the power of a design, which requires the specification of effect sizes. The seeming arbitrariness of the declared model can be mitigated by assessing the sensitivity of diagnosis to alternative models and strategies (see Section XX). Further, researchers can inform their substantive models with existing data, such as baseline surveys. Just as power calculators focus attention on minimum detectable effects, design declaration offers a tool to demonstrate design properties and how they change depending on researcher assumptions. The second component of a research design is the inquiry, often known as the estimand. This is a quantity that represents the true answer to the research question we are asking. Inquiries may be causal, as in the average treatment effect, or descriptive, as in the proportion of units who hold a certain characteristic. An inquiry is a function of the exogenous characteristics of units, of endogenous outcome variables, or both. It may be defined over all units in the population defined by the model, as in the average treatment effect for all units, or it may be defined over a subset of units, as in the conditional average treatment effect for women. Because we defined the distribution of the variables in the model, we can define the probability distribution of inquiries, which are a function of those variables. In the Gulzar and Khan (n.d.) study, the inquiry is the average difference in the rate of filing papers to run for office between people living in villages randomly assigned to receive an encouragement to vote focused on prosocial motivations for officeholding compared to those to whom personal benefits were emphasized. The data strategy defines how the researcher, along with research partners, intervenes in the world to generate an answer to the question posed by the inquiry. The researcher must select a sampling strategy for measurement, which could be to take measurements of all units or to first sample a subset of units. It includes the treatments and treatment assignment procedures. And it includes the measurement procedure itself, defining the set of survey questions or administrative data that will be collected from selected units. In short, the data strategy is everything the researcher does to obtain a set of data or observations used to answer the inquiry about the world. The data strategy in Gulzar and Khan (n.d.) entailed three steps: (1) randomly sampling 192 villages from among all villages in Haripur and Abbottabad districts, and using a random walk procedure to select 48 citizens in each village to participate in the experiment; (2) randomly-assigning each village with equal probability to one of three conditions (neutral, social benefits, or personal benefits); and (3) collecting administrative data on who filed papers to run and matching that back to pretreatment survey data on the 9,216 citizens. Sampling, treatment assignment, and measurement are the three common data strategy steps in an experiment; some experiments, instead, do not include a sampling step and instead assign treatments within a convenience sample. With the data that results from the data strategy, the answer strategy defines a set of procedures the researcher uses to translate the data into an answer to the inquiry. It is not simply the choice of an estimator, such as OLS or logit, but the full set of procedures from receiving the dataset to providing the answer in words, tables, and graphs. This includes data cleaning, data transformation, estimation, plotting, and interpretation. Not only the choice of OLS must be defined, but that we will focus attention on the coefficient estimate from the Z variable and assess uncertainty using a confidence interval and construct a coefficient plot in a certain way to visualize the inference. The answer strategy also includes all of the if-then procedures that researchers implicitly or explicitly take depending on initial results and features of the data. In a stepwise regression procedure, the answer strategy is not the final regression that results from iterative model selection, but that procedure itself – because the answer will depend on features of that change depending on sampling variability. Just like the values of the inquiry, the values of the estimates that result from the answer strategy have a probability distribution, because they are the result of the variables defined in the model (which have probability distributions) and the data strategy (sampling and treatment assignment are defined by probability distributions). Gulzar and Khan (n.d.) have a two-step answer strategy, fitting a linear model predicting whether a citizen ran for office (the outcome) with indicators for the social benefits and the personal benefits treatments, and then calculating the difference between the two coefficients as an estimate of whether social benefits are more or less effective than the personal benefits. Their answer strategy includes presenting a table with estimated difference, a standard error clustered on village to account for village-level random assignment, and a p-value calculated using permutation inference. MIDA captures the analysis-relevant features of a design, but it does not describe substantive elements, such as how theories are derived, how interventions are implemented, or even, qualitatively, how outcomes are measured. Yet many other aspects of a design that are not explicitly labeled in these features enter into this framework if they are analytically relevant. For example, if treatment effects decay, logistical details of data collection (such as the duration of time between a treatment being administered and endline data collection) may enter into the model. Similarly, if a researcher anticipates noncompliance, substantive knowledge of how treatments are taken up can be included in many parts of the design. Declaring a design is just like writing out a recipe. You can cook without writing out a recipe, but when you do, you can think through the whole process start to finish, you can critique the process, and you can modify it. References "],
["how-do-we-assess-the-quality-of-a-design.html", "2.2 How do we assess the quality of a design?", " 2.2 How do we assess the quality of a design? The ability to calculate distributions of answers, given a model, opens multiple avenues for assessment and critique. How good is the answer you expect to get from a given strategy? Would you do better, given some desideratum, with a different data strategy? With a different analysis strategy? How good is the strategy if the model is wrong in one way or another? To allow for this kind of of a design, we introduce two further concepts, both functions of research designs. These are quantities that a researcher or a third party could calculate with respect to a design. The first is a diagnostic statistic, which is a summary statistic generated from a “run” of a design—that is, the results given a possible realization of variables, given the model and data strategy. For example the statistic: \\(e=\\) “difference between the estimated and the actual average treatment effect” depends on the model (since the ATE depends on the model’s assumptions about potential outcomes). The statistic \\(s = \\mathbb{1}(p \\leq 0.05)\\), interpreted as “the result is considered statistically significant at the 5% level,” does not depend on the model but it does presuppose an answer strategy that reports a \\(p\\)-value. Diagnostic statistics are governed by probability distributions that arise because both the model and the data generation, given the model, may be stochastic. Second, a diagnosand is a summary of the distribution of a diagnostic statistic. For example, (expected) in the estimated treatment effect is \\(\\mathbb{E}(e)\\) and statistical is \\(\\mathbb{E}(s)\\). A design that can be declared in computer code can then be simulated in order to diagnose its properties. The approach to declaration that we advocate is one that conceives of a design as a set of steps, first defining the model, through a set of variable definitions; then the inquiry, a function of variables in the model; the data strategy, including functions to sample units and randomly assign treatments, if any; and the answer strategy, an estimation function that generates an estimated answer to the inquiry. A single simulation runs through these steps, calling each of these functions successively. A design diagnosis conducts \\(m\\) simulations, then summarizes the resulting distribution of diagnostic statistics in order to estimate the diagnosand. Diagnosands can be estimated with higher levels of precision by increasing \\(m\\). However, simulations are often computationally expensive. In order to assess whether researchers have conducted enough simulations to be confident in their diagnosand estimates, we recommend estimating the sampling distributions of the diagnosands via the nonparametric bootstrap.1 With the estimated diagnosand and its standard error, we can characterize our uncertainty about whether the range of likely values of the diagnosand compare favorably to reference values such as statistical power of \\(0.8\\).2 Diagnosis is an opportunity to write down what would make the study a success. We want to design in order to make success more likely. For a long time, researchers have classified studies as successful or not based on statistical significance. Accordingly, statistical power (the probability of a statistically significant result) has been the most front-of-mind diagnosand when researchers have set to designing studies. As we learn more about the pathologies of relying on the statistical significance, we learn that diagnosands beyond power are just as, if not more important. For example, the “credibility revolution” throughout social science has trained a laser-like focus on the bias diagnosand. Studies are coming under new criticism for lacking “strong identification,” which usually implies that the data and answer strategies could lead to biased answers depending on how incorrect the model is. Randomized experimentation promises unbiased answers, at least when the data and answer strategies are implemented well. What matters most in your research scenario? Is it statistical significance? If so, optimize your design with respect to power. Is what matters most in your research setting with the answer has the correct sign or not? Then diagnose how frequently your answer strategy yields an answer with the same sign as your inquiry. Diagnosis is an opportunity for you to articulate what would make your study a success and then to simulate how often you obtain that success. In their paper on simulating clinical trials through Monte Carlo, (???) provide helpful analytic formula for deriving Monte Carlo standard errors for several diagnosands (“performance measures”). In the companion software, we adopt a non-parametric bootstrap approach that is able to calculate standard errors for any user-provided diagnosand.↩ This procedure depends on the researcher choosing a “good” diagnosand estimator. In nearly all cases, diagnosands will be features of the distribution of a diagnostic statistic that, given i.i.d. sampling, can be consistently estimated via plug-in estimation (for example taking sample means). Our simulation procedure, by construction, yields i.i.d. draws of the diagnostic statistic.↩ "],
["how-can-we-best-plan-for-research.html", "2.3 How can we best plan for research?", " 2.3 How can we best plan for research? The subtitle of this book is “Declaration, Diagnosis, Redesign” to emphasize three important steps in the conceptualization of a research design. So far, we’ve outlined the first two points. First, you declare your design. Declaring a design entails separating out which parts of your idea belong in \\(M\\), \\(I\\), \\(D\\), and \\(A\\). The declaration process can be a challenge because mapping your ideas and excitement about your project into MIDA is not always straightforward. We promise it is a rewarding task. When you can express your research design in terms of these four components you are newly able to think about its properties. Once you’ve declared your design, you can diagnose it. Design diagnosis is the process of simulating your research design in order to understand the range of possible ways the study could turn out. It is in the diagnosis stage that we define the design properties that are most desirable in our research setting. We let computers do the simulations for us because imagining how design choices influence sampling distributions is — to put it lightly — cognitively demanding. The third step is redesign. Once your design has been declared, and you have learned to diagnose it with respect to the most important diagnosands, it is time to play with each of the design parameters to understand the implications of each for your most important diagnosands. This can mean a variety of things. Many diagnosands (power, RMSE) depend on the size of the study. We can redesign the study, varying the “sample size” feature of the data strategy to determine how big it needs to be to achieve a target diagnosand: 90% power, say, or an RMSE of 0.02. We could also vary an aspect of the answer strategy, say, the covariates used to adjust a regression model. Sometimes the changes to the data and answer strategies interact: if we use better covariates to increase the precision of the estimates in the answer strategy, we have to collect that information as a part of the data strategy. The redesign question now becomes, is it better to collect pre-treatment information from all subjects or is the money better spent on increasing the total number of subjects? Finally, redesign sometimes means changing the model. That is, sometimes we want to understand whether our design yields the right inferences even when the underlying data generating processes shift beneath our feet. In summary, redesign entails enumerating a set of possible designs given resource and theoretical constraints then picking the best one. In this book, we are asking that scholars add a new step to their workflow. We want scholars to formally declare and diagnose their research designs both in order to learn about them and to improve them. Much of the work of declaring and diagnosing designs is already part of how social scientists conduct research: grant proposals, IRB protocols, preanalysis plans, and dissertation prospectuses contain design information and justifications for why the design is appropriate for the question. The lack of a common language to describe designs and their properties, however, seriously hampers the utility of these practices for assessing and improving design quality. We hope that the inclusion of a declaration-diagnosis-redesign step to the research process can help address this basic difficulty. "],
["implications.html", "2.4 Implications", " 2.4 Implications We outline three phases of the scientific process during which the MIDA and declaration-diagnosis-redesign framework can assist study authors, readers, and research funders. Making design choices. The move towards increasing credibility of research in the social sciences places a premium on considering alternative data strategies and analysis strategies at early stages of research projects, not only because it reduces researcher discretion, but more importantly because it can improve the quality of the final research design. While there is nothing new about the idea of determining features such as sampling and estimation strategies ex ante, in practice many designs are finalized late in the research process, after data are collected. Frontloading design decisions is difficult not only because existing tools are rudimentary and often misleading, but because it is not clear in current practice what features of a design must be considered ex ante. We provide a framework for identifying which features affect the assessment of a design’s properties, declaring designs and diagnosing their inferential quality, and frontloading design decisions. Declaring the design’s features in code enables direct exploration of alternative data and analysis strategies using simulated data; evaluating alternative strategies through diagnosis; and exploring the robustness of a chosen strategy to alternative models. Researchers can undertake each step before study implementation or data collection. Communicating design choices. Bias in published results can arise for many reasons. For example, researchers may deliberately or inadvertently select analysis strategies because they produce statistically significant results. Proposed solutions to reduce this kind of bias focus on various types of preregistration of analysis strategies by researchers (Rennie 2004; Zarin and Tse 2008; Casey, Glennerster, and Miguel 2012; Nosek et al. 2015; Green and Lin 2016). Study registries are now operating in numerous areas of social science, including those hosted by the American Economic Association, Evidence in Governance and Politics, and the Center for Open Science. Bias may also arise from reviewers basing publication recommendations on statistical significance. Results-blind review processes are being introduced in some journals to address this form of bias (e.g. Findley et al. 2016). However, the effectiveness of design registries and results-blind review in reducing the scope for either form of publication bias depends on clarity over which elements must be included to describe the design. In practice, some registries rely on checklists and preanalysis plans exhibit great variation, ranging from lists of written hypotheses to all-but-results journal articles. In our view, the solution to this problem does not lie in ever-more-specific questionnaires, but rather in a new way of characterizing designs whose analytic features can be diagnosed through simulation. The actions to be taken by researchers are described by the data strategy and the answer strategy; these two features of a design are clearly relevant elements of a preregistration document. In order to know which design choices were made ex ante and which were arrived at ex post, researchers need to communicate their data and answer strategies unambiguously. However, assessing whether the data and answer strategies are any good usually requires specifying a model and an inquiry. Design declaration can clarify for researchers and third parties what aspects of a study need to be specified in order to meet standards for effective preregistration. Declaration of a design in code also enables a final and infrequently practiced step of the registration process, in which the researcher ``reports and reconciles’’ the final with the planned analysis. Identifying how and whether the features of a design diverge between ex ante and ex post declarations highlights deviations from the preanalysis plan. The magnitude of such deviations determines whether results should be considered exploratory or confirmatory. At present, this exercise requires a review of dozens of pages of text, such that differences (or similarities) are not immediately clear even to close readers. Reconciliation of designs declared in code can be conducted automatically, by comparing changes to the code itself (e.g., a move from the use of a stratified sampling function to simple random sampling) and by comparing key variables in the design such as sample sizes. Challenging Design Choices. The independent replication of the results of studies after their publication is an essential component of the shift toward more credible science. Replication — whether verification, reanalysis of the original data, or reproduction using fresh studies — provides incentives for researchers to be clear and transparent in their analysis strategies, and can build confidence in findings.3 In addition to rendering the design more transparent, design declaration can allow for a different approach to the re-analysis and critique of published research. A standard practice for replicators engaging in reanalysis is to propose a range of alternative strategies and assess the robustness of the data-dependent estimates to different analyses. The problem with this approach is that, when divergent results are found, third parties do not have clear grounds to decide which results to believe. This issue is compounded by the fact that, in changing the analysis strategy, replicators risk departing from the estimand of the original study, possibly providing different answers to different questions. In the worst case scenario, it can be difficult to determine what is learned both from the original study and from the replication. A more coherent strategy facilitated by design simulations would be to use a design declaration to conduct “design replication.” In a design replication, a scholar restates the essential design characteristics to learn about what the study could have revealed, not just what the original author reports was revealed. This helps to answer the question: under what conditions are the results of a study to be believed? By emphasizing abstract properties of the design, design replication provides grounds to support alternative analyses on the basis of the original authors’ intentions and not on the basis of the degree of divergence of results. Conversely, it provides authors with grounds to question claims made by their critics. References "],
["limitations.html", "2.5 Limitations", " 2.5 Limitations Designing high quality research is difficult and comes with many pitfalls, only a subset of which are ameliorated by the MIDA framework. Others we fail to address entirely and in some cases, we may even exacerbate them. We outline four concerns. The first is the worry that evaluative weight could get placed on essentially meaningless diagnoses. Given that design declaration includes declarations of conjectures about the world it is possible to choose inputs so that a design passes any diagnostic test set for it. For instance, a simulation-based claim to unbiasedness that incorporates all features of a design is still only good with respect to the precise conditions of the simulation (in contrast, analytic results, when available, may extend over general classes of designs). Still worse, simulation parameters might be selected because of their properties. A power analysis, for instance, may be useless if implausible parameters are chosen to raise power artificially. While MIDA may encourage more honest declarations, there is nothing in the framework that enforces them. As ever, garbage-in, garbage-out. Second, we see a risk that research may get evaluated on the basis of a narrow, but perhaps inappropriate set of diagnosands. Statistical power is often invoked as a key design feature – but even well-powered studies that are biased away from their targets of interest are of little theoretical use. The appropriateness of the diagnosand depends on the purposes of the study. As MIDA is silent on the question of a study’s purpose, it cannot guide researchers or critics to the appropriate set of diagnosands by which to evaluate a design. An advantage of the approach however is that the choice of diagnosands gets highlighted and new diagnosands can be generated in response to substantive concerns. Third, emphasis on the statistical properties of a design can obscure the substantive importance of a question being answered or other qualitative features of a design. A similar concern has been raised regarding the ``identification revolution’’ where a focus on identification risks crowding out attention to the importance of questions being addressed . Our framework can help researchers determine whether a particular design answers a question well (or at all), and it also nudges them to make sure that their questions are defined clearly and . It cannot, however, help researchers choose good questions. Finally, we see a risk that the variation in the suitability of design declaration to different research strategies may be taken as evidence of the relative superiority of different types of research strategies. While we believe that the range of strategies that can be declared and diagnosed is wider than what one might at first think possible, there is no reason to believe that all strong designs can be declared either ex ante or ex post. An advantage of our framework, we hope, is that it can help clarify when a strategy can or cannot be completely declared. When a design cannot be declared, nondeclarability is all the framework provides, and in such cases we urge caution in drawing conclusions about design quality. "],
["appendix-a-formal-definition-of-a-research-design-and-design-diagnosis.html", "2.6 Appendix: A formal definition of a research design and design diagnosis", " 2.6 Appendix: A formal definition of a research design and design diagnosis We formally define a research design. In doing so, we employ elements from Pearl’s (2009) approach to structural modeling, which provides a syntax for mapping design inputs to design outputs as well as the potential outcomes framework as presented, for example, in Imbens and Rubin (2015), which many social scientists use to clarify their inferential targets. We think of a research design, \\(\\Delta\\), as including four elements \\(&lt;M,I,D,A&gt;\\): A model, \\(M\\), of how the world works. In general following Pearl’s definition of a probabilistic causal model we will assume that a model contains three core elements. First, a specification of the variables \\(X\\) about which research is being conducted. This includes endogenous and exogenous variables (\\(V\\) and \\(U\\) respectively) and the ranges of these variables. In the formal literature this is sometimes called the signature of a model (Halpern 2000). Second, a specification of how each endogenous variable depends on other variables (the “functional relations” or, as in Imbens and Rubin (2015), “potential outcomes”), \\(F\\). Third, a probability distribution over exogenous variables, \\(P(U)\\). An inquiry, \\(I\\), about the distribution of variables, \\(X\\), perhaps given interventions on some variables. Using Pearl’s notation we can distinguish between questions that ask about the conditional values of variables, such as \\(\\Pr(X_1 | X_2 =1)\\) and questions that ask about values that would arise under interventions: \\(\\Pr(X_1 | do(X_2 = 1))\\). We let \\(a^M\\) denote the answer to \\(I\\) . Conditional on the model, \\(a^M\\) is the value of the estimand, the quantity that the researcher wants to learn about. A data strategy, \\(D\\), generates data \\(d\\) on \\(X\\). Data \\(d\\) arises, under model \\(M\\) with probability \\(P_M(d|D)\\). The data strategy includes sampling strategies and assignment strategies, which we denote with \\(P_S\\) and \\(P_Z\\) respectively. Measurement techniques are also a part of data strategies and can be thought of as a selection of observable variables that carry information about unobservable variables. An answer strategy, \\(A\\), that generates answer \\(a^A\\) using data \\(d\\). A key feature of this bare specification is that if \\(M\\), \\(D\\), and \\(A\\) are sufficiently well described, the answer to question \\(I\\) has a distribution \\(P_M(a^A|D)\\). Moreover, one can construct a distribution of comparisons of this answer to the correct answer, under \\(M\\), for example by assessing \\(P_M(a^M-a^A|D)\\). One can also compare this to results under different data or analysis strategies, \\(P_M(a^M-a^A|D&#39;)\\) and \\(P_M(a^M-a^{A&#39;}|D)\\), and to answers generated under alternative models, \\(P_M(a^{M&#39;}-a^{A}|D)\\), as long as these possess signatures that are consistent with inquiries and answer strategies. MIDA captures the analysis-relevant features of a design, but it does not describe substantive elements, such as how theories are derived or interventions are implemented. Yet many other aspects of a design that are not explicitly labeled in these features enter into this framework if they are analytically relevant. For example, logistical details of data collection such as the duration of time between a treatment being administered and endline data collection enter into the model if the longer time until data collection affects subject recall of the treatment. However, information in MIDA is typically insufficient to assess those substantive elements, an important and separate part of assessing the quality of a research study. The ability to calculate distributions of answers, given a model, opens multiple avenues for assessment and critique. How good is the answer you expect to get from a given strategy? Would you do better, given some desideratum, with a different data strategy? With a different analysis strategy? How good is the strategy if the model is wrong in some way or another? To allow for this kind of diagnosis of a design, we introduce two further concepts, both functions of research designs. These are quantities that a researcher or a third party could calculate with respect to a design. A Diagnostic Statistic is a summary statistic generated from a “run” of a design—that is, the results given a possible realization of variables, given the model and data strategy. A diagnostic statistic may or may not depend on the model as well as realized data. For example the statistic: \\(e=\\) “difference between the estimated and the actual average treatment effect” depends on the model (since the ATE depends on the model’s assumptions about potential outcomes). The statistic \\(s = \\mathbb{1}(p \\leq 0.05)\\), interpreted as “the result is considered statistically significant at the 5% level”,’’ does not depend on the model but it does presuppose an answer strategy that reports a \\(p\\) value. Diagnostic statistics are governed by probability distributions that arise because both the model and the data generation, given the model, may be stochastic. A Diagnosand is a summary of the distribution of a diagnostic statistic. For example, (expected) in the estimated treatment effect is \\(\\mathbb{E}(e)\\) and statistical is \\(\\mathbb{E}(s)\\). To illustrate, consider the following design. A model M specifies three variables \\(X\\), \\(Y\\) and \\(Z\\) (all defined on the reals). These form the signature. In additional we assume functional relationships between them that allow for the possibility of confounding (for example, \\(Y = bX + Z + \\epsilon_Y; X = Z+ \\epsilon_X\\), with \\(Z, \\epsilon_X, \\epsilon_Z\\) distributed standard normal). The inquiry \\(I\\) is ``what would be the average effect of a unit increase in \\(X\\) on \\(Y\\) in the population?’’ Note that this question depends on the signature of the model, but not the functional equations of the model (the answer provided by the model does of course depend on the functional equations). Consider now a data strategy, \\(D\\), in which data is gathered on \\(X\\) and \\(Y\\) for \\(n\\) randomly selected units. An answer \\(a^A\\), is then generated using ordinary least squares as the answer strategy, \\(A\\). We have specified all the components of MIDA. We now ask: How strong is this research design? One way to answer this question is with respect to the diagnosand “expected error.” Here the model’s functional equations provide an answer, \\(a^M\\) to the inquiry (for any draw of \\(\\beta\\)), and so the distribution of the expected error, given the model, \\(a^A-a^M\\), can be calculated. In this example the expected performance of the design may be poor, as measured by this diagnosand, because the data and analysis strategy do not handle the confounding described by the model. In comparison, better performance may be achieved through an alternative data strategy (e.g., where \\(D&#39;\\) randomly assigned \\(X\\) to \\(n\\) units before recording \\(X\\) and \\(Y\\)) or an alternative analysis strategy (e.g., \\(A&#39;\\) conditions on \\(Z\\)). These design evaluations depend on the model, and so one might reasonably ask how performance would look were the model different (for example if the underlying process involved nonlinearities). In all cases, the evaluation of a design depends on the assessment of a diagnosand, and comparing the diagnoses to what could be achieved under alternative designs. References "],
["software-primer.html", "Chapter 3 Software primer", " Chapter 3 Software primer You can implement the MIDA framework in any software package. Indeed, a design could be declared in writing or mathematical notation and then diagnosed using analytical formula.4 Social scientists use a number of tools for conducting statistical analysis: Stata, R, Python, Julia, SPSS, SAS, Mathematica, and more. Stata and R are most commonly used. We wrote DeclareDesign in the R statistical environment because of the availability of other tools for implementing research designs and because it is free-to-use. We have designed the rest of the book so that it can be read even if you do not use R, but you will have to translate the code into your own language of choice. On our Web site, we have a translation of core parts of the declaration and diagnosis process into Stata, Python, and Excel. In this section, we introduce you to DeclareDesign for R and how each step of the design-diagnose-redesign process can be implemented in it. However, we suggested in Section XX why analytical diagnoses may not be ideal for typical designs in the social sciences: they do not account for the specific features of research designs such as varying numbers of units per cluster and the interaction of choices about a data strategy and an answer strategy.↩ "],
["installing-r.html", "3.1 Installing R", " 3.1 Installing R This book relies on the statistical computing environment R, which you can download for free from CRAN. We also recommend the free program RStudio, which provides a friendly interface to R.5 Once you’ve got RStudio installed, open it up and install DeclareDesign and its related packages. These include three packages that enable specific steps in the research process (fabricatr for simulating social science data; randomizr, for random sampling and random assignment; and estimatr for design-based estimators). You can also install DesignLibrary, which gets standard designs up-and-running in one line. To install them, you can type: install.packages(c(&quot;DeclareDesign&quot;, &quot;fabricatr&quot;, &quot;randomizr&quot;, &quot;estimatr&quot;, &quot;DesignLibrary&quot;)) We also recommend you install and get to know the tidyverse suite of packages for data analysis, which we will use throughout the book: install.packages(&quot;tidyverse&quot;) In this chapter, we will introduce the DeclareDesign software and how to implement the MIDA framework within it. We will not provide a general introduction to R or to the tidyverse, because there are already many terrific introductions. We especially recommend R for Data Science, available for free on the Web. Both R and RStudio are available on Windows, Mac, and Linux.↩ "],
["where-we-are-going.html", "3.2 Where we are going", " 3.2 Where we are going We will build up to declaring and diagnosing a design in this section. But to get a sense of the goal, below is a simple 100-unit randomized experiment design declared, diagnosed, and redesigned. 3.2.1 Declaring a design # we should turn this into a picture labeling MIDA simple_design &lt;- # M: model # a 100-unit population with an unobserved shock &#39;e&#39; declare_population(N = 100, u = rnorm(N)) + # two potential outcomes, Y_Z_0 and Y_Z_1 # Y_Z_0 is the control potential outcome (what would happen if the unit is untreated) # it is equal to the unobserved shock &#39;u&#39; # Y_Z_1 is the treated potential outcome # it is equal to the control potential outcome plus a treatment effect of 0.25 declare_potential_outcomes(Y_Z_0 = u, Y_Z_1 = Y_Z_0 + 0.25) + # I: inquiry # we are interested in the average treatment effect in the population (PATE) declare_estimand(PATE = mean(Y_Z_1 - Y_Z_0)) + # D: data strategy # sampling: we randomly sample 50 of the 100 units in the population declare_sampling(n = 50) + # assignment: we randomly assign half of the 50 sampled units to treatment (half to control) declare_assignment(prob = 0.5) + # reveal outcomes: construct outcomes from the potential outcomes named Y depending on # the realized value of their assignment variable named Z declare_reveal(outcome_variables = Y, assignment_variables = Z) + # A: answer strategy # calculate the difference-in-means of Y depending on Z # we link this estimator to PATE because this is our estimate of our inquiry declare_estimator(Y ~ Z, model = difference_in_means, estimand = &quot;PATE&quot;) 3.2.2 Diagnosis To diagnose the design, we first define a set of diagnosands (see Section XX), which are statistical properties of the design. In this case, we select the bias (difference between the estimate and the estimand, which is the PATE); the root mean-squared error; and the statistical power of the design. # Select diagnosands simple_design_diagnosands &lt;- declare_diagnosands(select = c(bias, rmse, power)) We then diagnose the design, which involves simulating the design and again and again, and then calculate the diagnosands based on the simulations data. # Diagnose the design simple_design_diagnosis &lt;- diagnose_design(simple_design, diagnosands = simple_design_diagnosands, sims = 500) estimand_label estimator_label bias rmse power PATE estimator -0.0020108 0.2912167 0.146 3.2.3 Redesign We see that the power of the design is small, so we increase the number of sampled units from 50 to 100. replace_step creates a new design, swapping out the fourth step (sampling) for a modified sampling step. redesigned_simple_design &lt;- replace_step(simple_design, step = 4, new_step = declare_sampling(n = 100)) With the big picture of the declaration, diagnosis, and redesign of a simple design in mind, we now turn to building up from a single step to a full declared design. "],
["building-a-step-of-a-research-design.html", "3.3 Building a step of a research design", " 3.3 Building a step of a research design We begin learning about how to build a research design in DeclareDesign by declaring a single step: random assignment. We take as a starting point a fixed set of data, describing a set of voters in Los Angeles. The research project we are planning involves randomly assigning voters to receive a knock on their door from a canvasser (or not to receive a door knock). Our data look like this: ID age sex party precinct 001 62 F DEM 7356 002 69 M REP 8304 003 46 M GRN 6938 004 57 M GRN 2962 005 26 M GRN 2786 006 58 F DEM 7684 There are 100 voters in the dataset. 3.3.1 Using dplyr We plan to randomly assign 50 of the voters to treatment (door knock) and 50 to control (no door knock). We want to create an indicator variable Z, where 1 represents treatment and 0 control. In order to do this, we use R’s sample function: voter_file &lt;- voter_file %&gt;% mutate(Z = sample(c(0, 1), size = 100, replace = TRUE, prob = c(0.5, 0.5))) This says: draw a random sample with replacement 100 times (the number of voters) of 0’s and 1’s with probability 0.5 each. Recall that the %&gt;% operator sends a data frame to the dplyr verb mutate, which can add new columns to a data frame. This is a short dplyr “pipeline.”6 (See chapter XX of R4DS for an introduction.) Now our data frame voter_file includes the Z indicator: ID age sex party precinct Z 001 62 F DEM 7356 1 002 69 M REP 8304 0 003 46 M GRN 6938 1 004 57 M GRN 2962 1 005 26 M GRN 2786 0 006 58 F DEM 7684 0 We can make things a bit easier with the randomizr package, which includes common random assignment functions including simple random assignment used here (see Chapter XX for a description of common kinds of random assignment). You can instead write: voter_file &lt;- voter_file %&gt;% mutate(Z = simple_ra(N = 100, prob = 0.5)) We might use this dplyr pipeline to actually implement the random assignment for a study. But to diagnose the properties of a research design, we want to know what would happen under any possible random assignment. To do this, we will need to run the assignment step over and over again and save the results. 3.3.2 As a function To simulate the design in order to diagnose it, we need to turn the assignment step into a function. The function can then be run again and again, each time resulting in a different random assignment. In DeclareDesign, we are going to use a special kind of function: a tidy function, which takes in a data frame and returns back out a data frame. The new data frame may have an additional variable (such as a random assignment) or it may have fewer rows (due to sampling, for example). For our random assignment step, we want to create a tidy function that adds our assignment indicator Z to the data, but leaves it otherwise unchanged. We write: simple_random_assignment_function &lt;- function(data) { data %&gt;% mutate(Z = simple_ra(N = 100, prob = 0.5)) } We took the dplyr pipeline we built above, and put it on the inside of a tidy function. Now, when we run our random assignment function on the voter file, it adds in Z: simple_random_assignment_function(voter_file) ID age sex party precinct Z 001 62 F DEM 7356 0 002 69 M REP 8304 1 003 46 M GRN 6938 1 004 57 M GRN 2962 1 005 26 M GRN 2786 0 006 58 F DEM 7684 1 3.3.3 In DeclareDesign DeclareDesign makes writing each design step just a bit easier. Instead of writing a function each time, it writes a function for us. The core of DeclareDesign is a set of declare_* functions, including declare_assignment. Each one is a function factory, meaning it takes a set of parameters about your research design like the number of units and the random assignment probability as inputs, and returns a function as an output. Instead of writing the function simple_random_assignment_function as we did above, in DeclareDesign we declare it: simple_random_assignment_step &lt;- declare_assignment(prob = 0.5) simple_random_assignment_step is a tidy function. You can run the function on data: simple_random_assignment_step(voter_file) ID age sex party precinct Z Z_cond_prob 001 62 F DEM 7356 1 0.5 002 69 M REP 8304 0 0.5 003 46 M GRN 6938 0 0.5 004 57 M GRN 2962 0 0.5 005 26 M GRN 2786 0 0.5 006 58 F DEM 7684 0 0.5 A few parts of the declaration may seem a little bit odd. First, we did not tell R anything about the number of units in our dataset, as we did in the function and in the dplyr pipeline we wrote earlier. Second, we didn’t give it the data! This is because a step declaration creates a function that will work on any size dataset. We told declare_assignment that we want to assign treatment with probability 0.5 (and implicitly control with probability 1-0.5 = 0.5), regardless of how large the dataset is. We did not send the declaration the data, because declare_assignment automatically creates a tidy function for us, one that takes data and returns data with an assignment step. We will see in a moment how DeclareDesign uses these functions to simulate data from a research design. But you can always use the function yourself with your own data. In Chapter XX we describe how to implement your research design after you have conducted it, using the exact same functions you diagnosed the design with. This is one of the reasons we declare the assignment step — because we’ll learn about the properties of your design with the same code you can actually use to randomly assign treatment. Every step of a research design in MIDA can be written using one of the declare_* functions. In the next section, we walk through each step and how to declare it using DeclareDesign. This pipeline could be expressed in base R as voter_file$Z &lt;- sample(c(0, 1), size = 100, replace = TRUE, prob = c(0.5, 0.5))↩ "],
["research-design-steps.html", "3.4 Research design steps", " 3.4 Research design steps In this section, we walk through how to declare each step of a research design using DeclareDesign. In the next section, we build those steps into a research design, and then describe how to interrogate the design. 3.4.1 Model The model defines the structure of the world, both its size and background characteristics as well as how interventions in the world determine outcomes. In DeclareDesign, we split the model into two main design steps: the population and potential outcomes. There is always one population in a design, but there can be multiple sets of potential outcomes. 3.4.1.1 Population The population defines the number of units in the population, any multilevel structure to the data, and its background characteristics. We can define the population in several ways. In some cases, you may start a design with data on the population. When that happens, we do not to simulate it. We can simply declare the data as our population: declare_population(data = voter_file) ID age sex party precinct Z 001 62 F DEM 7356 1 002 69 M REP 8304 1 003 46 M GRN 6938 0 004 57 M GRN 2962 0 005 26 M GRN 2786 0 006 58 F DEM 7684 0 When we do not have complete data on the population, we simulate it. Relying on the data simulation functions from our fabricatr package, declare_population asks about the size and variables of the population: declare_population(N = 100, u = rnorm(N)) When we run the declared population function, we will get a different 100-unit dataset each time: ID u 001 1.4839998 002 0.8988917 003 -3.0057350 004 0.6757503 005 0.1079058 006 0.4559339 ID u 001 0.0649905 002 2.9407238 003 1.1198547 004 0.7876097 005 0.6407764 006 -0.7365158 ID u 001 -0.9435118 002 0.3627679 003 -0.1230749 004 -0.7747122 005 -0.4691835 006 0.2481613 The fabricatr package can simulate data for social science research including multilevel data structures like students in classrooms in schools. You can read the fabricatr Web site to get started simulating your data structure (link). A simple two-level data structure of individuals within households could be declared as: declare_population( households = add_level(N = 100, individuals_per_hh = sample(1:10, N, replace = TRUE)), individuals = add_level(N = individuals_per_hh, age = sample(1:100, N, replace = TRUE)) ) In every step of the research design process, you can short-circuit our default way of doing things and bring in your own code. This is useful when you have a complex design, or when you’ve already written code for your design and you want to use it directly. It works by setting the handler: complex_population_function &lt;- function(data, N_units) { data.frame(u = rnorm(N_units)) } declare_population(handler = complex_population_function, N_units = 100) 3.4.1.2 Potential outcomes Defining potential outcomes is as easy as a single expression per potential outcome. These may be a function of background characteristics, other potential outcomes, or other R functions.7 declare_potential_outcomes( Y_Z_0 = u, Y_Z_1 = Y_Z_0 + 0.25) des &lt;- declare_population(N = 100, u = rnorm(N)) + declare_potential_outcomes(Y_Z_0 = u, Y_Z_1 = Y_Z_0 + 0.25) draw_data(des) ID u Y_Z_0 Y_Z_1 001 -0.1311790 -0.1311790 0.1188210 002 1.1011911 1.1011911 1.3511911 003 0.8472447 0.8472447 1.0972447 004 0.9038887 0.9038887 1.1538887 005 0.1506122 0.1506122 0.4006122 006 -1.8921664 -1.8921664 -1.6421664 We also have a simpler interface to define all the potential outcomes at once as a function of a treatment assignment variable. The names of the potential outcomes are constructed from the outcome name (here Y on the lefthand side of the formula) and from the assignment_variables argument (here Z). declare_potential_outcomes(Y ~ u + 0.25 * Z, assignment_variables = Z) Either way of creating potential outcomes works; one may be easier or harder to code up in a given research design setting. 3.4.2 Inquiry To define your inquiry, declare your estimand, which is a function of background characteristics from your population, potential outcomes, or both. We define the average treatment effect for the experiment in our simple design as follows: declare_estimand(PATE = mean(Y_Z_1 - Y_Z_0)) Notice that we defined the PATE (the population average treatment effect), but said nothing special related to the population. In fact, it looks like we just defined the average treatment effect. This is because where you define the estimand in your design is going to determine whether it refers to the population, sample, or other form of estimand. We will see how to do this in a moment. 3.4.3 Data strategy The data strategy constitutes one or more steps representing interventions the researcher makes in the world from sampling to assignment to measurement. Typically, this may include sampling and assignment. 3.4.3.1 Sampling The sampling step relies on the randomizr package to conduct random sampling. See Section XX for an overview of the many kinds of sampling that are possible. We define a simple 50-unit sample from the population as follows: declare_sampling(n = 50) When we draw data from our simple design at this point, it will be smaller: from 100 units in the population to a data frame of 50 units representing the sample. In the data frame, we have an inclusion probability, the probability of being included in the sample. randomizr includes this by default. In this case, every unit in the population had an equal 0.5 probability of inclusion. ID u Y_Z_0 Y_Z_1 S_inclusion_prob 1 001 0.4060859 0.4060859 0.6560859 0.5 3 003 0.4240589 0.4240589 0.6740589 0.5 4 004 0.9057319 0.9057319 1.1557319 0.5 6 006 0.8445427 0.8445427 1.0945427 0.5 7 007 0.3581438 0.3581438 0.6081438 0.5 14 014 0.7424037 0.7424037 0.9924037 0.5 Sampling could also be non-random, which could be accomplished by using a handler. 3.4.3.2 Assignment Assignment also relies, by default, on the randomizr package for random assignment. Here, we define assignment as a 50% probability of assignment to treatment and 50% to control. declare_assignment(prob = 0.5) Assignment results in a data frame with an additional indicator Z of the assignment as well as the probability of assignment. Again, here the assignment probabilities are constant, but in other designs described in Section XX they are not and this is crucial information for the analysis stage. ID u Y_Z_0 Y_Z_1 S_inclusion_prob Z Z_cond_prob 001 -0.3199756 -0.3199756 -0.0699756 0.5 1 0.5 005 0.3980988 0.3980988 0.6480988 0.5 1 0.5 006 0.2591110 0.2591110 0.5091110 0.5 0 0.5 007 0.0237138 0.0237138 0.2737138 0.5 0 0.5 011 1.0837829 1.0837829 1.3337829 0.5 0 0.5 013 0.9040428 0.9040428 1.1540428 0.5 0 0.5 3.4.3.3 Other data strategies Random sampling and random assignment are not the only kinds of data strategies. Others may include merging in fixed administrative data from other sources, collapsing data across months or days, and other operations. You can include these as steps in your design too, using declare_step. Here, you must define a handler, as we did for using a custom function in declare_population. Some handlers that may prove useful are the dplyr verbs such as mutate and summarize, and the fabricate function from our fabricatr package. To add a variable using fabricate: declare_step(handler = fabricate, add_variable = rnorm(N)) If you have district-month data you may want to analyze at the district level, collapsing across months:8 collapse_data &lt;- function(data, collapse_by) { data %&gt;% group_by({{ collapse_by }}) %&gt;% summarize_all(mean, na.rm = TRUE) } declare_step(handler = collapse_data, collapse_by = district) 3.4.4 Answer strategy Through our model and data strategy steps, we have simulated a dataset with two key inputs to the answer strategy: an assignment variable and an outcome. In other answer strategies, pretreatment characteristics from the model might also be relevant. The data look like this: ID u Y_Z_0 Y_Z_1 S_inclusion_prob Z Z_cond_prob Y 001 -0.4153519 -0.4153519 -0.1653519 0.5 1 0.5 -0.1653519 002 1.0032993 1.0032993 1.2532993 0.5 0 0.5 1.0032993 006 0.1320780 0.1320780 0.3820780 0.5 1 0.5 0.3820780 007 -0.1392812 -0.1392812 0.1107188 0.5 1 0.5 0.1107188 008 0.5031416 0.5031416 0.7531416 0.5 0 0.5 0.5031416 012 -0.0771282 -0.0771282 0.1728718 0.5 1 0.5 0.1728718 Our estimator is the difference-in-means estimator, which compares outcomes between the group that was assigned to treatment and that assigned to control. We can calculate the difference-in-means estimate with a call to summarize from dplyr: simple_design_data %&gt;% summarize(DiM = mean(Y[Z == 1]) - mean(Y[Z == 0])) DiM -0.1209252 The estimatr package makes this easy and calculates the design-based standard error and a p-value and confidence interval for you: difference_in_means(Y ~ Z, data = simple_design_data) term estimate std.error statistic p.value conf.low conf.high df outcome Z -0.1209252 0.3134633 -0.3857713 0.7014032 -0.7514985 0.5096482 47.09491 Y Now, in order to declare our estimator, we can send the name of a model to declare_estimator. R has many models that work with declare_estimator, including lm, glm, the ictreg package from the list package, etc. The design-based estimators from estimatr can all be used. declare_estimator(Y ~ Z, model = difference_in_means, estimand = &quot;PATE&quot;) In this declaration, we also define the estimand we are targeting with the difference-in-means estimator.9 Typically, you will have an estimand that you are targeting, and sometimes you may consider targeting more than one and assessing how good your estimator estimates them. For example, you may want to know how good a job your instrumental variables job is at targeting the complier average causal effect, but also how close it gets on average to the average treatment effect. Typically, we think of potential outcomes as fixed and not random, and move random variables to the population.↩ The {{ }} syntax is handy for writing functions in dplyr where you want to be able reuse the function with different variable names. Here, the collapse_data function will group_by the variable you send to the argument collapse_by, which in our declaration we set to district. The pipeline within the function then calculates the mean in each district.↩ Sometimes, you may be interested just in the properties of an estimator, such as calculating its power. In this case, you need not define an estimand.↩ "],
["building-a-design-from-design-steps.html", "3.5 Building a design from design steps", " 3.5 Building a design from design steps In the last section, we defined a set of individual research steps. We draw one version of them together here: population &lt;- declare_population(N = 100, u = rnorm(N)) potential_outcomes &lt;- declare_potential_outcomes(Y_Z_0 = u, Y_Z_1 = Y_Z_0 + 0.25) estimand &lt;- declare_estimand(PATE = mean(Y_Z_1 - Y_Z_0)) sampling &lt;- declare_sampling(n = 50) assignment &lt;- declare_assignment(prob = 0.5) reveal &lt;- declare_reveal(outcome_variables = Y, assignment_variables = Z) estimator &lt;- declare_estimator(Y ~ Z, model = difference_in_means, estimand = &quot;PATE&quot;) To construct a research design object that we can operate on — diagnose it, redesign it, draw data from it, etc. — we add them together with the + operator. The + creates a design object. simple_design &lt;- population + potential_outcomes + estimand + sampling + assignment + reveal + estimator In the book, we’ll use a more compact way of writing a design, where we define it all at once with the +: simple_design &lt;- declare_population(N = 100, u = rnorm(N)) + declare_potential_outcomes(Y_Z_0 = u, Y_Z_1 = Y_Z_0 + 0.25) + declare_estimand(PATE = mean(Y_Z_1 - Y_Z_0)) + declare_sampling(n = 50) + declare_assignment(prob = 0.5) + declare_reveal(outcome_variables = Y, assignment_variables = Z) + declare_estimator(Y ~ Z, model = difference_in_means, estimand = &quot;PATE&quot;) 3.5.1 Order matters When defining a design, the order steps are included in the design via the + operator matters. Think of the order of your design as the causal order in which steps take place. population + potential_outcomes + estimand + sampling + assignment + reveal + estimator The order encodes several important aspects of the design: - First, the fact that the estimand follows the potential outcomes and comes before sampling and assignment means it is a population estimand, the population average treatment effect. This is because it is calculated on the data created so far. - The estimator comes after the assignment and reveal outcomes steps. If it didn’t, our difference-in-means would not work, because it wouldn’t have access to the treatment variable and the realized outcomes. "],
["simulating-a-research-design.html", "3.6 Simulating a research design", " 3.6 Simulating a research design Diagnosing a research design — learning about its properties — requires first simulating running the design over and over. We need to simulate the data generating process, then calculate the estimands, then calculate the estimates that will result. 3.6.1 In dplyr We first demonstrate how to use the tidy functions created by the declare_* functions in a dplyr pipeline to simulate a design once. We can run the population function, which generates the data structure, and then add the potential outcomes, and calculate the estimand as follows: population() %&gt;% potential_outcomes %&gt;% estimand estimand_label estimand PATE 0.25 This is the same thing as running the functions one at a time on each other: estimand(potential_outcomes(population())). Similarly, if we want to draw simulated estimates from the design, we again simulate a population, add potential outcomes, but now sample units, assign treatments to sampled units, reveal the outcomes, and calculate estimates: population() %&gt;% potential_outcomes %&gt;% sampling %&gt;% assignment %&gt;% reveal %&gt;% estimator estimator_label term estimate std.error statistic p.value conf.low conf.high df outcome estimand_label estimator Z 0.2227215 0.3264459 0.6822618 0.4985425 -0.4346286 0.8800717 45.36395 Y PATE 3.6.2 In DeclareDesign With simple design defined as an object, we can easily learn about what kind of data it generates, the values of its estimand and estimates, and other features with simple funtions in DeclareDesign. They chain together functions in a similar way to the dplyr pipelines abov. To draw simulated data based on the design, we use draw_data: draw_data(simple_design) ID u Y_Z_0 Y_Z_1 S_inclusion_prob Z Z_cond_prob Y 001 -1.3523271 -1.3523271 -1.1023271 0.5 1 0.5 -1.1023271 002 -0.7237033 -0.7237033 -0.4737033 0.5 1 0.5 -0.4737033 006 0.3585086 0.3585086 0.6085086 0.5 0 0.5 0.3585086 008 -0.0191530 -0.0191530 0.2308470 0.5 0 0.5 -0.0191530 009 -0.0550281 -0.0550281 0.1949719 0.5 1 0.5 0.1949719 010 0.7643561 0.7643561 1.0143561 0.5 1 0.5 1.0143561 draw_data runs all of the “data steps” in a design, which are both from the model (population and potential outcomes) and from the data strategy (typically sampling and assignment). To simulate the estimands from a single run of the design, we use draw_estimands. This runs two operations at once: it draws the data, and calculates the estimands at the point defined by the design. For example, in our design the estimand comes just after the potential outcomes. In this design, draw_estimands will run the first two steps and then calculate the estimands from the estimand function we declared: draw_estimands(simple_design) estimand_label estimand PATE 0.25 Similarly, we can simulate the estimates from a single run with draw_estimates which draws data and at the appropriate moment calculates estimates. draw_estimates(simple_design) estimator_label term estimate std.error statistic p.value conf.low conf.high df outcome estimand_label estimator Z -0.3010792 0.188783 -1.594843 0.1173209 -0.680667 0.0785086 47.93078 Y PATE To diagnose a design, we want a data frame that includes the estimates and estimands from many runs of a design. That is, we want to run the design, draw estimates and estimands, and then do that over and over and stack the results. This is exactly what simulate_design does: simulate_design(simple_design, sims = 500) design_label sim_ID estimand_label estimand estimator_label term estimate std.error statistic p.value conf.low conf.high df outcome simple_design 1 PATE 0.25 estimator Z 0.3175358 0.2423225 1.310385 0.1963148 -0.1697132 0.8047847 47.89786 Y simple_design 2 PATE 0.25 estimator Z 0.4425311 0.2481185 1.783547 0.0808223 -0.0563487 0.9414110 47.98461 Y simple_design 3 PATE 0.25 estimator Z 0.4963841 0.2786012 1.781702 0.0812151 -0.0639871 1.0567554 47.32619 Y simple_design 4 PATE 0.25 estimator Z -0.3844193 0.2548717 -1.508285 0.1384238 -0.8976614 0.1288229 45.30661 Y simple_design 5 PATE 0.25 estimator Z 0.7340671 0.2561452 2.865824 0.0062576 0.2184385 1.2496957 45.88444 Y "],
["diagnosing-a-research-design.html", "3.7 Diagnosing a research design", " 3.7 Diagnosing a research design The simulations data frame we created allows us to diagnose the design (calculate summary statistics from the simulations) directly. We can, for example, use the following dplyr pipeline to calculate the bias, root mean-squared error, and power for each estimator-estimand pair. simulations_df %&gt;% group_by(estimand_label, estimator_label) %&gt;% summarize(bias = mean(estimate - estimand), rmse = sqrt(mean((estimate - estimand)^2)), power = mean(p.value &lt; .05)) estimand_label estimator_label bias rmse power PATE estimator 0.0712198 0.3844849 0.2 In DeclareDesign, we do this in two steps. First, declare your diagnosands. These are functions of the simulations data. We have precoded several standard diagnosands (see Section XX). study_diagnosands &lt;- declare_diagnosands( select = c(bias, rmse, power), mse = mean((estimate - estimand)^2)) Next, take your simulations data and the diagnosands, and diagnose. This runs a single operation, which is to calculate the diagnosands on your simulations data, just like in the dplyr version above. diagnose_design(simulations_df, diagnosands = study_diagnosands) design_label estimand_label estimator_label term mse se(mse) bias se(bias) rmse se(rmse) power se(power) n_sims simple_design PATE estimator Z 0.1478286 0.0724626 0.0712198 0.2048244 0.3844849 0.0937049 0.2 0.1962373 5 We can also do this in a single step. When you send diagnose_design a design object, it will first run the simulations for you, then calculate the diagnosands from the simulations data frame that results. diagnose_design(simple_design, diagnosands = study_diagnosands) "],
["comparing-designs.html", "3.8 Comparing designs", " 3.8 Comparing designs In the diagnosis phase, you will often want to compare the properties of two designs to see which you prefer on the basis of the diagnosand values. We have two ways to compare. First, we can compare the designs themselves — what kinds of estimates and estimands do they produce, what steps are in the design. And we can compare the diagnoses. compare_designs(simple_design, redesigned_simple_design) To compare the diagnoses, we run a diagnosis for each one and then calculate the difference between each diagnosand for the two designs and conduct a statistical test of the null effect of no difference. compare_diagnoses(simple_design, redesigned_simple_design) 3.8.1 Comparing many variants of a design Often, we want to compare a large set of similar designs, varying key design parameters such as sample size, effect size, or the probability of treatment assignment. The easiest way to do this is to write a function that makes designs based on a set of these design inputs. We call these designers. Here’s a simple designer based on our running example: simple_designer &lt;- function(sample_size, effect_size) { declare_population(N = sample_size, u = rnorm(N)) + declare_potential_outcomes(Y_Z_0 = u, Y_Z_1 = Y_Z_0 + effect_size) + declare_estimand(PATE = mean(Y_Z_1 - Y_Z_0)) + declare_sampling(n = 50) + declare_assignment(prob = 0.5) + declare_reveal(outcome_variables = Y, assignment_variables = Z) + declare_estimator(Y ~ Z, model = difference_in_means, estimand = &quot;PATE&quot;) } To create a single design, based on our original parameters of a 100-unit sample size and a treatment effect of 0.25, we can run: simple_design &lt;- simple_designer(sample_size = 100, effect_size = 0.25) Now to simulate multiple designs, we can use the DeclareDesign function expand_design. Here we examine our simple design under several possible sample sizes, which we might want to do to conduct a minimum power analysis. We hold the effect size constant. simple_designs &lt;- expand_design(simple_designer, sample_size = c(100, 500, 1000), effect_size = 0.25) Our simulation and diagnosis tools can take a set of expanded designs (an R list) and will simulate all of them at once, creating a column called design_label to keep them apart. For example: diagnose_design(simple_designs) 3.8.2 Library of designs In our DesignLibrary package, we have created a set of common designs as designers, so you can get started quickly and also easily set up a range of design variants for comparison. library(DesignLibrary) b_c_design &lt;- block_cluster_two_arm_designer(N = 1000, N_blocks = 10) diagnose_design(b_c_design) "],
["how-to-use-this-book.html", "Chapter 4 How to use this book", " Chapter 4 How to use this book "],
["part-i-exercises.html", "4.1 Part I Exercises", " 4.1 Part I Exercises "],
["research-questions.html", "Chapter 5 Research questions", " Chapter 5 Research questions "],
["directed-acyclic-graphs-and-potential-outcomes.html", "Chapter 6 Directed Acyclic Graphs and Potential Outcomes", " Chapter 6 Directed Acyclic Graphs and Potential Outcomes We make use of two different formal languages for describing causal models: DAGs and potential outcomes. DAGs are “directed acyclic graphs,” where each node on a graph is a variable and the edges that connect them represent causal effects. DAGs emphasize a mechanical notion of causality: when the exposure variable changes, the outcome variable changes as a result. By contrast, the potential outcomes formalization emphasizes a counterfactual notion of causality. \\(Y_i(0)\\) is the outcome for unit \\(i\\) when the exposure variable is set to zero and \\(Y_i(1)\\) is the outcome when it is set to one. The difference between them is the effect of the treatment on the outcome for unit \\(i\\). Since at most only one potential outcome can ever be revealed, at least one of the two potential outcomes is necessarily counterfactual. Unrevealed potential outcomes are counter-to-facts; they do not exist. Nevertheless, the potential outcomes model is useful for thinking about what would have happened had things been different. Despite what you may have inferred from the sometimes heated disagreements between scholars who prefer one formalization to the other, DAGs and potential outcomes are compatible systems for thinking about causality. A theorem in one language is a theorem in the other (CITE???). We use both languages because they are useful for expressing different facets of research design. We’ll take the example of heterogeneous treatment effects to show how the two languages convey different aspects of the research setting. The DAG below has just four variables. \\(Y\\) is the outcome variable. It is affected by all of the other variables: \\(Z\\), the treatment variable, \\(X\\), a pre-treatment covariate, and \\(U\\), an unobserved source of variation in both \\(X\\) and \\(Y\\). Possibly the most important thing that a DAG can teach us is which research questions are even answerable in a given seeting. Here, the causal relationship between \\(X\\) and \\(Y\\) is confounded. Since we have no information about what is in \\(U\\), we can’t learn about the effects of \\(X\\) on \\(Y\\). However, the treatment variable \\(Z\\) has no edges leading in to it, which represents the idea that \\(Z\\) is randomly assigned or otherwise exogenous. The DAG shows that even though we can’t learn about the causal effects of \\(X\\) on \\(Y\\), we can learn about average causal effects of Z on Y. We could also draw descriptive inferences about the distributions of the observed variables like their ranges and averages or their variances and covariances. This DAG conveys beliefs about whether whether two variables are causally related, but it does not encode beliefs about how they are related. The DAG doesn’t show whether the effects are positive or negative, large or small, constant or heterogeneous. For example, we can’t read from the DAG whether the average effect of \\(Z\\) should be higher or lower depending on whether \\(X\\) is set to 0 or 1. This is no criticism of DAGs – they just don’t encode all of our causal beliefs about a system. A potential outcomes representation of this same system can fill in some of the details. More specifically, we will make the two potential outcomes of \\(Y\\) with respect to \\(Z\\) explicit, since we are focused on the effects of \\(Z\\) on \\(Y\\). We will skip writing out the potential outcomes of \\(X\\) and \\(Y\\) with respect to \\(U\\) to reduce complexity, though of course the causal process by which \\(U\\) affects its descendant variables could also be represented in potential outcomes. The untreated potential outcome is written \\(Y_i(Z = 0)\\) or \\(Y_i(0)\\) for short. Likewise, \\(Y_i(Z = 1)\\) (or \\(Y_i(1)\\)) is the treated potential outcome. Notice that the potential outcomes don’t appear on the DAG – only the revealed outcome \\(Y_i\\) does. That’s because the revealed \\(Y_i\\) is the output of a causal process that switches which potential outcome is revealed depending on the level of the treatment variable. The so-called “switching equation” can be written like this: \\[ Y_i = \\begin{cases} Y_i(0) &amp; Z = control \\\\ Y_i(1) &amp; Z = treatment \\\\ \\end{cases} \\] Sometimes the switching equation is written in a more compact algebraic form: \\(Y_i = Z * Y_i(1) + (1 - Z) * Y_i(0)\\). This expression works because in the treatment condition, Z is equal to 1, so the second half of the expression is zeroed out, and \\(Y_i = Y_i(1)\\). In the control condition, Z is equal to 0, so the first half of the expression is zeroed out. This works fine for binary treatment variables, but the piecewise notation extends more naturally to additional potential outcomes (treatment 1, treatment 2, placebo, etc). [INSET BOX on the Fundamental Problem of Causal Inference] The switching equation underlines the fact that at most, we can only observe the one potential outcome that happened to be revealed by the world. We either see $Y_i(1)$ or $Y_i(0)$, but we can never see both. This problem has famously been dubbed the &quot;Fundamental Problem of Causal Inference&quot; (Holland 1986). The problem is indeed fundamental and common to all causal inference settings. That said, the FUndamental Problem of Causal Inference is more *severe* in some settings than others. Consider the causal inference problem of measuring the average causal effect of a light switch on the lamp turning on. Suppose we flip the switch on at exactly 2020-07-08 12:33:38 EDT and the lamp illuminates. We literally can&#39;t know whether the lamp would be on or off if the switch were in the off position at exactly that moment -- that&#39;s the Fundamental Problem of Causal Inference at work. But we can nevertheless get a good sense of the average effect of the switch on the lamp by flicking it back and forth a few times. Conditional on everything else in the system working (the electricity to the house, the lightbulb filament, etc), the average effect of the switch on illumination is 100 percentage points. The fundamental problem just isn&#39;t a big deal in this case because we have fine control over the causal variable (the switch) and the outcome variable (is the light on?) is measured perfectly. Most social scientific research questions are much harder. Measuring the average effect of university education on earnings, for example, is a notoriously difficult research task. First, different kinds of people do and don&#39;t attend university; those who do attend have different experiences and develop different social networks; those who don&#39;t attend university *also* have heterogeneous experiences in their early careers. The Fundamental Problem of Causal Inference is quite severe in this case because it is very hard to know what the lives of well-educated offices worker would have been like had they not gone to college. Potential outcomes notation is especially useful for defining estimands. Estimands are the inferential target – what we call the Inquiry in the MIDA framework. The most common estimand is the Average Treatment Effect (ATE), which is written like this10: \\[ ATE \\def E[Y_i(1) - Y_i(0)] \\] We want to emphasize that the ATE is an average. The difference between \\(Y_i(1)\\) and \\(Y_i(0)\\) for unit \\(i\\) is an individual-level treatment effect (sometimes we’ll refer to \\(Y_i(1) - Y_i(0)\\) as \\(\\tau_i\\)). The ATE averages over all of the individual-level treatment effects in the relevant population. Some units will have a \\(tau_i\\) that is higher than the ATE, some will have a \\(\\tau_i\\) that is lower. We emphasize this because sometimes people mistakenly thing that by focusing on an ATE, researchers are “assuming” that everyone experiences the same treatment effect. This is not true. The ATE is just a single-number summary of a possibly very heterogeneous set of responses to treatment. We can also use potential outcomes notation to define other, more complicated estimands. Throughout the book, we’ll describe a series of them – local average treatment effects, average treatment effects on the treated, average direct effects, average indirect effects, and spillover effects, to name a few – all using potential outcomes notation. The expectation operator \\(E[]\\) is a way of describing the average of a random variable. In general, \\(E[X] = \\sum_{x \\in X} x * pr(X = x)\\). Here we are slightly abusing the notation, since in a fixed population, \\(Y_i(1)\\) and \\(Y_i(0)\\) are not random variables. We could write the ATE as \\(\\frac{1}{N}\\sum_1^N Y_i(1) - Y_i(0)\\), or we could just imagine that we drawing one unit at random from the fixed population; the expectation operator can be defined with respect to this imaginary random variable in order to save ourselves some notational headaches.↩ "],
["the-same-dag-can-support-multiple-causal-theories-.html", "6.1 The same DAG can support multiple causal theories.", " 6.1 The same DAG can support multiple causal theories. In this example, we’ll illustrate how using both DAGs and potential outcomes together, we can describe and learn about research designs in a more complete way. The DAG above encodes the beliefs that \\(Y\\) is a function of \\(Z\\), \\(X\\), and \\(U\\) and also that we can learn about the effects of \\(Z\\) on \\(Y\\) but not \\(X\\) on \\(Y\\). The DAG is silent, however, on the question of whether the effect of \\(Z\\) is on average different for units with different values of \\(X\\). In other words, the DAG doesn’t reflect beliefs about treatment effect heterogeneity. Using potential outcomes, we can write down three new estimands: the conditional average treatment effect (CATE) of \\(Z\\) on \\(Y\\) for units with \\(X = 1\\), the CATE among units with \\(X = 0\\), and the difference-in-cates: \\[ \\begin{align} CATE_{(X = 1)} &amp;\\def E[Y_i(1) - Y_i(0) | X = 1] \\\\ CATE_{(X = 0)} &amp;\\def E[Y_i(1) - Y_i(0) | X = 0] \\\\ Diff-in-CATEs &amp;\\def E[Y_i(1) - Y_i(0) | X = 1] - E[Y_i(1) - Y_i(0) | X = 0] \\end{align} \\] Suppose we think that the effect of treatment is larger for units with \\(X = 1\\), which is to say we think the difference-in-cates is positive. We can supplement the DAG representation of the causal model with a design declaration that includes these beliefs. diff_in_cates &lt;- 0.5 design &lt;- declare_population(N = 100, U = rnorm(N), X = rbinom(N, 1, prob = pnorm(0.5 * U + rnorm(N)))) + declare_potential_outcomes(Y ~ 0.5 * X + 0.5 * Z + diff_in_cates * X * Z + 0.5 * U) + declare_estimands( ATE = mean(Y_Z_1 - Y_Z_0), CATE_1 = mean(Y_Z_1[X == 1] - Y_Z_0[X == 1]), CATE_0 = mean(Y_Z_1[X == 0] - Y_Z_0[X == 0]), DiC = mean(Y_Z_1[X == 1] - Y_Z_0[X == 1]) - mean(Y_Z_1[X == 0] - Y_Z_0[X == 0]) ) estimand_label estimand ATE 0.725 CATE_1 1.000 CATE_0 0.500 DiC 0.500 We could incorporate different beliefs about the causal model by changing the diff_in_cates parameter. However, notice that regardless of the value of the interaction (either zero or some other number), the DAG looks the same. So if you want to design your study for the difference-in-CATEs, then you’ll need to go beyond the DAG to write down your beliefs about the extent of heterogeneity using another tool. "],
["summary.html", "6.2 Summary", " 6.2 Summary DAGs and potential outcomes are two languages for expressing beliefs about causality. The DAG formulation emphasizes a mechanistic understanding of causal systems: if I change this here, that will change there. The potential outcomes formulation emphasizes counterfactuals: we can either observe what happens there if I change this here or if I don’t, but not both. The two languages are useful for describing different parts of a research design. DAGs excel at describing whole causal models and indicate which variables we can learn the causal effects of. Potential outcomes are useful for precisely defining causal estimands like the ATE, the CATEs, and the difference-in-CATEs. Design declaration uses the language of potential outcomes in order to encode more specific beliefs (such as beliefs about heterogeneity) than can be represented on the DAG. "],
["specifying-the-model.html", "Chapter 7 Specifying the model", " Chapter 7 Specifying the model In this chapter, we will talk about how to go about the tricky task of specifying a \\(M\\)odel and why it is so important to do so. We are not talking about “statistical models” used to estimate unknown parameters—those are part of your answer strategy. We are talking about specifying simulation models: a set of assumptions about how your data could be generated, encoded in math or computer language, which we can use to generate imaginary datasets. The distinction is a subtle but important one. We should only trust the inferences we draw from a statistical model if we think they would be good inferences to draw based on a plausible \\(M\\)odel of how the data was generated. \\(M\\)odels often lurk where you might not expect them. When researchers do experiments, they often compare the average observed outcome in the treatment group to the average observed outcome in the control group in order to get an estimate of the average treatment effect.11 This comparison requires no statistical modelling of the distributions of the observed variables: it follows from the fact that the average difference in two potential outcomes is equal to the difference in the averages of those two potential outcomes. So it seems as though such strategies are completely agnostic as to how the data was generated. Implicit in the very definition of the average treatment effect here, however, is a \\(M\\)odel: specifically, one in which any particular draw of the random assignment is able to reveal no more than two potential outcomes for any given individual—this is sometimes referred to as a “no spillovers” assumption. But many social processes violate this assumption. If I do something differently every time you get treated and I do not, then I have at least three potential outcomes: one in which I am treated, one in which neither of us are treated, and one in which you are treated but I am not. The average observed difference-in-means across random assignments might diverge quite substantially from the average effect we are interested in (say, of putting just one person in treatment and the rest in control). So even the simplest, most agnostic research design is relying on the plausibility of an implicit \\(M\\)odel in order to generate insights that are correct on average. In practice, we never get to know whether our \\(M\\)odel is right. For example, we can never really know how many potential outcomes there are. So we also cannot know if we got the right answer. What we can do, however, is study our procedure for generating answers under \\(M\\)odels of the world that we find plausible. That way, we do get to know which answers we should and should not trust, given the assumptions we are willing to make about the world. …of putting one person in treatment and everyone else in control↩ "],
["declaring-exogenous-and-endogenous-variables-in-the-population.html", "7.1 Declaring exogenous and endogenous variables in the population", " 7.1 Declaring exogenous and endogenous variables in the population Remember from Chapter X that declaring a \\(M\\)odel means specifying exogenous and endogenous variables, how they are related to one another, and the probability distributions that determine which values the variables take. That might sound hard but it does not need to be. The following line of code contains a fully specified model: population &lt;- declare_population(N = 8, e = runif(N), X = rnorm(N, e, 1)) An exogenous variable is one whose values do not depend on the values of any other variable in the \\(M\\)odel. Here, e is an exogenous variable: it is simply a random draw from the uniform distribution between 0 and 1, inclusive. So long as N stays fixed, no other variables influence the values that e takes on. An endogenous variable is one whose values can depend on another variable. Here, because X is defined as a function of the value that e takes, we say X is endogenous to its “parent,” e (we use the term “parent” to describe the variables that appear in the function defining an endogenous variable). The code says that the value that X takes for a given individual is equal to a random draw from a normal distribution whose mean is equal to their value of e and whose standard deviation is equal to 1. The population function we just declared generates random datasets with eight observations of e and X. Try running this code a few times to see: population() ## ID e X ## 1 1 0.9148060 1.3190744 ## 2 2 0.9370754 0.8309509 ## 3 3 0.2861395 1.7976615 ## 4 4 0.8304476 0.7357886 ## 5 5 0.6417455 2.6601692 ## 6 6 0.5190959 0.4563819 ## 7 7 0.7365883 2.0414580 ## 8 8 0.1346666 2.4213120 As we will see below, the population function is a useful place to declare all of the exogenous variables and many of the endogenous variables in our study, along with some pretty complicated relationships between them. If we want to learn about our inferences, however, our model also has to define potential outcomes. "],
["declaring-potential-outcomes.html", "7.2 Declaring potential outcomes", " 7.2 Declaring potential outcomes Remember from section X that potential outcomes are the building blocks of your inquiry—they describe counterfactual states of the world that could exist, depending on some parent variable. Say the parent variable is whether you are contacted by an election campaign canvasser, who tries to convince you to vote for Jane Doe, a candidate running in your district. Then you might have at least two potential outcomes that depend on the value of this variable: whether you vote for Jane Doe when a canvasser from her campaign contacts you, on the one hand, and whether you vote for Jane Doe when that canvasser does not contact you, on the other. The difference in those two states of the world describes the treatment effect of the election campaign canvasser on whether you vote for Jane Doe. Say we did an experiment in which that canvasser visits five out of ten people who answer doors, selected at random. Then we could define the potential outcomes of the people they visit as endogenous variables: the “untreated” potential outcome of the experiment would encode whether each door-answerer would vote for Jane Doe if they were not contacted by the canvasser, and the “treated” potential ouctcome would encode whether they vote for Jane Doe if they were contacted by the canvasser. In addiiton to the untreated and treated potential outcomes of this experiment, we can add a third, distinct, variable: the vote choice that we would actually observe in an experiment where we actually assigned half of the people to canvassing. To reiterate, if a researcher wants to declare a study with two treatment conditions and no spillovers, her model will have to include at least three variables: The untreated potential outcome The treated potential outcome The observed outcome That is why DeclareDesign has a whole step devoted to declaring potential outcomes. The declare_potential_outcomes() step is crucial because it splits up endogenous variables into the counterfactual sets of values they could take, given the variables they depend on. Consider this declaration: potential_outcomes &lt;- declare_potential_outcomes(Y_Z_0 = .5 &lt; e, Y_Z_1 = .5 &lt; e + .05) Here, we have added two variables to the dataset, Y_Z_0 and Y_Z_1. They are both endogenous, in the sense that they depend on the value that e takes and on the value that Z takes.12 So Y_Z_1 is a list of the hypothetical values Y could take, if you were to set a specific variable it depends on, Z, to take the value of 1. Suppose, for example, that e represents a person’s utility from voting for Jane Doe. Then, the code says that each person will vote for Jane Doe if e is greater than .5 when they’re untreated, and if they are treated, they vote for Jane Doe when e + .05 is greater than .5. So, the \\(M\\)odel stipulates that canvassing makes people roughly five percentage points more likely to vote for Jane Doe. A variable’s potential outcomes can either be expressed by defining each potential outcome explicitly, as we do above, or through what we call a “potential outcomes function:” potential_outcomes &lt;- declare_potential_outcomes(Y ~ .5 &lt; e + .05 * Z) By default, declare_potential_outcomes() assumes that the functional equation for Y will include a binary Z that it can split on to create Y_Z_0 and Y_Z_1. But you can also tell the function to split on any other variables that take on any kind of value. For example, potential_outcomes &lt;- declare_potential_outcomes( income ~ employed + education + u, assignment_variables = list(employed = c(&quot;No&quot;,&quot;Yes&quot;), education = c(10,12))) will create four new variables: income_employed_No_education_10, income_employed_No_education_12, income_employed_Yes_education_10, and income_employed_Yes_education_12. In this setup, income is the observed variable and the other four variables are the potential outcomes that would result from assigning individuals to the corresponding values of employed and education. The resultant function, potential_outcomes, knows not to create potential outcomes corresponding to values of u because u does not appear in the list of assignment_variables. 7.2.1 Potential outcomes can include variables that are not yet defined Careful readers may have picked up on something a bit confusing in the preceding paragraphs: we have defined potential outcomes in terms of a variable that has not yet been realized in our design. simple_design &lt;- population + potential_outcomes + estimand + sampling + assignment + reveal_outcomes + estimator The potential outcomes Y_Z_1 and Y_Z_0 are already defined in terms of Z in the second step of our design, well before the variable Z gets created in the fifth step, assignment. How is this possible? Oddly, perhaps, the values of potential outcomes do not depend on the actual values that the parent variables happen to take13—that is what makes them potential outcomes and not plain old outcomes. Imagine, for example, that your design involves assigning everyone to the control, so that \\(Z_i = 0~~\\forall~~i\\) in practice. In that case, we still define the treated potential outcome, \\(Y_i(Z_i = 1)\\), exactly as before. We do not need to know what values Z will actually take in order to define the values that Y could potentially take. What this requires at the model specification stage is some forwards-looking: you are going to need to model the imaginary states of the world that could happen before they happen. So, for example, if you have a different potential outcome depending on whether you are treated and whether you are sampled, then you need to define potential outcomes in terms of treatment assignment and sampling before either of these steps have occurred. More on this below. 7.2.2 When you do and do not need to define potential outcomes So when should you split endogenous variables into all of the counterfactual values they can take on (potential outcomes), and when should you leave them whole? In other words, which variables belong in your population and which belong in your potential_outcomes? Why do we split on variables like Z and not on variables like u or e? The short answer is that it all depends on your \\(I\\)nquiry. Let us take a simple example using the population declaration above. Suppose that you were interested in the average effect of \\(Z\\) on \\(Y\\), for any given value that \\(X\\) can take: \\(E[Y_i(1) - Y_i(0)]\\). Then your inquiry depends only on Y_Z_1 and Y_Z_0. Those are the only potential outcomes you need. What if you want to know the effect of \\(Z\\) on \\(Y\\) among groups for whom \\(X\\) happened to equal 1 or happened to equal 0: \\(E[Y_i(1) - Y_i(0)\\mid X_i = x]\\). Here, you are not interested in the causal effect of X, just in whether, descriptively, the effect of Z on Y just happens to different among people for whom X == 1. There is no need to split on X in this case: you just need to look at the difference in Y_Z_1 and Y_Z_0 among the people for whom X is equal to 1 or to 0, which can be achieved through subsetting. Now, let us imagine that you were interested in the causal effects of both X and Z on Y. For example, you might want to know whether X causes the effects of Z on Y to be bigger: \\[E[(Y_i(Z_i = 1,X_i = 1) - Y_i(Z_i = 0,X_i = 1)) - \\] \\[~~~~~(Y_i(Z_i = 1,X_i = 0) - Y_i(Z_i = 0,X_i = 0))].\\] That is a claim about the counterfactual states of Y as a function of both variables, and would require something such as: potential_outcomes &lt;- declare_potential_outcomes( Y_X_0_Z_0 = .5 &lt; e, Y_X_0_Z_1 = .5 &lt; e + .05, Y_X_1_Z_0 = .5 &lt; e, Y_X_1_Z_1 = .5 &lt; e + .05 + .05) Where Y_X_0_Z_1, for example, is a variable that lists every individual’s potential Y outcome if X were set to 0 and Z were set to 1. In this example, we have stipulated that X increases the effect of Z by .05. To see this, note that we can rewrite the potential outcomes declaration above using a potential outcomes function: declare_potential_outcomes(Y ~ .5 &lt; e + Z * .05 + Z * X * .05, assignment_variables = list(Z = 0:1, X = 0:1)) What if the value of \\(X\\) were itself a function of \\(Z\\), and your inquiry focused on the effect of \\(X\\) on \\(Y\\)—\\(E[Y_i(X_i = 1) - Y_i(X_i = 0)]\\)? In that case, you might model two sets of potential outcomes: the potential outcomes of \\(X\\) as a function of \\(Z\\), and the potential outcomes of \\(Y\\) as a function of \\(X\\): potential_outcomes &lt;- declare_potential_outcomes( X_Z_0 = .5 &lt; e * 0.75, X_Z_1 = .5 &lt; e * 1.25, Y_X_0 = .5 &lt; e, Y_X_1 = .5 &lt; e + .05) Here, we are assuming Z is exogenous in the sense that it is randomly assigned. However, X, the causal variable of interest, is endogenous to e and to Z. In fact, if we are interested in the effect of X on Y, e is no longer background noise here as it was in the examples above: it has become a confounder of the causal effect in which we are interested . To see this, note that higher levels of e make both X and Y more likely to be TRUE. This is the sort of setup in which analysts would typically use an “instrumental variables” approach (callout to IV).14 In practice, there are always many many more potential outcomes lurking in your study than you need to model. Notice, for example, that we did not model the potential outcomes of Y as a function of specific levels of e above – unless we care about the causal effect of e on some outcome, there is little reason to model the counterfactuals it gives rise to. In the following two sections, we walk through some more concrete advice on which kinds of potential outcomes you need to consider modelling. Specifically, we think you should focus on potential outcomes generated through two processes: manipulation and interference. 7.2.3 Manipulation creates potential outcomes Manipulation is some real or imagined intervention in the world that sets the values of a parent of one of your outcomes. Perhaps the most obvious manipulation is assignment to treatment—a coin flip, for example, is a manipulation that sets \\(Z\\) to 1 for roughly half of the people and to 0 for the others. In quasi-experimental designs, we imagine a quasi-assignment: a non-random policy intervention might set some constituencies to have a change in their electoral rules and not others, for example. In that case, we imagine every constituency’s potential outcome had the policy intervention taken place there, and vice versa. But there are many other kinds of manipulations: measurement and sampling are two obvious examples. When you randomly sample someone and conduct a survey with them, you set their sampling status to “Sampled” and their measurement status to “Measured.” In general, you should consider modeling any manipulation that might affect the value of your \\(I\\)nquiry. A good rule of thumb is that there will be at least as many potential outcomes as the Cartesian product of the range of the manipulated variables. Let us suppose, for example, that you have a treatment variable with three values, \\(Z \\in \\{1,2,3\\}\\), and you think that there might be Hawthorne effects – e.g., an effect of having your outcomes measured, \\(M \\in \\{0,1\\}\\). That implies you should have six potential outcomes, \\(Y_i(Z_i,M_i)\\): \\(Y_i(1,0)\\), \\(Y_i(2,0)\\), \\(Y_i(3,0)\\), \\(Y_i(1,1)\\),\\(Y_i(2,1)\\), and \\(Y_i(3,1)\\). The first three represent states of the world revealed by assigning someone to the different arms of the treatment when they are not measured, and the latter three those same treatment outcomes when measured. The following potential outcomes declaration suggests a Hawthorne effect: hawthorne_POs &lt;- declare_potential_outcomes( Y_Z_1_M_0 = .5 &lt; e, Y_Z_2_M_0 = .5 &lt; e + .05, Y_Z_3_M_0 = .5 &lt; e + .05, Y_Z_1_M_1 = .5 &lt; e + .05, Y_Z_2_M_1 = .5 &lt; e + .05 + .05, Y_Z_3_M_1 = .5 &lt; e + .05 + .05) Note, however, that our design now requires some modifications: simple_design has an inquiry defined in terms of the simpler potential outcomes, Y_Z_1 and Y_Z_0, which no longer exist. We need to clarify that we are interested in the effect of treatment without any measurement effects (here, we will say we are interested in the treatment 2 versus 1 comparison). Second, simple_design had no variable M to split on: we need a step in which we manipulate the measurement variable to be 1 for everyone in the sample. Finally, simple_design revealed Y purely as a function of Z, but we need to declare that the observed Y will correspond to the values of both Z and M: assignment &lt;- declare_assignment(conditions = c(1,2,3)) estimand_no_m &lt;- declare_estimand(ate_2_no_m = mean(Y_Z_2_M_0 - Y_Z_1_M_0)) measurement &lt;- declare_step(M = 1, handler = fabricate) reveal_outcomes_measurement &lt;- declare_reveal(Y, c(Z, M)) hawthorne_design &lt;- population + hawthorne_POs + estimand_no_m + sampling + assignment + measurement + reveal_outcomes_measurement + estimator From here, it is easy to modify our design with measurement effects to stipulate a model in which there is an interaction between treatment and measurement—this is often referred to as an “experimenter demand” effect, and can be more problematic for inference than a simple Hawthorne effect (you can do some simple algebra with the estimand declaration to see why): experimenter_demand_POs &lt;- declare_potential_outcomes( Y_Z_1_M_0 = .5 &lt; e, Y_Z_2_M_0 = .5 &lt; e + .05, Y_Z_3_M_0 = .5 &lt; e + .05, Y_Z_1_M_1 = .5 &lt; e, Y_Z_2_M_1 = .5 &lt; e + .05 + .05, Y_Z_3_M_1 = .5 &lt; e + .05 + .10) demand_design &lt;- replace_step(design = hawthorne_design, step = &quot;hawthorne_POs&quot;, new_step = experimenter_demand_POs) The very same logic can be applied to defining interactions between different treatment arms, effects from sampling, and other interactive effects. 7.2.4 Interference creates potential outcomes We stated in the previous section that there are usually at least as many potential outcomes as the Cartesian product of the ranges of the manipulated parents. But that statement assumes that the value of each individual’s potential outcome depends only on the value of their own parent variables. When one individual’s potential outcomes depend on the value of a manipulated variable of any other unit in the study, we refer to this as “interference.” Interference is a generic concept that includes social processes such as spillovers, social comparisons, contagion, communication, displacement, deterrence, and persistence [CITE GG p256-6]. Depending on your model, interference can generate a much, much larger space of potential outcomes. The formal notation for interference is that \\(Y_i(Z_i) \\neq Y_i(\\mathbf{Z})\\). Here, \\(\\mathbf{Z}\\) denotes the entire vector of random assignments: the math says that an individual’s potential outcome expressed in terms of their own assignment is not the same thing as their outcome expressed in terms of everyone’s assignment. Accordingly, the most general potential outcomes model in the presence of interference is one in which every conceivable realization of \\(\\mathbf{Z}\\) is mapped to exactly one potential outcome. Consider an employee-of-the-month experiment, in which one of three individuals is randomly assigned to be employee of the month (reference to GG). Suppose that employees 1 and 2 do not like each other. We will define potential outcomes in the following manner: \\(Y_i(j)\\), where \\(j\\) denotes the index of the treated individual. So, for example, \\(Y_2(3)\\) is the potential outcome of the second individual when individual 3 is assigned to treatment. interference_design &lt;- declare_population(N = 3, e = runif(N)) + declare_potential_outcomes( Y_J_1 = c(.5 &lt; e[1] + 1, .5 &lt; e[2] - 1, .5 &lt; e[3]), Y_J_2 = c(.5 &lt; e[1] - 1, .5 &lt; e[2] + 1, .5 &lt; e[3]), Y_J_3 = c(.5 &lt; e[1], .5 &lt; e[2], .5 &lt; e[3] + 1)) + declare_assignment(conditions = c(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;), assignment_variable = &quot;J&quot;) + declare_step(Z = as.numeric(ID == J), handler = fabricate) + declare_reveal(Y, J) + declare_estimator(Y ~ Z, model = lm_robust) Notice we did not declare an estimand here. That is because the inquiry in a design with interference requires particular attention (callout to inquiry section). As you can see, the space of potential outcomes can expand very quickly when we allow for every possible way in which the treatment can be assigned to affect outcomes differently. Under complete random assignment of \\(m\\) of \\(N\\) people to treatment or control, there are \\(N\\) choose \\(m\\) ways of assigning treatment, for example. For a simple design in which ten people are assigned to treatment and control in equal proportions, there are 252 potential outcomes to consider. The problem of specifying the potential outcomes in your \\(M\\)odel can quickly become intractable. Often, we do not actually expect outcomes to differ quite so much: if there are 100 employees in the company and Sally does not know Jim or Tracy, she might be indifferent between the two worlds in which either Jim or Tracy wins employee-of-the-month. Generalizing this principle, one might be able to cut down on the number of potential outcomes by considering spillovers limited to social networks within the company. Where possible, a good way to handle spillovers is to specify a \\(M\\)odel in which they function like any other treatment. Such \\(M\\)odels rely on qualitative knowledge, which can be wrong. Let us say, for example, that you were worried about spillovers in an experiment, but: 1) you know that people’s interactions are restricted by some ordering, such as a queue; 2) you strongly suspected that a person being treated only affects the outcomes of the next person in the queue, but not the person after. For example, if the treatment involved providing some randomly selected people in a queue with extra information and the outcome of interest was customer satisfaction, those who fall in the queue behind treated individuals might get frustrated at observing how long the person in front of them is taking. In this case, we can think of two treatments: \\(Z\\), being directly treated, and \\(S\\), having the person before you treated: spillover_POs &lt;- declare_potential_outcomes( Y_Z_0_S_0 = .5 &lt; e, Y_Z_1_S_0 = .5 &lt; e + .05, Y_Z_0_S_1 = .5 &lt; e - .05 / 2, Y_Z_1_S_1 = .5 &lt; e + .05 / 2) neighbors &lt;- declare_step(next_neighbor = c(N,(1:(N-1))), S = Z[next_neighbor], handler = fabricate) reveal_spillovers &lt;- declare_reveal(Y, c(Z, S)) spillover_design &lt;- population + spillover_POs + sampling + assignment + neighbors + reveal_spillovers + estimator Now, instead of \\(N\\) choose \\(m\\) potential outcomes, there are four—for any sample size. We have contained the problem of proliferating potential outcomes. Of course, showing that one’s strategy is robust to the spillovers specified in this \\(M\\)odel might not be convincing to a skeptic who contends spillovers might also affect the second or third person behind the treated individual. Moreso than ever, the validity of inferences depend on the robustness of the \\(A\\)nswer strategy to plausible \\(M\\)odels. 7.2.5 What to do with potential outcomes whose parents are continuous So far we have focused on potential outcomes whose parents take on discrete values. But another way in which potential outcomes can proliferate is if there are infinitely many places at which to consider splitting the outcome on the parent variable, as is the case with continuous parents. [INCLUDE EXAMPLE FROM PAPER] 7.2.6 Principal strata can be defined in terms of potential outcomes One important set of \\(I\\)nquiries comprise estimands that are specific to certain causal “types” in the population. For example, in designs in which not everyone assigned to treatment actually takes treatment, researchers are often interested in the treatment effect among those actually treated (often referred to as “compliers”). Similarly, in designs where some units do not report outcomes and where reporting is possibly a function of treatment, researchers are often interested in the effect among those who would report in either treatment or control. These causal types—referred to as “principal strata” in the literature [CITE imbens rubin]—can be usefully modeled in terms of potential outcomes. Consider the case of one-sided non-compliance: compliance_POs &lt;- declare_potential_outcomes( D_Z_0 = 0, D_Z_1 = ifelse(order(e) &gt; 4, 1, 0), Y_D_0 = .5 &lt; e, Y_D_1 = .5 &lt; e + .05) ate_estimand &lt;- declare_estimand(ate = mean(Y_D_1 - Y_D_0)) cace_estimand &lt;- declare_estimand(cace = mean(Y_D_1 - Y_D_0), subset = D_Z_0 == 0 &amp; D_Z_1 == 1) Here, treatment status, \\(D\\), is a potential outcome of treatment assignment, \\(Z\\). The CACE estimand is easily defined in terms of the causal type: it is the average effect among people for whom \\(D_i(Z_i = 0) =0, D_i(Z_i = 1) = 1\\). Similarly, we can think of attrition as a potential outcome. Here, people with the lowest four values of e do not report outcomes if assigned to control, but everyone reports when they are assigned to treatment. Whenever someone does not report we only observe an NA. attrition_POs &lt;- declare_potential_outcomes( R_Z_0 = ifelse(order(e) &lt;= 4, 1, 0), R_Z_1 = 1, Y_R_0_Z_0 = NA, Y_R_0_Z_1 = NA, Y_R_1_Z_0 = .5 &lt; e, Y_R_1_Z_1 = .5 &lt; e + .05) 7.2.7 Develop a null model You might be wondering whether \\(M\\)odels are simply a way of coding hypotheses. The two are similar but not quite the same: many \\(M\\)odels can correspond to the same hypothesis. One particularly important hypothesis is the so-called null hypothesis that the average effect of \\(Z\\) on \\(Y\\) is equal to zero. Consider the following three models: population &lt;- declare_population(N = 8, e = rnorm(N, 0, 1)) model_1 &lt;- population + declare_potential_outcomes(Y ~ e) model_2 &lt;- population + declare_potential_outcomes(Y ~ e + e * 2 * Z) model_3 &lt;- population + declare_potential_outcomes(Y ~ ifelse(e &gt; .5, Z * .2, -Z * .2)) Each of these models is consistent with the null hypothesis. In the first, treatment and control outcomes are exactly the same: \\(Z\\) does not even appear in the functional equation for \\(Y\\), there is no effect for any unit in the sample. This is sometimes referred to as a “sharp null hypothesis,” and is a specific case of the more general null. In model_2, \\(Z\\) increases the variance of \\(Y\\), but because the mean of \\(e\\) is 0, there is no average difference in the means of the treated and control outcomes: they are both zero. In model_3, there are large positive treatment effects for those with \\(e_i &gt; .5\\) and large negative treatment effects for those with \\(e_i \\leq .5\\). On average, the effects offset each other leading to an average effect of \\(Z\\) on \\(Y\\) that is equal to zero. Since there are infinitely many ways of parameterizing this average effect of zero (replace .2 with any number in model_3), there are infinitely many models that correspond to the single null hypothesis that the average effect of Z on Y is equal to zero. Despite the fact that there are many such null models to consider, and the fact that most researchers do not design a project expecting that null hypotheses are true, we see great value in declaring a so-called “null model.” By “null model,” we have in mind something like the potential outcomes in model_1: there is no relationship whatsoever between the outcome of interest and the treatment(s) of interest. You can you learn at least three important things from a “null model” design, or “null design.” First, the power of your null design is an important quantity: it is none other than the rate of false positives for your design, otherwise known as the type 1 error rate. It is important to know if the probability with which you (erroneously) reject the null of a zero average effect is equal to your \\(\\alpha\\): the rate at which you stipulate that you are comfortable erroneously rejecting the null (usually 5%). You might be quite happy to see that your non-null design exhibits great power with small effects, for example. But if you check your null design and see that your power is above 10%, then you know you have a problem: you are rejecting the null of no effect at twice the rate you should be for a stated error rate of 5%. Second, the ability to define the false positive rate for one estimator gives you the ability to define the false positive rate over all of the estimators in your design. This is often referred to as the family-wise error rate: how often at least one of the tests you run erroneously rejects the null of no effect. If your tests are completely unrelated to one another, this rate will just be \\(\\alpha^k\\), where \\(k\\) is the number of tests. But often rejecting one test implies you are more likely to reject another: if an erroneous rejection results from chance imbalance on a variable that is correlated with another variable tested against the same treatment, then you are likely to reject that test too. At the extreme, if you run \\(k\\) equivalent tests, the familywise error rate will be 5%: rejecting one means you reject the rest, failing to reject one means failing to reject the rest. So, as we show in section X, if you use your actual realized data to generate a null design, you can figure out what your actual familywise error rate is under the global null of no effect for any unit or outcome. From there, you can figure out what testwise \\(\\alpha\\) you would need to apply in order to reject any test in the family 5% of the time. Often, this will be a lot less punitive than out-of-the-box corrections for multiple comparisons. Third, and maybe most importantly for practical purposes, a null design does not require any specification about effect sizes. A perennial issue in power analysis is that calculating power requires specification of some arbitrary effect size. But often the very reason we do a study is to determine what the size of an effect is. More meaningful, we think, is using diagnosis to determine the smallest effect you could detect with 80% power. And for that, you only need a good estimate of the standard error, which does not depend on the effect size—it can be derived from a null design. When diagnosing a null design in order to calculate the MDE, it may be worth considering potential outcomes of the form declared in model_2: when trying to determine the standard error, it is important to consider whether the treatment may change the variance in the outcome, even if it does not change the mean. 7.2.8 Consider heterogeneity treatment effects may be stochastic—this can matter for variance estimation treatment effects may vary systematically—many otherwise unbiased designs become biased when this happens, so consider it by default In DeclareDesign, potential outcomes are labelled starting with the outcome (here, Y) followed by a specific parent variable whose effect we are interested in (here, Z), and the value that parent is set to (here 1 and 0), all separated by underscores.↩ Include reference to Pearl and truncation here? E.g. \\(Pr(Y~ \\mid ~do(X)) ~~|| ~~Pr(X)\\)↩ One shortcoming of this \\(M\\)odel is that it has “baked in” the assumption of an exclusion restriction: Y is only affected by Z through Z’s effect on X—Z does not appear in the expression for Y. Returning to the fish example: showing that an instrumental variables answer strategy performs well under this \\(M\\)odel would not be convincing to someone who worried about a particular, unmodeled, violation of the exclusion restriction.↩ "],
["declaring-populations.html", "7.3 Declaring populations", " 7.3 Declaring populations 7.3.1 Use data, if you have it can use it to estimate population parameters for RNGs can build off / bootstrap existing dataset start with a null design to calculate MDE 7.3.2 Incorporate hierarchy into your model Some examples of nested models Time-series models Stochastic group sizes "],
["without-a-model-you-can-get-the-answer-pretty-wrong.html", "7.4 Without a model you can get the answer pretty wrong", " 7.4 Without a model you can get the answer pretty wrong Of course, some research designs require much more \\(M\\)odel than others. In an experiment, we typically worry about everything that happens post-assignment when we draw up a \\(M\\)odel: attrition, compliance, and so forth. But in observational studies, in addition to those issues, the \\(M\\)odel for pretreatment variables can matter a lot. Take the simplest possible design, in which we want to know the effect of a non-randomly assigned treatment, \\(Z\\), on \\(Y\\). Say you have a pretreatment covariate, \\(X\\), that is correlated with both \\(Z\\) and \\(Y\\). Should you control for \\(X\\)? Some authors might argue that, because the values of \\(X\\) are realized prior to the values of \\(Z\\) and time cannot flow backwards, \\(X\\) is causally antecedent to \\(Z\\) and so it follows you cannot do worse by including it. Rosenbaum (2002), for instance, argues that “there is little to no reason to avoid adjustment for a true covariate, a variable describing subjects before treatment.” However, as Greenland, Pearl, and Robins (1999) have shown, it turns out that if \\(Z\\) and \\(Y\\) are not confounded but \\(X\\) and \\(Y\\) are, then controlling for \\(X\\) introduces additional spurious dependency between \\(Z\\) and \\(Y\\) that can create considerable bias. If the question is “should you control for \\(X\\)?” the answer is always “it depends on your \\(M\\)odel.” So, you really need to specify a \\(M\\)odel: doing so helps determine the conditions under which your answers are credible. Without knowing the conditions under which we can believe your answers, it is hard to know whether to believe your answer. References "],
["defining-the-inquiry.html", "Chapter 8 Defining the Inquiry ", " Chapter 8 Defining the Inquiry "],
["classes-of-estimands.html", "8.1 Classes of estimands", " 8.1 Classes of estimands A well defined research design usually requires a well defined question and the quality of a design can often be assessed in terms of how well the question can be answered. In all that follows we will make use of the concept of an estimand, which we take to be quantity that you seek to estimate, it is the correct answer to the question you are asking. It’s October in an election year. Your inquiry is: “how many voters will vote Democrat in November?” The true answer is 66,221,143. This true answer is your estimand, you seek to estimate this number now, even though the election has not happened yet. On the basis of a survey your best guess is 65, 112, 114. This is your estimate for this estimand. In this case the estimand is a number and one that will eventually be revealed, letting you assess how well your estimate measures up against your estimand. But in social science inequiry estimands can take many different forms. We describe eight families of estimand. These different families reflect different social scientific orientations and often different It turns out that many estimands can be thought of as summaries of potential outcomes. TO describe these we will imagine a simple model, in which \\(Y\\) is thought to depend on \\(X\\) and \\(M\\) and \\(M\\) in turn is thought to depend on \\(X\\). We represent this barebones model using a DAG in figure ??. ADD NUMBERS WITH POTENTIAL OUTCOMES USE ONLY PO NOTATION 8.1.1 Descriptive estimands Descriptive estimands can also require inference, not simply measurement. Simplest case level estimand: \\[X = 1\\] “Yes” An example of a descriptive estimand is: \\[E_{i\\in N}(Y)\\] \\[E_{i\\in N}(Y | X=1)\\] This descriptive estimand is to be distinguished from the counterfactual estimand: \\[\\Pr(Y=1 | X \\leftarrow 1)\\] where \\(\\leftarrow\\) is interpreted to mean that \\(X\\) is “set” to the indicated value. Moments; covariance 8.1.2 Simple causal estimands The simplest causal estimand is the outcome that a unit (or group) would have under a possibly counterfactual condition. The expected potential outcome: \\[\\Pr(Y=1 | X \\leftarrow 1)\\] The average treatment effect is a summary of such potential outcomes across two conditions. \\[\\Pr(Y=1 | X \\leftarrow 1) - \\Pr(Y=1 | X \\leftarrow 0)\\] Defined over a population N, the average treatment effect is written using potential outcomes notation as: \\[E_{i\\in N}(Y(X=1) - Y(X=0))\\] These simple estimands might condition on observational quantities, giving rise to the conditional average treatment effect: \\[\\Pr(Y=1 | X \\leftarrow 1, M = 1) - \\Pr(Y=1 | X \\leftarrow 0, M = 1)\\] or, perhaps, controlled conditional average treatment effects: \\[\\Pr(Y=1 | X \\leftarrow 1, M \\leftarrow 1) - \\Pr(Y=1 | X \\leftarrow 0, M \\leftarrow 1)\\] or, perhaps differences in effects: Causes of effects estimand: 8.1.3 Local estimands / EStiamnds over latent classes, principal strata Complier average is also a CATE RDD is also a CATE Estimands as summaries 8.1.4 Study dependent estimands Expected ATT vs realized ATT 8.1.5 Complex counterfactuals 8.1.6 Parametric estimands Dimensionality reduction Model parameters 8.1.7 Vector-valued inquiries (set of predictions for N units), The diagnostic statistic is ith respect to the vector e.g. the CEF QCA estimand. 8.1.8 Models as estimands "],
["selecting-estimands.html", "8.2 Selecting estimands", " 8.2 Selecting estimands 8.2.1 You are responsible for your estimand 8.2.2 Estimands to purpose. Choice of inquiry is not valueless 8.2.3 Estimand scope:What is the set of units which you want to learn the answer about? Know what ATE averages over Implication of estimand definition for analysis 8.2.4 Unknown estimands: Inquiries for discovery "],
["crafting-a-data-strategy.html", "Chapter 9 Crafting a data strategy", " Chapter 9 Crafting a data strategy The data strategy is what researchers do in the world in order to collect information about it. Depending on the design, it could include decisions about any or or all of the following: how to sample or select cases, how to assign treatments, or how to measure outcomes. These choices apply to all kinds of research. In experimental research, a large focus is given to the assignment of treatments. How many treatment conditions should there be? Should we use a simple coin flip to decide who recieves treatment, or should we use a more complicated strategy like blocking? Experimenters are of course also very concerned with sampling and measurement procedures, but it is the random assignment to treatments that make experiments distinctive among research designs. Quantitative descriptive research, on the other hand, often has an inquiry like the population average of some outcome variable. Since the goal here is to draw inferences about a population on the basis of a sample, we need to pay special attention to the procedure by which units are selected into the sample. We might use a random sampling procedure in order to generate a design-based justification for generalizing from samples to population. Nonrandom sampling procedures are also possible: convenience sampling, respondent-driven sampling, and snowball sampling are all data strategies that do not include an explictly random component. Once we have selected units into the sample, we need to measure them in some way. The tools we use to measure are a critical part of the data strategy. For many social scientific studies, a main way we collect information is through surveys. A huge methodological literature on survey administration has developed to help guide researchers who have to design questionnaires. Bad survey questions yield distorted or noisy responses. They can be distored if responses are systematically biased away from the true latent target the question is designed to measure, in which case the question has low validity. They can be noisy if (hypothetically) you would obtain different answers each time you asked the same person the same question, in which case the question has low reliability. Beyond surveys, we might use administrative data to collect outcomes. The concerns about validity and reliability do not disappear once we move out of the survey environment. The information that shows up in an administrative database is itself the result of many human decisions, each of which has the possibility of increasing or decreasing the distance between the measurement and the thing to be measured. Researchers have to choose good sampling, assignment, and measurement techniques that, when combined and applied to the world, will produce information that is ready for analysis. We will discuss answer strategies – the set of analysis choices about what to do with the data once it’s collected – in the next chapter. The data and answer strategies are of course intimately interconnected. How you analyze data depends deeply on how it was collected and how you collect data depends just as deeply on how you plan to analyze it. For the moment, we are thinking through the many choices we might make as part of the data strategy, but of course they will have to be considered in concert with the answer strategy in any applied research design setting. The data strategy is a set of procedures that result in a dataset. It is important to keep these two concepts straight. If you apply data strategy \\(D\\), it produces dataset \\(d\\). The data \\(d\\) is the result of the data strategy \\(D\\). We say \\(d\\) is “the” result of \\(D\\), since when we apply the data strategy to the world, we only do so once and we obtain the data that we obtain. But when we are crafting a data strategy, we have to think about the many datasets that the data strategy could have produced. Some of the datasets might be really excellent. For example, in good datasets, we achieve good covariate balance across the treatment and control groups. Or we might draw a sample whose distribution of observable characteristics looks really similar to the population. But some of the datasets might be worse: because of the vagaries of randomization, the particular realizations of the random assignment or random sampling might more more or less balanced. We do not have to settle for data strategies that can produce worse datasets! We want to choose data strategy \\(D\\) that is likely to result in a high-quality dataset \\(d\\). "],
["choosing-a-sampling-procedure.html", "9.1 Choosing a sampling procedure", " 9.1 Choosing a sampling procedure simple, complete, stratified, clustered, stratified and clustered weighted sampling (over/undersampling) quota sampling "],
["choosing-an-assignment-procedure.html", "9.2 Choosing an assignment procedure", " 9.2 Choosing an assignment procedure simple, complete, blocked, clustered, blocked and clustered point restricted randomization no assignment procedure at all multiple arms "],
["choosing-a-measurement-procedure.html", "9.3 Choosing a measurement procedure", " 9.3 Choosing a measurement procedure Should this be where we do the first bit of distinction between latent and observed more T (david McKenzie). How frequently to measure. Andy - arguing against intermediate measurement? multiple measurements of Y. make a scale "],
["section.html", "9.4 ", " 9.4 just downloading the data. Did you offload the data strategy possibly ambiguous where the data strategy ends and the analysis strategy ends. SOMEone did parts of the datastrategy "],
["choosing-an-answer-strategy.html", "Chapter 10 Choosing an answer strategy", " Chapter 10 Choosing an answer strategy "],
["what-belongs-in-an-answer-strategy.html", "10.1 What belongs in an answer strategy", " 10.1 What belongs in an answer strategy 10.1.1 estimate-estimand pairs once you have the data, you need to have a procedure to develop an answer or a decision from it. it should be quantitative or qualitative. you will connect an estimator to an estimand, and the estimator designed to produces estimates of the estimand. distinguish estimate/estimator, using the notation from the paper (am Am etc.). a function to produce an estimate and measure(s) of uncertainty of the estimate. may be as simple as a mean or difference-in-means, as in our simple design: estimates_df &lt;- difference_in_means(Y ~ Z, data = simple_design_data) kable(tidy(estimates_df)) term estimate std.error statistic p.value conf.low conf.high df outcome Z 0.5 0.5 1 0.5 -5.853102 6.853102 1 Y in this case, there is a single statistic (the average difference between outcomes in treated and controlled) that represents the estimate. this is our guess of the estimand, the average treatment effect. 10.1.2 measures of uncertainty in addition, we have several statistics that assess the uncertainty of the estimate, here the standard error and a frequentist confidence interval. the answer strategy is not just how you get to the answer, but how sure you are of it. we often also have statistics related to hypothesis testing, here a test statistics and p-value under the null hypothesis of a zero average treatment effect. our “answer” may either be the estimate of the average treatment effect, or in some cases the decision, is there a non-zero average treatment effect. your answer strategy is the full set of steps from first seeing the data until the estimate of the estimand you present in the paper, which is usually more than just the estimate, its uncertainty measure, and associated hypothesis test. 10.1.3 procedures procedures, if any, by which you explore the data and determine a final set of estimates are part of the answer strategy. for example, we sometimes find that the model we planned to run to analyze the data cannot be estimated. in these cases, there is an iterative estimation procedure in which a first model is run, changes to the specification are made, and a second or third model is presented as the result. that full set of steps – a decision tree, depending on what is estimable – is the answer strategy and we can evaluate whether it is a good one not only under the realized data but under other possible realizations where the decision tree would be the same but the decisions different. procedures where you run two procedures and pick the best fit or preferred on some dimensions show example of a procedure of this form (model selection?) where the coverage is off if you don’t account for the multi step report_if_significant &lt;- function(data){ fit_nocov &lt;- lm_robust(Y ~ Z, data) fit_cov &lt;- lm_robust(Y ~ Z + X, data) # select fit with lower p.value on Z if(fit_cov$p.value[2] &lt; fit_nocov$p.value[2]){ fit_selected &lt;- fit_cov } else { fit_selected &lt;- fit_nocov } fit_selected %&gt;% tidy %&gt;% filter(term == &quot;Z&quot;) } design &lt;- declare_population( N = 100, X = rbinom(N, 1, 0.5), u = rnorm(N) ) + declare_potential_outcomes(Y ~ 0.25 * Z + 10 * X + u) + declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + declare_assignment(prob = 0.5) + declare_reveal(Y, Z) + declare_estimator(Y ~ Z, model = lm_robust, label = &quot;nocov&quot;, estimand = &quot;ATE&quot;) + declare_estimator(Y ~ Z, model = lm_robust, label = &quot;cov&quot;, estimand = &quot;ATE&quot;) + declare_estimator( handler = label_estimator(report_if_significant), label = &quot;select-lower-p-value&quot;, estimand = &quot;ATE&quot;) dg &lt;- diagnose_design(design, sims = sims) procedures for testing assumptions of identification strategy before running analysis, such as falsification or placebo tests. in these tests, you run a test and only analyze the data using the analysis strategy you proposed if it passes the test indicating a failure to reject the null of no violation of the assumptions. show example of RDD where the method is biased if you don’t use the assumption test but the procedure is unbiased conditional_on_placebo_test &lt;- function(data) { placebo_test &lt;- lm_robust(Y_placebo ~ Z, data) %&gt;% tidy %&gt;% filter(term == &quot;Z&quot;) estimate &lt;- lm_robust(Y ~ Z, data) %&gt;% tidy %&gt;% filter(term == &quot;Z&quot;) if(placebo_test$p.value &lt;= 0.05) { tibble(estimate = NA, reject = TRUE, term = &quot;Z&quot;) } else { estimate %&gt;% mutate(reject = FALSE) } } library(sn) placebo_design &lt;- declare_population(N = 100, u = rsn(n = N, xi = 0, omega = 1, alpha = 10)) + declare_potential_outcomes(Y ~ 0.25 * Z + u) + declare_potential_outcomes(Y_placebo = 0.1 + 1.2 * u) + declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + declare_assignment(prob = 0.5) + declare_estimator(Y ~ Z, model = lm_robust, estimand = &quot;ATE&quot;, label = &quot;unconditional&quot;) + declare_estimator(handler = label_estimator(conditional_on_placebo_test), estimand = &quot;ATE&quot;, label = &quot;conditional&quot;) simulations_df &lt;- simulate_design(placebo_design, sims = sims) # diag &lt;- diagnose_design(placebo_design, sims = sims) simulations_df %&gt;% group_by(estimator_label) %&gt;% summarize(bias = mean(estimate - estimand, na.rm = TRUE)) ## # A tibble: 2 x 2 ## estimator_label bias ## &lt;chr&gt; &lt;dbl&gt; ## 1 conditional -0.0403 ## 2 unconditional -0.00311 precommittment is part of the answer strategy 10.1.4 robustness checks robustness checks are part of the answer strategy. often, a single estimator is presented as the main analysis but then a series of alternative specifications are displayed in an appendix (such as including or excluding covariates and their interactions, different subsets of the data, or alternative statistical models). the purpose is to provide readers with evidence about how dependent the main results are on the specification, data subset, and statistical model used. when this is the case, the decision a reader makes based on their inferences about the estimand from the estimate depend not only on the main estimate but also the robustness checks. as a result, we want to assess the properties of the two together. We illustrate with a simple analysis of the correlation between two variables y1 and y2, who have a true positive correlation. y2 is also a function of an observed covariate x and measurement error. Our main analysis is a bivariate regression predicting y2 with y1. We compare this answer strategy to one in which we run that analysis, but also run a robustness check controlling for x. We do this because as the analyst we are unsure of the true DGP and wish to demonstrate to reviewer’s that our results are not dependent on the functional form we choose. bivariate_correlation_decision &lt;- function(data) { fit &lt;- lm_robust(y2 ~ y1, data) %&gt;% tidy %&gt;% filter(term == &quot;y1&quot;) tibble(decision = fit$p.value &lt;= 0.05) } interacted_correlation_decision &lt;- function(data) { fit &lt;- lm_robust(y2 ~ y1 + x, data) %&gt;% tidy %&gt;% filter(term == &quot;y1&quot;) tibble(decision = fit$p.value &lt;= 0.05) } robustness_check_decision &lt;- function(data) { main_analysis &lt;- bivariate_correlation_decision(data) robustness_check &lt;- interacted_correlation_decision(data) tibble(decision = main_analysis$decision == TRUE &amp; robustness_check$decision == TRUE) } robustness_checks_design &lt;- declare_population( N = 100, x = rnorm(N), y1 = rnorm(N), y2 = 0.15 * y1 + 0.01 * x + rnorm(N) ) + declare_estimand(y1_y2_are_related = TRUE) + declare_estimator(handler = label_estimator(bivariate_correlation_decision), label = &quot;bivariate&quot;) + declare_estimator(handler = label_estimator(robustness_check_decision), label = &quot;robustness-check&quot;) decision_diagnosis &lt;- declare_diagnosands(correct = mean(decision == estimand), keep_defaults = FALSE) diag &lt;- diagnose_design(robustness_checks_design, sims = sims, diagnosands = decision_diagnosis) We evaluate the two answer strategies in terms of the rate of correctly deciding there is a correlation between y2 and y1. In the main analysis, this means we judge there is a correlation when the p-value is below \\(0.05\\). In our robustness check answer strategy, we decide there is a correlation when both the main analysis and the robustness check return p-values below \\(0.05\\) on the coefficient on y1. We see that we are more likely to correctly judge there is a correlation in the simpler analysis strategy. This is because we added an additional criterion to our decision; both criteria, due to random noise, sometimes fail to reject the null of no correlation. Our second answer strategy is more robust in the sense that we have stronger evidence of a correlation when we run the two analyses together. But we are also less likely to decide (correctly) that there is a relationship. The robustness check is conservative. This exercise highlights that the properties of an answer strategy with secondary analyses will be different than the properties of the main analysis alone. If we planned (or conducted) robustness checks, we may wish to know how good the pair of strategies is together. distinguish this from changes to the model where we do robustnesss vis a vis a fixed answer and data strategy. i.e. two notions of “robustness”. one is fix I D A and change M, is this “design” robust to changes in M. the other is, within a given run, is the estimate “robust” to changing the estimation procedure, so this is a diagnostic statistic. note I must be defined across these changes in M. Using the MIDA way of thinking about designs, we can also think of another notion of the “robustness” of a design. The typical way we think of robustness checks is multiple secondary analyses conditional on the observed data to build confidence in an analysis of that fixed data. However, the motivation for these robustness checks is uncertainty about the true data generating process. By declaring a design in terms of MIDA, we can think about the robustness of a single estimator to multiple possible true data generating processes. An estimator that is robust in this sense is one that is unbiased with low uncertainty regardless of, say, the true functional form between y1 and y2. To determine whether an estimator is robust, we can redefine a set of designs with different functional forms and assess the rate of correct decisions of our robustness checks strategy under each different model. robustness_checks_design &lt;- robustness_checks_design + declare_estimator(handler = label_estimator(interacted_correlation_decision), label = &quot;interacted&quot;) robustness_checks_design_dgp2 &lt;- replace_step( robustness_checks_design, step = 1, new_step = declare_population( N = 100, x = rnorm(N), y1 = rnorm(N), y2 = 0.15 * y1 + 0.01 * x + 0.05 * y1 * x + rnorm(N) ) ) robustness_checks_design_dgp3 &lt;- replace_step( robustness_checks_design, step = 1, new_step = declare_population( N = 100, x = rnorm(N), y1 = 0.15 * x + rnorm(N), y2 = 0.15 * x + rnorm(N) ) ) robustness_checks_design_dgp3 &lt;- replace_step( robustness_checks_design_dgp3, step = 2, new_step = declare_estimand(y1_y2_are_related = FALSE) ) decision_diagnosis &lt;- declare_diagnosands(correct = mean(decision == estimand), keep_defaults = FALSE) diag &lt;- diagnose_design( robustness_checks_design, robustness_checks_design_dgp2, robustness_checks_design_dgp3, sims = sims, diagnosands = decision_diagnosis) 10.1.5 presentation of results how you present the estimates — graphically, in tables, and in text — are all parts of the answer strategy. this is because the inferences readers make about the estimand from your paper do not just come from the numerical estimate. in some cases, the number may not even be presented exactly, and instead a graphic of the estimate and its confidence interval is what readers rely on. lots of advice to present graphically (cite), what are implications of that? the decisions made from your results by readers are not just a function of numerical estimates but how they are presented. We explore this by comparing two possible graphical displays of conditional avareage treatment effects in an experiment. A common presentational format is to present the average treatment effect in one group and then the other along with confidence intervals. Inferences are made — either by the author, or by readers — as a function of whether one is significant and not the other. If that is true, the inference is that there is a difference in CATEs. An alternative is to present the estimated difference along with the two effects. The inferences can then directly be based on whether the confidence interval of the difference crosses zero. We illustrate these two visual answer strategies below: We now demonstrate that the answer strategy on the left is flawed. XXYY describe sims. 10.1.6 multiple comparisons your answer strategy should take into account how many statistical tests you are conducting, not just focus on the estimate-estimand pair. when you present the results from many null hypothesis tests, the rate of falsely rejecting at least one of those tests even when all are true goes up, due to the multiple comparisons problem. if you plan to adjust for this problem, those adjustments are part of your answer strategy, because they will typically adjust the p-values you report and the decisions readers make with them. as this seection has highlighted, the answer strategy is intimately connected with the data strategy. people often think of their entire research design as the answer strategy. but they can’t be separated. 10.1.7 what you will do when data goes sideways to compare answer strategies, you can imagine the estimators that are possible if things go well as well as if things go wrong, when there is missing data or there are outliers in variables. a good answer strategy (which might be a single estimator, or a procedure if-this-then-that) can handle both states of the world. procedures for addressing deviations from expected analyses are part of the answer strategy. whether a study has a PAP or not, we often have a way we expect to analyze the data if things go well. when they do not – because data are missing, there is noncompliance to an intervention, or the study is suspended for example – the answers will change. these procedures determine the answer the study provides (or in some cases does not), so are part of the answer strategy. standard operating procedures (lin and green) are documents that systematize these procedures in advange. point is about what you do not the model aspects of this properties of imputation procedure standard operating procedures "],
["how-to-select-an-answer-strategy.html", "10.2 How to select an answer strategy", " 10.2 How to select an answer strategy introduce classes of estimators: qual/quant, frequentist/bayesian, design based and model based (logit probit etc.) talk about issue of model-based vs design based, as separated from the model you assume in M. in model based you run a procedure that assumes a dgp, which may or may not be connected to the M. your data strategy should shape your answer strategy (analyse as you randomize) assignment strategies (blocks and/or clusters, heterogeneous assignment probabilities, etc.) sampling strategies (strata and/or clusters, heterogeneous sampling probabilities, etc.) this is true not just for experiments but for surveys (how did you sample), natural experiments (how did nature assign the treatment), and other designs you can select an answer strategy in advance, by simulating data. when estimators are selected with the data in hand, choices are often made in response to the realized data through examining model fit statistics that appear ideal in the context of this data, but are not ideal from the perspective of other data that could have been collected. we want answer strategies that perform well no matter how the data turn out. issues are: overfitting, selecting a suboptimal estimator from design perspective that passes a model fit statistic (show this is worse in a simple example) "],
["diagnosis-1.html", "Chapter 11 Diagnosis", " Chapter 11 Diagnosis "],
["diagnosing-a-single-design.html", "11.1 Diagnosing a single design", " 11.1 Diagnosing a single design Definition and practical details of Monte Carlo and diagnosands (and discussion of formulae) [JC] Graphic of simulations (of multiple runs) "],
["how-do-you-select-diagnosands.html", "11.2 How do you select diagnosands?", " 11.2 How do you select diagnosands? Diagnose given the purposes of the study Single shot vs repeated designs (MSE vs bias) Moral questions (Type 1 vs Type 2 errors) Power for biased designs Standard diagnosands (paragraph on each of the diagnosands in our defaults) Ways of getting answers to a question wrong Diagnosing inferential statistics (SE bias vs. coverage, error rates for ps, Bayes?) How to select diagnosands (some sort of decision tree?) Multiple estimates / inquiries [JC] How to think about uncertainty about model parameters (multiple designs?) Diagnosands that are a function of “multiple designs” like MDE Conditional diagnosands different? Uncertainty of diagnosands (bootstrapping etc.) "],
["diagnosis-to-assess-the-robustness-of-designs-to-models-gb.html", "11.3 Diagnosis to assess the robustness of designs to models [GB]", " 11.3 Diagnosis to assess the robustness of designs to models [GB] Hold inquiry constant! (read Richard Crump “Moving the Goalposts”) Hold three constant, vary one of MIDA at a time M: ICC, null model, alternative DAGs, heterogeneity I: "],
["redesign-1.html", "Chapter 12 Redesign", " Chapter 12 Redesign "],
["part-ii-exercises.html", "Chapter 13 Part II Exercises", " Chapter 13 Part II Exercises Imai, King, and Stuart (2008) Imagine you are a reviewer on a paper that claims smoking causes a smoking addiction. The data offered in support of this claim shows that all 100 subjects who smoke are addicted to smoking and that all 100 subject who do not smoke are not addicted. is not addicted to smoking is addicted to smoking does not smoke 0 100 smokes 100 0 Do the data support the conclusion that smoking causes addiction to smoking? Now imagine you are a reviewer on a paper that claims sailing causes a sailing addiction. The data offered in support of this claim shows that all 100 subjects who sail are addicted to sailing and that all 100 subject who do not sail are not addicted. Do the data support the conclusion that sailing causes addiction to sailing? is not addicted to sailing is addicted to sailing does not sail 0 100 sails 100 0 How do your answers to (a) and (b) differ? Sketch (either in words or with a DAG) the alternative causal models for smoking and sailing that could have produced the two datasets. Propose a data and answer strategy that would distinguish between the two causal models you described in (c). The Complier Average Causal Effect (CACE) is defined as the average effect of treatment on a subset of subjects who “comply” with their treatment assignment, i.e., if assigned to treatment, they take treatment but if assigned to control, they do not take treatment. The Average Treatment Effect on the Treated (ATT) is defined as the average effect of the treatment on those subjects who were treated. These two estimands are subtley different. Declare a design and draw_estimands from it to demonstrate that these two estimands can be different. Hint: the ATT depends on the data strategy but the CACE does not. References "],
["design-library.html", "Chapter 14 Design Library", " Chapter 14 Design Library This section of the book enumerates a series of common social science research designs. Each entry will include description of the design in terms of MIDA and also a declaration of the design in code. We’ll often diagnose designs over the range of values of some design parameters in order to point out especially interesting or unusual features of the design. Our goal in this section is not to provide a comprehensive accounting of all empirical research designs. It’s also not to describe any of the particular designs in exhaustive detail, because we are quite sure that in order for these designs to be useful for any practical purpose, they will need to be modified. The entries in the design library are not recipes that, if you follow the instructions, out will come high-quality research. Instead, we hope that the entries provide inspiration for how to tailor a particular class of designs – the blocked-and-clustered randomized trial or the catch-and-release design – to your own research setting. The basic structure of the design library entry will be useful, but the specifics about plausible ranges of outcomes, sample size constraints, etc, will be different in each particular setting. We’ve split up designs by Inquiry and by Data strategy. Inquires can be descriptive or causal and Data strategies can be observational or experimental. This leads to four categories of research: Observational descriptive, Experimental descriptive, Observational Causal, and Experimental causal. A third dimension along which studies can vary is whether the Answer strategy is qualitative or quantitative. If we include this dimension in our typology, we’d end up with eight broad categories of research design. We don’t see the qualitative-quantitative difference in answer strategy to be as fundamental as the differences in inquiry and data strategy, so we’ll just include both qualitative and quantitative designs in each of our four categories. Besides, social scientists always appreciate a good two-by-two: In the broadest terms, descriptive inquiries can be described as \\(f(\\mathbf{Y(Z = Realized)})\\), where \\(f()\\) is some function and \\(\\mathbf{Y(Z = Realized)}\\) is a vector of realized outcomes. That is, descriptive designs seek to summarize (using \\(f()\\)) the world as it is (as represented by \\(\\mathbf{Y(Z = Realized)}\\)). Descriptive designs can be better or worse at answering that inquiry. The quality of descriptive research designs depends on the extent of measurement, sampling, and estimation error. Causal inquiries can be described as \\(f(\\mathbf{Y(Z)})\\), where \\(Z\\) is not a realized vector of treatments, but is instead is a vector that could take on counterfactual values. A standard causal inquiry is the Average Treatment Effect, in which \\(f()\\) is the function that takes the average of the difference between two potential outcome vectors, \\(Y(Z = 1)\\) and \\(Y(Z = 0)\\). But there are many causal inquiries beyond the ATE – the thing they all have in common is that they are functions not of realized outcomes, but of potential outcomes. The quality of causal research designs depends on everything that a descriptive design depends on, but also on the understanding and quality of the mechanism that assigns units to treatment conditions. All research designs suffer from some kind of missing data problem. Rubin pointed out missing data in surveys come from people you didn’t survey or people who refused to answer. In causal inference problems, the data that are missing are the potential outcomes that were not revealed by the world. In Descriptive studies, the data that are missing are the true values of the things to be measured. Measurement error is a missing data problem too! Observational research designs are typified by researchers having no impact on the units under study. They simply record the outcomes that happened in the world and would have happened even if the study did not occur. Experimental research designs are more active – they cause some potential outcomes to be revealed but not others. In this way, researchers have an impact on the units they study. For this reason, experimental studies tend to raise more ethical questions than do observational studies. Experimenters literally change what potential outcomes become realized outcomes. Sometimes the lines between types of research become blurry. The Hawthorne effect is the name given to the idea that measuring a thing changes it. If there are Hawthorne effects, than observational research designs also change which potential outcomes are revealed. That is, if there is a difference between Y(Z = measured) and Y(Z = unmeasured), then the act of observation changes that which is observed. Passive data collection methods are sometimes preferred on these grounds. "],
["observational-designs-for-descriptive-inference.html", "Chapter 15 Observational designs for descriptive inference", " Chapter 15 Observational designs for descriptive inference section introduction "],
["random-sampling.html", "15.1 Random sampling", " 15.1 Random sampling 15.1.1 Simple random sampling Often we are interested in features of a population, but data on the entire population is prohibitively expensive to collect. Instead, researchers obtain data on a small fraction of the population and use measurements taken on that sample to draw inferences about the population. Imagine we seek to estimate the average political ideology of residents of the small town of Portola, California, on a left-right scale that varies from 1 (most liberal) to 7 (most conservative). We draw a simple random sample in which all residents have an equal chance of inclusion in the study. It’s a straightforward design but formally declaring it will make it easy to assess its properties. 15.1.1.1 Design Declaration Model: Even for this most basic of designs, researchers bring to bear a background model of the world. As described in Chapter 1, the three elements of a model are the signature, probability distributions over variables, and functional equations among variables. The signature here is a specification of the variable of interest, \\(Y\\), with a well defined domain (seven possible values between 1 and 7). In the code declaration below, we assume a uniform distribution over these 7 values. This choice is a speculation about the population distribution of \\(Y\\); some features of the design diagnosis will depend on the choice of distribution. The functional equations seem absent here as there is only one variable in the model. We could consider an elaboration of the model that includes three variables: the true outcome, \\(Y\\); the decision to measure the outcome, \\(M\\); and the measured outcome, \\(Y^M\\). We ignore this complication for now under the assumption that \\(Y = Y^M\\), i.e., that \\(Y\\) is measured perfectly. Finally, the model also includes information about the size of the population. Portola, California, has a population of approximately 2100 people as of 2010, so \\(N = 2100\\). Inquiry: Our inquiry is the population mean of \\(Y\\): \\(\\frac{1}{N} \\sum_1^N Y_i = \\bar{Y}\\). Data strategy: In simple random sampling, we draw a random sample without replacement of size \\(n\\), where every member of the population has an equal probability of inclusion in the sample, \\(\\frac{n}{N}\\). When \\(N\\) is very large relative to \\(n\\), units are drawn approximately independently. In this design we measure \\(Y\\) for \\(n=100\\) units in the sample; the other \\(N-n\\) units are not measured. Answer strategy: We estimate the population mean with the sample mean estimator: \\(\\widehat{\\overline{Y}} = \\frac{1}{n} \\sum_1^n Y_i\\). Even though our inquiry implies our answer should be a single number, an answer strategy typically also provides statistics that help us assess the uncertainty around that single number. To construct a 95% confidence interval around our estimate, we calculate the standard error of the sample mean, then approximate the sampling distribution of the sample mean estimator using a formula that includes a finite population correction. In particular, we approximate the estimated sampling distribution by a \\(t\\) distribution with \\(n - 1\\) degrees of freedom. In the code for our answer strategy, we spell out each step in turn. # Model ------------------------------------------------------------------- N &lt;- 2100 fixed_population &lt;- declare_population(N = N, Y = sample(1:7, N, replace = TRUE))() population &lt;- declare_population(data = fixed_population) # Inquiry ----------------------------------------------------------------- estimand &lt;- declare_estimand(Ybar = mean(Y)) # Data Strategy ----------------------------------------------------------- n &lt;- 100 sampling &lt;- declare_sampling(n = n) # Answer Strategy --------------------------------------------------------- estimator &lt;- declare_estimator(Y ~ 1, model = lm_robust, estimand = estimand, label = &quot;Sample Mean Estimator&quot;) # Design ------------------------------------------------------------------ design &lt;- population + estimand + sampling + estimator diagnosands &lt;- declare_diagnosands(select = c(bias, coverage, mean_estimate, sd_estimate)) 15.1.1.2 Takeaways With the design declared we can run a diagnosis and plot results from Monte Carlo simulations of the design: diagnosis &lt;- diagnose_design( design, sims = sims, bootstrap_sims = b_sims, diagnosands = diagnosands) The diagnosis indicates that under simple random sampling, the sample mean estimator of the population mean is unbiased. The graph on the left shows the sampling distribution of the estimator: it’s centered directly on the true value of the inquiry. Confidence intervals also have a sampling distribution – they change depending on the idiosyncrasies of each sample we happen to draw. The figure on the right shows that the 95% of the time the confidence intervals cover the true value of the estimand, as they should. As sample size grows, the sampling distribution of the estimator gets tighter, but the coverage of the confidence intervals stays at 95% – just the properties we would want out of our answer strategy. Things work well here it seems. In the exercises we suggest some small modifications of the design that point to conditions under which things might break down. 15.1.2 Stratified and clustered random sampling Researchers often cannot randomly sample at the individual level because it may, among other reasons, be too costly or logistically impractical. Instead, they may choose to randomly sample households, political precincts, or any group of individuals in order to draw inferences about the population. This strategy may be cheaper and simpler but may also introduce risks of less precise estimates. Say we are interested in the average party ideology in the entire state of California. Using cluster sampling, we randomly sample counties within the state, and within each selected county, randomly sample individuals to survey. Assuming enough variation in the outcome of interest, the random assignment of equal-sized clusters yields unbiased but imprecise estimates. By sampling clusters, we select groups of individuals who may share common attributes. Unlike simple random sampling, we need to take account of this intra-cluster correlation in our estimation of the standard error.15 The higher the degree of within-cluster similarity, the more variance we observe in cluster-level averages and the more imprecise are our estimates.16 We address this by considering cluster-robust standard errors in our answer strategy below. 15.1.2.1 Design Declaration Model: We specify the variable of interest \\(Y\\) (political ideology, say) as a discrete variable ranging from 1 (most liberal) to 7 (most conservative). We do not define a functional model since we are interested in the population mean of \\(Y\\). The model also includes information about the number of sampled clusters and the number of individuals per cluster. Inquiry: Our estimand is the population mean of political identification \\(Y\\). Because we employed random sampling, we can expect the value of the sample mean (\\(\\widehat{\\overline{y}}\\)) to approximate the true population parameter (\\(\\widehat{\\overline{Y}}\\)). Data strategy: Sampling follows a two-stage strategy. We first draw a random sample 30 counties in California, and in each county select 20 individuals at random. This guarantees that each county has the same probability of being included in the sample and each resident within a county the same probability of being in the sample. In this design we estimate \\(Y\\) for n = 600 respondents. Answer strategy: We estimate the population mean with the sample mean estimator: \\(\\widehat{\\overline{Y}} = \\frac{1}{n} \\sum_1^n Y_i\\), and estimate standard errors under the assumption of independent and heteroskedastic errors as well as cluster-robust standard errors to take into account correlation of errors within clusters. Below we demonstrate the the imprecision of our estimated \\(\\widehat{\\overline{Y}}\\) when we cluster standard errors and when we do not in the presence of an intracluster correlation coefficient (ICC) of 0.402. N_blocks &lt;- 1 N_clusters_in_block &lt;- 1000 N_i_in_cluster &lt;- 50 n_clusters_in_block &lt;- 30 n_i_in_cluster &lt;- 20 icc &lt;- 0.402 # M: Model fixed_pop &lt;- declare_population( block = add_level(N = N_blocks), cluster = add_level(N = N_clusters_in_block), subject = add_level(N = N_i_in_cluster, latent = draw_normal_icc(mean = 0, N = N, clusters = cluster, ICC = icc), Y = draw_ordered(x = latent, breaks = qnorm(seq(0, 1, length.out = 8))) ) )() cluster_sampling_design &lt;- declare_population(data = fixed_pop) + # I: Inquiry declare_estimand(Ybar = mean(Y)) + # D: Data Strategy declare_sampling(strata = block, clusters = cluster, n = n_clusters_in_block, sampling_variable = &quot;Cluster_Sampling_Prob&quot;) + declare_sampling(strata = cluster, n = n_i_in_cluster, sampling_variable = &quot;Within_Cluster_Sampling_Prob&quot;) + # A: Answer Strategy declare_estimator(Y ~ 1, model = lm_robust, clusters = cluster, estimand = &quot;Ybar&quot;, label = &quot;Clustered Standard Errors&quot;) 15.1.2.2 Takeaways diagnosis &lt;- diagnose_design(cluster_sampling_design, sims = sims) Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Power Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand cluster_sampling_design Ybar Clustered Standard Errors (Intercept) 500 0.01 0.25 1.00 0.95 3.97 0.25 0.25 0.00 3.97 (0.01) (0.01) (0.00) (0.01) (0.01) (0.01) (0.00) (0.00) (0.00) To appreciate the role of clustering better we also plot simulated values of our estimand with standard errors not clustered and with clustered standard errors. To do this we first add an additional estimator to the design that does not take account of clusters. new_design &lt;- cluster_sampling_design + declare_estimator(Y ~ 1, model = lm_robust, estimand = &quot;Ybar&quot;, label = &quot;Naive Standard Errors&quot;) diagnosis &lt;- diagnose_design(new_design, sims = sims) The figure above may give us the impression that our estimate with clustered standard errors is less precise, when in fact, it correctly accounts for the uncertainty surrounding our estimates. The blue lines in the graph demonstrate the estimates from simulations which contain our estimand. As our table and graphs show, the share of these simulations over the total number of simulations, also known as coverage, is (correctly) close to 95% in estimations with clustered standard errors and 54% in estimations without clustered standard errors. As expected, the mean estimate itself and the bias is the same in both specifications. 15.1.2.3 Exercises Modify the declaration to change the distribution of \\(Y\\) from being uniform to something else: perhaps imagine that more extreme ideologies are more prevalent than moderate ones. Is the sample mean estimator still unbiased? Interpret your answer. Change the sampling procedure to favor units with higher values of ideology. Is the sample mean estimator still unbiased? Interpret your answer. Modify the estimation function to use this formula for the standard error: \\(\\widehat{se} \\equiv \\frac{\\widehat\\sigma}{\\sqrt{n}}\\). This equation differs from the one used in our declaration (it ignores the total population size \\(N\\)). Check that the coverage of this new design is incorrect when \\(N=n\\). Assess how large \\(N\\) has to be for the difference between these procedures not to matter. The intra-cluster correlation coefficient (ICC) can be calculated directly and is a feature of this design.↩ In ordinary least square (OLS) models, we assume errors are independent (error terms between individual observations are uncorrelated with each other) and homoskedastic (the size of errors is homogeneous across individuals). In reality, this is often not the case with cluster sampling.↩ "],
["multilevel-regression-and-poststratification.html", "15.2 Multilevel regression and poststratification", " 15.2 Multilevel regression and poststratification You can use the global bib file via rmarkdown cites like this: Imai, King, and Stuart (2008) district population_size prop_white prop_black prop_asian prop_hispanic_other prop_democrat prop_republican 1 41995 0.6848196 0.2414573 0.0325039 0.0500774 0.5185611 0.2436329 2 41076 0.2573035 0.6714870 0.0072305 0.0904421 0.7334309 0.0992582 3 40878 0.3384706 0.5112775 0.0085376 0.2235677 0.6786992 0.1176531 4 41287 0.8557415 0.0381718 0.0790564 0.0357498 0.3469342 0.3956847 5 40722 0.8473061 0.0709199 0.0597957 0.0208732 0.3942886 0.3638110 6 41985 0.8783613 0.0674765 0.0123854 0.0431821 0.4042878 0.3675094 # US population delaware_population_df &lt;- fabricate( data = delaware_senate_districts_df, individuals = add_level( N = population_size, race_white = rbinom(N, 1, prob = prop_white), race_black = rbinom(N, 1, prob = prop_black), race_asian = rbinom(N, 1, prob = prop_black), race_hispanic_other = rbinom(N, 1, prob = prop_hispanic_other), pid_republican = rbinom(N, 1, prob = prop_republican), pid_democrat = rbinom(N, 1, prob = prop_democrat) ) ) %&gt;% select(-starts_with(&quot;prop_&quot;), -population_size) # population weights for MRP mrp_weights &lt;- delaware_population_df %&gt;% group_by(district, race_white, race_black, race_asian, race_hispanic_other, pid_republican, pid_democrat) %&gt;% summarize(n_cell = n()) %&gt;% group_by(district) %&gt;% mutate(proportion_cell = n_cell/sum(n_cell)) %&gt;% select(-n_cell) %&gt;% ungroup delaware_population_df &lt;- mrp_weights %&gt;% select(district, proportion_cell) %&gt;% right_join(delaware_population_df) # Lax and Philips APSR 2009 # Policies are coded dichotomously, 1 for the progay policy and 0 otherwise: Adoption (9 states allow second-parent adoption in all jurisdictions) design &lt;- declare_population( data = delaware_population_df, districts = modify_level(district_effect = rnorm(N)), individuals = modify_level( noise = rnorm(N, mean = district_effect), policy_support = rbinom(N, 1, prob = pnorm( 0.25 + 0.2 * race_white - 0.1 * race_black - 0.2 * race_hispanic_other - 0.1 * pid_democrat + 0.15 * pid_republican + noise)) ) ) + declare_estimand(handler = function(data) { data %&gt;% group_by(district) %&gt;% summarize(estimand = mean(policy_support)) %&gt;% ungroup %&gt;% mutate(estimand_label = &quot;mean_policy_support&quot;) }) + declare_sampling(n = 500) + declare_estimator(handler = tidy_estimator(function(data) { data %&gt;% group_by(district) %&gt;% summarize(estimate = mean(policy_support)) }), label = &quot;strata_means&quot;, estimand = &quot;mean_policy_support&quot;) + # this estimator owes code to https://timmastny.rbind.io/blog/multilevel-mrp-tidybayes-brms-stan/ declare_estimator(handler = tidy_estimator(function(data) { model_fit &lt;- glmer( formula = policy_support ~ race_white + race_black + race_asian + race_hispanic_other + pid_democrat + pid_republican + (1 | district), data = data, family = binomial(link = &quot;logit&quot;)) data %&gt;% mutate( support_predicted = prediction(model_fit, data = ., allow.new.levels = TRUE, type = &quot;response&quot;), support_predicted_weighted = support_predicted * proportion_cell ) %&gt;% group_by(district) %&gt;% summarize(estimate = sum(support_predicted_weighted)) }), label = &quot;mrp_mle&quot;, estimand = &quot;mean_policy_support&quot;) dat &lt;- draw_data(design) draw_estimates(design) sims &lt;- simulate_design(design, sims = 3) diag &lt;- diagnose_design(design, sims = 100, diagnosands = declare_diagnosands(select = bias), add_grouping_variables = &quot;state&quot;) This chunk is set to echo = TRUE and eval = do_diagnosis simulations_pilot &lt;- simulate_design(design, sims = sims) Right after you do simulations, you want to save the simulations rds. Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. kable(head(simulations_pilot)) design_label sim_ID estimator_label term estimate std.error statistic p.value conf.low conf.high df outcome design 1 estimator Z 0.9624445 0.1705951 5.641688 0.0000002 0.6234399 1.301449 88.32465 Y design 2 estimator Z 0.9532166 0.2124610 4.486548 0.0000200 0.5315217 1.374911 96.66334 Y design 3 estimator Z 0.9674071 0.2137695 4.525468 0.0000170 0.5431863 1.391628 97.96114 Y design 4 estimator Z 1.0545908 0.1942247 5.429747 0.0000004 0.6690878 1.440094 96.58766 Y design 5 estimator Z 1.0708566 0.1897958 5.642153 0.0000002 0.6942127 1.447501 97.99051 Y design 6 estimator Z 0.7197320 0.1964379 3.663916 0.0004076 0.3297870 1.109677 95.63827 Y References "],
["inference-about-unobserved-variables.html", "15.3 Inference about unobserved variables", " 15.3 Inference about unobserved variables "],
["structural-estimation.html", "15.4 Structural estimation", " 15.4 Structural estimation "],
["experimental-designs-for-descriptive-inference.html", "Chapter 16 Experimental designs for descriptive inference", " Chapter 16 Experimental designs for descriptive inference Section introduction "],
["audit-experiments.html", "16.1 Audit experiments", " 16.1 Audit experiments A basic requirement of a good research design is that the question it seeks to answer does in fact have an answer, at least under plausible models of the world. In our framework, this means that an inquiry \\(I\\) must have an associated answer \\(a^M\\), which refers to the answer under the model. Interestingly, we sometimes might not be conscious that the questions we ask do not have answers. Fortunately, when we ask a computer to answer such a question, it complains. How could a question not have an answer? Answerless questions can arise when inquiries depend on variables that do not exist or are undefined for some units. In other words, when there is a mismatch between the model and the inquiry, we’re asking a question about something that doesn’t exist. Consider an audit experiment (see Audit Experiment Design) that seeks to assess the effects of an email from a Latino name (versus a White name) on whether and how well election officials respond to requests for information. For example, do they use a positive or negative tone. These questions seem reasonable enough. The problem, however, is that if there are officials who don’t send responses, tone is undefined. More subtly, if there is an official that does send an email but would not have sent it in a different treatment condition, then tone is undefined for one of their potential outcomes. 16.1.1 Design Declaration Model: The model has two outcome variables, \\(R_i\\) and \\(Y_i\\). \\(R_i\\) stands for “response” and is equal to 1 if a response is sent, and 0 otherwise. \\(Y_i\\) is the tone of the response and is normally distributed when it is defined. \\(Z_i\\) is the treatment and equals 1 if the email is sent using a Latino name and 0 otherwise. The table below shows the potential outcomes for four possible types of subjects, depending on the potential outcomes of \\(R_i\\). A types always respond regardless of treatment and D types never respond, regardless of treatment. B types respond if and only if they are treated, whereas C types respond if and only if they are not treated. The table also includes columns for the potential outcomes of \\(Y_i\\), showing which potential outcome subjects would express depending on their type. The key thing to note is that for the B, C, and D types, the effect of treatment on \\(Y_i\\) is undefined because messages never sent have no tone. The last (and very important) feature of our model is that the outcomes \\(Y_i\\) are possibly correlated with subject type. Even though both \\(E[Y_i(1) | \\text{Type} = A]\\) and \\(E[Y_i(1) | \\text{Type} = B]\\) exist, there’s no reason to expect that they are the same. In the design we assume a distribution of types with 40% A, 5% B, 10% C, and 45% D. Causal Types Type \\(R_i(0)\\) \\(R_i(1)\\) \\(Y_i(0)\\) \\(Y_i(1)\\) A 1 1 \\(Y_i(0)\\) \\(Y_i(1)\\) B 0 1 NA \\(Y_i(1)\\) C 1 0 \\(Y_i(0)\\) NA D 0 0 NA NA Inquiry: We have two inquiries. The first is straightforward: \\(E[R_i(1) - R_i(0)]\\) is the Average Treatment Effect on response. The second inquiry is the undefined inquiry that does not have an answer: \\(E[Y_i(1) - Y_i(0)]\\). We will also consider a third inquiry, which is defined: \\(E[Y_i(1) - Y_i(0) | \\mathrm{Type} = A]\\), which is the average effect of treatment on tone among \\(A\\) types. Data strategy: The data strategy will be to use complete random assignment to assign 250 of 500 units to treatment. Answer strategy: We’ll try to answer all three inquiries with the difference-in-means estimator, but as the diagnosis will reveal, this strategy works well for some inquiries but not others. # Model ------------------------------------------------------------------- population &lt;- declare_population( N = 500, type = sample(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;), size = N, replace = TRUE, prob = c(.40, .05, .10, .45))) potential_outcomes &lt;- declare_potential_outcomes( R_Z_0 = type %in% c(&quot;A&quot;, &quot;C&quot;), R_Z_1 = type %in% c(&quot;A&quot;, &quot;B&quot;), Y_Z_0 = ifelse(R_Z_0, rnorm(n = sum(R_Z_0), mean = .1*(type == &quot;A&quot;) - 2*(type == &quot;C&quot;)), NA), Y_Z_1 = ifelse(R_Z_1, rnorm(n = sum(R_Z_1), mean = .2*(type == &quot;A&quot;) + 2*(type == &quot;B&quot;)), NA) ) # Inquiry ----------------------------------------------------------------- estimand_1 &lt;- declare_estimand(ATE_R = mean(R_Z_1 - R_Z_0)) estimand_2 &lt;- declare_estimand(ATE_Y = mean(Y_Z_1 - Y_Z_0)) estimand_3 &lt;- declare_estimand( ATE_Y_for_As = mean(Y_Z_1[type == &quot;A&quot;] - Y_Z_0[type == &quot;A&quot;])) # Data Strategy ----------------------------------------------------------- assignment &lt;- declare_assignment(m = 250) # Answer Strategy --------------------------------------------------------- estimator_1 &lt;- declare_estimator(R ~ Z, estimand = estimand_1, label = &quot;ATE_R&quot;) estimator_2 &lt;- declare_estimator(Y ~ Z, estimand = estimand_2, label = &quot;ATE_Y&quot;) estimator_3 &lt;- declare_estimator(Y ~ Z, estimand = estimand_3, label = &quot;ATE_YA&quot;) # Design ------------------------------------------------------------------ design &lt;- population + potential_outcomes + assignment + estimand_1 + estimand_2 + estimand_3 + declare_reveal(outcome_variables = c(&quot;R&quot;, &quot;Y&quot;)) + estimator_1 + estimator_2 + estimator_3 16.1.2 Takeaways We now diagnose the design: diagnosis &lt;- diagnose_design(design, sims = sims) Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Power Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand design ATE_R ATE_R Z 100 0.00 0.04 0.20 0.98 -0.05 0.04 0.04 0.00 -0.05 (0.00) (0.00) (0.04) (0.01) (0.00) (0.00) (0.00) (0.00) (0.00) design ATE_Y ATE_Y Z 100 NA NA 0.91 NA 0.54 0.19 0.15 NA NA NA NA (0.03) NA (0.02) (0.01) (0.00) NA NA design ATE_Y_for_As ATE_YA Z 100 0.31 0.34 0.91 0.52 0.54 0.19 0.15 0.04 0.22 (0.01) (0.01) (0.03) (0.04) (0.02) (0.01) (0.00) (0.02) (0.01) We learn three things from the design diagnosis. First, as expected, our experiment is unbiased for the average treatment effect on response. Next, we see that our second inquiry, as well as our diagnostics for it, are undefined. The diagnosis tells us that our definition of potential outcomes produces a definition problem for the estimand. Note that the diagnosands that are defined, including power, depend only on the answer strategy and not on the estimand. Finally, our third estimand – the average effects for the \\(A\\) types – is defined but our estimates are biased. The reason for this is that we cannot tell from the data which types are the \\(A\\) types: we are not conditioning on the correct subset. Indeed, we are unable to condition on the correct subset. If a subject responds in the treatment group, we don’t know if she is an \\(A\\) or a \\(B\\) type; in the control group, we can’t tell if a responder is an \\(A\\) or a \\(C\\) type. Our difference-in-means estimator of the ATE on \\(Y\\) among \\(A\\)s will be off whenever \\(A\\)s have different outcomes from \\(B\\)s and \\(C\\)s. In some cases, the problem might be resolved by changing the inquiry. Closely related estimands can often be defined, perhaps by redefining \\(Y\\) (e.g., emails never sent have a tone of zero). Some redefinitions of the problem, as in the one we examine above, require estimating effects for unobserved subgroups which is a difficult challenge. 16.1.3 Applications This kind of problem is surprisingly common. Here are three more distinct instances of the problem: \\(Y\\) is the decision to vote Democrat (\\(Y=1\\)) or Republican (\\(Y=0\\)), \\(R\\) is the decision to turn out to vote and \\(Z\\) is a campaign message. The decision to vote may depend on treatment but if subjects do not vote then \\(Y\\) is undefined. \\(Y\\) is the weight of infants, \\(R\\) is whether a child is born and \\(Z\\) is a maternal health intervention. Fertility may depend on treatment but the weight of unborn (possibly never conceived) babies is not defined. \\(Y\\) is the charity to whom contributions are made during fundraising and \\(R\\) is whether anything is contributed and \\(Z\\) is an encouragement to contribute. The identity of beneficiaries is not defined if there are no contributions. All of these problem exhibit a form of post treatment bias (see section Post treatment bias) but the issue goes beyond picking the right estimator. Our problem here is conceptual: the effect of treatment on the outcome just doesn’t exist for some subjects. 16.1.4 Exercises The amount of bias on the third estimand depends on both the distribution of types and the correlation of types with the potential outcomes of Y. Modify the declaration so that the estimator of the effect on Y is unbiased, changing only the distribution of types. Repeat the exercise, changing only the correlation of type with the potential outcomes of \\(Y\\). Try approaching the problem by redefining the inquiry, seeking to assess the effect of treatment on the share of responses with positive tone. "],
["experiments-for-sensitive-questions.html", "16.2 Experiments for sensitive questions", " 16.2 Experiments for sensitive questions setup: a descriptive estimand, the proportion holding sensitive characteristic; two experimental designs to recover it, list experiments and randomized response if identification assumptions are violated (focus on ceiling/floor), estimates of ATE still unbiased but not for the descriptive estimand compare design where the ceiling/floor categories are minimized through Glynn (2013) design advice to use negatively-correlated items and a high prevalence and a low prevalence item both designs exhibit bias-variance tradeoff (more control of variance with RR) 16.2.1 List experiments Sometimes, subjects might not tell the truth when asked about certain attitudes or behaviors. Responses may be affected by sensitivity bias, or the tendency of survey subjects to dissemble for fear of negative repercussions if some reference group learns their true response (Blair, Coppock, and Moor 2018). In such cases, standard survey estimates based on direct questions will be biased. One class of solutions to this problem is to obscure individual responses, providing protection from social or legal pressures. When we obscure responses systematically through an experiment, we can often still identify average quantities of interest. One such design is the list experiment (introduced by Miller (1984)), which asks respondents for the count of the number of `yes’ responses to a series of questions including the sensitive item, rather than for a yes or no answer on the sensitive item itself. List experiments give subjects cover by aggregating their answer to the sensitive item with responses to other questions. During the 2016 Presidential Election in the U.S., some observers were concerned that pre-election estimates of support for Donald Trump might have been downward biased by “Shy Trump Supporters” – survey respondents who supported Trump in their hearts, but were embarrassed to admit it to pollsters. To assess this possibility, Coppock (2017) obtained estimates of Trump support that were free of social desirability bias using a list experiment. Subjects in the control and treatment groups were asked: “Here is a list of [three/four] things that some people would do and some people would not. Please tell me HOW MANY of them you would do. We do not want to know which ones of these you would do, just how many. Here are the [three/four] things:” Control Treatment If it were up for a vote, I would vote to raise the minimum wage to 15 dollars an hour If it were up for a vote, I would vote to raise the minimum wage to 15 dollars an hour If it were up for a vote, I would vote to repeal the Affordable Care Act, also known as Obamacare If it were up for a vote, I would vote to repeal the Affordable Care Act, also known as Obamacare If it were up for a vote, I would vote to ban assault weapons If it were up for a vote, I would vote to ban assault weapons If the 2016 presidential election were being held today and the candidates were Hillary Clinton (Democrat) and Donald Trump (Republican), I would vote for Donald Trump. The treatment group averaged 1.843 items while the control group averaged 1.548 items, for a difference-in-means estimate 0.296. We will show this is an unbiased estimator for the average treatment effect between being asked to respond to the treated list and the control list (invoking the usual assumptions of randomized experiments, including SUTVA). But our estimand is the proportion of people who support Donald Trump. For the difference-in-means to be an unbiased estimator of the proportion of respondents who would say “yes” to the sensitive item, we invoke two additional assumptions: no design effects and no “liars” (see Imai 2011). The first highlights the fact that we need a good estimate of the average control item count from the control group (in this example 1.843). We use that to net out the control item count from responses to the treated group (what is left is the sensitive item proportion). When respondents provide a different control item count in the treated group than in the control group, for example because they evaluate items relatively and the inclusion of the sensitive item changes their answers (see Flavin and Keane 2011), the design breaks down. The no liars assumption says that respondents provide truthful answers to the sensitive item within the count. The justification for the assumption is that the plausible cover of being asked within a count makes it possible for respondents to answer truthfully. The estimate is, under these assumptions, free from sensitivity bias, but it’s also much higher variance. The 95% confidence interval for the list experiment estimate is nearly 14 percentage points wide, whereas the the 95% confidence interval for the (possibly biased!) direct question asked of the same sample is closer to 4 percentage points. The choice between list experiments and direct question is therefore a bias-variance tradeoff. List experiments may have less bias, but they are higher variance. Direct questions may be biased, but they have less variance. 16.2.1.1 Declaration Model: Our model includes subjects’ true support for Donald Trump and whether or not they are “shy”. These two variables combine to determine how subjects will respond when asked directly about Trump support. The potential outcomes model combines three types of information to determine how subjects will respond to the list experiment: their responses to the three nonsensitive control items, their true support for Trump, and whether they are assigned to see the treatment or the control list. Notice that our definition of the potential outcomes embeds the no liars and no design effects assumptions required for the list experiment design. We also have a global parameter that reflects our expectations about the proportion of Trump supporters who are shy. It’s set at 6%, which is large enough to make a difference for polling, but not so large as to be implausible. Inquiry: Our estimand is the proportion of voters who actually plan to vote for Trump. Data strategy: First we sample 500 respondents from the U.S. population at random, then we randomly assign 250 of the 500 to treatment and the remainder to control. In the survey, we ask subjects both the direct question and the list experiment question. Answer strategy: We estimate the proportion of truthful Trump voters in two ways. First, we take the mean of answers to the direct question. Second, we take the difference in means in the responses to the list experiment question. # Model ------------------------------------------------------------------- proportion_shy &lt;- .06 list_design &lt;- # Model declare_population( N = 5000, # true trump vote (unobservable) truthful_trump_vote = draw_binary(.45, N), # shy voter (unobservable) shy = draw_binary(proportion_shy, N), # direct question response (1 if Trump supporter and not shy, 0 otherwise) Y_direct = if_else(truthful_trump_vote == 1 &amp; shy == 0, 1, 0), # nonsensitive list experiment items raise_minimum_wage = draw_binary(.8, N), repeal_obamacare = draw_binary(.6, N), ban_assault_weapons = draw_binary(.5, N) ) + declare_potential_outcomes( Y_list_Z_0 = raise_minimum_wage + repeal_obamacare + ban_assault_weapons, Y_list_Z_1 = Y_list_Z_0 + truthful_trump_vote ) + # Inquiry declare_estimand(proportion_truthful_trump_vote = mean(truthful_trump_vote), ATE = mean(Y_list_Z_1 - Y_list_Z_0)) + # Data Strategy declare_sampling(n = 500) + declare_assignment(prob = .5) + declare_reveal(Y_list) + # Answer Strategy declare_estimator( Y_direct ~ 1, model = lm_robust, term = &quot;(Intercept)&quot;, estimand = &quot;proportion_truthful_trump_vote&quot;, label = &quot;direct&quot;) + declare_estimator( Y_list ~ Z, model = difference_in_means, estimand = c(&quot;proportion_truthful_trump_vote&quot;, &quot;ATE&quot;), label = &quot;list&quot;) simulations_list &lt;- simulate_design(list_design, sims = sims) The plot shows the sampling distribution of the direct and list experiment estimators. The sampling distribution of the direct question is tight but biased; the list experiment (if the requisite assumptions hold) is unbiased, but higher variance. The choice between these two estimators of the prevalence rate depends on which – bias or variance – is more important in a particular setting. See Blair, Coppock, and Moor (2018) for an extended discussion of how the choice of research design depends deeply on the purpose of the project. 16.2.1.2 Violations of identifying assumptions In our model, the definition of the treated potential outcome, Y_list_Z_1 = Y_list_Z_0 + truthful_trump_vote, bakes in the no design effects and no liars assumptions. The first component is the control item count Y_list_Z_0, which ensures the respondent’s count of control items is the same in both groups. The second is the true trump vote, which assumes no liars. What do we learn from this experimental design if these assumptions do not hold? We examine the case of “ceiling effects,” in which respondents whose control item count is the maximum (in the example, they would vote yes to all three control items) withhold their true support for Trump in the treatment group. We thus redefine the treated potential outcome to be a function of the original count, but those who would respond 4 (all control items plus Trump support) instead respond 3. These are the “liars.” list_design_ceiling &lt;- replace_step( list_design, step = 2, new_step = declare_potential_outcomes( Y_list_Z_0 = raise_minimum_wage + repeal_obamacare + ban_assault_weapons, Y_list_Z_1_no_liars = Y_list_Z_0 + truthful_trump_vote, Y_list_Z_1 = ifelse(Y_list_Z_1_no_liars == 4, 3, Y_list_Z_1_no_liars)) ) diagnosis_list_ceiling &lt;- diagnose_design(list_design_ceiling, sims = sims, bootstrap_sims = b_sims) estimator_label estimand_label bias rmse list ATE -0.022 0.091 direct proportion_truthful_trump_vote -0.026 0.031 list proportion_truthful_trump_vote -0.127 0.153 We see that the list experiment is still an unbiased estimator for the average difference in responses to the treatment list and the shorter control list (ATE). But under ceiling effects, it is no longer an unbiased estimator for the proportion of truthful Trump vote. Indeed, it is more unbiased than the direct question. The divergence illustrates a common feature of experimental designs for descriptive inference: the average treatment effect can be estimated without bias under SUTVA and randomization with these designs, but additional assumptions are required in order to add an interpretation of the ATE as the descriptive quantity of interest. The burden is on the researcher to demonstrate the credibility of these additional assumptions. The experimental design alone is not sufficient justification. 16.2.1.3 Addressing potential assumption violations by design Researchers may bolster the assumptions to identify the descriptive estimand through changes to the data strategy or the answer strategy. Changes to the data strategy for the list experiment aim to reduce the risk of design effects and violations of the no liars assumptions. For example, when there is a risk of ceiling effects, Glynn (2013) proposes selecting control items that are inversely correlated. With three items, if two of the items are perfectly negatively correlated (i.e., if you say “yes” to one item you say “no” to the other), then the control item count will always be below the maximum of three when ceiling effects bite. We illustrate this design change by replacing the population declaration for the design with ceiling effects. This change is in the population, but is really a part of the data strategy because it involves the choice of measurement tool (which control items the researcher selects to ask respondents). We see that the design is stil lunbiased for the ATE and now is unbiased for the proportion of truthful Trump vote. This is because there are no longer ceiling effects, which represented a violation of the no design effects assumption required to interpret the ATE as the proportion of truthful Trump vote. Changes the answer strategy have been proposed to address both the no design effects and no liars assumption. Blair and Imai (2012) propose a statistical test for the design effects assumption; if it does not pass, they suggest not analyzing the list experiment data (i.e., this is a procedure that makes up an answer strategy). Scholars have also identified improvements to the answer strategy to address violations of no liars: Blair and Imai (2012) provides a model that adjusts for ceiling and floor effects and Li (2019) provides a bounds approach that relaxes the assumption. 16.2.2 Randomized response technique library(rr) rr_forced_known &lt;- function(data) { fit &lt;- try(rrreg(Y_forced_known ~ 1, data = data, p = 2/3, p0 = 1/6, p1 = 1/6, design = &quot;forced-known&quot;)) pred &lt;- try(as.data.frame(predict(fit, avg = TRUE, quasi.bayes = TRUE))) if(class(fit) != &quot;try-error&quot; &amp; class(pred) != &quot;try-error&quot;) { names(pred) &lt;- c(&quot;estimate&quot;, &quot;std.error&quot;, &quot;conf.low&quot;, &quot;conf.high&quot;) pred$p.value &lt;- with(pred, 2 * pnorm(-abs(estimate / std.error))) } else { pred &lt;- data.frame(estimate = NA, std.error = NA, conf.low = NA, conf.high = NA, p.value = NA, error = TRUE) } pred } rr_mirrored &lt;- function(data) { fit &lt;- try(rrreg(Y_mirrored ~ 1, data = data, p = 2/3, design = &quot;mirrored&quot;)) pred &lt;- try(as.data.frame(predict(fit, avg = TRUE, quasi.bayes = TRUE))) if(class(fit) != &quot;try-error&quot; &amp; class(pred) != &quot;try-error&quot;) { names(pred) &lt;- c(&quot;estimate&quot;, &quot;std.error&quot;, &quot;conf.low&quot;, &quot;conf.high&quot;) pred$p.value &lt;- with(pred, 2 * pnorm(-abs(estimate / std.error))) } else { pred &lt;- data.frame(estimate = NA, std.error = NA, conf.low = NA, conf.high = NA, p.value = NA, error = TRUE) } pred } proportion_shy &lt;- .06 rr_design &lt;- declare_population( N = 100, # true trump vote (unobservable) truthful_trump_vote = draw_binary(.45, N), # shy voter (unobservable) shy = draw_binary(proportion_shy, N), # Direct question response (1 if Trump supporter and not shy, 0 otherwise) Y_direct = as.numeric(truthful_trump_vote == 1 &amp; shy == 0)) + declare_estimand(sensitive_item_proportion = mean(truthful_trump_vote)) + declare_potential_outcomes(Y_forced_known ~ (dice == 1) * 0 + (dice %in% 2:5) * truthful_trump_vote + (dice == 6) * 1, conditions = 1:6, assignment_variable = &quot;dice&quot;) + declare_potential_outcomes(Y_mirrored ~ (coin == &quot;heads&quot;) * truthful_trump_vote + (coin == &quot;tails&quot;) * (1 - truthful_trump_vote), conditions = c(&quot;heads&quot;, &quot;tails&quot;), assignment_variable = &quot;coin&quot;) + declare_assignment(prob_each = rep(1/6, 6), conditions = 1:6, assignment_variable = &quot;dice&quot;) + declare_assignment(prob_each = c(2/3, 1/3), conditions = c(&quot;heads&quot;, &quot;tails&quot;), assignment_variable = &quot;coin&quot;) + declare_reveal(Y_forced_known, dice) + declare_reveal(Y_mirrored, coin) + declare_estimator(handler = tidy_estimator(rr_forced_known), label = &quot;forced_known&quot;, estimand = &quot;sensitive_item_proportion&quot;) + declare_estimator(handler = tidy_estimator(rr_mirrored), label = &quot;mirrored&quot;, estimand = &quot;sensitive_item_proportion&quot;) + declare_estimator(Y_direct ~ 1, model = lm_robust, term = &quot;(Intercept)&quot;, label = &quot;direct&quot;, estimand = &quot;sensitive_item_proportion&quot;) rr_design &lt;- set_diagnosands(rr_design, diagnosands = declare_diagnosands(select = c(mean_estimate, bias, rmse, power))) rr_diagnosis &lt;- diagnose_design(rr_design, sims = sims, bootstrap_sims = b_sims) kable(reshape_diagnosis(rr_diagnosis)) Design Label Estimand Label Estimator Label Term N Sims Mean Estimate Bias RMSE Power rr_design sensitive_item_proportion direct (Intercept) 10 0.43 -0.02 0.03 1.00 (0.01) (0.00) (0.00) (0.00) rr_design sensitive_item_proportion forced_known NA 10 0.45 -0.00 0.02 1.00 (0.02) (0.01) (0.00) (0.00) rr_design sensitive_item_proportion mirrored NA 10 0.42 -0.03 0.09 0.90 (0.04) (0.03) (0.02) (0.10) 16.2.2.1 Bias-variance tradeoff rr_designs &lt;- redesign(rr_design, proportion_shy = seq(from = 0, to = 0.5, by = 0.05), N = seq(from = 500, to = 5000, by = 500)) rr_tradeoff_diagnosis &lt;- diagnose_design(rr_designs, sims = sims, bootstrap_sims = b_sims) 16.2.3 References References "],
["conjoint-experimetns.html", "16.3 Conjoint experimetns", " 16.3 Conjoint experimetns You can use the global bib file via rmarkdown cites like this: Imai, King, and Stuart (2008) design &lt;- declare_population(N = 100, u = rnorm(N)) + declare_potential_outcomes(Y ~ Z + u) + declare_assignment(prob = 0.5) + declare_reveal(Y, Z) + declare_estimator(Y ~ Z, model = difference_in_means) This chunk is set to echo = TRUE and eval = do_diagnosis simulations_pilot &lt;- simulate_design(design, sims = sims) Right after you do simulations, you want to save the simulations rds. Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. kable(head(simulations_pilot)) design_label sim_ID estimator_label term estimate std.error statistic p.value conf.low conf.high df outcome design 1 estimator Z 0.9624445 0.1705951 5.641688 0.0000002 0.6234399 1.301449 88.32465 Y design 2 estimator Z 0.9532166 0.2124610 4.486548 0.0000200 0.5315217 1.374911 96.66334 Y design 3 estimator Z 0.9674071 0.2137695 4.525468 0.0000170 0.5431863 1.391628 97.96114 Y design 4 estimator Z 1.0545908 0.1942247 5.429747 0.0000004 0.6690878 1.440094 96.58766 Y design 5 estimator Z 1.0708566 0.1897958 5.642153 0.0000002 0.6942127 1.447501 97.99051 Y design 6 estimator Z 0.7197320 0.1964379 3.663916 0.0004076 0.3297870 1.109677 95.63827 Y References "],
["behavioral-games.html", "16.4 Behavioral games", " 16.4 Behavioral games "],
["observational-designs-for-causal-inference.html", "Chapter 17 Observational designs for causal inference", " Chapter 17 Observational designs for causal inference section introduction "],
["selection-on-observables.html", "17.1 Selection on observables", " 17.1 Selection on observables (matching and regression etc.) 17.1.1 Classic Confounding We want to know the effect of Z on Y, but it’s confounded by X DIM is biased, OLS is unbiased because we happen to get the functional forms right enough. Figure 17.1: DAG with one observed confounder design_1 &lt;- declare_population(N = 100, U_z = rnorm(N), U_x = rnorm(N), U_y = rnorm(N), X = U_x) + declare_potential_outcomes(Y ~ 0.5*Z + X + U_y) + declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + declare_assignment(prob_unit = pnorm(U_z + U_x), simple = TRUE) + declare_estimator(Y ~ Z, estimand = &quot;ATE&quot;, label = &quot;DIM&quot;) + declare_estimator(Y ~ Z + X, model = lm, estimand = &quot;ATE&quot;, label = &quot;OLS&quot;) dx_1 &lt;- diagnose_design(design_1, sims = sims, bootstrap_sims = b_sims) dx_1 ## ## Research design diagnosis based on 100 simulations. Diagnosand estimates with bootstrapped standard errors in parentheses (20 replicates). ## ## Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Power ## design_1 ATE DIM Z 100 0.96 0.99 1.00 ## (0.02) (0.02) (0.00) ## design_1 ATE OLS Z 100 0.01 0.23 0.61 ## (0.02) (0.01) (0.03) ## Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand ## 0.05 1.46 0.27 0.27 0.00 0.50 ## (0.02) (0.02) (0.01) (0.00) (0.00) (0.00) ## 0.96 0.51 0.23 0.23 0.00 0.50 ## (0.02) (0.02) (0.01) (0.00) (0.00) (0.00) 17.1.2 What if the functional form is wrong? Oh no, the functional form is wrong, so even though we’re controlling for all confounders, there’s still bias. Solution: matching might do a better job since it’s sort of a “nonparametric” form of covariate control. design_2 &lt;- declare_population(N = 100, U_z = rnorm(N), U_x = rnorm(N), U_y = rnorm(N), X = U_x) + declare_potential_outcomes(Y ~ 0.5*Z + X + X^2 + U_y) + declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + declare_assignment(prob_unit = pnorm(U_z + U_x + U_x^2), simple = TRUE) + declare_estimator(Y ~ Z, estimand = &quot;ATE&quot;, label = &quot;DIM&quot;) + declare_estimator(Y ~ Z + X, model = lm, estimand = &quot;ATE&quot;, label = &quot;OLS&quot;) dx_2 &lt;- diagnose_design(design_2, sims = sims, bootstrap_sims = b_sims) dx_2 ## ## Research design diagnosis based on 100 simulations. Diagnosand estimates with bootstrapped standard errors in parentheses (20 replicates). ## ## Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Power ## design_2 ATE DIM Z 100 1.30 1.34 1.00 ## (0.03) (0.04) (0.00) ## design_2 ATE OLS Z 100 0.75 0.84 0.91 ## (0.02) (0.03) (0.03) ## Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand ## 0.03 1.80 0.35 0.33 0.00 0.50 ## (0.02) (0.03) (0.03) (0.00) (0.00) (0.00) ## 0.44 1.25 0.38 0.36 0.00 0.50 ## (0.04) (0.02) (0.04) (0.01) (0.00) (0.00) 17.1.3 What if you have unobserved confounding? Figure 17.2: DAG with unobserved confounding design_3 &lt;- declare_population(N = 100, U_z = rnorm(N), U_x = rnorm(N), U_y = correlate(rnorm, given = U_z, rho = 0.9), X = U_x) + declare_potential_outcomes(Y ~ 0.5*Z + X + U_y) + declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + declare_assignment(prob_unit = pnorm(U_z + U_x), simple = TRUE) + declare_estimator(Y ~ Z, estimand = &quot;ATE&quot;, label = &quot;DIM&quot;) + declare_estimator(Y ~ Z + X, model = lm, estimand = &quot;ATE&quot;, label = &quot;OLS&quot;) dx_3 &lt;- diagnose_design(design_3, sims = sims, bootstrap_sims = b_sims) dx_3 ## ## Research design diagnosis based on 100 simulations. Diagnosand estimates with bootstrapped standard errors in parentheses (20 replicates). ## ## Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Power ## design_3 ATE DIM Z 100 1.73 1.74 1.00 ## (0.02) (0.02) (0.00) ## design_3 ATE OLS Z 100 1.02 1.03 1.00 ## (0.02) (0.02) (0.00) ## Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand ## 0.00 2.23 0.20 0.22 0.00 0.50 ## (0.00) (0.02) (0.01) (0.00) (0.00) (0.00) ## 0.00 1.52 0.16 0.19 0.00 0.50 ## (0.00) (0.02) (0.01) (0.00) (0.00) (0.00) 17.1.4 What if the observed covariate is post-treatment? Figure 17.3: DAG with one observed mediator design_4 &lt;- declare_population(N = 100, U_z = rnorm(N), U_m = rnorm(N), U_y = rnorm(N)) + declare_potential_outcomes(M ~ 0.5*Z + U_m) + declare_potential_outcomes(Y ~ 0.5*Z + (0.5*Z + U_m) + U_y) + declare_assignment(prob_unit = pnorm(U_z), simple = TRUE) + declare_reveal(c(M, Y), Z) + declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + declare_estimator(Y ~ Z, estimand = &quot;ATE&quot;, label = &quot;DIM&quot;) + declare_estimator(Y ~ Z + M, model = lm, estimand = &quot;ATE&quot;, label = &quot;OLS&quot;) dx_4 &lt;- diagnose_design(design_4, sims = sims, bootstrap_sims = b_sims) dx_4 ## ## Research design diagnosis based on 100 simulations. Diagnosand estimates with bootstrapped standard errors in parentheses (20 replicates). ## ## Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Power ## design_4 ATE DIM Z 100 0.04 0.29 0.97 ## (0.03) (0.02) (0.02) ## design_4 ATE OLS Z 100 -0.49 0.54 0.65 ## (0.02) (0.02) (0.06) ## Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand ## 0.91 1.04 0.29 0.28 0.00 1.00 ## (0.02) (0.03) (0.02) (0.00) (0.00) (0.00) ## 0.37 0.51 0.23 0.21 0.00 1.00 ## (0.04) (0.02) (0.02) (0.00) (0.00) (0.00) "],
["instrumental-variables.html", "17.2 Instrumental variables", " 17.2 Instrumental variables This one with continuous instruments You can use the global bib file via rmarkdown cites like this: Imai, King, and Stuart (2008) design &lt;- declare_population(N = 100, u = rnorm(N)) + declare_potential_outcomes(Y ~ Z + u) + declare_assignment(prob = 0.5) + declare_reveal(Y, Z) + declare_estimator(Y ~ Z, model = difference_in_means) This chunk is set to echo = TRUE and eval = do_diagnosis simulations_pilot &lt;- simulate_design(design, sims = sims) Right after you do simulations, you want to save the simulations rds. Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. kable(head(simulations_pilot)) design_label sim_ID estimator_label term estimate std.error statistic p.value conf.low conf.high df outcome design 1 estimator Z 0.9624445 0.1705951 5.641688 0.0000002 0.6234399 1.301449 88.32465 Y design 2 estimator Z 0.9532166 0.2124610 4.486548 0.0000200 0.5315217 1.374911 96.66334 Y design 3 estimator Z 0.9674071 0.2137695 4.525468 0.0000170 0.5431863 1.391628 97.96114 Y design 4 estimator Z 1.0545908 0.1942247 5.429747 0.0000004 0.6690878 1.440094 96.58766 Y design 5 estimator Z 1.0708566 0.1897958 5.642153 0.0000002 0.6942127 1.447501 97.99051 Y design 6 estimator Z 0.7197320 0.1964379 3.663916 0.0004076 0.3297870 1.109677 95.63827 Y References "],
["difference-in-differences.html", "17.3 Difference-in-differences", " 17.3 Difference-in-differences 17.3.1 Two-period two-group setting Show that comparison of T and C in period 2 is biased and comparison of T between period 1 and 2 is biased, but DiD unbiased in presence of confounding in treatment assignment (unit with higher unit shock is always treated) and time trends N_units &lt;- 2 N_time_periods &lt;- 2 two_period_two_group_design &lt;- declare_population( units = add_level(N = N_units, unit_shock = rnorm(N, sd = 0.5)), periods = add_level(N = N_time_periods, nest = FALSE, time = (1:N_time_periods) - N_time_periods + 1), unit_period = cross_levels(by = join(units, periods), unit_time_shock = rnorm(N, sd = 0.01)) ) + # internal note: the unbiasedness obtains whether or not there is a unit-time shock declare_potential_outcomes( Y_Z_0 = unit_shock + 0.5 * time + unit_time_shock, # common pretreatment trend Y_Z_1 = Y_Z_0 + 1) + declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0), subset = time == 1) + declare_assignment(Z = unit_shock == max(unit_shock), handler = mutate) + declare_reveal( Y = case_when(Z == 0 | time &lt; 1 ~ Y_Z_0, TRUE ~ Y_Z_1), handler = mutate) + declare_estimator(estimate = (mean(Y[Z == 1 &amp; time == 1]) - mean(Y[Z == 0 &amp; time == 1])) - (mean(Y[Z == 1 &amp; time == 0]) - mean(Y[Z == 0 &amp; time == 0])), estimator_label = &quot;DiD&quot;, handler = summarize, label = &quot;DiD&quot;) + declare_estimator(estimate = mean(Y[Z == 1 &amp; time == 1]) - mean(Y[Z == 1 &amp; time == 0]), estimator_label = &quot;Diff&quot;, handler = summarize, label = &quot;Over-Time&quot;) + declare_estimator(estimate = mean(Y[Z == 1 &amp; time == 1]) - mean(Y[Z == 0 &amp; time == 1]), estimator_label = &quot;DiM&quot;, handler = summarize, label = &quot;DiM&quot;) diagnosis_two_period_two_group &lt;- diagnose_design( two_period_two_group_design, diagnosands = declare_diagnosands(select = bias), sims = sims, bootstrap_sims = b_sims) kable(get_diagnosands(diagnosis_two_period_two_group)) design_label estimand_label estimator_label bias se(bias) n_sims two_period_design ATE DiD -0.0421836 0.0671367 1000 two_period_design ATE Diff 0.4619315 0.0487573 1000 two_period_design ATE DiM 1.1511461 0.0547887 1000 17.3.2 Parallel trends assumption Introduce assumption and visual test # add an additional pretreatment time period in order to visually test for parallel pre-trends three_period_two_group_design &lt;- redesign(two_period_two_group_design, N_time_periods = 3) draw_data(three_period_two_group_design) %&gt;% group_by(Z, time) %&gt;% summarize(Y = mean(Y)) %&gt;% mutate(Z_color = factor(Z, levels = c(FALSE, TRUE), labels = c(&quot;Untreated&quot;, &quot;Treated&quot;))) %&gt;% ggplot(aes(time, Y, color = Z_color)) + geom_line() + scale_color_discrete(&quot;&quot;) + scale_x_discrete(&quot;Time&quot;, limits = c(-1, 0, 1)) Formal test (DID on T = -1 and T = 0 periods, i.e. a year backward from the DiD) There is a result that shows that the two-step procedure of the parallel trends assumption then DID if test passes that shows poor coverage of SEs in final DID (https://arxiv.org/abs/1804.01208). Cite here. 17.3.3 Multi-period design Switch to regression context with 20 periods, 100 units and show same results hold with two-way FE (controlling for one period before T is insufficient to remove bias) N_units &lt;- 20 N_time_periods &lt;- 20 multi_period_design &lt;- declare_population( units = add_level(N = N_units, unit_shock = rnorm(N), unit_treated = 1*(unit_shock &gt; median(unit_shock)), unit_treatment_start = sample(2:(N_time_periods - 1) - N_time_periods + 1, N, replace = TRUE)), periods = add_level(N = N_time_periods, nest = FALSE, time = (1:N_time_periods) - N_time_periods + 1), unit_period = cross_levels(by = join(units, periods), noise = rnorm(N), pretreatment = 1*(time &lt; unit_treatment_start)) ) + declare_potential_outcomes( Y_Z_0 = unit_shock + 0.5 * time + noise, # common pretreatment trend Y_Z_1 = Y_Z_0 + 0.2) + declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0), subset = time == 1) + declare_assignment(Z = 1*(unit_treated &amp; pretreatment == FALSE), handler = fabricate) + declare_reveal(Y, Z) + declare_estimator(Y ~ Z + time, fixed_effects = ~ units + periods, model = lm_robust, label = &quot;twoway-fe&quot;, estimand = &quot;ATE&quot;) diagnosis_multi_period_multi_group &lt;- diagnose_design(multi_period_design, diagnosands = declare_diagnosands(select = bias), sims = sims, bootstrap_sims = b_sims) kable(get_diagnosands(diagnosis_multi_period_multi_group)) design_label estimand_label estimator_label term bias se(bias) n_sims multi_period_design ATE twoway-fe Z -0.0005784 0.0066689 1000 Show that in case where some units switch back and forth between T and C during panel there is bias (point to Imai and Kim appear with weighted FE estimator to fix this) "],
["regression-discontinuity.html", "17.4 Regression Discontinuity", " 17.4 Regression Discontinuity Regression discontinuity designs exploit substantive knowledge that treatment is assigned in a particular way: everyone above a threshold is assigned to treatment and everyone below it is not. Even though researchers do not control the assignment, substantive knowledge about the threshold serves as a basis for a strong identification claim. Thistlewhite and Campbell introduced the regression discontinuity design in the 1960s to study the impact of scholarships on academic success. Their insight was that students with a test score just above a scholarship cutoff were plausibly comparable to students whose scores were just below the cutoff, so any differences in future academic success could be attributed to the scholarship itself. Regression discontinuity designs identify a local average treatment effect: the average effect of treatment exactly at the cutoff. The main trouble with the design is that there is vanishingly little data exactly at the cutoff, so any answer strategy needs to use data that is some distance away from the cutoff. The further away from the cutoff we move, the larger the threat of bias. We’ll consider an application of the regression discontinuity design that examines party incumbency advantage – the effect of a party winning an election on its vote margin in the next election. 17.4.1 Design Declaration Model: Regression discontinuity designs have four components: A running variable, a cutoff, a treatment variable, and an outcome. The cutoff determines which units are treated depending on the value of the running variable. In our example, the running variable \\(X\\) is the Democratic party’s margin of victory at time \\(t-1\\); and the treatment, \\(Z\\), is whether the Democratic party won the election in time \\(t-1\\). The outcome, \\(Y\\), is the Democratic vote margin at time \\(t\\). We’ll consider a population of 1,000 of these pairs of elections. A major assumption required for regression discontinuity is that the conditional expectation functions for both treatment and control potential outcomes are continuous at the cutoff.17 To satisfy this assumption, we specify two smooth conditional expectation functions, one for each potential outcome. The figure plots \\(Y\\) (the Democratic vote margin at time \\(t\\)) against \\(X\\) (the margin at time \\(t-1\\)). We’ve also plotted the true conditional expectation functions for the treated and control potential outcomes. The solid lines correspond to the observed data and the dashed lines correspond to the unobserved data. cutoff &lt;- .5 control &lt;- function(X) { as.vector(poly(X, 4, raw = TRUE) %*% c(.7, -.8, .5, 1))} treatment &lt;- function(X) { as.vector(poly(X, 4, raw = TRUE) %*% c(0, -1.5, .5, .8)) + .15} rd_design &lt;- # Model ------------------------------------------------------------------- declare_population( N = 1000, X = runif(N, 0, 1) - cutoff, noise = rnorm(N, 0, .1), Z = 1 * (X &gt; 0) ) + declare_potential_outcomes(Y ~ Z * treatment(X) + (1 - Z) * control(X) + noise) + # Inquiry ----------------------------------------------------------------- declare_estimand(LATE = treatment(0) - control(0)) + # Data Strategy ----------------------------------------------------------------- declare_reveal(Y, Z) + # Answer Strategy --------------------------------------------------------- declare_estimator(formula = Y ~ poly(X, 4) * Z, model = lm_robust, estimand = &quot;LATE&quot;) Inquiry: Our estimand is the effect of a Democratic win in an election on the Democratic vote margin of the next election, when the Democratic vote margin of the first election is zero. Formally, it is the difference in the conditional expectation functions of the control and treatment potential outcomes when the running variable is exactly zero. The black vertical line in the plot shows this difference. Data strategy: We collect data on the Democratic vote share at time \\(t-1\\) and time \\(t\\) for all 1,000 pairs of elections. There is no sampling or random assignment. Answer strategy: We will approximate the treated and untreated conditional expectation functions to the left and right of the cutoff using a flexible regression specification estimated via OLS. In particular, we fit each regression using a fourth-order polynomial. Much of the literature on regression discontinuity designs focuses on the tradeoffs among answer strategies, with many analysts recommending against higher-order polynomial regression specifications. We use one here to highlight how well such an answer strategy does when it matches the functional form in the model. We discuss alternative estimators in the exercises. rd_diagnosis &lt;- diagnose_design(rd_design, sims = sims, bootstrap_sims = b_sims) Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. summary(rd_diagnosis) ## ## Research design diagnosis based on 100 simulations. Diagnosand estimates with bootstrapped standard errors in parentheses (20 replicates). ## ## Design Label Estimand Label Estimator Label Term N Sims Bias RMSE ## rd_design LATE estimator poly(X, 4)1 100 2.08 24.82 ## (2.36) (1.50) ## Power Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand ## 0.06 0.95 2.23 24.85 26.52 0.33 0.15 ## (0.02) (0.02) (2.36) (1.54) (0.28) (0.21) (0.00) 17.4.2 Takeaways We highlight three takeaways. First, the power of this design is very low: with 1,000 units we do not achieve even 10% statistical power. However, our estimates of the uncertainty are not too wide: the coverage probability indicates that our confidence intervals indeed contain the estimand 95% of the time as they should. Our answer strategy is highly uncertain because the fourth-order polynomial specification in regression model gives weights to the data that greatly increase the variance of the estimator (Gelman and Imbens (2017)). In the exercises we explore alternative answer strategies that perform better. Second, the design is biased because polynomial approximations of the average effect at exactly the point of the threshold will be inaccurate in small samples (Sekhon and Titiunik (2017)), especially as units farther away from the cutoff are incorporated into the answer strategy. We know that the estimated bias is not due to simulation error by examining the bootstrapped standard error of the bias estimates. Finally, from the figure, we can see how poorly the average effect at the threshold approximates the average effect for all units. The average treatment effect among the treated (to the right of the threshold in the figure) is negative, whereas at the threshold it is positive. This clarifies that the estimand of the regression discontinuity design, the difference at the cutoff, is only relevant for a small – and possibly empty – set of units very close to the cutoff. 17.4.3 Further Reading Since its rediscovery by social scientists in the late 1990s, the regression discontinuity design has been widely used to study diverse causal effects such as: prison on recidivism (Mitchell et al. (2017)); China’s one child policy on human capital (Qin, Zhuang, and Yang (2017)); eligibility for World Bank loans on political liberalization (Carnegie and Samii (2017)); and anti-discrimination laws on minority employment (Hahn, Todd, and Van der Klaauw (1999)). We’ve discussed a “sharp” regression discontinuity design in which all units above the threshold were treated and all units below were untreated. In fuzzy regression discontinuity designs, some units above the cutoff remain untreated or some units below take treatment. This setting is analogous to experiments that experience noncompliance and may require instrumental variables approaches to the answer strategy (see Compliance is a Potential Outcome). Geographic regression discontinuity designs use distance to a border as the running variable: units on one side of the border are treated and units on the other are untreated. Keele and Titiunik (2016) use such a design to study whether voters are more likely to turn out when they have the opportunity to vote directly on legislation on so-called ballot initiatives. A complication of this design is how to measure distance to the border in two dimensions. 17.4.4 Exercises Gelman and Imbens (2017) point out that higher order polynomial regression specifications lead to extreme regression weights. One approach to obtaining better estimates is to select a bandwidth, \\(h\\), around the cutoff, and run a linear regression. Declare a sampling procedure that subsets the data to a bandwidth around the threshold, as well as a first order linear regression specification, and analyze how the power, bias, RMSE, and coverage of the design vary as a function of the bandwidth. The rdrobust estimator in the rdrobust package implements a local polynomial estimator that automatically selects a bandwidth for the RD analysis and bias-corrected confidence intervals. Declare another estimator using the rdrobust function and add it to the design. How does the coverage and bias of this estimator compare to the regression approaches declared above? Reduce the number of polynomial terms of the the treatment() and control() functions and assess how the bias of the design changes as the potential outcomes become increasingly linear as a function of the running variable. Redefine the population function so that units with higher potential outcome are more likely to locate just above the cutoff than below it. Assess whether and how this affects the bias of the design. References "],
["process-tracing.html", "17.5 Process tracing", " 17.5 Process tracing "],
["synthetic-controls.html", "17.6 Synthetic controls", " 17.6 Synthetic controls Modeled after the example here: https://www.mitpressjournals.org/doi/abs/10.1162/REST_a_00429?casa_token=o-zWqCima50AAAAA:yiEERZfdhAUoHV0-xBYNjgdljvgfRXrriR8foG7X8nHSUAMFrLcw2vWY8e9pHzmRT24MMAIv9hvKpQ Did the 2007 Legal Arizona Workers Act Reduce the State’s Unauthorized Immigrant Population? Sarah Bohn, Magnus Lofstrom, and Steven Raphael The Review of Economics and Statistics 2014 96:2, 258-269 Abstract: We test for an effect of Arizona’s 2007 Legal Arizona Workers Act (LAWA) on the proportion of the state’s population characterized as noncitizen Hispanic. We use the synthetic control method to select a group of states against which Arizona’s population trends can be compared. We document a notable and statistically significant reduction in the proportion of the Hispanic noncitizen population in Arizona. The decline observed matches the timing of LAWA’s implementation, deviates from the time series for the synthetic control group, and stands out relative to the distribution of placebo estimates for other states in the nation. Outline: (1) how does synth work? - declaration: set up states with time trends and levels that are both correlated with a type and following the linear model assumed by SCM - try three estimators: (1) difference-in-difference; (2) single difference in treated period; and (3) difference in treated period weighted by Synth weights. - show that synth works under its assumptions; plot of time series of treat and synthetic control; plot of the time series from all units to illustrate which are picked (sorted by weights) (2) what are synth’s assumptions? - linear model; treated unit is in convex hull of control units’ pretreatment time series (3) how to diagnose when you are outside the convex hull - declaration outside the convex hull and use the Abadie diagnostic demonstrating a poor match. (possibly explore power of this diagnostic) - show that synth is biased in this setting. augsynth is not. # tidy function that takes data and just adds the synthetic control weights to it synth_weights_tidy &lt;- function(data) { dataprep.out &lt;- dataprep( foo = data, predictors = &quot;prop_non_hispanic_below_hs&quot;, predictors.op = &quot;mean&quot;, time.predictors.prior = 1998:2006, dependent = &quot;prop_non_hispanic_below_hs&quot;, unit.variable = &quot;state_number&quot;, time.variable = &quot;year&quot;, treatment.identifier = 4, controls.identifier = c(1:3, 5:50), # states without Arizona time.optimize.ssr = 1998:2006, time.plot = 1998:2009) capture.output(fit &lt;- synth(data.prep.obj = dataprep.out)) tab &lt;- synth.tab(dataprep.res = dataprep.out, synth.res = fit) data %&gt;% left_join(tab$tab.w %&gt;% mutate(synth_weights = w.weights) %&gt;% dplyr::select(synth_weights, unit.numbers), by = c(&quot;state_number&quot; = &quot;unit.numbers&quot;)) %&gt;% mutate(synth_weights = replace(synth_weights, state_number == 4, 1)) } augsynth_tidy &lt;- function(data) { fit &lt;- augsynth(prop_non_hispanic_below_hs ~ legal_worker_act, state, year, t_int = 2007, data = data) res &lt;- summary(fit)$att %&gt;% filter(Time == 2007) %&gt;% select(Estimate, Std.Error) names(res) &lt;- c(&quot;estimate&quot;, &quot;std.error&quot;) res$p.value &lt;- 2 * pt(-abs(res$estimate/res$std.error), df = nrow(data) - 15) res$conf.low &lt;- res$estimate - 1.96 * res$std.error res$conf.high &lt;- res$estimate + 1.96 * res$std.error res } # note need to clean up the range of the data, currently over 1 design &lt;- declare_population( states = add_level( N = 50, state = state.abb, state_number = as.numeric(as.factor(state)), state_shock = runif(N, -.15, .15), border_state = state %in% c(&quot;AZ&quot;, &quot;CA&quot;, &quot;NM&quot;, &quot;TX&quot;), state_shock = ifelse(border_state, .2, state_shock) ), years = add_level( N = 12, nest = FALSE, year = 1998:2009, post_treatment_period = year &gt;= 2007, year_shock = runif(N, -.025, .025), year_trend = year - 1998 ), obs = cross_levels( by = join(states, years), # treatment indicator: legal_worker_act = if_else(post_treatment_period == TRUE &amp; state == &quot;AZ&quot;, 1, 0), state_year_shock = runif(N, -.025, .025), prop_non_hispanic_below_hs_baseline = 0.4 + state_shock + year_shock + (.01 + .05 * border_state) * year_trend + state_year_shock ) ) + declare_potential_outcomes( prop_non_hispanic_below_hs ~ prop_non_hispanic_below_hs_baseline + 0.25 * legal_worker_act, assignment_variable = legal_worker_act) + declare_estimand( ATE_AZ = mean(prop_non_hispanic_below_hs_legal_worker_act_1 - prop_non_hispanic_below_hs_legal_worker_act_0), subset = legal_worker_act == TRUE) + declare_reveal(prop_non_hispanic_below_hs, legal_worker_act) + declare_step(handler = synth_weights_tidy) + declare_estimator( prop_non_hispanic_below_hs ~ legal_worker_act, subset = year &gt;= 2007, weights = synth_weights, model = lm_robust, label = &quot;synth&quot;) + declare_estimator( prop_non_hispanic_below_hs ~ legal_worker_act, subset = year &gt;= 2007, model = lm_robust, label = &quot;unweighted&quot;) + declare_estimator( prop_non_hispanic_below_hs ~ I(state == &quot;AZ&quot;) + post_treatment_period + legal_worker_act, term = &quot;legal_worker_act&quot;, model = lm_robust, label = &quot;unweighted_did&quot;) + declare_estimator(handler = tidy_estimator(augsynth_tidy), label = &quot;augsynth&quot;) state_data &lt;- draw_data(design) state_data %&gt;% dplyr::select(state, synth_weights) %&gt;% distinct %&gt;% arrange(-synth_weights) %&gt;% head ## state synth_weights ## 1 AZ 1.000 ## 2 TX 0.942 ## 3 NM 0.026 ## 4 CA 0.016 ## 5 AL 0.001 ## 6 AK 0.001 state_data %&gt;% ggplot() + geom_line(aes(year, prop_non_hispanic_below_hs)) + facet_wrap(~ state) state_data %&gt;% mutate(treatment_state = factor(state == &quot;AZ&quot;, levels = c(FALSE, TRUE), labels = c(&quot;Synthethic Control&quot;, &quot;Arizona&quot;))) %&gt;% group_by(treatment_state, year) %&gt;% summarize(prop_non_hispanic_below_hs = weighted.mean(prop_non_hispanic_below_hs, w = synth_weights)) %&gt;% ggplot(aes(x = year, y = prop_non_hispanic_below_hs, color = treatment_state)) + geom_line() + geom_vline(xintercept = 2007) + scale_x_continuous(breaks = scales::pretty_breaks()) + annotate(&quot;text&quot;, x = 2006.7, y = 1.7, label = &quot;Law Introduced in 2007&quot;, hjust = &quot;right&quot;, family = &quot;Palatino&quot;) + labs(color = &quot;&quot;) + xlab(&quot;&quot;) + ylab(&quot;Proportion Non-Hispanic Below H.S. Education&quot;) + dd_theme() simulations &lt;- simulate_design(design, sims = sims) Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. synth_diagnosands &lt;- declare_diagnosands(select = c(&quot;bias&quot;, &quot;rmse&quot;, &quot;coverage&quot;)) diagnosis &lt;- diagnose_design(simulations, diagnosands = synth_diagnosands, bootstrap_sims = b_sims) kable(reshape_diagnosis(diagnosis)) Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Coverage design ATE_AZ augsynth NA 1000 0.00 0.02 0.66 (0.00) (0.00) (0.01) design ATE_AZ synth legal_worker_act 1000 0.01 0.02 1.00 (0.00) (0.00) (0.00) design ATE_AZ unweighted legal_worker_act 1000 0.66 0.66 0.00 (0.00) (0.00) (0.00) design ATE_AZ unweighted_did legal_worker_act 1000 0.28 0.28 0.00 (0.00) (0.00) (0.00) we see that Synth outperforms either method 17.6.1 When there are not good controls, standard synth will get the wrong answer # declaration outside the convex hull design_outside_hull &lt;- replace_step( design, step = 2, new_step = declare_potential_outcomes( prop_non_hispanic_below_hs ~ prop_non_hispanic_below_hs_baseline + 0.25 * legal_worker_act + 0.2 * (state == &quot;AZ&quot;), assignment_variable = legal_worker_act)) state_data_outside_hull &lt;- draw_data(design_outside_hull) simulations_outside_hull &lt;- simulate_design(design_outside_hull, sims = sims) Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. diagnosis_outside_hull &lt;- diagnose_design(simulations_outside_hull, diagnosands = synth_diagnosands, bootstrap_sims = b_sims) kable(reshape_diagnosis(diagnosis_outside_hull)) Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Coverage design_outside_hull ATE_AZ augsynth NA 1000 -0.01 0.03 0.75 (0.00) (0.00) (0.01) design_outside_hull ATE_AZ synth legal_worker_act 1000 0.20 0.20 0.00 (0.00) (0.00) (0.00) design_outside_hull ATE_AZ unweighted legal_worker_act 1000 0.86 0.86 0.00 (0.00) (0.00) (0.00) design_outside_hull ATE_AZ unweighted_did legal_worker_act 1000 0.28 0.28 0.00 (0.00) (0.00) (0.00) # plot the synthetic control constructed in this way (it usually picks just texas and is highly biased) state_data_outside_hull %&gt;% mutate(treatment_state = factor(state == &quot;AZ&quot;, levels = c(FALSE, TRUE), labels = c(&quot;Synthethic Control&quot;, &quot;Arizona&quot;))) %&gt;% group_by(treatment_state, year) %&gt;% summarize(prop_non_hispanic_below_hs = weighted.mean(prop_non_hispanic_below_hs, w = synth_weights)) %&gt;% ggplot(aes(x = year, y = prop_non_hispanic_below_hs, color = treatment_state)) + geom_line() + geom_vline(xintercept = 2007) + scale_x_continuous(breaks = scales::pretty_breaks()) + annotate(&quot;text&quot;, x = 2006.7, y = 1.7, label = &quot;Law Introduced in 2007&quot;, hjust = &quot;right&quot;, family = &quot;Palatino&quot;) + labs(color = &quot;&quot;) + xlab(&quot;&quot;) + ylab(&quot;Proportion Non-Hispanic Below H.S. Education&quot;) + dd_theme() 17.6.2 References "],
["policing-and-post-treatment-bias.html", "17.7 Policing and post-treatment bias", " 17.7 Policing and post-treatment bias Imai, King, and Stuart (2008) Knox, Lowe, and Mummolo (2020) (https://www.cambridge.org/core/journals/american-political-science-review/article/administrative-records-mask-racially-biased-policing/66BC0F9998543868BB20F241796B79B8) study the statistical biases that accompany estimates of racial bias in police use of force when presence in the dataset (being stopped by police) is conditioned on an outcome that is a downstream consequence of race. They show the estimate is not identified unless additional modelling assumptions are brought to bear. Gaebler et al. (2020) (https://5harad.com/papers/post-treatment-bias.pdf) study the same question and make such modeling assumptions (subset ignorability, definition 2). In a twitter thread (https://twitter.com/jonmummolo/status/1275790509647241222?s=20), Mummolo shows the three DAGs that are compatible with subset ignorability. We agree with Mummolo that these DAGs assume away causal paths that are very plausible. This document provides a design declaration for this setting and shows how estimates of the controlled direct effect (effect of race on force among the stopped) are biased unless those paths are set to zero by assumption. References "],
["design-declaration-4.html", "Chapter 18 Design Declaration", " Chapter 18 Design Declaration There are four variables: (D: minority, M: stop, U: suspicion (unobserved), Y: force) and five paths: D_M = 1 # effect of minority on stop U_M = 1 # effect of suspicion on stop D_Y = 1 # effect of minority on force U_Y = 1 # effect of suspicion on force M_Y = 1 # effect of stop on force This basic design allows all five paths. design_1 &lt;- declare_population(N = 1000, D = rbinom(N, size = 1, prob = 0.5), U = rnorm(N)) + declare_potential_outcomes(M ~ rbinom(N, size = 1, prob = pnorm(D_M * D + U_M * U)), assignment_variable = &quot;D&quot;) + declare_reveal(M, D) + declare_potential_outcomes(Y ~ rnorm(N, D_Y * D + M_Y * M + U_Y * U), conditions = list(D = c(0, 1), M = c(0, 1))) + declare_reveal(outcome_variables = &quot;Y&quot;, assignment_variables = c(&quot;D&quot;, &quot;M&quot;)) + declare_estimand(CDE = mean(Y_D_1_M_1 - Y_D_0_M_1)) + declare_estimator(Y ~ D, subset = M == 1, estimand = &quot;CDE&quot;) We redesign the design 3 times, removing one path at a time, then simulate all four designs. # no effect of D on M design_2 &lt;- redesign(design_1, D_M = 0) # no effect of U on M design_3 &lt;- redesign(design_1, U_M = 0) # no effect of U on Y design_4 &lt;- redesign(design_1, U_Y = 0) This chunk is set to echo = TRUE and eval = do_diagnosis simulations &lt;- simulate_designs(design_1, design_2, design_3, design_4, sims = sims) Right after you do simulations, you want to save the simulations rds. This plot confirms that unless one of those implausible assumptions hold, estimates of the CDE are biased. "],
["experimental-designs-for-causal-inference.html", "Chapter 19 Experimental designs for causal inference", " Chapter 19 Experimental designs for causal inference Section introduction "],
["two-arm-trials.html", "19.1 Two arm trials", " 19.1 Two arm trials You can use the global bib file via rmarkdown cites like this: Imai, King, and Stuart (2008) design &lt;- declare_population(N = 100, u = rnorm(N)) + declare_potential_outcomes(Y ~ Z + u) + declare_assignment(prob = 0.5) + declare_reveal(Y, Z) + declare_estimator(Y ~ Z, model = difference_in_means) This chunk is set to echo = TRUE and eval = do_diagnosis simulations_pilot &lt;- simulate_design(design, sims = sims) Right after you do simulations, you want to save the simulations rds. Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. kable(head(simulations_pilot)) design_label sim_ID estimator_label term estimate std.error statistic p.value conf.low conf.high df outcome design 1 estimator Z 0.9624445 0.1705951 5.641688 0.0000002 0.6234399 1.301449 88.32465 Y design 2 estimator Z 0.9532166 0.2124610 4.486548 0.0000200 0.5315217 1.374911 96.66334 Y design 3 estimator Z 0.9674071 0.2137695 4.525468 0.0000170 0.5431863 1.391628 97.96114 Y design 4 estimator Z 1.0545908 0.1942247 5.429747 0.0000004 0.6690878 1.440094 96.58766 Y design 5 estimator Z 1.0708566 0.1897958 5.642153 0.0000002 0.6942127 1.447501 97.99051 Y design 6 estimator Z 0.7197320 0.1964379 3.663916 0.0004076 0.3297870 1.109677 95.63827 Y References "],
["two-arm-trials-and-designs-with-blocking-and-clustering.html", "19.2 Two-arm trials and designs with blocking and clustering", " 19.2 Two-arm trials and designs with blocking and clustering "],
["multiarm-designs.html", "19.3 Multiarm Designs", " 19.3 Multiarm Designs You can use the global bib file via rmarkdown cites like this: Imai, King, and Stuart (2008) design &lt;- declare_population(N = 100, u = rnorm(N)) + declare_potential_outcomes(Y ~ Z + u) + declare_assignment(prob = 0.5) + declare_reveal(Y, Z) + declare_estimator(Y ~ Z, model = difference_in_means) This chunk is set to echo = TRUE and eval = do_diagnosis simulations_pilot &lt;- simulate_design(design, sims = sims) Right after you do simulations, you want to save the simulations rds. Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. kable(head(simulations_pilot)) design_label sim_ID estimator_label term estimate std.error statistic p.value conf.low conf.high df outcome design 1 estimator Z 0.9624445 0.1705951 5.641688 0.0000002 0.6234399 1.301449 88.32465 Y design 2 estimator Z 0.9532166 0.2124610 4.486548 0.0000200 0.5315217 1.374911 96.66334 Y design 3 estimator Z 0.9674071 0.2137695 4.525468 0.0000170 0.5431863 1.391628 97.96114 Y design 4 estimator Z 1.0545908 0.1942247 5.429747 0.0000004 0.6690878 1.440094 96.58766 Y design 5 estimator Z 1.0708566 0.1897958 5.642153 0.0000002 0.6942127 1.447501 97.99051 Y design 6 estimator Z 0.7197320 0.1964379 3.663916 0.0004076 0.3297870 1.109677 95.63827 Y References "],
["encouragement-designs.html", "19.4 Encouragement designs", " 19.4 Encouragement designs Idea for this one would be to show how violations of no defiers and excludability lead to bias. types &lt;- c(&quot;Always-Taker&quot;, &quot;Never-Taker&quot;, &quot;Complier&quot;, &quot;Defier&quot;) direct_effect_of_encouragement &lt;- 0.0 proportion_defiers &lt;- 0.0 design &lt;- declare_population( N = 500, type = sample( types, N, replace = TRUE, prob = c(0.1, 0.1, 0.8 - proportion_defiers, proportion_defiers) ), noise = rnorm(N) ) + declare_potential_outcomes( D ~ case_when( Z == 0 &amp; type %in% c(&quot;Never-Taker&quot;, &quot;Complier&quot;) ~ 0, Z == 1 &amp; type %in% c(&quot;Never-Taker&quot;, &quot;Defier&quot;) ~ 0, Z == 0 &amp; type %in% c(&quot;Always-Taker&quot;, &quot;Defier&quot;) ~ 1, Z == 1 &amp; type %in% c(&quot;Always-Taker&quot;, &quot;Complier&quot;) ~ 1 ) ) + declare_potential_outcomes( Y ~ 0.5 * (type == &quot;Complier&quot;) * D + 0.25 * (type == &quot;Always-Taker&quot;) * D + 0.75 * (type == &quot;Defier&quot;) * D + direct_effect_of_encouragement * Z + noise, assignment_variables = c(&quot;D&quot;, &quot;Z&quot;) ) + declare_estimand(CACE = mean((Y_D_1_Z_1 + Y_D_1_Z_0) / 2 - (Y_D_0_Z_1 + Y_D_0_Z_0) / 2), subset = type == &quot;Complier&quot;) + declare_assignment(prob = 0.5) + declare_reveal(D, assignment_variable = &quot;Z&quot;) + declare_reveal(Y, assignment_variables = c(&quot;D&quot;, &quot;Z&quot;)) + declare_estimator(Y ~ D | Z, model = iv_robust, estimand = &quot;CACE&quot;) designs &lt;- redesign( design, proportion_defiers = seq(0, 0.3, length.out = 5), direct_effect_of_encouragement = seq(0, 0.3, length.out = 5) ) simulations &lt;- simulate_design(designs, sims = sims) gg_df &lt;- simulations %&gt;% group_by(proportion_defiers, direct_effect_of_encouragement) %&gt;% summarize(bias = mean(estimate - estimand)) ggplot(gg_df, aes( proportion_defiers, bias, group = direct_effect_of_encouragement, color = direct_effect_of_encouragement )) + geom_point() + geom_line() 19.4.1 References "],
["stepped-wedge-designs.html", "19.5 Stepped wedge designs", " 19.5 Stepped wedge designs Plan with this vignette: Show why you would want to do stepped wedge Increases power There are more estimands Show conditions under which FE give you wrong answer, draw connection to two-way FE papers in econ Show how to get answer right under hetfx Show additional tradeoffs in sample allocation 19.5.1 Design Declaration Model: Inquiry: Data strategy: Answer strategy: The weights consider p_00 &lt;- p_W1 &lt;- p_W2 &lt;- p_W3 &lt;- 1/4 design &lt;- declare_population( t = add_level(N = 3, u_t = rnorm(N), trend = as.numeric(t), p = c(p_W1, p_W1 + p_W2, p_W1 + p_W2 + p_W3)), i = add_level(N = 8, u_i = rnorm(N), nest = FALSE), obs = cross_levels(by = join(t, i), u_ti = rnorm(N))) + declare_potential_outcomes( Y_Z_0 = u_i + u_t + u_ti, Y_Z_1 = u_i + u_t + u_ti + trend) + declare_assignment(clusters = i, conditions = 1:4, prob_each = c(p_W1, p_W2, p_W3, p_00), assignment_variable = &quot;wave&quot;) + declare_step(Z = as.numeric(t &gt;= wave), ip = 1 / (Z * p + (1 - Z) * (1 - p)), handler = fabricate) + declare_reveal(Y, Z) + declare_estimand(ate = mean(Y_Z_1 - Y_Z_0)) + declare_estimator(Y ~ Z, model = lm_robust, label = &quot;Unweighted SW&quot;) + declare_estimator(Y ~ Z, model = lm_robust, label = &quot;Weighted SW&quot;, weights = ip) draw_data(design) %&gt;% mutate(i = fct_reorder(i, wave), Assignment = ifelse(Z == 1, &quot;Treatment&quot;, &quot;Control&quot;)) %&gt;% ggplot(aes(x = t, y = i, fill = Assignment)) + geom_tile(color = &quot;white&quot;) + scale_fill_grey(start = .9,end = .5) + geom_text(aes(label = round(ip,1))) + dd_theme() # Diagnose design diagnoses &lt;- diagnose_design(design) reshape_diagnosis(diagnoses) %&gt;% kable() Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Power Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand design ate Unweighted SW Z 500 0.31 0.84 0.87 0.91 2.31 0.78 0.69 0.00 2.00 (0.04) (0.03) (0.02) (0.01) (0.04) (0.02) (0.01) (0.00) (0.00) design ate Weighted SW Z 500 -0.04 0.74 0.69 0.96 1.96 0.74 0.77 0.00 2.00 (0.04) (0.02) (0.02) (0.01) (0.04) (0.02) (0.01) (0.00) (0.00) Change assignment allocation # Diagnose design no_SW &lt;- redesign(design, p_00 = .5, p_W1 = .5, p_W2 = 0, p_W3 = 0) no_control &lt;- redesign(design, p_00 = 0, p_W1 = .34, p_W2 = .33, p_W3 = .33) more_earlier &lt;- redesign(design, p_00 = .5, p_W1 = .30, p_W2 = .10, p_W3 = .10) sw_comparisons &lt;- diagnose_designs(no_SW, no_control, more_earlier) reshape_diagnosis(sw_comparisons) %&gt;% kable() Design Label p_00 p_W1 p_W2 p_W3 Estimand Label Estimator Label Term N Sims Bias RMSE Power Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand no_SW 0.5 0.5 0 0 ate Unweighted SW Z 500 -0.06 0.86 0.71 0.88 1.94 0.86 0.68 0.00 2.00 (0.04) (0.03) (0.02) (0.01) (0.04) (0.03) (0.01) (0.00) (0.00) no_SW 0.5 0.5 0 0 ate Weighted SW Z 500 -0.06 0.86 0.71 0.88 1.94 0.86 0.68 0.00 2.00 (0.04) (0.03) (0.02) (0.01) (0.04) (0.03) (0.01) (0.00) (0.00) no_control 0 0.34 0.33 0.33 ate Unweighted SW Z 500 0.37 1.02 0.83 0.83 2.37 0.95 0.70 0.00 2.00 (0.05) (0.03) (0.02) (0.02) (0.05) (0.03) (0.01) (0.00) (0.00) no_control 0 0.34 0.33 0.33 ate Weighted SW Z 500 0.01 0.84 0.71 0.92 2.01 0.84 0.76 0.00 2.00 (0.04) (0.03) (0.02) (0.01) (0.04) (0.03) (0.01) (0.00) (0.00) more_earlier 0.5 0.3 0.1 0.1 ate Unweighted SW Z 500 0.17 0.79 0.80 0.92 2.17 0.77 0.70 0.00 2.00 (0.03) (0.02) (0.02) (0.01) (0.03) (0.02) (0.01) (0.00) (0.00) more_earlier 0.5 0.3 0.1 0.1 ate Weighted SW Z 500 -0.00 0.77 0.74 0.93 2.00 0.77 0.71 0.00 2.00 (0.03) (0.02) (0.02) (0.01) (0.03) (0.02) (0.01) (0.00) (0.00) 19.5.2 References "],
["randomized-saturation-design.html", "19.6 Randomized Saturation Design", " 19.6 Randomized Saturation Design Randomized saturation designs (Baird et al. (2018)) offer researchers a way to estimate the diffusion of intervention effects within some geographic or social network. Most approaches work by first cluster-assigning non-overlapping groups of individuals to treatment saturations, then block-assigning individuals to treatment in the proportions determined by the saturations. Asunka et al. (2019), for example, wanted to know if the presence of election monitors at ballot stations would displace violence and fraud to other ballot stations. They randomized constituencies to low, medium, and high levels of saturation, and then randomized ballot stations to have election monitoring or not in low, medium, or high concentrations, depending on the randomized saturation. In the original study, the authors did not include a zero-saturation condition. Here, we declare a simplified version of their design in which a zero-saturation condition is included. Main points to develop: Randomized saturation is great when you get the model right. Though, show how IPW reduces the power to detect main effect, especially if there’s no spillover. Randomized saturation assumes a model that may be wrong. In particular, spillovers are restricted to containers. But this might not be correct. 19.6.1 Design Declaration Model: Potential outcomes are defined in terms of S—the saturation—and Z—whether or not a ballot station is treated. We model spillovers in two ways. In the first, the amount of spillover that affects a unit is determined by how many other units in its network are treated. In the second, the amount of spillover a unit receives is determined by whether that unit’s geographic neighbor is treated, irrespective of whether they share a network. Inquiry: We want to know the effect of having high and medium levels of saturation versus low saturation in the control: \\(E[Y_i(Z_i = 0, S_i = \\text{high})-Y_i(Z_i = 0, S_i = \\text{low})]\\) and \\(E[Y_i(Z_i = 0, S_i = \\text{medium})-Y_i(Z_i = 0, S_i = \\text{low})]\\). We also want to know the “direct effect”–e.g. what happens to those directly treated if we disregard spillovers. Here it is defined over potential outcomes that the experiment does not reveal, since no one is treated in low-saturation constituencies: \\(E[Y_i(Z_i = 1, S_i = \\text{low})-Y_i(Z_i = 0, S_i = \\text{low})]\\). Data strategy: We assign entire groups of individual ballot stations to one of three saturations: low (0%), medium (50%), and high (75%). We then randomize individuals within groups to treatment or control in the proportions dictated by the saturation. Thus, the saturation is cluster-randomized, whereas treatment is block-randomized. Answer strategy: We weight each individual by the inverse of the probability that they find themselves in the condition they’re in. To estimate spillovers, we run one regression comparing high and one regression comparing medium to low saturation control units. To estimate the direct effect, we run a regression of the outcome on the treatment indicatior on the full sample, controlling for saturation. N_individuals &lt;- 60 N_groups &lt;- 15 G_per_saturation &lt;- c(5,5,5) design &lt;- declare_population(N = N_individuals, X = 1:N, U = rnorm(N), G = ntile(X, N_groups)) + declare_assignment(assignment_variable = &quot;S&quot;, clusters = G, conditions = c(&quot;low&quot;,&quot;med&quot;,&quot;high&quot;), m_each = G_per_saturation) + declare_assignment(prob = 0, blocks = G, assignment_variable = &quot;Z_S_low&quot;) + declare_assignment(prob = .5, blocks = G, assignment_variable = &quot;Z_S_med&quot;) + declare_assignment(prob = .75, blocks = G, assignment_variable = &quot;Z_S_high&quot;) + declare_step( spillover_low = ave(Z_S_low, G, FUN = sum) * .1, spillover_med = ave(Z_S_med, G, FUN = sum) * .1, spillover_high = ave(Z_S_high, G, FUN = sum) * .1, handler = fabricate, label = &quot;spillover&quot;) + declare_potential_outcomes( Y ~ Z * -.20 + U + spillover_low * (S == &quot;low&quot;) + spillover_med * (S == &quot;med&quot;) + spillover_high * (S == &quot;high&quot;), conditions = list(Z = c(0,1), S = c(&quot;low&quot;,&quot;med&quot;,&quot;high&quot;))) + declare_estimand(high = mean(Y_Z_0_S_high - Y_Z_0_S_low), med = mean(Y_Z_0_S_med - Y_Z_0_S_low), ate_no_spill = mean(Y_Z_1_S_low - Y_Z_0_S_low)) + declare_reveal(Z,S) + declare_step( w = 1 / (S_cond_prob * (Z_S_low_cond_prob * (S == &quot;low&quot;) + Z_S_med_cond_prob * (S == &quot;med&quot;) + Z_S_high_cond_prob * (S == &quot;high&quot;))), handler = fabricate) + declare_reveal(Y,c(Z, S)) + declare_estimator(model = lm_robust, formula = Y ~ S, subset = Z == 0 &amp; S %in% c(&quot;high&quot;,&quot;low&quot;), estimand = &quot;high&quot;, weights = w, label = &quot;high vs low&quot;) + declare_estimator(model = lm_robust, formula = Y ~ S, subset = Z == 0 &amp; S %in% c(&quot;med&quot;,&quot;low&quot;), weights = w, estimand = &quot;med&quot;, label = &quot;med vs low&quot;) + declare_estimator(model = lm_robust, formula = Y ~ Z + S, term = &quot;Z&quot;, weights = w, estimand = &quot;ate_no_spill&quot;, label = &quot;main effect&quot;) Here’s what our hypothetical country looks like: draw_data(design) %&gt;% ggplot(aes(x = 1, y = X, color = as.factor(G))) + geom_point() + scale_color_discrete(&quot;Ballot station&quot;) + scale_y_continuous(&quot;Latitude&quot;) + scale_x_continuous(&quot;Longitude&quot;) + geom_hline(yintercept = seq(1,N_individuals,by = N_individuals / N_groups) - .5) Let’s diagnose diagnosis &lt;- diagnose_design(design, sims = sims) Our diagnosis shows this design does a pretty great job, under this model of spillovers: diagnosis %&gt;% reshape_diagnosis() %&gt;% kable() Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Power Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand design ate_no_spill main effect Z 500 0.01 0.35 0.10 0.93 -0.19 0.35 0.33 0.08 -0.20 (0.01) (0.01) (0.01) (0.01) (0.01) (0.01) (0.00) (0.04) (0.00) design high high vs low Shigh 500 -0.05 0.51 0.12 0.90 0.25 0.50 0.48 0.13 0.30 (0.02) (0.02) (0.02) (0.01) (0.02) (0.02) (0.01) (0.05) (0.00) design med med vs low Smed 500 0.00 0.39 0.09 0.93 0.20 0.39 0.38 0.04 0.20 (0.02) (0.01) (0.01) (0.01) (0.02) (0.01) (0.00) (0.03) (0.00) It’s particularly nice, since we’re able to estimate the direct effect (whose constitutive POs we never observe) by partialling out spillovers. Show here: power tradeoffs for main effects versus spillovers, in terms of proportion of sample allocated to the “low” versus other conditions and also in terms of IPW (equivalent sample size with everyone in the .5 condition) Now, we consider a model of spillovers in which fraud is displaced latitudinally, from one neighbor to the next. Say, because there are roads traveling north and fraudsters disregard boundaries (in reality, they are unlikely to do so). distal_design &lt;- replace_step(design = design, step = &quot;spillover&quot;, new_step = declare_step(next_neighbor = c(N,1:(N-1)), spillover_low = Z_S_low[next_neighbor], spillover_med = Z_S_med[next_neighbor], spillover_high = Z_S_high[next_neighbor], handler = fabricate) ) distal_diagnosis &lt;- diagnose_design(distal_design, sims = sims) When there are next-neighbor spillovers that ignore boundaries, the estimator is biased again distal_diagnosis %&gt;% reshape_diagnosis() %&gt;% kable() Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Power Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand distal_design ate_no_spill main effect Z 500 -0.25 0.43 0.23 0.88 -0.45 0.36 0.36 0.00 -0.20 (0.02) (0.01) (0.02) (0.01) (0.02) (0.01) (0.00) (0.00) (0.00) distal_design high high vs low Shigh 500 0.20 0.54 0.48 0.91 0.95 0.50 0.48 0.00 0.75 (0.02) (0.02) (0.02) (0.01) (0.02) (0.02) (0.01) (0.00) (0.00) distal_design med med vs low Smed 500 0.15 0.42 0.33 0.92 0.65 0.40 0.41 0.00 0.50 (0.02) (0.01) (0.02) (0.01) (0.02) (0.01) (0.00) (0.00) (0.00) References "],
["multi-study-designs.html", "Chapter 20 Multi-study designs", " Chapter 20 Multi-study designs section introduction "],
["papers-with-multiple-studies.html", "20.1 Papers with multiple studies", " 20.1 Papers with multiple studies In many research projects, we seek to evaluate multiple observable implications for a single theory. [Examples: Psychology; APSR articles that have an experiment plus observational work; replication efforts.] In such cases, a single piece of evidence does not constitute sufficient evidence to validate the theory as a whole. Rather, we believe in the theory when multiple pieces of evidence support it. Conventions around what constitutes a convincing pattern of evidence vary. Some researchers will not believe a theory unless each piece of evidence in support of it is statistically significant. Less stringent approaches simply seek evidence “consistent” with the theory, such as the observation that all effects are signed in the predicted direction. Here, we declare an \\(N\\)-study design, and examine the consequences of these two different approaches to evaluating a theory in light of multiple studies. We show that, under multiple observable implications generated by the same process, conditioning on significance can lead one strongly astray. Generally speaking, looking at the sign of effects is more probative because it is much less prone to false negatives. With small numbers of studies, however, the risks of false positives are high. 20.1.1 Design Declaration Model: We declare \\(N\\) populations, all of which are governed by the same data-generating process: X is exogenous and standard normally-distributed, M is standard-normally distributed and correlated with X by rho, and Y is a function of the main effect of X as well as the interaction between X and M, with the size and sign of the direct and interactive effects determined by tau and gamma, respectively. Inquiry: We want to know, in a global sense, if our theory is “right.” Here, that means that the effect of X on Y is positive and increasing with M, and that X affects M. When tau is positive, our theory is correct. When it is zero or negative, our theory is incorrect. Data strategy: We conduct and collect independent datasets on \\(N\\) datasets of size n. In the example below, we conduct three studies, assuming we only observe X and Y in the first, only M and X in the second, and Y, M, and X, in the third. Answer strategy: Using linear regression, we estimate the bivariate correlation between X and Y in study 1, the bivariate correlation between X and M in study 2, and the interaction between X and M on Y in study 3. n1 &lt;- 100 n2 &lt;- 100 n3 &lt;- 100 rho &lt;- .5 gamma &lt;- tau &lt;- .2 generate_study_sample &lt;- function(n, rho, tau, gamma, data){ fabricate(N = n, X = rnorm(N), M = rnorm(N, X * rho, sqrt(1 - rho^2)), U = rnorm(N), Y = tau * X + gamma * M * X + U) } three_study_design &lt;- # Study 1 -- Bivariate correlation between X and Y declare_population(n = n1, tau = tau, gamma = gamma, rho = rho, handler = generate_study_sample) + declare_estimator(Y ~ X, term = &quot;X&quot;, model = lm_robust, label = &quot;Study 1&quot;) + # Study 2 -- Bivariate correlation between M and X declare_population(n = n2, tau = tau, gamma = gamma, rho = rho, handler = generate_study_sample) + declare_estimator(M ~ X, term = &quot;X&quot;, model = lm_robust, label = &quot;Study 2&quot;) + # Study 3 -- Interaction in X and M declare_population(n = n3, tau = tau, gamma = gamma, rho = rho, handler = generate_study_sample) + declare_estimator(Y ~ X + M + X:M, term = &quot;X:M&quot;, model = lm_robust, label = &quot;Study 3&quot;) 20.1.2 Takeaways Let us compare the performance of the “all significant” versus “all signed” approaches to theory confirmation when the theory is “correct” (tau and gamma positive), versus when it is “incorrect” (both parameters zero). In the first approach, a theory is deemed “supported” by the evidence when all effects are significant. In the second, the theory is supported by the evidence when the signs of all effects are positive. # Simulate design simulations &lt;- simulate_design(three_study_design) # Simulate null design null_three_study_design &lt;- redesign(three_study_design, tau = 0, gamma = 0, rho = 0) null_simulations &lt;- simulate_design(null_three_study_design) In the first three rows of the table, the theory is correct in that tau, gamma, and rho are positive, in the second three rows it is incorrect because both the main effect and interaction are zero. The “power” column tells us the proportion of simulations in which the effect is significant at the \\(\\alpha = .05\\) level, the “all significant” column tells us the proportion of simulations in which all of the studies have significant effects, the “positive” column tells us how often the study found a positively signed result, while the “all positive column” tells us the proportion of simulations where all studies had a positively signed result. tau estimator_label gamma power all_significant positive all_positive 0.2 Study 1 0.2 0.406 0.256 0.968 0.938 0.2 Study 2 0.2 0.998 0.256 1.000 0.938 0.2 Study 3 0.2 0.616 0.256 0.966 0.938 0.0 Study 1 0.0 0.058 0.000 0.500 0.114 0.0 Study 2 0.0 0.052 0.000 0.506 0.114 0.0 Study 3 0.0 0.060 0.000 0.530 0.114 Notice that, because the studies are independent, the probability that all are significant is equal to the product of their power: Pr(all studies significant) = Pr(Study 1 significant) \\(\\times\\)…\\(\\times\\) Pr(Study \\(N\\) significant). Thus, if you only believe a theory if the studies conducted to test it yield significant results, and those studies are all powered at the conventionally accepted level of 80%, you erroneously reject the theory with a probability of \\(.8^N\\). If you do three, conventionally well-powered, randomized studies each shooting at the right quantity of interest, then in almost half of the cases where you are right, you will think you are wrong. Furthermore, notice how detrimental the addition of a small study can be by this metric, even if it gets at an important mechanism. As soon as you condition your inference about the theory being correct on the significance of all the observable implications, a low-powered test can sharply increase the risk of false rejection. What can we say about the risk of false positives? The power of the individual studies is where it should be: for a stated error rate of $= $5%, the studies are every so slightly conservative. However, the “all significant” desideratum creates a rejection rate that is too high. Some of these problems, though not all, are alleviated when we disregard significance and just look at signs. When the theory is right, there is a very good chance that all of the effects we estimate are positive: we surmise the theory is correct roughly 94% of the time when it actually is. When the theory is not correct in the sense that the true effects are zero, random error means that they are positive half the time and negative the other half. Consequently, the probability of erroneously accepting the theory based on the sign of effects when the true underlying effects are zero is equal to \\(.5^N\\). Here, that means we erroneously infer we are right at a relatively high rate of 12% of simulations. In the design above, the rates at which we rejected or accepted theories seemed to depend on the number of studies we considered. In the graph below, we look at twenty-nine different \\(N\\)-study designs, all of which seek to confirm a theory by replicating evidence for it \\(N\\) times. The first design is made of two studies, each independtly evaluating the hypothesis that \\(Y\\) is positively correlated with \\(X\\). Again, we consider how conditioning an inference about the theory on whether all results are significant or all results are positive affects error rates. Again, we see that significance is not a probative way to look for observable theoretical implications. As soon as there are more than four studies, there is virtually no chance of confirming even a true theory by this metric. We see a sort of reverse multiple-comparisons problem: increasing the number of tests makes false rejections increasingly more likely when we are interested in the joint probability of all the tests saying the same thing. Unless the number of studies is small, whether all produce significant results essentially yields no information about whether the theory is correct. By contrast, looking at signs only can be highly probative. In this application, the optimal number of studies is about eight. At that point, there is virtually no chance of erroneously inferring that the theory is correct when the effects are zero, but when the theory is correct there is a good chance (almost 75%) that all of the available evidence will be signed accordingly. As the number of studies increases, so too does the probability of discordant results, and using the unanimity of signs to judge whether the theory is correct becomes increasingly unwise. "],
["multi-site-studies.html", "20.2 Multi-site studies", " 20.2 Multi-site studies 20.2.1 Unbiased estimates of out-of-sample sites in presence of heterogeneous effects starting point is fixed budget and you’re thinking about two possible designs: (1) a single large study in one context or (2) a set of five studies in five different contexts with the same intervention and outcome measures When there are heterogeneous effects, you can get good predictions out of sample even when average effects differ substantially (and you do better with multiple sites when sites in the the population have different proportions of subject types that are correlated with het fx) Two notable features of the design: - there must be het fx for this to work (otherwise our estimates get biased toward zero because of overfitting to the het variables) - we have to have information about the covariate in the population and the sample (here we used the proportion of people in each het type) Findings: - these two strategies are both unbiased - the design with five sites has half the RMSE of the one-site design. this is because of the variation in the proportions of types across sites. - interestingly there is poor coverage (anti-conservative) when you use the single site design (when you have contextual variation as well, i.e. effect differs across sites for reasons not captured by the het fx, coverage is off for all designs. will keep this point out, seems like too much and you don’t need contextual effects to get different effects across sites, those come from different proportions of types) meta_re_estimator &lt;- function(data){ site_estimates_df &lt;- data %&gt;% group_by(site) %&gt;% do(tidy(lm_robust(Y ~ Z, data = .))) %&gt;% filter(term == &quot;Z&quot;) %&gt;% ungroup meta_fit &lt;- rma(estimate, std.error, data = site_estimates_df, method = &quot;REML&quot;) with(meta_fit, tibble( estimate = as.vector(beta), std.error = se, p.value = pval, conf.low = ci.lb, conf.high = ci.ub)) } post_strat_estimator &lt;- function(data, pr_types_population) { if(length(unique(data$site)) &gt; 1) { fit &lt;- lm_robust(Y ~ Z*as.factor(subject_type) + as.factor(site), data = data) tidy(fit) } else { fit &lt;- lm_robust(Y ~ Z*as.factor(subject_type), data = data) } alpha &lt;- .05 lh_fit &lt;- try({ linearHypothesis( fit, hypothesis.matrix = paste(paste(paste(pr_types_population[91:100][-1], &quot;*&quot;, matchCoefs(fit, &quot;Z&quot;), sep = &quot;&quot;), collapse = &quot; + &quot;), &quot; = 0&quot;), level = 1 - alpha) }) if(!inherits(lh_fit, &quot;try-error&quot;)) { tibble(estimate = drop(attr(lh_fit, &quot;value&quot;)), std.error = sqrt(diag(attr(lh_fit, &quot;vcov&quot;))), df = fit$df.residual, statistic = estimate / std.error, p.value = 2 * pt(abs(statistic), df, lower.tail = FALSE), conf.low = estimate + std.error * qt(alpha / 2, df), conf.high = estimate + std.error * qt(1 - alpha / 2, df)) } else { tibble(error = TRUE) } } # need to have biased sampling to get bias here # two kinds of populations, one in which the study type determines the subject types and you select on study type # a second kind where study type determines study shock # in second type if you adjust for subject type then you will be able to unbiased recover global multi_site_designer &lt;- function( N_sites = 10, n_study_sites = 5, n_subjects_per_site = 1000, feasible_effect = 0, subject_type_effects = seq(from = -0.1, to = 0.1, length.out = 10), pr_types = c( # rows are sites, columns are types 0.005, 0.005, 0.09, 0.15, 0.25, 0.1, 0, 0.1, 0.15, 0.15, 0.1, 0.15, 0.15, 0.15, 0.25, 0.005, 0, 0.1, 0.09, 0.005, 0.15, 0.15, 0.15, 0.005, 0.005, 0, 0.25, 0.09, 0.1, 0.1, 0, 0.15, 0.005, 0.09, 0.005, 0.15, 0.25, 0.1, 0.1, 0.15, 0.005, 0.1, 0.09, 0.25, 0.15, 0.15, 0.005, 0, 0.1, 0.15, 0.005, 0.15, 0.25, 0.1, 0, 0.1, 0.005, 0.15, 0.09, 0.15, 0.15, 0.15, 0.005, 0.25, 0.1, 0.15, 0.09, 0.005, 0.1, 0, 0.25, 0.1, 0.15, 0, 0.005, 0.15, 0.15, 0.1, 0.005, 0.09, 0.005, 0.1, 0.1, 0.15, 0, 0.25, 0.15, 0.09, 0.005, 0.15, 0.005, 0.09, 0.15, 0.1, 0, 0.1, 0.15, 0.005, 0.25, 0.15) ) { declare_population( site = add_level(N = N_sites, feasible_site = sample(c(rep(1, 8), rep(0, 2)), N, replace = FALSE)), subject_types = add_level( N = 10, subject_type = 1:10, subject_type_effect = subject_type_effects, type_proportion = pr_types, N_subjects = ceiling(2500 * type_proportion) ), subjects = add_level(N = N_subjects, noise = rnorm(N)) ) + declare_potential_outcomes(Y ~ Z * (0.1 + subject_type_effect + feasible_effect * feasible_site) + noise) + declare_estimand(ATE_feasible = mean(Y_Z_1 - Y_Z_0), subset = feasible_site == FALSE) + # true effect for feasible sites declare_sampling(clusters = site, strata = feasible_site, strata_n = c(0, n_study_sites)) + declare_sampling(strata = site, n = n_subjects_per_site) + declare_assignment(blocks = site, prob = 0.5) + declare_estimand(study_site_ATE = mean(Y_Z_1 - Y_Z_0)) + declare_estimator(handler = tidy_estimator(post_strat_estimator), pr_types_population = pr_types, label = &quot;post-strat&quot;) } single_site_large_design &lt;- multi_site_designer(n_study_sites = 1, n_subjects_per_site = 2500) small_study_five_sites &lt;- multi_site_designer(n_study_sites = 5, n_subjects_per_site = 500) simulations_small_large &lt;- simulate_design(single_site_large_design, small_study_five_sites, sims = sims) diagnosis_small_large &lt;- diagnose_design(simulations_small_large %&gt;% filter(!is.na(estimate) &amp; !is.na(std.error) &amp; !is.na(statistic) &amp; !is.na(p.value) &amp; !is.na(conf.low) &amp; !is.na(conf.high)), bootstrap_sims = b_sims) kable(get_diagnosands(diagnosis_small_large)) design_label estimand_label estimator_label bias se(bias) rmse se(rmse) power se(power) coverage se(coverage) mean_estimate se(mean_estimate) sd_estimate se(sd_estimate) mean_se se(mean_se) type_s_rate se(type_s_rate) mean_estimand se(mean_estimand) n_sims single_site_large_design ATE_feasible post-strat 0.0031475 0.0119653 0.3629044 0.0129728 0.1025126 0.009576 0.9356784 0.0077135 0.1040090 0.0119606 0.3633932 0.0130575 0.3182932 0.0065550 0.1666667 0.0371110 0.1008615 0.0003499 995 single_site_large_design study_site_ATE post-strat 0.0036542 0.0119708 0.3636432 0.0131462 0.1025126 0.009576 0.9356784 0.0076497 0.1040090 0.0119606 0.3633932 0.0130575 0.3182932 0.0065550 0.1666667 0.0371110 0.1003548 0.0005471 995 small_study_five_sites ATE_feasible post-strat 0.0156817 0.0058686 0.1857540 0.0064505 0.1419940 0.010929 0.9385700 0.0076310 0.1160789 0.0058907 0.1857092 0.0065791 0.1689084 0.0021306 0.0212766 0.0124541 0.1003972 0.0003763 993 small_study_five_sites study_site_ATE post-strat 0.0151117 0.0058942 0.1864589 0.0065325 0.1419940 0.010929 0.9395770 0.0076094 0.1160789 0.0058907 0.1857092 0.0065791 0.1689084 0.0021306 0.0212766 0.0124541 0.1009671 0.0001825 993 20.2.2 Bayesian estimation can improve estimates of effects for sampled sites you can improve site-level effect estimates by analyzing with simple Bayesian model because of its shrinkage property, even when the Bayesian model is wrong about distribution of effects in population this is the point from the blog post; I will modify the above design so it can also make this point, switching between the normal distribution and uniform distribution for the fx distribution stan_model &lt;- &quot; data { int&lt;lower=0&gt; J; // number of sites real y[J]; // estimated effects real&lt;lower=0&gt; sigma[J]; // s.e. of effect estimates } parameters { real mu; real&lt;lower=0&gt; tau; real eta[J]; } transformed parameters { real theta[J]; real tau_sq = tau^2; for (j in 1:J) theta[j] = mu + tau * eta[j]; } model { target += normal_lpdf(eta | 0, 1); target += normal_lpdf(y | theta, sigma); } &quot; stan_re_estimator &lt;- function(data) { site_estimates_df &lt;- data %&gt;% group_by(site) %&gt;% do(tidy(lm_robust(Y ~ Z, data = .))) %&gt;% filter(term == &quot;Z&quot;) %&gt;% ungroup J &lt;- nrow(site_estimates_df) df &lt;- list(J = J, y = site_estimates_df$estimate, sigma = site_estimates_df$std.error) fit &lt;- stan(model_code = stan_model, data = site_estimates_df) fit_sm &lt;- summary(fit)$summary data.frame(estimate = fit_sm[,1][c(&quot;mu&quot;, &quot;tau&quot;, &quot;theta[1]&quot;, &quot;prob_pos&quot;)]) } bayes_estimator &lt;- declare_estimator(handler = stan_re_estimator) 20.2.3 when things break down: confounded sampling none of these designs work when you’re trying to make predictions for sites that are systematically different, i.e. are not in the same population as the sampling frame the design was set up to include several sites where researchers could not feasibly set up experiments. in the original design, effects do not depend on whether sites are feasible for the experiment. when effects do vary, there are systematic differences for those target sites. those differences might come from three sources: mean effect size differs in places that are sampled vs not sampled; individual-level het fx sizes that systematically differ in places that are sampled to study vs others; covariate profiles that do not exist in sites outside the sampling frame. I introduce effects in the first way and show there is substantial bias. small_study_five_sites_feasible_effects &lt;- multi_site_designer(n_study_sites = 5, n_subjects_per_site = 500, feasible_effect = -0.25) simulations_feasible_effects &lt;- simulate_design(small_study_five_sites_feasible_effects, sims = sims) diagnosis_feasible_effects &lt;- diagnose_design(simulations_feasible_effects %&gt;% filter(!is.na(estimate) &amp; !is.na(std.error) &amp; !is.na(statistic) &amp; !is.na(p.value) &amp; !is.na(conf.low) &amp; !is.na(conf.high)), bootstrap_sims = b_sims) kable(get_diagnosands(diagnosis_feasible_effects)) design_label estimand_label estimator_label bias se(bias) rmse se(rmse) power se(power) coverage se(coverage) mean_estimate se(mean_estimate) sd_estimate se(sd_estimate) mean_se se(mean_se) type_s_rate se(type_s_rate) mean_estimand se(mean_estimand) n_sims small_study_five_sites_feasible_effects ATE_feasible post-strat -0.0203957 0.0053320 0.1680045 0.0051913 0.0804829 0.008917 0.9557344 0.0064813 0.0803788 0.0053339 0.1667939 0.0052236 0.163637 0.00196 0.075 0.0315973 0.1007745 0.0003584 994 small_study_five_sites_feasible_effects study_site_ATE post-strat 0.2299906 0.0053397 0.2842747 0.0048958 0.0804829 0.008917 0.6549296 0.0155612 0.0803788 0.0053339 0.1667939 0.0052236 0.163637 0.00196 0.925 0.0315973 -0.1496118 0.0002008 994 Other points I decided to abandon to keep this simple: - tradeoff: context-specific interventions and comparability of intervention effects - tradeoff: comparability and fidelity to context in outcome measurement "],
["part-iii-exercises.html", "Chapter 21 Part III Exercises", " Chapter 21 Part III Exercises Measuring sensitive traits with direct questions may be biased due to sensitivity bias: subjects may perceive (rightly or wrongly) that someone such as the enumerator, their neighbors, or the authorities will impose costs on them if they provide a dispreferred answer. In sensitive settings, direct questions provide a comparatively precise answer, but they are biased. One alternative to direct questions is the list experiment (see section XXX). Under their assumptions, list experiments are unbiased, but they can be quite imprecise. Thus, the choice between list experiments and direct questions includes a bias-variance tradeoff. Following the formulas given in _____, the variance of the direct question estimator is the following, where \\(\\pi^*\\) is the true prevalence rate and \\(\\delta\\) is sensitivity bias. \\[ \\frac{1}{N - 1} \\bigg\\{ \\pi^* (1 - \\pi^*) + \\delta (1 - \\delta) - 2(\\delta - \\pi^*\\delta) \\bigg\\} \\] The variance of the list experiment is given by this expression, where \\(\\V(Y_i(0)\\) is the variance of the control item count and \\(\\cov(Y_i(0), D_i^*)\\) is the covariance of the control item count with the densitive trait. \\[ \\frac{1}{N-1} \\bigg\\{ \\pi^*(1-\\pi^*) + 4 \\V(Y_i(0)) + 4 \\cov(Y_i(0), D_i^*) \\bigg\\} \\] Our goal is to compare the direct question and list experiment designs with respect to the RMSE diagnosand. Recall that RMSE equals the square root of variance plus bias squared: \\(RMSE = \\sqrt{Variance + Bias^2}\\). Assume the following design parameters: \\(\\delta = 0.10\\), \\(\\pi^* = 0.50\\), \\(\\V(Y_i(0) = 0.075\\), \\(\\cov(Y_i(0), D_i^*) = 0.025\\). What is the RMSE of the direct question when \\(N\\) = 100? What is the RMSE of the list experiment when \\(N\\) = 100? Make a figure with \\(N\\) on the horizontal axis and RMSE on the vertical axis. Plot the RMSE for both designs over a range of sample sizes from 100 to 2000. Hint: you’ll need to write a function for each design that takes \\(N\\) as an input and returns RMSE. You can get started by filling out this starter function: direct_rmse &lt;- function(N){ # stuff goes here} How large does the sample size need to be before the list experiment is preferred to the direct question on RMSE grounds? Comment on how your answer to (d) would change if \\(\\delta\\) were equal to 0.2? What are the implications for the choice between list experiments and direct questions? "],
["putting-designs-to-use.html", "Chapter 22 Putting Designs to Use", " Chapter 22 Putting Designs to Use "],
["before-studies.html", "22.1 Before studies", " 22.1 Before studies 22.1.1 Pre-Analysis Plans 22.1.2 Registered Reports 22.1.3 Standard Operating Procedures 22.1.4 Evaluating and Supporting Research "],
["after-studies.html", "22.2 After studies", " 22.2 After studies 22.2.1 Reconciliation 22.2.2 Replication 22.2.3 Peers: Better scholarly critique 22.2.4 Combining designs Job market papers with multiple studies / three paper paradigm in psych (is it one design targeting same inquiry?) [JC 1p] Multi-site studies – take a design from another study and use it for another one Knowledge accumulation "],
["improving-designs-in-the-social-sciences.html", "22.3 Improving designs in the social sciences", " 22.3 Improving designs in the social sciences 22.3.1 A library of research designs as objects "],
["part-iv-exercises.html", "22.4 Part IV Exercises", " 22.4 Part IV Exercises "],
["references-4.html", "References", " References "]
]
