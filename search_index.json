[
["index.html", "Research Design: Declare, Diagnose, Redesign Welcome", " Research Design: Declare, Diagnose, Redesign Graeme Blair, Jasper Cooper, Alexander Coppock, and Macartan Humphreys Welcome "],
["preamble.html", "Chapter 1 Preamble", " Chapter 1 Preamble With this book, we hope to promote a new, comprehensive way of thinking about research designs in the social sciences. We hope this way of thinking will make research designs more transparent and more robust. But we also hope it will make research design easier, easier to produce good designs, but also easier to share designs and build off of the designs that others have developed. The core idea is to start think of a design as an object that can be interrogated. The design encodes your beliefs about the world, it describes your questions, and it lays out how you go about answering those questions, in terms both of what data you use and how you use it. A key idea is that all of these features can be provided in code and if done right the information provided is enough to be able to simulate a run of the design and assess its properties. For a researcher, being able to simulate a design puts you in a powerful position as you can then start assessing the conditions under which a design perfoms well or badly. For a reader, a complete declaration lets you quickly examine the analytic core of a design. As you work through designs in this book we hope you will develop a nose for quickly parsing what goes in the model, what are the kinds of inquiries you should expect to see, what are the data strategies, what are analysis strategies, and how these all link together. "],
["mida.html", "Chapter 2 MIDA", " Chapter 2 MIDA Research designs are the procedures we use to learn about the world as we imagine it. Research designs vary according to how we imagine the world, the questions we want answer, the information we collect, and how we summarize the information in order to yield an answer. Interviews, surveys, case studies, randomized experiments, laboratory games, [etc], are all research designs All empirical research designs gather and analyze information in order to answer a question about the world, stated in terms of a background model. These four features form the basis of the “MIDA” framework that we will rely on to describe designs. MIDA stands for “model,” “inquiry,” “data strategy,” and “answer strategy.” The first two components of a design are theoretical. The model of the world that is in researchers minds may correspond to the real world more or less well, but the model is certainly not the same thing as the real world. The inquiry is a question about the world, expressed in terms of the model. Because the inquiry can only be expressed in model terms, it is also theoretical. By contrast, the second two components of a research design - the data strategy and answer strategy - take place in the real world and are not theoretical. The data strategy describes how the research will obtain new information, so includes choices like case selection, sampling procedures, randomization protocols, questionaire design, participant observation techniques, and the like. The answer strategy is how you process the new information. The answer strategy is more than just the choice of estimator – it’s the full set of choices that map the data into the table and figures in the report. For quantitative designs, these choices include how “raw data” are processed into “clean data,” which estimators will be applied, and which results will be presented. For qualitative designs, the answer strategy includes procedures for summarizing interviews, field notes, or original source documents. Formally, we defined a causal , \\(M\\), of how the world works, following Pearl’s definition of a probabilistic causal model . A casual model itself is made up of three components: The variables \\(X\\) and their ranges. The variables in \\(X\\) can themselves be partitioned into endogenous (\\(V\\)) and exogenous (\\(U\\)) variables. The functional relationships (\\(F\\)) between variables. The functional relationships describe how each variable in the model does or does not causally affect the others. Sometimes these functional relationships are described by potential outcomes functions. A probability distribution over the exogenous variables, \\(P(U)\\). Many features of a probabalistic causal model can be encoded in a Directed Acyclic Graph, or DAG. It shows the variables and the presence or absence of functional relationships between variables. It does not encode the functional forms of those relationships or even the strength of the causal effect. It is difficult to represent heterogeneity on a DAG. The DAG does not show the \\(P(U)\\). The inquiry generates the answer-under-the-model, or \\(a^M\\). "],
["model-inquiry-data-strategy-model-strategy.html", "2.1 Model-Inquiry-Data Strategy-Model Strategy", " 2.1 Model-Inquiry-Data Strategy-Model Strategy The specification of a problem requires a description of the world and the question to be asked about the world as described. The answering requires a description of what information is used and how conclusions are reached given the information. At its most basic we think of a research design, \\(\\Delta\\), as including four elements \\(&lt;M,I,D,A&gt;\\): A , \\(M\\), of how the world works. In general following Pearl’s definition of a probabilistic causal model we will assume that a model contains three core elements. First, a specification of the variables \\(X\\) about which research is being conducted. This includes endogenous and exogenous variables (\\(V\\) and \\(U\\) respectively) and the ranges of these variables. In the formal literature this is sometimes called the of a model (Halpern 2000). Second, a specification of how each endogenous variable depends on other variables (the functional relations'' or, as in @Imbens2015,potential outcomes’’), \\(F\\). Third, a probability distribution over exogenous variables, \\(P(U)\\). An , \\(I\\), about the distribution of variables, \\(X\\), perhaps given interventions on some variables. Using Pearl’s notation we can distinguish between questions that ask about the conditional values of variables, such as \\(\\Pr(X_1 | X_2 =1)\\) and questions that ask about values that would arise under interventions: \\(\\Pr(X_1 | do(X_2 = 1))\\). We let \\(a^M\\) denote the answer to \\(I\\) . Conditional on the model, \\(a^M\\) is the value of the estimand, the quantity that the researcher wants to learn about. A strategy, \\(D\\), generates data \\(d\\) on \\(X\\). Data \\(d\\) arises, under model \\(M\\) with probability \\(P_M(d|D)\\). The data strategy includes sampling strategies and assignment strategies, which we denote with \\(P_S\\) and \\(P_Z\\) respectively. Measurement techniques are also a part of data strategies and can be thought of as a selection of observable variables that carry information about unobservable variables. An answer strategy, \\(A\\), that generates answer \\(a^A\\) using data \\(d\\). A key feature of this bare specification is that if \\(M\\), \\(D\\), and \\(A\\) are sufficiently well described, the answer to question \\(I\\) has a distribution \\(P_M(a^A|D)\\). Moreover, one can construct a distribution of comparisons of this answer to the correct answer, under \\(M\\), for example by assessing \\(P_M(a^M-a^A|D)\\). One can also compare this to results under different data or analysis strategies, \\(P_M(a^M-a^A|D&#39;)\\) and \\(P_M(a^M-a^{A&#39;}|D)\\), and to answers generated under alternative models, \\(P_M(a^{M&#39;}-a^{A}|D)\\), as long as these possess signatures that are consistent with inquiries and answer strategies. MIDA captures the analysis-relevant features of a design, but it does not describe substantive elements, such as how theories are derived or interventions are implemented. Yet many other aspects of a design that are not explicitly labeled in these features enter into this framework if they are analytically relevant. For example, logistical details of data collection such as the duration of time between a treatment being administered and endline data collection enter into the model if the longer time until data collection affects subject recall of the treatment. However, information in {} is typically insufficient to assess those substantive elements, an important and separate part of assessing the quality of a research study. References "],
["diagnosands-and-diagnosis.html", "2.2 Diagnosands and diagnosis", " 2.2 Diagnosands and diagnosis The ability to calculate distributions of answers, given a model, opens multiple avenues for assessment and critique. How good is the answer you expect to get from a given strategy? Would you do better, given some desideratum, with a different data strategy? With a different analysis strategy? How good is the strategy if the model is wrong in some way or another? To allow for this kind of diagnosis of a design, we introduce two further concepts, both functions of research designs. These are quantities that a researcher or a third party could calculate with respect to a design. A {} is a summary statistic generated from a “run” of a design—that is, the results given a possible realization of variables, given the model and data strategy. A diagnostic statistic may or may not depend on the model as well as realized data. For example the statistic: \\(e=\\) “difference between the estimated and the actual average treatment effect” depends on the model (since the ATE depends on the model’s assumptions about potential outcomes). The statistic \\(s = \\mathbb{1}(p \\leq 0.05)\\), interpreted as “the result is considered statistically significant at the 5% level”,’’ does not depend on the model but it does presuppose an answer strategy that reports a \\(p\\) value. Diagnostic statistics are governed by probability distributions that arise because both the model and the data generation, given the model, may be stochastic. A Diagnosand is a summary of the distribution of a diagnostic statistic. For example, (expected) in the estimated treatment effect is \\(\\mathbb{E}(e)\\) and statistical is \\(\\mathbb{E}(s)\\). To illustrate, consider the following design. A model M specifies three variables \\(X\\), \\(Y\\) and \\(Z\\) (all defined on the reals). These form the signature. In additional we assume functional relationships between them that allow for the possibility of confounding (for example, \\(Y = bX + Z + \\epsilon_Y; X = Z+ \\epsilon_X\\), with \\(Z, \\epsilon_X, \\epsilon_Z\\) distributed standard normal). The inquiry \\(I\\) is ``what would be the average effect of a unit increase in \\(X\\) on \\(Y\\) in the population?’’ Note that this question depends on the signature of the model, but not the functional equations of the model (the answer provided by the model does of course depend on the functional equations). Consider now a data strategy, \\(D\\), in which data is gathered on \\(X\\) and \\(Y\\) for \\(n\\) randomly selected units. An answer \\(a^A\\), is then generated using ordinary least squares as the answer strategy, \\(A\\). We have specified all the components of MIDA. We now ask: How strong is this research design? One way to answer this question is with respect to the diagnosand “expected error.” Here the model’s functional equations provide an answer, \\(a^M\\) to the inquiry (for any draw of \\(\\beta\\)), and so the distribution of the expected error, given the model, \\(a^A-a^M\\), can be calculated. In this example the expected performance of the design may be poor, as measured by this diagnosand, because the data and analysis strategy do not handle the confounding described by the model. In comparison, better performance may be achieved through an alternative data strategy (e.g., where \\(D&#39;\\) randomly assigned \\(X\\) to \\(n\\) units before recording \\(X\\) and \\(Y\\)) or an alternative analysis strategy (e.g., \\(A&#39;\\) conditions on \\(Z\\)). These design evaluations depend on the model, and so one might reasonably ask how performance would look were the model different (for example if the underlying process involved nonlinearities). In all cases, the evaluation of a design depends on the assessment of a diagnosand, and comparing the diagnoses to what could be achieved under alternative designs. In section X we discuss possible choices of diagnosands and operate a set of these "],
["what-is-a-complete-design-declaration.html", "2.3 What is a Complete Design Declaration?", " 2.3 What is a Complete Design Declaration? A declaration of a research design that is in some sense complete is required in order to implement it, communicate its essential features, and to assess its properties. Yet existing definitions make clear that there is no single conception of a complete research design: the Consolidated Standards of Reporting Trials (CONSORT) Statement widely used in medicine includes 22 features and other proposals range from nine to 60 components. We propose a conditional notion of completeness: we say a design is ``diagnosand-complete’’ for a given diagnosand if that diagnosand can be calculated from the declared design. Thus a design that is diagnosand complete for one diagnosand may not be for another. Consider for example the diagnosand statistical power. Power is the probability that a p-value is lower than a critical value. Thus, power-completeness requires that the answer strategy return a p value. It does not, however, require a well-defined estimand. In contrast, Bias- or RMSE-completeness does not require a hypothesis test, but does require the specification of an estimand. Diagnosand-completeness is a desirable property to the extent that it means a diagnosand can be calculated. How useful this is depends however on how useful the diagnosand is for decision making. Thus evaluating completeness should focus first on whether diagnosands for which completeness holds are indeed useful ones. This usefulness depends in part on whether the information on which diagnoses are made is believable. A design may be bias-complete for instance under the assumptions of a particular spillover structure, for example. Readers may disagree with these assumptions but there are still gains from the declaration as the grounds for claims for unbiasedness are clear and the effects of deviations from model assumptions can be assessed. In practice, different research communities set different standards for what constitutes sufficient information to make such conjectures about the world plausible. "],
["declaration-diagnosis-redesign.html", "2.4 Declaration-Diagnosis-Redesign", " 2.4 Declaration-Diagnosis-Redesign "],
["software-primer.html", "Chapter 3 Software primer", " Chapter 3 Software primer library(DeclareDesign) set.seed(1) # M -- Model population &lt;- declare_population(N = 5, u = rnorm(N)) potential_outcomes &lt;- declare_potential_outcomes(Y_Z_0 = 0, Y_Z_1 = 1 + u) # I -- Inquiries: A query defined in terms of potential outcomes estimand_1 &lt;- declare_estimand(PATE = mean(Y_Z_1 - Y_Z_0)) estimand_2 &lt;- declare_estimand(SATE = mean(Y_Z_1 - Y_Z_0)) # D -- Data Strategy: Researcher interventions on the world sampling &lt;- declare_sampling(n = 4) assignment &lt;- declare_assignment() reveal_Y &lt;- declare_reveal(Y,Z) # A -- Answer steps estimate &lt;- declare_estimator(Y~Z, estimand = c(&quot;PATE&quot;, &quot;SATE&quot;)) design &lt;- population + potential_outcomes + estimand_2 + sampling + estimand_1 + assignment + reveal_Y+ estimate simulations &lt;- simulate_design(design, sims = 1) "],
["a-simple-design-declaration.html", "3.1 A simple design declaration", " 3.1 A simple design declaration Here we’re going to walk through a design The way DD works is that it generates a dataset, and passes that dataset along, modifying or summarizing it at each step 3.1.1 Parameters tau &lt;- .10 N &lt;- 8 N_sampled &lt;- 4 3.1.2 \\(M\\)odel population &lt;- declare_population(N = N, e = runif(N)) potential_outcomes &lt;- declare_potential_outcomes(Y_Z_0 = .5 &lt; e, Y_Z_1 = .5 &lt; e + tau) 3.1.3 \\(I\\)nquiry estimand &lt;- declare_estimand(PATE = mean(Y_Z_1 - Y_Z_0)) 3.1.4 \\(D\\)ata strategy sampling &lt;- declare_sampling(n = N_sampled) assignment &lt;- declare_assignment(prob = .5) reveal_outcomes &lt;- declare_reveal(Y, Z) 3.1.5 \\(A\\)nswer strategy estimator &lt;- declare_estimator(Y ~ Z, label = &quot;DiM&quot;, estimand = &quot;PATE&quot;) 3.1.6 Putting it all together simple_design &lt;- population + potential_outcomes + estimand + sampling + assignment + reveal_outcomes + estimator 3.1.7 Appending, Modifying, Expanding simple_design &lt;- simple_design + linear_estimator simple_design &lt;- redesign(simple_design, N = 100) simple_designs &lt;- redesign(simple_design, N = c(50,100,200)) null_potential_outcomes &lt;- declare_potential_outcomes(Y_Z_0 = .5 &lt; e, Y_Z_1 = Y_Z_0) simple_designs &lt;- replace_step(simple_design, &quot;potential_outcomes&quot;, null_potential_outcomes) "],
["research-questions.html", "Chapter 4 Research questions", " Chapter 4 Research questions "],
["specifying-the-model.html", "Chapter 5 Specifying the model", " Chapter 5 Specifying the model In section X, we said that a model has three things: a set of endogenous and exogenous variables; a set of equations defining how those variables are related; and a set of probability distributions over the exogenous variables. In this chapter, we’ll talk about how to go about the tricky task of specifying those three things and why it’s so important to do so. Key learning points: Learning depends on models because they are the only way to understand how your answer and data strategy perform with respect to your inquiry The key thing a model does is posit potential outcomes—those are the building blocks of what you can ask (your \\(I\\)nquiry) and what you can observe (your \\(A\\)nswer strategy) How to take control of your potential outcomes: you will learn what counterfactual states of the world you need model explicitly and which ones you don’t "],
["models-versus-modelling.html", "5.1 Models versus modelling", " 5.1 Models versus modelling In this chapter, we’re not talking about “statistical models” used to estimate unknown parameters—that’s part of your answer strategy. We’re talking about specifying simulation models: a set of assumptions encoded in math or computer language that can be used to generate imaginary datasets. The distinction is a subtle but important one. To see why, let’s talk about fish. How many fish are there in the ocean, anyway? It turns out we don’t know the answer to this question: we just can’t count all the fish. We do have answers, though: scientists estimate there are 3.5 trillion fish in the ocean as of 2017. But if we can’t count them, how do we get this answer and why should we ever believe it? It all depends on comparisons between \\(M\\)odels and modelling. Let’s say our \\(M\\)odel states that \\(N\\) fish exist. We want to see how well a statistical modelling strategy would do at guessing \\(N\\). Our \\(M\\)odel further assumes the fish don’t die, reproduce, or migrate. They don’t have any favorite spots in the sea, so they’re equally likely to be caught no matter where you cast your net. Moreover, the fish are brave fish: having been caught once and released, the fish don’t shy away from being caught again. One of the simplest modelling techniques to estimate the size of an unknown population is the capture-recapture technique: Net \\(n\\) fish, tag them, and release them; catch another \\(K\\) fish, count the \\(k\\) that are tagged (from the previous netting), and release them. This leads to a pretty simple statistical model, \\(n/N = k/K \\Rightarrow \\hat{N} = Kn/k\\). In our [design library entry on sample-resample designs][Sample-resample designs] we show that this design does a fairly good job of getting the answer right, under this \\(M\\)odel. In reality though, we know that the posited \\(M\\)odel of the world is very unrealistic: fish die, migrate, reproduce, favor certain spots, drop their tags, and avoid nets at different rates. So, if you were told you that the 3.5 million number came from the simple modelling exercise above, you’d rightly distrust it. What fisheries scientists actually do is draw on a wealth of research to build really complex and much more realistic \\(M\\)odels on computers, which are capable of simulating fish stocks. With those \\(M\\)odels and the imaginary but known fish populations they generate, we can test the performance of more sophisticated statistical modelling strategies. If the statistical models do a pretty good job of guessing the size of the imaginary fish populations, then we should feel more inclined to trust the numbers they give us when we go out and use them in the real world. In other words, the distinction between \\(M\\)odels and modelling here gives us something quite profound: though we may never know how many fish are really in the ocean, we can know how many fish we would guess are in the ocean were our assumptions about the ocean correct. We never get to know if we guessed correctly, but we do get to know which guesses we should and shouldn’t trust, given the assumptions we’re willing to make about the world. "],
["without-a-model-you-can-get-the-answer-pretty-wrong.html", "5.2 Without a model you can get the answer pretty wrong", " 5.2 Without a model you can get the answer pretty wrong Most of our causal and descriptive inferences are based on the plausibility of an implicit \\(M\\)odel—in fact, \\(M\\)odels often lurk where you might not expect them. For example, many experimental designs appear not to rely on \\(M\\)odels in order to generate plausible inferences, because their answer strategies don’t engage in any modelling of variables. The linearity of expectations implies that \\(E[Y_i(Z_i = 1) - Y_i(Z_i = 0)]\\) is equal to \\(E[Y_i(Z_i = 1)] - E[Y_i(Z_i = 0)]\\)—in English, the average difference in potential outcomes is equal to the difference in average potential outcomes. Many researchers take this to mean that they can simply compare the average observed outcome in the treatment group, \\(E[Y_i |Z_i = 1]\\) to the average observed outcome in the control group, \\(E[Y_i |Z_i = 0]\\), in order to get an unbiased estimate of the average treatment effect.1 Since this comparison requires no statistical modelling of the distributions of the observed variables, it seems as though such strategies are completely agnostic as to how the world works. Implicit in the very definition of the estimand here, however, is a \\(M\\)odel: specifically, one in which any particular draw of the random assignment is able to reveal no more than two potential outcomes for any given individual—this is sometimes referred to as the Stable Unit Treatment Value Assumption (SUTVA). But many social processes violate this assumption. If I do something differently every time you get treated and I don’t, then I have at least three potential outcomes: one in which I’m treated, one in which neither of us are treated, and one in which you’re treated but I’m not. The average observed difference-in-means across random assignments might diverge quite substantially from the true average effect of putting just one person in treatment and the rest in control, for example. (Callout to section on spillovers). So even the simplest, most agnostic research design is relying on the plausibility of an implicit model in order for to generate insights that are correct on average. Of course, some research designs require much more \\(M\\)odel than others. In an experiment, we typically worry about everything that happens post-assignment when we draw up a \\(M\\)odel: attrition, compliance, and so forth. But in observational studies, in addition to those issues, the \\(M\\)odel for pretreatment variables can matter a lot. Take the simplest possible design, in which we want to know the effect of a non-randomly assigned treatment, \\(Z\\), on \\(Y\\). Say you have a pretreatment covariate, \\(X\\), that is correlated with both \\(Z\\) and \\(Y\\). Should you control for \\(X\\)? Some authors might argue that, because the values of \\(X\\) are realized prior to the values of \\(Z\\) and time can’t flow backwards, \\(X\\) is causally antecedent to \\(Z\\) and so it follows you can’t do worse by including it. Rosenbaum (2002), for instance, argues that “there is little to no reason to avoid adjustment for a true covariate, a variable describing subjects before treatment.” However, as (???) have shown, it turns out that if \\(Z\\) and \\(Y\\) are not confounded but \\(X\\) and \\(Y\\) are, then controlling for \\(X\\) introduces additional spurious dependency between \\(Z\\) and \\(Y\\) that can create considerable bias. If the question is “should you control for \\(X\\)?” the answer is always “it depends on your \\(M\\)odel.” So, you really need to specify a \\(M\\)odel: doing so helps determine the conditions under which your answers are credible. Without knowing the conditions under which we can believe your answers, it’s hard to know whether to believe your answer. \\(M\\)odels play a second and equally important role: they define the ‘’potential outcomes’’ of the individuals in your study. Those are critical, because they form the basis of both your inquiry—what you want to know—and your answer strategy—what you can observe. References "],
["declaring-exogenous-and-endogenous-variables-in-the-population.html", "5.3 Declaring exogenous and endogenous variables in the population", " 5.3 Declaring exogenous and endogenous variables in the population Remember from Chapter X that declaring a \\(M\\)odel means specifying exogenous and endogenous variables, how they are related to one another, and the probability distributions that determine which values the variables take. Further below, we’ll talk about how to use pre-existing data to build a \\(M\\)odel, but suppose for now that we must build it entirely from scratch. An exogenous variable is one whose values do not depend on the values of any other variable in the \\(M\\)odel. By contrast, an endogenous variable is one whose values can depend on another variable. We use the term “parent” to describe the variables that endogenous variables depend on directly. The following line of code, taken and modified from the \\(M\\)odel from our [A simple design declaration][simple design declaration], contains a fully specified model: population &lt;- declare_population(N = 8, e = runif(N), X = rnorm(N, e, 1)) Here, e is an exogenous variable: it is simply a random draw from the uniform distribution between 0 and 1, inclusive. So long as \\(N\\) stays fixed, no other variables influence the values that e takes on. By contrast, X is defined as a function of the value that e takes, so it is endogenous. The code says that the value that X takes for a given individual is equal to a random draw from a normal distribution whose mean is equal to their value of e and whose standard deviation is equal to 1. Say, for example, that e took the value .35 for the fifth individual in the study. In that case, the fifth individual will get a value of X that should be close to, but not exactly, .35. Our model has two variables, one of which is uniformly distributed and the other of which is normally distributed, with the mean of X depending on the value of e. We refer to e as a parent of X because it appears in the expression for X. "],
["declaring-potential-outcomes.html", "5.4 Declaring potential outcomes", " 5.4 Declaring potential outcomes The population function we just declared generates random datasets with eight observations of e and X. Try running this code a few times to see: population() ## ID e X ## 1 1 0.9148060 1.3190744 ## 2 2 0.9370754 0.8309509 ## 3 3 0.2861395 1.7976615 ## 4 4 0.8304476 0.7357886 ## 5 5 0.6417455 2.6601692 ## 6 6 0.5190959 0.4563819 ## 7 7 0.7365883 2.0414580 ## 8 8 0.1346666 2.4213120 The population function is a useful place to declare all of the exogenous variables and many of the endogenous variables in our study. We’ll talk below about how to build more sophisticated population structures, with hierarchical dependencies and randomly-generated sample sizes. But a key step in building the model comes after the declaration of the population function and it’s important to talk about it first: declaring the potential outcomes. So what are the potential outcomes? Consider the case of an outcome, \\(Y\\), that responds to a (possibly randomized) treatment assignment, \\(Z\\). For example, \\(Z\\) might be a coin-flip that assigns people to treatment when it comes up heads (\\(Z = 1\\)) and to control when it comes up tails (\\(Z = 0\\)). We can express the \\(i\\)’th individual’s outcome when the assignment variable takes value \\(z\\) as \\(Y_i(Z_i = z)\\). For a binary treatment, this gives rise to at least three different variables: \\(Y_i\\), \\(Y_i(Z_i = 1)\\), and \\(Y_i(Z_i = 0)\\). The first variable is a list of the actual observed outcomes for any given realization of treatment assignment: if you flip the coin eight times, for example, \\(Y_i\\) will be a variable that is eight observations long that tells you what happened to the people who did and didn’t get assigned to treatment. The second variable is the potential outcome of an individual if you set their treatment assignment to ‘treated’—i.e., a list of everyone’s outcomes imagining that you only flipped heads. And the third is the potential outcome of an individual if you set their treatment assignment to ‘untreated’—a list of every person’s outcomes in a world where you only flip tails. What’s so important about defining these imaginary potential outcomes? Why do we need all three of these variables, and not just \\(Y_i\\), the values we actually observe? The point is that potential outcomes are the building blocks of your inquiry—if you’re not interested in the counterfactual differences in an outcome generated by some other variable, then you don’t need to worry about the corresponding potential outcomes. But if you want to know a causal effect, you have to model the state of the world when the cause is present and the one in which it is absent. A causal effect is defined as the difference between those two states of the world. That’s why DeclareDesign has a whole step devoted to declaring potential outcomes. The declare_potential_outcomes() step is crucial because it splits up endogenous variables into the counterfactual sets of values they could take, given the variables they depend on. Consider this declaration: potential_outcomes &lt;- declare_potential_outcomes(Y_Z_0 = .5 &lt; e, Y_Z_1 = .5 &lt; e + .05) Here, we have added two variables to the dataset, Y_Z_0 and Y_Z_1. They are both endogenous, in the sense that they depend on the value that e takes and on the value that Z takes. In our framework, potential outcomes are labelled starting with the outcome (here, Y) followed by a specific parent variable whose effect we’re interested in (here, Z), and the value that parent is set to (here 1 and 0), all separated by underscores. So Y_Z_1 is a list of the hypothetical values Y could take, if you were to set a specific variable it depends on, Z, to take the value of 1. In English, the code in the potential outcomes declaration reads, “the value of \\(Y\\) when \\(Z\\) is zero is TRUE if \\(e\\) is greater than .5 and FALSE otherwise, while the value of \\(Y\\) when \\(Z\\) is one is TRUE if \\(e\\) plus .05 is greater than .5.” So, setting \\(Z\\) to one makes \\(Y\\) roughly five percentage points more likely to be TRUE, on average. In math, we could write this: \\(Y_i(Z_i = 0) = \\mathbb{I}(.5 &lt; e_i)\\) and \\(Y_i(Z_i = 1) = \\mathbb{I}(.5 &lt; e_i + .05)\\). Notice that this provides a pretty easy expression for the observed outcome: \\(Y_i = \\mathbb{I}(.5 &lt; e_i + Z_i \\times .05)\\). So the potential outcomes for a variable can either be expressed by defining each potential outcome explicitly, as we do above, or through what we call a “potential outcomes function:” potential_outcomes &lt;- declare_potential_outcomes(Y ~ .5 &lt; e + .05 * Z) By default, declare_potential_outcomes() assumes that the functional equation for Y will include a binary Z that it can split on to create Y_Z_0 and Y_Z_1. But you can also tell the function to split on any other variables that take on any kind of value. For example, potential_outcomes &lt;- declare_potential_outcomes( income ~ employed + education + u, assignment_variables = list(employed = c(&quot;No&quot;,&quot;Yes&quot;), education = c(10,12))) will create four new variables: income_employed_No_education_10, income_employed_No_education_12, income_employed_Yes_education_10, and income_employed_Yes_education_12. In this setup, income is the observed variable and the other four variables are the potential outcomes that would result from assigning individuals to the corresponding values of employed and education. The resultant function, potential_outcomes, knows not to create potential outcomes corresponding to values of u because u does not appear in the list of assignment_variables. 5.4.1 Potential outcomes can include variables that are not yet defined Careful readers may have picked up on something a bit confusing in the preceding paragraphs: we have defined potential outcomes in terms of a variable that has not yet been realized in our design. simple_design &lt;- population + potential_outcomes + estimand + sampling + assignment + reveal_outcomes + estimator The potential outcomes Y_Z_1 and Y_Z_0 are already defined in terms of Z in the second step of our design, well before the variable Z gets created in the fifth step, assignment. How is this possible? Oddly, perhaps, the values of potential outcomes don’t depend on the actual values that the parent variables happen to take2—that’s what makes them potential outcomes and not plain old outcomes. Imagine, for example, that your design involves assigning everyone to the control, so that \\(Z_i = 0~~\\forall~~i\\) in practice. In that case, we still define the treated potential outcome, \\(Y_i(Z_i = 1)\\), exactly as before. We don’t need to know what values Z will actually take in order to define the values that Y could potentially take. What this requires at the model specification stage is some forwards-looking: you are going to need to model the imaginary states of the world that could happen before they happen. So, for example, if you have a different potential outcome depending on whether you are treated and whether you are sampled, then you need to define potential outcomes in terms of treatment assignment and sampling before either of these steps have occurred. More on this below. 5.4.2 When you do and don’t need to define potential outcomes So when should you split endogenous variables into all of the counterfactual values they can take on (potential outcomes), and when should you leave them whole? In other words, which variables belong in your population and which belong in your potential_outcomes? Why do we split on variables like Z and not on variables like u or e? The short answer is that it all depends on your \\(I\\)nquiry. Let’s take a simple example using the population declaration above. Suppose that you were interested in the average effect of \\(Z\\) on \\(Y\\), for any given value that \\(X\\) can take: \\(E[Y_i(1) - Y_i(0)]\\). Then your inquiry depends only on Y_Z_1 and Y_Z_0. Those are the only potential outcomes you need. What if you want to know the effect of \\(Z\\) on \\(Y\\) among groups for whom \\(X\\) happened to equal 1 or happened to equal 0: \\(E[Y_i(1) - Y_i(0)\\mid X_i = x]\\). Here, you’re not interested in the causal effect of X, just in whether, descriptively, the effect of Z on Y just happens to different among people for whom X == 1. There’s no need to split on X in this case: you just need to look at the difference in Y_Z_1 and Y_Z_0 among the people for whom X is equal to 1 or to 0, which can be achieved through subsetting. Now, let’s imagine that you were interested in the causal effects of both X and Z on Y. For example, you might want to know whether X causes the effects of Z on Y to be bigger: \\[E[(Y_i(Z_i = 1,X_i = 1) - Y_i(Z_i = 0,X_i = 1)) - \\] \\[~~~~~(Y_i(Z_i = 1,X_i = 0) - Y_i(Z_i = 0,X_i = 0))].\\] That’s a claim about the counterfactual states of Y as a function of both variables, and would require something such as: potential_outcomes &lt;- declare_potential_outcomes( Y_X_0_Z_0 = .5 &lt; e, Y_X_0_Z_1 = .5 &lt; e + .05, Y_X_1_Z_0 = .5 &lt; e, Y_X_1_Z_1 = .5 &lt; e + .05 + .05) Where Y_X_0_Z_1, for example, is a variable that lists every individual’s potential Y outcome if X were set to 0 and Z were set to 1. In this example, we have stipulated that X increases the effect of Z by .05. To see this, note that we can rewrite the potential outcomes declaration above using a potential outcomes function: declare_potential_outcomes(Y ~ .5 &lt; e + Z * .05 + Z * X * .05, assignment_variables = list(Z = 0:1, X = 0:1)) What if the value of \\(X\\) were itself a function of \\(Z\\), and your inquiry focused on the effect of \\(X\\) on \\(Y\\)—\\(E[Y_i(X_i = 1) - Y_i(X_i = 0)]\\)? In that case, you might model two sets of potential outcomes: the potential outcomes of \\(X\\) as a function of \\(Z\\), and the potential outcomes of \\(Y\\) as a function of \\(X\\): potential_outcomes &lt;- declare_potential_outcomes( X_Z_0 = .5 &lt; e * 0.75, X_Z_1 = .5 &lt; e * 1.25, Y_X_0 = .5 &lt; e, Y_X_1 = .5 &lt; e + .05) Here, we are assuming Z is exogenous in the sense that it is randomly assigned. However, X, the causal variable of interest, is endogenous to e and to Z. In fact, if we’re interested in the effect of X on Y, e is no longer background noise here as it was in the examples above: it has become a confounder of the causal effect in which we’re interested . To see this, note that higher levels of e make both X and Y more likely to be TRUE. This is the sort of setup in which analysts would typically use an “instrumental variables” approach (callout to IV).3 In practice, there are always many many more potential outcomes lurking in your study than you need to model. Notice, for example, that we didn’t model the potential outcomes of Y as a function of specific levels of e above – unless we care about the causal effect of e on some outcome, there is little reason to model the counterfactuals it gives rise to. In the following two sections, we walk through some more concrete advice on which kinds of potential outcomes you need to consider modelling. Specifically, we think you should focus on potential outcomes generated through two processes: manipulation and interference. 5.4.3 Manipulation creates potential outcomes Manipulation is some real or imagined intervention in the world that sets the values of a parent of one of your outcomes. Perhaps the most obvious manipulation is assignment to treatment—a coin flip, for example, is a manipulation that sets \\(Z\\) to 1 for roughly half of the people and to 0 for the others. In quasi-experimental designs, we imagine a quasi-assignment: a non-random policy intervention might set some constituencies to have a change in their electoral rules and not others, for example. In that case, we imagine every constituency’s potential outcome had the policy intervention taken place there, and vice versa. But there are many other kinds of manipulations: measurement and sampling are two obvious examples. When you randomly sample someone and conduct a survey with them, you set their sampling status to “Sampled” and their measurement status to “Measured.” In general, you should consider modeling any manipulation that might affect the value of your \\(I\\)nquiry. A good rule of thumb is that there will be at least as many potential outcomes as the Cartesian product of the range of the manipulated variables. Let’s suppose, for example, that you have a treatment variable with three values, \\(Z \\in \\{1,2,3\\}\\), and you think that there might be Hawthorne effects – e.g., an effect of having your outcomes measured, \\(M \\in \\{0,1\\}\\). That implies you should have six potential outcomes, \\(Y_i(Z_i,M_i)\\): \\(Y_i(1,0)\\), \\(Y_i(2,0)\\), \\(Y_i(3,0)\\), \\(Y_i(1,1)\\),\\(Y_i(2,1)\\), and \\(Y_i(3,1)\\). The first three represent states of the world revealed by assigning someone to the different arms of the treatment when they’re not measured, and the latter three those same treatment outcomes when measured. The following potential outcomes declaration suggests a Hawthorne effect: hawthorne_POs &lt;- declare_potential_outcomes( Y_Z_1_M_0 = .5 &lt; e, Y_Z_2_M_0 = .5 &lt; e + .05, Y_Z_3_M_0 = .5 &lt; e + .05, Y_Z_1_M_1 = .5 &lt; e + .05, Y_Z_2_M_1 = .5 &lt; e + .05 + .05, Y_Z_3_M_1 = .5 &lt; e + .05 + .05) Note, however, that our design now requires some modifications: simple_design has an inquiry defined in terms of the simpler potential outcomes, Y_Z_1 and Y_Z_0, which no longer exist. We need to clarify that we’re interested in the effect of treatment without any measurement effects (here, we’ll say we’re interested in the treatment 2 versus 1 comparison). Second, simple_design had no variable M to split on: we need a step in which we manipulate the measurement variable to be 1 for everyone in the sample. Finally, simple_design revealed Y purely as a function of Z, but we need to declare that the observed Y will correspond to the values of both Z and M: assignment &lt;- declare_assignment(conditions = c(1,2,3)) estimand_no_m &lt;- declare_estimand(ate_2_no_m = mean(Y_Z_2_M_0 - Y_Z_1_M_0)) measurement &lt;- declare_step(M = 1, handler = fabricate) reveal_outcomes_measurement &lt;- declare_reveal(Y, c(Z, M)) hawthorne_design &lt;- population + hawthorne_POs + estimand_no_m + sampling + assignment + measurement + reveal_outcomes_measurement + estimator From here, it’s easy to modify our design with measurement effects to stipulate a model in which there is an interaction between treatment and measurement—this is often referred to as an “experimenter demand” effect, and can be more problematic for inference than a simple Hawthorne effect (you can do some simple algebra with the estimand declaration to see why): experimenter_demand_POs &lt;- declare_potential_outcomes( Y_Z_1_M_0 = .5 &lt; e, Y_Z_2_M_0 = .5 &lt; e + .05, Y_Z_3_M_0 = .5 &lt; e + .05, Y_Z_1_M_1 = .5 &lt; e, Y_Z_2_M_1 = .5 &lt; e + .05 + .05, Y_Z_3_M_1 = .5 &lt; e + .05 + .10) demand_design &lt;- replace_step(design = hawthorne_design, step = &quot;hawthorne_POs&quot;, new_step = experimenter_demand_POs) The very same logic can be applied to defining interactions between different treatment arms, effects from sampling, and other interactive effects. 5.4.4 Interference creates potential outcomes We stated in the previous section that there are usually at least as many potential outcomes as the Cartesian product of the ranges of the manipulated parents. But that statement assumes that the value of each individual’s potential outcome depends only on the value of their own parent variables. When one individual’s potential outcomes depend on the value of a manipulated variable of any other unit in the study, we refer to this as “interference.” Interference is a generic concept that includes social processes such as spillovers, social comparisons, contagion, communication, displacement, deterrence, and persistence [CITE GG p256-6]. Depending on your model, interference can generate a much, much larger space of potential outcomes. The formal notation for interference is that \\(Y_i(Z_i) \\neq Y_i(\\mathbf{Z})\\). Here, \\(\\mathbf{Z}\\) denotes the entire vector of random assignments: the math says that an individual’s potential outcome expressed in terms of their own assignment is not the same thing as their outcome expressed in terms of everyone’s assignment. Accordingly, the most general potential outcomes model in the presence of interference is one in which every conceivable realization of \\(\\mathbf{Z}\\) is mapped to exactly one potential outcome. Consider an employee-of-the-month experiment, in which one of three individuals is randomly assigned to be employee of the month (reference to GG). Suppose that employees 1 and 2 don’t like each other. We’ll define potential outcomes in the following manner: \\(Y_i(j)\\), where \\(j\\) denotes the index of the treated individual. So, for example, \\(Y_2(3)\\) is the potential outcome of the second individual when individual 3 is assigned to treatment. interference_design &lt;- declare_population(N = 3, e = runif(N)) + declare_potential_outcomes( Y_J_1 = c(.5 &lt; e[1] + 1, .5 &lt; e[2] - 1, .5 &lt; e[3]), Y_J_2 = c(.5 &lt; e[1] - 1, .5 &lt; e[2] + 1, .5 &lt; e[3]), Y_J_3 = c(.5 &lt; e[1], .5 &lt; e[2], .5 &lt; e[3] + 1)) + declare_assignment(conditions = c(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;), assignment_variable = &quot;J&quot;) + declare_step(Z = as.numeric(ID == J), handler = fabricate) + declare_reveal(Y, J) + declare_estimator(Y ~ Z, model = lm_robust) Notice we didn’t declare an estimand here. That’s because the inquiry in a design with interference requires particular attention (callout to inquiry section). As you can see, the space of potential outcomes can expand very quickly when we allow for every possible way in which the treatment can be assigned to affect outcomes differently. Under complete random assignment of \\(m\\) of \\(N\\) people to treatment or control, there are \\(N\\) choose \\(m\\) ways of assigning treatment, for example. For a simple design in which ten people are assigned to treatment and control in equal proportions, there are 252 potential outcomes to consider. The problem of specifying the potential outcomes in your \\(M\\)odel can quickly become intractable. Often, we don’t actually expect outcomes to differ quite so much: if there are 100 employees in the company and Sally doesn’t know Jim or Tracy, she might be indifferent between the two worlds in which either Jim or Tracy wins employee-of-the-month. Generalizing this principle, one might be able to cut down on the number of potential outcomes by considering spillovers limited to social networks within the company. Where possible, a good way to handle spillovers is to specify a \\(M\\)odel in which they function like any other treatment. Such \\(M\\)odels rely on qualitative knowledge, which can be wrong. Let’s say, for example, that you were worried about spillovers in an experiment, but: 1) you know that people’s interactions are restricted by some ordering, such as a queue; 2) you strongly suspected that a person being treated only affects the outcomes of the next person in the queue, but not the person after. For example, if the treatment involved providing some randomly selected people in a queue with extra information and the outcome of interest was customer satisfaction, those who fall in the queue behind treated individuals might get frustrated at observing how long the person in front of them is taking. In this case, we can think of two treatments: \\(Z\\), being directly treated, and \\(S\\), having the person before you treated: spillover_POs &lt;- declare_potential_outcomes( Y_Z_0_S_0 = .5 &lt; e, Y_Z_1_S_0 = .5 &lt; e + .05, Y_Z_0_S_1 = .5 &lt; e - .05 / 2, Y_Z_1_S_1 = .5 &lt; e + .05 / 2) neighbors &lt;- declare_step(next_neighbor = c(N,(1:(N-1))), S = Z[next_neighbor], handler = fabricate) reveal_spillovers &lt;- declare_reveal(Y, c(Z, S)) spillover_design &lt;- population + spillover_POs + sampling + assignment + neighbors + reveal_spillovers + estimator Now, instead of \\(N\\) choose \\(m\\) potential outcomes, there are four—for any sample size. We’ve contained the problem of proliferating potential outcomes. Of course, showing that one’s strategy is robust to the spillovers specified in this \\(M\\)odel might not be convincing to a skeptic who contends spillovers might also affect the second or third person behind the treated individual. Moreso than ever, the validity of inferences depend on the robustness of the \\(A\\)nswer strategy to plausible \\(M\\)odels. 5.4.5 What to do with potential outcomes whose parents are continuous So far we have focused on potential outcomes whose parents take on discrete values. But another way in which potential outcomes can proliferate is if there are infinitely many places at which to consider splitting the outcome on the parent variable, as is the case with continuous parents. [INCLUDE EXAMPLE FROM PAPER] 5.4.6 Principal strata can be defined in terms of potential outcomes One important set of \\(I\\)nquiries comprise estimands that are specific to certain causal “types” in the population. For example, in designs in which not everyone assigned to treatment actually takes treatment, researchers are often interested in the treatment effect among those actually treated (often referred to as “compliers”). Similarly, in designs where some units do not report outcomes and where reporting is possibly a function of treatment, researchers are often interested in the effect among those who would report in either treatment or control. These causal types—referred to as “principal strata” in the literature [CITE imbens rubin]—can be usefully modeled in terms of potential outcomes. Consider the case of one-sided non-compliance: compliance_POs &lt;- declare_potential_outcomes( D_Z_0 = 0, D_Z_1 = ifelse(order(e) &gt; 4, 1, 0), Y_D_0 = .5 &lt; e, Y_D_1 = .5 &lt; e + .05) ate_estimand &lt;- declare_estimand(ate = mean(Y_D_1 - Y_D_0)) cace_estimand &lt;- declare_estimand(cace = mean(Y_D_1 - Y_D_0), subset = D_Z_0 == 0 &amp; D_Z_1 == 1) Here, treatment status, \\(D\\), is a potential outcome of treatment assignment, \\(Z\\). The CACE estimand is easily defined in terms of the causal type: it is the average effect among people for whom \\(D_i(Z_i = 0) =0, D_i(Z_i = 1) = 1\\). Similarly, we can think of attrition as a potential outcome. Here, people with the lowest four values of e do not report outcomes if assigned to control, but everyone reports when they’re assigned to treatment. Whenever someone does not report we only observe an NA. attrition_POs &lt;- declare_potential_outcomes( R_Z_0 = ifelse(order(e) &lt;= 4, 1, 0), R_Z_1 = 1, Y_R_0_Z_0 = NA, Y_R_0_Z_1 = NA, Y_R_1_Z_0 = .5 &lt; e, Y_R_1_Z_1 = .5 &lt; e + .05) 5.4.7 Develop a null model You might be wondering whether \\(M\\)odels are simply a way of coding hypotheses. The two are similar but not quite the same: many \\(M\\)odels can correspond to the same hypothesis. One particularly important hypothesis is the so-called null hypothesis that the average effect of \\(Z\\) on \\(Y\\) is equal to zero. Consider the following three models: population &lt;- declare_population(N = 8, e = rnorm(N, 0, 1)) model_1 &lt;- population + declare_potential_outcomes(Y ~ e) model_2 &lt;- population + declare_potential_outcomes(Y ~ e + e * 2 * Z) model_3 &lt;- population + declare_potential_outcomes(Y ~ ifelse(e &gt; .5, Z * .2, -Z * .2)) Each of these models is consistent with the null hypothesis. In the first, treatment and control outcomes are exactly the same: \\(Z\\) does not even appear in the functional equation for \\(Y\\), there is no effect for any unit in the sample. This is sometimes referred to as a “sharp null hypothesis,” and is a specific case of the more general null. In model_2, \\(Z\\) increases the variance of \\(Y\\), but because the mean of \\(e\\) is 0, there is no average difference in the means of the treated and control outcomes: they’re both zero. In model_3, there are large positive treatment effects for those with \\(e_i &gt; .5\\) and large negative treatment effects for those with \\(e_i \\leq .5\\). On average, the effects offset each other leading to an average effect of \\(Z\\) on \\(Y\\) that is equal to zero. Since there are infinitely many ways of parameterizing this average effect of zero (replace .2 with any number in model_3), there are infinitely many models that correspond to the single null hypothesis that the average effect of Z on Y is equal to zero. Despite the fact that there are many such null models to consider, and the fact that most researchers don’t design a project expecting that null hypotheses are true, we see great value in declaring a so-called “null model.” By “null model,” we have in mind something like the potential outcomes in model_1: there is no relationship whatsoever between the outcome of interest and the treatment(s) of interest. You can you learn at least three important things from a “null model” design, or “null design.” First, the power of your null design is an important quantity: it is none other than the rate of false positives for your design, otherwise known as the type 1 error rate. It is important to know if the probability with which you (erroneously) reject the null of a zero average effect is equal to your \\(\\alpha\\): the rate at which you stipulate that you’re comfortable erroneously rejecting the null (usually 5%). You might be quite happy to see that your non-null design exhibits great power with small effects, for example. But if you check your null design and see that your power is above 10%, then you know you have a problem: you are rejecting the null of no effect at twice the rate you should be for a stated error rate of 5%. Second, the ability to define the false positive rate for one estimator gives you the ability to define the false positive rate over all of the estimators in your design. This is often referred to as the family-wise error rate: how often at least one of the tests you run erroneously rejects the null of no effect. If your tests are completely unrelated to one another, this rate will just be \\(\\alpha^k\\), where \\(k\\) is the number of tests. But often rejecting one test implies you are more likely to reject another: if an erroneous rejection results from chance imbalance on a variable that is correlated with another variable tested against the same treatment, then you’re likely to reject that test too. At the extreme, if you run \\(k\\) equivalent tests, the familywise error rate will be 5%: rejecting one means you reject the rest, failing to reject one means failing to reject the rest. So, as we show in section X, if you use your actual realized data to generate a null design, you can figure out what your actual familywise error rate is under the global null of no effect for any unit or outcome. From there, you can figure out what testwise \\(\\alpha\\) you would need to apply in order to reject any test in the family 5% of the time. Often, this will be a lot less punitive than out-of-the-box corrections for multiple comparisons. Third, and maybe most importantly for practical purposes, a null design doesn’t require any specification about effect sizes. A perennial issue in power analysis is that calculating power requires specification of some arbitrary effect size. But often the very reason we do a study is to determine what the size of an effect is. More meaningful, we think, is using diagnosis to determine the smallest effect you could detect with 80% power. And for that, you only need a good estimate of the standard error, which does not depend on the effect size—it can be derived from a null design. When diagnosing a null design in order to calculate the MDE, it may be worth considering potential outcomes of the form declared in model_2: when trying to determine the standard error, it’s important to consider whether the treatment may change the variance in the outcome, even if it doesn’t change the mean. 5.4.8 Consider heterogeneity treatment effects may be stochastic—this can matter for variance estimation treatment effects may vary systematically—many otherwise unbiased designs become biased when this happens, so consider it by default Include reference to Pearl and truncation here? E.g. \\(Pr(Y~ \\mid ~do(X)) ~~|| ~~Pr(X)\\)↩ One shortcoming of this \\(M\\)odel is that it has “baked in” the assumption of an exclusion restriction: Y is only affected by Z through Z’s effect on X—Z does not appear in the expression for Y. Returning to the fish example: showing that an instrumental variables answer strategy performs well under this \\(M\\)odel would not be convincing to someone who worried about a particular, unmodeled, violation of the exclusion restriction.↩ "],
["declaring-populations.html", "5.5 Declaring populations", " 5.5 Declaring populations 5.5.1 Use data, if you have it can use it to estimate population parameters for RNGs can build off / bootstrap existing dataset start with a null design to calculate MDE 5.5.2 Incorporate hierarchy into your model Some examples of nested models Time-series models Stochastic group sizes "],
["defining-the-inquiry.html", "Chapter 6 Defining the inquiry", " Chapter 6 Defining the inquiry "],
["crafting-a-data-strategy.html", "Chapter 7 Crafting a data strategy", " Chapter 7 Crafting a data strategy "],
["choosing-an-answer-strategy.html", "Chapter 8 Choosing an answer strategy", " Chapter 8 Choosing an answer strategy "],
["diagnosis.html", "Chapter 9 Diagnosis", " Chapter 9 Diagnosis "],
["diagnosing-a-single-design.html", "9.1 Diagnosing a single design", " 9.1 Diagnosing a single design Definition and practical details of Monte Carlo and diagnosands (and discussion of formulae) [JC] Graphic of simulations (of multiple runs) "],
["how-do-you-select-diagnosands.html", "9.2 How do you select diagnosands?", " 9.2 How do you select diagnosands? Diagnose given the purposes of the study Single shot vs repeated designs (MSE vs bias) Moral questions (Type 1 vs Type 2 errors) Power for biased designs Standard diagnosands (paragraph on each of the diagnosands in our defaults) Ways of getting answers to a question wrong Diagnosing inferential statistics (SE bias vs. coverage, error rates for ps, Bayes?) How to select diagnosands (some sort of decision tree?) Multiple estimates / inquiries [JC] How to think about uncertainty about model parameters (multiple designs?) Diagnosands that are a function of “multiple designs” like MDE Conditional diagnosands different? Uncertainty of diagnosands (bootstrapping etc.) "],
["diagnosis-to-assess-the-robustness-of-designs-to-models-gb.html", "9.3 Diagnosis to assess the robustness of designs to models [GB]", " 9.3 Diagnosis to assess the robustness of designs to models [GB] Hold inquiry constant! (read Richard Crump “Moving the Goalposts”) Hold three constant, vary one of MIDA at a time M: ICC, null model, alternative DAGs, heterogeneity I: "],
["redesign.html", "Chapter 10 Redesign", " Chapter 10 Redesign "],
["design-library.html", "Chapter 11 Design Library", " Chapter 11 Design Library This section of the book ennumerates a series of common and not-so-common social science research design. Each entry will include description of the design in terms of MIDA and also a declaration of the design in code. We’ll often diagnose designs over the range of values of some design parameters in order to point out especially interesting or unusual features of the design. Our goal in this section is not to provide a comprehensive accounting of all empirical research designs. It’s also not to describe any of the particular designs in exhaustive detail, because we are quite sure that in order for these designs to be useful for any practical purpose, they will need to be modified. The entries in the design library are not recipies that, if you follow the instructions, out will come high-quality research. Instead, we hope that the entries provide inspiration for how to tailor a particular class of designs – the blocked-and-clustered randomized trial or the catch-and-release design – to your own research setting. The basic structure of the design library entry will be useful, but the specifics about plausible ranges of outcomes, sample size constraints, etc, will be different in each particular setting. We’ve split up designs by Inquiry and by Data strategy. Inquires can be descripitve or causal and Data strategies can be observational or experimental. This leads to four categories of research: Observational descriptive, Experimental descriptive, Observational Causal, and Experimental causal. A third dimension along which studies can vary is whether the Answer strategy is qualitative or quantitative. If we include this dimension in our typology, we’d end up with eight broad categories of research design. We don’t see the qualitative-quantitative difference in answer strategy to be as fundamental as the differences in inquiry and data strategy, so we’ll just include both qualitative and quantitative designs in each of our four categories. Besides, social scientists always appreciate a good two-by-two: In the broadest terms, descriptive inquiries can be described as \\(f(\\mathbf{Y(Z = Realized)})\\), where \\(f()\\) is some function and \\(\\mathbf{Y(Z = Realized)}\\) is a vector of realized outcomes. That is, descriptive designs seek to summarize (using \\(f()\\)) the world as it is (as represented by \\(\\mathbf{Y(Z = Realized)}\\)). Descripitive designs can be better or worse at answering that inquiry. The quality of descriptive research designs depends on the extent of measurement, sampling, and estimation error. Causal inquiries can be described as \\(f(\\mathbf{Y(Z)})\\), where \\(Z\\) is not a realized vector of treatments, but is instead is a vector that could take on counterfactual values. A standard causal inquiry is the Average Treatment Effect, in which \\(f()\\) is the function that takes the average of the difference between two potential outcome vectors, \\(Y(Z = 1)\\) and \\(Y(Z = 0)\\). But there are many causal inquiries beyond the ATE – the thing they all have in common is that they are functions not of realized outcomes, but of potential outcomes. The quality of causal reseach designs depends on everything that a descriptive design depends on, but also on the understanding and quality of the mechanism that assigns units to treatment conditions. All research designs suffer from some kind of missing data problem. Rubin pointed out missing data in surveys come from people you didn’t survey or people who refused to answer. In causal inference problems, the data that are missing are the potential outcomes that were not revealed by the world. In Descriptive studies, the data that are missing are the true values of the things to be measured. Measurement error is a missing data problem too! Observational research designs are typified by reseachers having no impact on the units under study. They simply record the outcomes that happened in the world and would have happened even if the study did not occur. Experimental research designs are more active – they cause some potential outcomes to be revealed but not others. In this way, researchers have an impact on the units they study. For this reason, experimental studies tend to raise more ethical questions than do observational studies. Experimenters literally change what potential outcomes become realized outcomes. Sometimes the lines between types of research become blurry. The Hawthorne effect is the name given to the idea that measuring a thing changes it. If there are hawthorne effects, than observational research designs also change which potential outcomes are revealed. That is, if there is a difference between Y(Z = measured) and Y(Z = unmeasured), then the act of observation changes that which is observed. Passive data collection methods are sometimes preferred on these grounds. "],
["observational-designs-for-descriptive-inference.html", "Chapter 12 Observational Designs for Descriptive Inference", " Chapter 12 Observational Designs for Descriptive Inference blah blah about this section of designs "],
["random-sampling.html", "12.1 Random sampling", " 12.1 Random sampling 12.1.1 Simple random sampling Often we are interested in features of a population, but data on the entire population is prohibitively expensive to collect. Instead, researchers obtain data on a small fraction of the population and use measurements taken on that sample to draw inferences about the population. Imagine we seek to estimate the average political ideology of residents of the small town of Portola, California, on a left-right scale that varies from 1 (most liberal) to 7 (most conservative). We draw a simple random sample in which all residents have an equal chance of inclusion in the study. It’s a straightforward design but formally declaring it will make it easy to assess its properties. 12.1.1.1 Design Declaration Model: Even for this most basic of designs, researchers bring to bear a background model of the world. As described in Chapter 1, the three elements of a model are the signature, probability distributions over variables, and functional equations among variables. The signature here is a specification of the variable of interest, \\(Y\\), with a well defined domain (seven possible values between 1 and 7). In the code declaration below, we assume a uniform distribution over these 7 values. This choice is a speculation about the population distribution of \\(Y\\); some features of the design diagnosis will depend on the choice of distribution. The functional equations seem absent here as there is only one variable in the model. We could consider an elaboration of the model that includes three variables: the true outcome, \\(Y\\); the decision to measure the outcome, \\(M\\); and the measured outcome, \\(Y^M\\). We ignore this complication for now under the assumption that \\(Y = Y^M\\), i.e., that \\(Y\\) is measured perfectly. Finally, the model also includes information about the size of the population. Portola, California, has a population of approximately 2100 people as of 2010, so \\(N = 2100\\). Inquiry: Our inquiry is the population mean of \\(Y\\): \\(\\frac{1}{N} \\sum_1^N Y_i = \\bar{Y}\\). Data strategy: In simple random sampling, we draw a random sample without replacement of size \\(n\\), where every member of the population has an equal probability of inclusion in the sample, \\(\\frac{n}{N}\\). When \\(N\\) is very large relative to \\(n\\), units are drawn approximately independently. In this design we measure \\(Y\\) for \\(n=100\\) units in the sample; the other \\(N-n\\) units are not measured. Answer strategy: We estimate the population mean with the sample mean estimator: \\(\\widehat{\\overline{Y}} = \\frac{1}{n} \\sum_1^n Y_i\\). Even though our inquiry implies our answer should be a single number, an answer strategy typically also provides statistics that help us assess the uncertainty around that single number. To construct a 95% confidence interval around our estimate, we calculate the standard error of the sample mean, then approximate the sampling distribution of the sample mean estimator using a formula that includes a finite population correction. In particular, we approximate the estimated sampling distribution by a \\(t\\) distribution with \\(n - 1\\) degrees of freedom. In the code for our answer strategy, we spell out each step in turn. # Model ------------------------------------------------------------------- N &lt;- 2100 fixed_population &lt;- declare_population(N = N, Y = sample(1:7, N, replace = TRUE))() population &lt;- declare_population(data = fixed_population) # Inquiry ----------------------------------------------------------------- estimand &lt;- declare_estimand(Ybar = mean(Y)) # Data Strategy ----------------------------------------------------------- n &lt;- 100 sampling &lt;- declare_sampling(n = n) # Answer Strategy --------------------------------------------------------- estimator &lt;- declare_estimator(Y ~ 1, model = lm_robust, estimand = estimand, label = &quot;Sample Mean Estimator&quot;) # Design ------------------------------------------------------------------ design &lt;- population + estimand + sampling + estimator diagnosands &lt;- declare_diagnosands(select = c(bias, coverage, mean_estimate, sd_estimate)) 12.1.1.2 Takeaways With the design declared we can run a diagnosis and plot results from Monte Carlo simulations of the design: diagnosis &lt;- diagnose_design( design, sims = sims, bootstrap_sims = b_sims, diagnosands = diagnosands) The diagnosis indicates that under simple random sampling, the sample mean estimator of the population mean is unbiased. The graph on the left shows the sampling distribution of the estimator: it’s centered directly on the true value of the inquiry. Confidence intervals also have a sampling distribution – they change depending on the idiosyncrasies of each sample we happen to draw. The figure on the right shows that the 95% of the time the confidence intervals cover the true value of the estimand, as they should. As sample size grows, the sampling distribution of the estimator gets tighter, but the coverage of the confidence intervals stays at 95% – just the properties we would want out of our answer strategy. Things work well here it seems. In the exercises we suggest some small modifications of the design that point to conditions under which things might break down. 12.1.2 Stratified and clustered random sampling Researchers often cannot randomly sample at the individual level because it may, among other reasons, be too costly or logistically impractical. Instead, they may choose to randomly sample households, political precincts, or any group of individuals in order to draw inferences about the population. This strategy may be cheaper and simpler but may also introduce risks of less precise estimates. Say we are interested in the average party ideology in the entire state of California. Using cluster sampling, we randomly sample counties within the state, and within each selected county, randomly sample individuals to survey. Assuming enough variation in the outcome of interest, the random assignment of equal-sized clusters yields unbiased but imprecise estimates. By sampling clusters, we select groups of individuals who may share common attributes. Unlike simple random sampling, we need to take account of this intra-cluster correlation in our estimation of the standard error.4 The higher the degree of within-cluster similarity, the more variance we observe in cluster-level averages and the more imprecise are our estimates.5 We address this by considering cluster-robust standard errors in our answer strategy below. 12.1.2.1 Design Declaration Model: We specify the variable of interest \\(Y\\) (political ideology, say) as a discrete variable ranging from 1 (most liberal) to 7 (most conservative). We do not define a functional model since we are interested in the population mean of \\(Y\\). The model also includes information about the number of sampled clusters and the number of individuals per cluster. Inquiry: Our estimand is the population mean of political identification \\(Y\\). Because we employed random sampling, we can expect the value of the sample mean (\\(\\widehat{\\overline{y}}\\)) to approximate the true population parameter (\\(\\widehat{\\overline{Y}}\\)). Data strategy: Sampling follows a two-stage strategy. We first draw a random sample 30 counties in California, and in each county select 20 individuals at random. This guarantees that each county has the same probability of being included in the sample and each resident within a county the same probability of being in the sample. In this design we estimate \\(Y\\) for n = 600 respondents. Answer strategy: We estimate the population mean with the sample mean estimator: \\(\\widehat{\\overline{Y}} = \\frac{1}{n} \\sum_1^n Y_i\\), and estimate standard errors under the assumption of independent and heteroskedastic errors as well as cluster-robust standard errors to take into account correlation of errors within clusters. Below we demonstrate the the imprecision of our estimated \\(\\widehat{\\overline{Y}}\\) when we cluster standard errors and when we do not in the presence of an intracluster correlation coefficient (ICC) of 0.402. N_blocks &lt;- 1 N_clusters_in_block &lt;- 1000 N_i_in_cluster &lt;- 50 n_clusters_in_block &lt;- 30 n_i_in_cluster &lt;- 20 icc &lt;- 0.402 # M: Model fixed_pop &lt;- declare_population( block = add_level(N = N_blocks), cluster = add_level(N = N_clusters_in_block), subject = add_level(N = N_i_in_cluster, latent = draw_normal_icc(mean = 0, N = N, clusters = cluster, ICC = icc), Y = draw_ordered(x = latent, breaks = qnorm(seq(0, 1, length.out = 8))) ) )() cluster_sampling_design &lt;- declare_population(data = fixed_pop) + # I: Inquiry declare_estimand(Ybar = mean(Y)) + # D: Data Strategy declare_sampling(strata = block, clusters = cluster, n = n_clusters_in_block, sampling_variable = &quot;Cluster_Sampling_Prob&quot;) + declare_sampling(strata = cluster, n = n_i_in_cluster, sampling_variable = &quot;Within_Cluster_Sampling_Prob&quot;) + # A: Answer Strategy declare_estimator(Y ~ 1, model = lm_robust, clusters = cluster, estimand = &quot;Ybar&quot;, label = &quot;Clustered Standard Errors&quot;) 12.1.2.2 Takeaways diagnosis &lt;- diagnose_design(cluster_sampling_design, sims = sims) Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Power Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand cluster_sampling_design Ybar Clustered Standard Errors (Intercept) 500 0.01 0.25 1.00 0.95 3.97 0.25 0.25 0.00 3.97 (0.01) (0.01) (0.00) (0.01) (0.01) (0.01) (0.00) (0.00) (0.00) To appreciate the role of clustering better we also plot simulated values of our estimand with standard errors not clustered and with clustered standard errors. To do this we first add an additional estimator to the design that does not take account of clusters. new_design &lt;- cluster_sampling_design + declare_estimator(Y ~ 1, model = lm_robust, estimand = &quot;Ybar&quot;, label = &quot;Naive Standard Errors&quot;) diagnosis &lt;- diagnose_design(new_design, sims = sims) The figure above may give us the impression that our estimate with clustered standard errors is less precise, when in fact, it correctly accounts for the uncertainty surrounding our estimates. The blue lines in the graph demonstrate the estimates from simulations which contain our estimand. As our table and graphs show, the share of these simulations over the total number of simulations, also known as coverage, is (correctly) close to 95% in estimations with clustered standard errors and 54% in estimations without clustered standard errors. As expected, the mean estimate itself and the bias is the same in both specifications. 12.1.2.3 Exercises Modify the declaration to change the distribution of \\(Y\\) from being uniform to something else: perhaps imagine that more extreme ideologies are more prevalent than moderate ones. Is the sample mean estimator still unbiased? Interpret your answer. Change the sampling procedure to favor units with higher values of ideology. Is the sample mean estimator still unbiased? Interpret your answer. Modify the estimation function to use this formula for the standard error: \\(\\widehat{se} \\equiv \\frac{\\widehat\\sigma}{\\sqrt{n}}\\). This equation differs from the one used in our declaration (it ignores the total population size \\(N\\)). Check that the coverage of this new design is incorrect when \\(N=n\\). Assess how large \\(N\\) has to be for the difference between these procedures not to matter. The intra-cluster correlation coefficient (ICC) can be calculated directly and is a feature of this design.↩ In ordinary least square (OLS) models, we assume errors are independent (error terms between individual observations are uncorrelated with each other) and homoskedastic (the size of errors is homogeneous across individuals). In reality, this is often not the case with cluster sampling.↩ "],
["multilevel-regression-and-poststratification.html", "12.2 Multilevel regression and poststratification", " 12.2 Multilevel regression and poststratification You can use the global bib file via rmarkdown cites like this: Imai, King, and Stuart (2008) # demographic summary statistics by state us_state_demographics_2000 &lt;- fabricate( state = add_level( N = 50, state_name = state.abb, state_population = sample(8000:10000, N, replace = TRUE), proportion_female = runif(N, min = .45, max = .55), proportion_old = runif(N, min = .25, max = .65)) ) # US population us_population_2000 &lt;- fabricate( data = us_state_demographics_2000, individuals = add_level( N = state_population, female = draw_binary(N = N, prob = proportion_female), old = draw_binary(N = N, prob = proportion_old) ) ) us_population_2000 &lt;- us_population_2000 %&gt;% select(-state_population, -proportion_female, -proportion_old) # population weights for MRP mrp_weights &lt;- us_population_2000 %&gt;% select(state, state_name, individuals, female, old) %&gt;% group_by(state, female, old) %&gt;% summarize(n_cell = n()) %&gt;% group_by(state) %&gt;% mutate(proportion_cell = n_cell/sum(n_cell)) %&gt;% select(-n_cell) us_population_2000 &lt;- us_population_2000 %&gt;% left_join(mrp_weights) # Lax and Philips APSR 2009 # Policies are coded dichotomously, 1 for the progay policy and 0 otherwise: Adoption (9 states allow second-parent adoption in all jurisdictions) design &lt;- declare_population(data = us_population_2000) + declare_potential_outcomes( policy_support = draw_binary(latent = 1 + .2 * female - .5 * old, link = &quot;probit&quot;)) + declare_estimand(handler = function(data) { data %&gt;% group_by(state) %&gt;% summarize(estimand = mean(policy_support)) %&gt;% ungroup %&gt;% transmute(estimand_label = paste0(&quot;state&quot;, state, sep = &quot;&quot;), estimand)}) + declare_sampling(n = 500) + declare_estimator(policy_support ~ 1, model = lm_robust) # declare_estimator(handler = mrp) # need to figure out how to write the mrp function # 1. write my own stan model # 2. compile it (using model()) # 3. draw samples from it # 4. get out samples using tidybayes and post-stratify using dplyr # mrp &lt;- function(data){ # brm_fit &lt;- brm( # policy_support ~ (1 | female) + (1 | old) + (1 | state), # data = data, # family = bernoulli(), # prior = c( # set_prior(&quot;normal(0, 0.2)&quot;, class = &#39;sd&#39;, group = &quot;old&quot;), # set_prior(&quot;normal(0, 0.2)&quot;, class = &#39;sd&#39;, group = &quot;female&quot;), # set_prior(&quot;normal(0, 0.2)&quot;, class = &#39;sd&#39;, group = &quot;state&quot;) # ) # ) # # brm_fit %&gt;% # spread_draws(~state) %&gt;% # summarize_at( # vars(female, old), # funs(mean, sd, low, high, equivprop) # ) # # # %&gt;% # # gather(coefficient, value) %&gt;% # # rename(Stat = coefficient) %&gt;% # # separate(Stat, c(&quot;coefficient&quot;, &quot;Stat&quot;), &quot;_&quot;) %&gt;% # # spread(Stat, value) # # # } # # # library(rstan) # # dat &lt;- draw_data(design) # # draw_estimates(design) This chunk is set to echo = TRUE and eval = do_diagnosis simulations_pilot &lt;- simulate_design(design, sims = sims) Right after you do simulations, you want to save the simulations rds. Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. kable(head(simulations_pilot)) design_label sim_ID estimator_label term estimate std.error statistic p.value conf.low conf.high df outcome design 1 estimator Z 0.9624445 0.1705951 5.641688 0.0000002 0.6234399 1.301449 88.32465 Y design 2 estimator Z 0.9532166 0.2124610 4.486548 0.0000200 0.5315217 1.374911 96.66334 Y design 3 estimator Z 0.9674071 0.2137695 4.525468 0.0000170 0.5431863 1.391628 97.96114 Y design 4 estimator Z 1.0545908 0.1942247 5.429747 0.0000004 0.6690878 1.440094 96.58766 Y design 5 estimator Z 1.0708566 0.1897958 5.642153 0.0000002 0.6942127 1.447501 97.99051 Y design 6 estimator Z 0.7197320 0.1964379 3.663916 0.0004076 0.3297870 1.109677 95.63827 Y References "],
["inference-about-unobserved-variables.html", "12.3 Inference about unobserved variables", " 12.3 Inference about unobserved variables "],
["structural-estimation.html", "12.4 Structural estimation", " 12.4 Structural estimation "],
["experimental-designs-for-descriptive-inference.html", "Chapter 13 Experimental Designs for Descriptive Inference", " Chapter 13 Experimental Designs for Descriptive Inference "],
["audit-experiments.html", "13.1 Audit experiments", " 13.1 Audit experiments A basic requirement of a good research design is that the question it seeks to answer does in fact have an answer, at least under plausible models of the world. In our framework, this means that an inquiry \\(I\\) must have an associated answer \\(a^M\\), which refers to the answer under the model. Interestingly, we sometimes might not be conscious that the questions we ask do not have answers. Fortunately, when we ask a computer to answer such a question, it complains. How could a question not have an answer? Answerless questions can arise when inquiries depend on variables that do not exist or are undefined for some units. In other words, when there is a mismatch between the model and the inquiry, we’re asking a question about something that doesn’t exist. Consider an audit experiment (see Audit Experiment Design) that seeks to assess the effects of an email from a Latino name (versus a White name) on whether and how well election officials respond to requests for information. For example, do they use a positive or negative tone. These questions seem reasonable enough. The problem, however, is that if there are officials who don’t send responses, tone is undefined. More subtly, if there is an official that does send an email but would not have sent it in a different treatment condition, then tone is undefined for one of their potential outcomes. 13.1.1 Design Declaration Model: The model has two outcome variables, \\(R_i\\) and \\(Y_i\\). \\(R_i\\) stands for “response” and is equal to 1 if a response is sent, and 0 otherwise. \\(Y_i\\) is the tone of the response and is normally distributed when it is defined. \\(Z_i\\) is the treatment and equals 1 if the email is sent using a Latino name and 0 otherwise. The table below shows the potential outcomes for four possible types of subjects, depending on the potential outcomes of \\(R_i\\). A types always respond regardless of treatment and D types never respond, regardless of treatment. B types respond if and only if they are treated, whereas C types respond if and only if they are not treated. The table also includes columns for the potential outcomes of \\(Y_i\\), showing which potential outcome subjects would express depending on their type. The key thing to note is that for the B, C, and D types, the effect of treatment on \\(Y_i\\) is undefined because messages never sent have no tone. The last (and very important) feature of our model is that the outcomes \\(Y_i\\) are possibly correlated with subject type. Even though both \\(E[Y_i(1) | \\text{Type} = A]\\) and \\(E[Y_i(1) | \\text{Type} = B]\\) exist, there’s no reason to expect that they are the same. In the design we assume a distribution of types with 40% A, 5% B, 10% C, and 45% D. Causal Types Type \\(R_i(0)\\) \\(R_i(1)\\) \\(Y_i(0)\\) \\(Y_i(1)\\) A 1 1 \\(Y_i(0)\\) \\(Y_i(1)\\) B 0 1 NA \\(Y_i(1)\\) C 1 0 \\(Y_i(0)\\) NA D 0 0 NA NA Inquiry: We have two inquiries. The first is straightforward: \\(E[R_i(1) - R_i(0)]\\) is the Average Treatment Effect on response. The second inquiry is the undefined inquiry that does not have an answer: \\(E[Y_i(1) - Y_i(0)]\\). We will also consider a third inquiry, which is defined: \\(E[Y_i(1) - Y_i(0) | \\mathrm{Type} = A]\\), which is the average effect of treatment on tone among \\(A\\) types. Data strategy: The data strategy will be to use complete random assignment to assign 250 of 500 units to treatment. Answer strategy: We’ll try to answer all three inquiries with the difference-in-means estimator, but as the diagnosis will reveal, this strategy works well for some inquiries but not others. # Model ------------------------------------------------------------------- population &lt;- declare_population( N = 500, type = sample(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;), size = N, replace = TRUE, prob = c(.40, .05, .10, .45))) potential_outcomes &lt;- declare_potential_outcomes( R_Z_0 = type %in% c(&quot;A&quot;, &quot;C&quot;), R_Z_1 = type %in% c(&quot;A&quot;, &quot;B&quot;), Y_Z_0 = ifelse(R_Z_0, rnorm(n = sum(R_Z_0), mean = .1*(type == &quot;A&quot;) - 2*(type == &quot;C&quot;)), NA), Y_Z_1 = ifelse(R_Z_1, rnorm(n = sum(R_Z_1), mean = .2*(type == &quot;A&quot;) + 2*(type == &quot;B&quot;)), NA) ) # Inquiry ----------------------------------------------------------------- estimand_1 &lt;- declare_estimand(ATE_R = mean(R_Z_1 - R_Z_0)) estimand_2 &lt;- declare_estimand(ATE_Y = mean(Y_Z_1 - Y_Z_0)) estimand_3 &lt;- declare_estimand( ATE_Y_for_As = mean(Y_Z_1[type == &quot;A&quot;] - Y_Z_0[type == &quot;A&quot;])) # Data Strategy ----------------------------------------------------------- assignment &lt;- declare_assignment(m = 250) # Answer Strategy --------------------------------------------------------- estimator_1 &lt;- declare_estimator(R ~ Z, estimand = estimand_1, label = &quot;ATE_R&quot;) estimator_2 &lt;- declare_estimator(Y ~ Z, estimand = estimand_2, label = &quot;ATE_Y&quot;) estimator_3 &lt;- declare_estimator(Y ~ Z, estimand = estimand_3, label = &quot;ATE_YA&quot;) # Design ------------------------------------------------------------------ design &lt;- population + potential_outcomes + assignment + estimand_1 + estimand_2 + estimand_3 + declare_reveal(outcome_variables = c(&quot;R&quot;, &quot;Y&quot;)) + estimator_1 + estimator_2 + estimator_3 13.1.2 Takeaways We now diagnose the design: diagnosis &lt;- diagnose_design(design, sims = sims) Design Label Estimand Label Estimator Label Term N Sims Bias RMSE Power Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand design ATE_R ATE_R Z 100 0.00 0.04 0.20 0.98 -0.05 0.04 0.04 0.00 -0.05 (0.00) (0.00) (0.04) (0.01) (0.00) (0.00) (0.00) (0.00) (0.00) design ATE_Y ATE_Y Z 100 NA NA 0.91 NA 0.54 0.19 0.15 NA NA NA NA (0.03) NA (0.02) (0.01) (0.00) NA NA design ATE_Y_for_As ATE_YA Z 100 0.31 0.34 0.91 0.52 0.54 0.19 0.15 0.04 0.22 (0.01) (0.01) (0.03) (0.04) (0.02) (0.01) (0.00) (0.02) (0.01) We learn three things from the design diagnosis. First, as expected, our experiment is unbiased for the average treatment effect on response. Next, we see that our second inquiry, as well as our diagnostics for it, are undefined. The diagnosis tells us that our definition of potential outcomes produces a definition problem for the estimand. Note that the diagnosands that are defined, including power, depend only on the answer strategy and not on the estimand. Finally, our third estimand – the average effects for the \\(A\\) types – is defined but our estimates are biased. The reason for this is that we cannot tell from the data which types are the \\(A\\) types: we are not conditioning on the correct subset. Indeed, we are unable to condition on the correct subset. If a subject responds in the treatment group, we don’t know if she is an \\(A\\) or a \\(B\\) type; in the control group, we can’t tell if a responder is an \\(A\\) or a \\(C\\) type. Our difference-in-means estimator of the ATE on \\(Y\\) among \\(A\\)s will be off whenever \\(A\\)s have different outcomes from \\(B\\)s and \\(C\\)s. In some cases, the problem might be resolved by changing the inquiry. Closely related estimands can often be defined, perhaps by redefining \\(Y\\) (e.g., emails never sent have a tone of zero). Some redefinitions of the problem, as in the one we examine above, require estimating effects for unobserved subgroups which is a difficult challenge. 13.1.3 Applications This kind of problem is surprisingly common. Here are three more distinct instances of the problem: \\(Y\\) is the decision to vote Democrat (\\(Y=1\\)) or Republican (\\(Y=0\\)), \\(R\\) is the decision to turn out to vote and \\(Z\\) is a campaign message. The decision to vote may depend on treatment but if subjects do not vote then \\(Y\\) is undefined. \\(Y\\) is the weight of infants, \\(R\\) is whether a child is born and \\(Z\\) is a maternal health intervention. Fertility may depend on treatment but the weight of unborn (possibly never conceived) babies is not defined. \\(Y\\) is the charity to whom contributions are made during fundraising and \\(R\\) is whether anything is contributed and \\(Z\\) is an encouragement to contribute. The identity of beneficiaries is not defined if there are no contributions. All of these problem exhibit a form of post treatment bias (see section Post treatment bias) but the issue goes beyond picking the right estimator. Our problem here is conceptual: the effect of treatment on the outcome just doesn’t exist for some subjects. 13.1.4 Exercises The amount of bias on the third estimand depends on both the distribution of types and the correlation of types with the potential outcomes of Y. Modify the declaration so that the estimator of the effect on Y is unbiased, changing only the distribution of types. Repeat the exercise, changing only the correlation of type with the potential outcomes of \\(Y\\). Try approaching the problem by redefining the inquiry, seeking to assess the effect of treatment on the share of responses with positive tone. "],
["experiments-for-sensitive-questions.html", "13.2 Experiments for sensitive questions", " 13.2 Experiments for sensitive questions 13.2.1 List experiments Sometimes, subjects might not tell the truth when asked about certain attitudes or behaviors. Responses may be affected by sensitivity bias, or the tendancy of survey subjects to dissemble for fear of negative repercussions if some reference group learns their true response (Blair, Coppock, and Moor 2018). In such cases, standard survey estimates based on direct questions will be biased. One class of solutions to this problem is to obscure individual responses, providing protection from social or legal pressures. When we obscure responses systematically through an experiment, we can often still identify average quantities of interest. One such design is the list experiment (introduced by Miller (1984)), which asks respondents for the count of the number of `yes’ responses to a series of questions including the sensitive item, rather than for a yes or no answer on the sensitive item itself. List experiments give subjects cover by aggregating their answer to the sensitive item with responses to other questions. During the 2016 Presidential Election in the U.S., some observers were concerned that pre-election estimates of support for Donald Trump might have been downwardly biased by “Shy Trump Supporters” – survey respondents who supported Trump in their hearts, but were embarrased to admit it to pollsters. To assess this possibility, Coppock (2017) obtained estimates of Trump support that were free of social desirability bias using a list experiment. Subjects in the control and treatment groups were asked: “Here is a list of [three/four] things that some people would do and some people would not. Please tell me HOW MANY of them you would do. We do not want to know which ones of these you would do, just how many. Here are the [three/four] things:” Control Treatment If it were up for a vote, I would vote to raise the minimum wage to 15 dollars an hour If it were up for a vote, I would vote to raise the minimum wage to 15 dollars an hour If it were up for a vote, I would vote to repeal the Affordable Care Act, also known as Obamacare If it were up for a vote, I would vote to repeal the Affordable Care Act, also known as Obamacare If it were up for a vote, I would vote to ban assault weapons If it were up for a vote, I would vote to ban assault weapons If the 2016 presidential election were being held today and the candidates were Hillary Clinton (Democrat) and Donald Trump (Republican), I would vote for Donald Trump. The treatment group averaged 1.843 items while the control group averaged 1.548 items, for a difference-in-means estimate of support for Donald Trump of 29.6% (note that this estimate is representative of US adults and not of US adults who would actually vote). The trouble with this estimate is that, while it’s plausibly free from social desirability bias, it’s also much higher variance. The 95% confidence interval for the list experiment estimate is nearly 14 percentage points wide, whereas the the 95% confidence interval for the (possibly biased!) direct question asked of the same sample is closer to 4 percentage points. The choice between list experiments and direct question is therefore a bias-variance tradeoff. List experiments may have less bias, but they are higher variance. Direct questions may be biased, but they have less variance. 13.2.1.1 Declaration Model: Our model includes subjects’ true support for Donald Trump and whether or not they are “shy”. These two variables combine to determine how subjects will respond when asked directly about Trump support. The potential outcomes model combines three types of information to determine how subjects will respond to the list experiment: their responses to the three nonsensitive control items, their true support for Trump, and whether they are assigned to see the treatment or the control list. Notice that our definition of the potential outcomes embeds the “No Liars” and “No Design Effects” assumptions required for the list experiment design (see Blair and Imai 2012 for more on these assumptions). We also have a global parameter that reflects our expectations about the proportion of Trump supporters who are shy. It’s set at 6%, which is large enough to make a difference for polling, but not so large as to be implausible. Inquiry: Our estimand is the proportion of voters who actually plan to vote for Trump. Data strategy: First we sample 500 respondents from the U.S. population at random, then we randomly assign 250 of the 500 to treatment and the remainder to control. In the survey, we ask subjects both the direct question and the list experiment question. Answer strategy: We estimate the proportion of truthful Trump voters in two ways. First, we take the mean of answers to the direct question. Second, we take the difference in means in the responses to the list experiment question. # Model ------------------------------------------------------------------- proportion_shy &lt;- .06 list_design &lt;- # Model declare_population( N = 5000, # true trump vote (unobservable) truthful_trump_vote = draw_binary(.45, N), # shy voter (unobservable) shy = draw_binary(proportion_shy, N), # Direct question response (1 if Trump supporter and not shy, 0 otherwise) Y_direct = as.numeric(truthful_trump_vote == 1 &amp; shy == 0), # Nonsensitive list experiment items raise_minimum_wage = draw_binary(.8, N), repeal_obamacare = draw_binary(.6, N), ban_assault_weapons = draw_binary(.5, N) ) + declare_potential_outcomes( Y_list_Z_0 = raise_minimum_wage + repeal_obamacare + ban_assault_weapons, Y_list_Z_1 = Y_list_Z_0 + truthful_trump_vote ) + # Inquiry declare_estimand(proportion_truthful_trump_vote = mean(truthful_trump_vote)) + # Data Strategy declare_sampling(n = 500) + declare_assignment(prob = .5) + declare_reveal(Y_list) + # Answer Strategy declare_estimator( Y_direct ~ 1, model = lm_robust, term = &quot;(Intercept)&quot;, estimand = &quot;proportion_truthful_trump_vote&quot;, label = &quot;direct&quot; ) + declare_estimator( Y_list ~ Z, model = difference_in_means, estimand = &quot;proportion_truthful_trump_vote&quot;, label = &quot;list&quot;) simulations_list &lt;- simulate_design(list_design, sims = sims) The plot shows the sampling distribution of the direct and list experimetn estimators. The sampling distribution of the direct question is tight but biased; the list experiment (if the requisite assumptions hold) is unbiased, but higher variance. The choice between these two estimators of the prevalence rate depends on which – bias or variance – is more important in a particular setting. See Blair, Coppock, and Moor (2018) for an extended discussion of how the choice of research design depends deeply on the purpose of the project. 13.2.2 Randomized response technique 13.2.2.1 Declaration library(rr) rr_forced_known_tidy &lt;- function(data) { fit &lt;- rrreg(Y_forced_known ~ 1, data = data, p = 2/3, p0 = 1/6, p1 = 1/6, design = &quot;forced-known&quot;) pred &lt;- as.data.frame(predict(fit, avg = TRUE, quasi.bayes = TRUE)) names(pred) &lt;- c(&quot;estimate&quot;, &quot;std.error&quot;, &quot;conf.low&quot;, &quot;conf.high&quot;) pred$p.value &lt;- with(pred, 2 * pnorm(-abs(estimate / std.error))) pred } rr_mirrored_tidy &lt;- function(data) { fit &lt;- rrreg(Y_mirrored ~ 1, data = data, p = 2/3, design = &quot;mirrored&quot;) pred &lt;- as.data.frame(predict(fit, avg = TRUE, quasi.bayes = TRUE)) names(pred) &lt;- c(&quot;estimate&quot;, &quot;std.error&quot;, &quot;conf.low&quot;, &quot;conf.high&quot;) pred$p.value &lt;- with(pred, 2 * pnorm(-abs(estimate / std.error))) pred } proportion_shy &lt;- .06 rr_design &lt;- declare_population( N = 100, # true trump vote (unobservable) truthful_trump_vote = draw_binary(.45, N), # shy voter (unobservable) shy = draw_binary(proportion_shy, N), # Direct question response (1 if Trump supporter and not shy, 0 otherwise) Y_direct = as.numeric(truthful_trump_vote == 1 &amp; shy == 0)) + declare_estimand(sensitive_item_proportion = mean(truthful_trump_vote)) + declare_potential_outcomes(Y_forced_known ~ (dice == 1) * 0 + (dice %in% 2:5) * truthful_trump_vote + (dice == 6) * 1, conditions = 1:6, assignment_variable = &quot;dice&quot;) + declare_potential_outcomes(Y_mirrored ~ (coin == &quot;heads&quot;) * truthful_trump_vote + (coin == &quot;tails&quot;) * (1 - truthful_trump_vote), conditions = c(&quot;heads&quot;, &quot;tails&quot;), assignment_variable = &quot;coin&quot;) + declare_assignment(prob_each = rep(1/6, 6), conditions = 1:6, assignment_variable = &quot;dice&quot;) + declare_assignment(prob_each = c(2/3, 1/3), conditions = c(&quot;heads&quot;, &quot;tails&quot;), assignment_variable = &quot;coin&quot;) + declare_reveal(Y_forced_known, dice) + declare_reveal(Y_mirrored, coin) + declare_estimator(handler = tidy_estimator(rr_forced_known_tidy), label = &quot;forced_known&quot;, estimand = &quot;sensitive_item_proportion&quot;) + declare_estimator(handler = tidy_estimator(rr_mirrored_tidy), label = &quot;mirrored&quot;, estimand = &quot;sensitive_item_proportion&quot;) + declare_estimator(Y_direct ~ 1, model = lm_robust, term = &quot;(Intercept)&quot;, label = &quot;direct&quot;, estimand = &quot;sensitive_item_proportion&quot;) rr_design &lt;- set_diagnosands(rr_design, diagnosands = declare_diagnosands(select = c(mean_estimate, bias, rmse, power))) 13.2.3 References References "],
["conjoint-experimetns.html", "13.3 Conjoint experimetns", " 13.3 Conjoint experimetns You can use the global bib file via rmarkdown cites like this: Imai, King, and Stuart (2008) design &lt;- declare_population(N = 100, u = rnorm(N)) + declare_potential_outcomes(Y ~ Z + u) + declare_assignment(prob = 0.5) + declare_reveal(Y, Z) + declare_estimator(Y ~ Z, model = difference_in_means) This chunk is set to echo = TRUE and eval = do_diagnosis simulations_pilot &lt;- simulate_design(design, sims = sims) Right after you do simulations, you want to save the simulations rds. Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. kable(head(simulations_pilot)) design_label sim_ID estimator_label term estimate std.error statistic p.value conf.low conf.high df outcome design 1 estimator Z 0.9624445 0.1705951 5.641688 0.0000002 0.6234399 1.301449 88.32465 Y design 2 estimator Z 0.9532166 0.2124610 4.486548 0.0000200 0.5315217 1.374911 96.66334 Y design 3 estimator Z 0.9674071 0.2137695 4.525468 0.0000170 0.5431863 1.391628 97.96114 Y design 4 estimator Z 1.0545908 0.1942247 5.429747 0.0000004 0.6690878 1.440094 96.58766 Y design 5 estimator Z 1.0708566 0.1897958 5.642153 0.0000002 0.6942127 1.447501 97.99051 Y design 6 estimator Z 0.7197320 0.1964379 3.663916 0.0004076 0.3297870 1.109677 95.63827 Y References "],
["behavioral-games.html", "13.4 Behavioral games", " 13.4 Behavioral games "],
["experimental-designs-for-causal-inference.html", "Chapter 14 Experimental Designs for Causal Inference", " Chapter 14 Experimental Designs for Causal Inference "],
["two-arm-trials.html", "14.1 Two arm trials", " 14.1 Two arm trials You can use the global bib file via rmarkdown cites like this: Imai, King, and Stuart (2008) design &lt;- declare_population(N = 100, u = rnorm(N)) + declare_potential_outcomes(Y ~ Z + u) + declare_assignment(prob = 0.5) + declare_reveal(Y, Z) + declare_estimator(Y ~ Z, model = difference_in_means) This chunk is set to echo = TRUE and eval = do_diagnosis simulations_pilot &lt;- simulate_design(design, sims = sims) Right after you do simulations, you want to save the simulations rds. Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. kable(head(simulations_pilot)) design_label sim_ID estimator_label term estimate std.error statistic p.value conf.low conf.high df outcome design 1 estimator Z 0.9624445 0.1705951 5.641688 0.0000002 0.6234399 1.301449 88.32465 Y design 2 estimator Z 0.9532166 0.2124610 4.486548 0.0000200 0.5315217 1.374911 96.66334 Y design 3 estimator Z 0.9674071 0.2137695 4.525468 0.0000170 0.5431863 1.391628 97.96114 Y design 4 estimator Z 1.0545908 0.1942247 5.429747 0.0000004 0.6690878 1.440094 96.58766 Y design 5 estimator Z 1.0708566 0.1897958 5.642153 0.0000002 0.6942127 1.447501 97.99051 Y design 6 estimator Z 0.7197320 0.1964379 3.663916 0.0004076 0.3297870 1.109677 95.63827 Y References "],
["two-arm-trials-and-designs-with-blocking-and-clustering.html", "14.2 Two-arm trials and designs with blocking and clustering", " 14.2 Two-arm trials and designs with blocking and clustering "],
["multiarm-designs.html", "14.3 Multiarm Designs", " 14.3 Multiarm Designs You can use the global bib file via rmarkdown cites like this: Imai, King, and Stuart (2008) design &lt;- declare_population(N = 100, u = rnorm(N)) + declare_potential_outcomes(Y ~ Z + u) + declare_assignment(prob = 0.5) + declare_reveal(Y, Z) + declare_estimator(Y ~ Z, model = difference_in_means) This chunk is set to echo = TRUE and eval = do_diagnosis simulations_pilot &lt;- simulate_design(design, sims = sims) Right after you do simulations, you want to save the simulations rds. Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. kable(head(simulations_pilot)) design_label sim_ID estimator_label term estimate std.error statistic p.value conf.low conf.high df outcome design 1 estimator Z 0.9624445 0.1705951 5.641688 0.0000002 0.6234399 1.301449 88.32465 Y design 2 estimator Z 0.9532166 0.2124610 4.486548 0.0000200 0.5315217 1.374911 96.66334 Y design 3 estimator Z 0.9674071 0.2137695 4.525468 0.0000170 0.5431863 1.391628 97.96114 Y design 4 estimator Z 1.0545908 0.1942247 5.429747 0.0000004 0.6690878 1.440094 96.58766 Y design 5 estimator Z 1.0708566 0.1897958 5.642153 0.0000002 0.6942127 1.447501 97.99051 Y design 6 estimator Z 0.7197320 0.1964379 3.663916 0.0004076 0.3297870 1.109677 95.63827 Y References "],
["encouragement-designs.html", "14.4 Encouragement designs", " 14.4 Encouragement designs Idea for this one would be to show how violations of no defiers and excludability lead to bias. types &lt;- c(&quot;Always-Taker&quot;, &quot;Never-Taker&quot;, &quot;Complier&quot;, &quot;Defier&quot;) direct_effect_of_encouragement &lt;- 0.0 proportion_defiers &lt;- 0.0 design &lt;- declare_population( N = 500, type = sample( types, N, replace = TRUE, prob = c(0.1, 0.1, 0.8 - proportion_defiers, proportion_defiers) ), noise = rnorm(N) ) + declare_potential_outcomes( D ~ case_when( Z == 0 &amp; type %in% c(&quot;Never-Taker&quot;, &quot;Complier&quot;) ~ 0, Z == 1 &amp; type %in% c(&quot;Never-Taker&quot;, &quot;Defier&quot;) ~ 0, Z == 0 &amp; type %in% c(&quot;Always-Taker&quot;, &quot;Defier&quot;) ~ 1, Z == 1 &amp; type %in% c(&quot;Always-Taker&quot;, &quot;Complier&quot;) ~ 1 ) ) + declare_potential_outcomes( Y ~ 0.5 * (type == &quot;Complier&quot;) * D + 0.25 * (type == &quot;Always-Taker&quot;) * D + 0.75 * (type == &quot;Defier&quot;) * D + direct_effect_of_encouragement * Z + noise, assignment_variables = c(&quot;D&quot;, &quot;Z&quot;) ) + declare_estimand(CACE = mean((Y_D_1_Z_1 + Y_D_1_Z_0) / 2 - (Y_D_0_Z_1 + Y_D_0_Z_0) / 2), subset = type == &quot;Complier&quot;) + declare_assignment(prob = 0.5) + declare_reveal(D, assignment_variable = &quot;Z&quot;) + declare_reveal(Y, assignment_variables = c(&quot;D&quot;, &quot;Z&quot;)) + declare_estimator(Y ~ D | Z, model = iv_robust, estimand = &quot;CACE&quot;) designs &lt;- redesign( design, proportion_defiers = seq(0, 0.3, length.out = 5), direct_effect_of_encouragement = seq(0, 0.3, length.out = 5) ) simulations &lt;- simulate_design(designs, sims = sims) gg_df &lt;- simulations %&gt;% group_by(proportion_defiers, direct_effect_of_encouragement) %&gt;% summarize(bias = mean(estimate - estimand)) ggplot(gg_df, aes( proportion_defiers, bias, group = direct_effect_of_encouragement, color = direct_effect_of_encouragement )) + geom_point() + geom_line() 14.4.1 References "],
["stepped-wedge-designs.html", "14.5 Stepped wedge designs", " 14.5 Stepped wedge designs You can use the global bib file via rmarkdown cites like this: Imai, King, and Stuart (2008) design &lt;- declare_population(N = 100, u = rnorm(N)) + declare_potential_outcomes(Y ~ Z + u) + declare_assignment(prob = 0.5) + declare_reveal(Y, Z) + declare_estimator(Y ~ Z, model = difference_in_means) This chunk is set to echo = TRUE and eval = do_diagnosis simulations_pilot &lt;- simulate_design(design, sims = sims) Right after you do simulations, you want to save the simulations rds. Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. kable(head(simulations_pilot)) design_label sim_ID estimator_label term estimate std.error statistic p.value conf.low conf.high df outcome design 1 estimator Z 0.9624445 0.1705951 5.641688 0.0000002 0.6234399 1.301449 88.32465 Y design 2 estimator Z 0.9532166 0.2124610 4.486548 0.0000200 0.5315217 1.374911 96.66334 Y design 3 estimator Z 0.9674071 0.2137695 4.525468 0.0000170 0.5431863 1.391628 97.96114 Y design 4 estimator Z 1.0545908 0.1942247 5.429747 0.0000004 0.6690878 1.440094 96.58766 Y design 5 estimator Z 1.0708566 0.1897958 5.642153 0.0000002 0.6942127 1.447501 97.99051 Y design 6 estimator Z 0.7197320 0.1964379 3.663916 0.0004076 0.3297870 1.109677 95.63827 Y References "],
["partial-population-design.html", "14.6 Partial Population Design", " 14.6 Partial Population Design You can use the global bib file via rmarkdown cites like this: Imai, King, and Stuart (2008) design &lt;- declare_population(N = 100, u = rnorm(N)) + declare_potential_outcomes(Y ~ Z + u) + declare_assignment(prob = 0.5) + declare_reveal(Y, Z) + declare_estimator(Y ~ Z, model = difference_in_means) This chunk is set to echo = TRUE and eval = do_diagnosis simulations_pilot &lt;- simulate_design(design, sims = sims) Right after you do simulations, you want to save the simulations rds. Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. kable(head(simulations_pilot)) design_label sim_ID estimator_label term estimate std.error statistic p.value conf.low conf.high df outcome design 1 estimator Z 0.9624445 0.1705951 5.641688 0.0000002 0.6234399 1.301449 88.32465 Y design 2 estimator Z 0.9532166 0.2124610 4.486548 0.0000200 0.5315217 1.374911 96.66334 Y design 3 estimator Z 0.9674071 0.2137695 4.525468 0.0000170 0.5431863 1.391628 97.96114 Y design 4 estimator Z 1.0545908 0.1942247 5.429747 0.0000004 0.6690878 1.440094 96.58766 Y design 5 estimator Z 1.0708566 0.1897958 5.642153 0.0000002 0.6942127 1.447501 97.99051 Y design 6 estimator Z 0.7197320 0.1964379 3.663916 0.0004076 0.3297870 1.109677 95.63827 Y References "],
["partial-population-design-for-spillover-analysis.html", "14.7 Partial population design for spillover analysis", " 14.7 Partial population design for spillover analysis "],
["observational-designs-for-causal-inference.html", "Chapter 15 Observational Designs for causal inference", " Chapter 15 Observational Designs for causal inference "],
["selection-on-observables.html", "15.1 Selection on observables", " 15.1 Selection on observables (matching and regression etc.) You can use the global bib file via rmarkdown cites like this: Imai, King, and Stuart (2008) design &lt;- declare_population(N = 100, u = rnorm(N)) + declare_potential_outcomes(Y ~ Z + u) + declare_assignment(prob = 0.5) + declare_reveal(Y, Z) + declare_estimator(Y ~ Z, model = difference_in_means) This chunk is set to echo = TRUE and eval = do_diagnosis simulations_pilot &lt;- simulate_design(design, sims = sims) Right after you do simulations, you want to save the simulations rds. Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. kable(head(simulations_pilot)) design_label sim_ID estimator_label term estimate std.error statistic p.value conf.low conf.high df outcome design 1 estimator Z 0.9624445 0.1705951 5.641688 0.0000002 0.6234399 1.301449 88.32465 Y design 2 estimator Z 0.9532166 0.2124610 4.486548 0.0000200 0.5315217 1.374911 96.66334 Y design 3 estimator Z 0.9674071 0.2137695 4.525468 0.0000170 0.5431863 1.391628 97.96114 Y design 4 estimator Z 1.0545908 0.1942247 5.429747 0.0000004 0.6690878 1.440094 96.58766 Y design 5 estimator Z 1.0708566 0.1897958 5.642153 0.0000002 0.6942127 1.447501 97.99051 Y design 6 estimator Z 0.7197320 0.1964379 3.663916 0.0004076 0.3297870 1.109677 95.63827 Y References "],
["instrumental-variables.html", "15.2 Instrumental variables", " 15.2 Instrumental variables This one with continuous instruments You can use the global bib file via rmarkdown cites like this: Imai, King, and Stuart (2008) design &lt;- declare_population(N = 100, u = rnorm(N)) + declare_potential_outcomes(Y ~ Z + u) + declare_assignment(prob = 0.5) + declare_reveal(Y, Z) + declare_estimator(Y ~ Z, model = difference_in_means) This chunk is set to echo = TRUE and eval = do_diagnosis simulations_pilot &lt;- simulate_design(design, sims = sims) Right after you do simulations, you want to save the simulations rds. Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. kable(head(simulations_pilot)) design_label sim_ID estimator_label term estimate std.error statistic p.value conf.low conf.high df outcome design 1 estimator Z 0.9624445 0.1705951 5.641688 0.0000002 0.6234399 1.301449 88.32465 Y design 2 estimator Z 0.9532166 0.2124610 4.486548 0.0000200 0.5315217 1.374911 96.66334 Y design 3 estimator Z 0.9674071 0.2137695 4.525468 0.0000170 0.5431863 1.391628 97.96114 Y design 4 estimator Z 1.0545908 0.1942247 5.429747 0.0000004 0.6690878 1.440094 96.58766 Y design 5 estimator Z 1.0708566 0.1897958 5.642153 0.0000002 0.6942127 1.447501 97.99051 Y design 6 estimator Z 0.7197320 0.1964379 3.663916 0.0004076 0.3297870 1.109677 95.63827 Y References "],
["difference-in-differences.html", "15.3 Difference in differences", " 15.3 Difference in differences # simulations_pilot &lt;- simulate_design(design, sims = sims) "],
["regression-discontinuity.html", "15.4 Regression Discontinuity", " 15.4 Regression Discontinuity Regression discontinuity designs exploit substantive knowledge that treatment is assigned in a particular way: everyone above a threshold is assigned to treatment and everyone below it is not. Even though researchers do not control the assignment, substantive knowledge about the threshold serves as a basis for a strong identification claim. Thistlewhite and Campbell introduced the regression discontinuity design in the 1960s to study the impact of scholarships on academic success. Their insight was that students with a test score just above a scholarship cutoff were plausibly comparable to students whose scores were just below the cutoff, so any differences in future academic success could be attributed to the scholarship itself. Regression discontinuity designs identify a local average treatment effect: the average effect of treatment exactly at the cutoff. The main trouble with the design is that there is vanishingly little data exactly at the cutoff, so any answer strategy needs to use data that is some distance away from the cutoff. The further away from the cutoff we move, the larger the threat of bias. We’ll consider an application of the regression discontinuity design that examines party incumbency advantage – the effect of a party winning an election on its vote margin in the next election. 15.4.1 Design Declaration Model: Regression discontinuity designs have four components: A running variable, a cutoff, a treatment variable, and an outcome. The cutoff determines which units are treated depending on the value of the running variable. In our example, the running variable \\(X\\) is the Democratic party’s margin of victory at time \\(t-1\\); and the treatment, \\(Z\\), is whether the Democratic party won the election in time \\(t-1\\). The outcome, \\(Y\\), is the Democratic vote margin at time \\(t\\). We’ll consider a population of 1,000 of these pairs of elections. A major assumption required for regression discontinuity is that the conditional expectation functions for both treatment and control potential outcomes are continuous at the cutoff.6 To satisfy this assumption, we specify two smooth conditional expectation functions, one for each potential outcome. The figure plots \\(Y\\) (the Democratic vote margin at time \\(t\\)) against \\(X\\) (the margin at time \\(t-1\\)). We’ve also plotted the true conditional expectation functions for the treated and control potential outcomes. The solid lines correspond to the observed data and the dashed lines correspond to the unobserved data. cutoff &lt;- .5 control &lt;- function(X) { as.vector(poly(X, 4, raw = TRUE) %*% c(.7, -.8, .5, 1))} treatment &lt;- function(X) { as.vector(poly(X, 4, raw = TRUE) %*% c(0, -1.5, .5, .8)) + .15} rd_design &lt;- # Model ------------------------------------------------------------------- declare_population( N = 1000, X = runif(N, 0, 1) - cutoff, noise = rnorm(N, 0, .1), Z = 1 * (X &gt; 0) ) + declare_potential_outcomes(Y ~ Z * treatment(X) + (1 - Z) * control(X) + noise) + # Inquiry ----------------------------------------------------------------- declare_estimand(LATE = treatment(0) - control(0)) + # Data Strategy ----------------------------------------------------------------- declare_reveal(Y, Z) + # Answer Strategy --------------------------------------------------------- declare_estimator(formula = Y ~ poly(X, 4) * Z, model = lm_robust, estimand = &quot;LATE&quot;) Inquiry: Our estimand is the effect of a Democratic win in an election on the Democratic vote margin of the next election, when the Democratic vote margin of the first election is zero. Formally, it is the difference in the conditional expectation functions of the control and treatment potential outcomes when the running variable is exactly zero. The black vertical line in the plot shows this difference. Data strategy: We collect data on the Democratic vote share at time \\(t-1\\) and time \\(t\\) for all 1,000 pairs of elections. There is no sampling or random assignment. Answer strategy: We will approximate the treated and untreated conditional expectation functions to the left and right of the cutoff using a flexible regression specification estimated via OLS. In particular, we fit each regression using a fourth-order polynomial. Much of the literature on regression discontinuity designs focuses on the tradeoffs among answer strategies, with many analysts recommending against higher-order polynomial regression specifications. We use one here to highlight how well such an answer strategy does when it matches the functional form in the model. We discuss alternative estimators in the exercises. rd_diagnosis &lt;- diagnose_design(rd_design, sims = sims, bootstrap_sims = b_sims) Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. summary(rd_diagnosis) ## ## Research design diagnosis based on 100 simulations. Diagnosand estimates with bootstrapped standard errors in parentheses (20 replicates). ## ## Design Label Estimand Label Estimator Label Term N Sims Bias ## rd_design LATE estimator poly(X, 4)1 100 2.08 ## (2.36) ## RMSE Power Coverage Mean Estimate SD Estimate Mean Se Type S Rate ## 24.82 0.06 0.95 2.23 24.85 26.52 0.33 ## (1.50) (0.02) (0.02) (2.36) (1.54) (0.28) (0.21) ## Mean Estimand ## 0.15 ## (0.00) 15.4.2 Takeaways We highlight three takeaways. First, the power of this design is very low: with 1,000 units we do not achieve even 10% statistical power. However, our estimates of the uncertainty are not too wide: the coverage probability indicates that our confidence intervals indeed contain the estimand 95% of the time as they should. Our answer strategy is highly uncertain because the fourth-order polynomial specification in regression model gives weights to the data that greatly increase the variance of the estimator (Gelman and Imbens (2017)). In the exercises we explore alternative answer strategies that perform better. Second, the design is biased because polynomial approximations of the average effect at exactly the point of the threshold will be inaccurate in small samples (Sekhon and Titiunik (2017)), especially as units farther away from the cutoff are incorporated into the answer strategy. We know that the estimated bias is not due to simulation error by examining the bootstrapped standard error of the bias estimates. Finally, from the figure, we can see how poorly the average effect at the threshold approximates the average effect for all units. The average treatment effect among the treated (to the right of the threshold in the figure) is negative, whereas at the threshold it is positive. This clarifies that the estimand of the regression discontinuity design, the difference at the cutoff, is only relevant for a small – and possibly empty – set of units very close to the cutoff. 15.4.3 Further Reading Since its rediscovery by social scientists in the late 1990s, the regression discontinuity design has been widely used to study diverse causal effects such as: prison on recidivism (Mitchell et al. (2017)); China’s one child policy on human capital (Qin, Zhuang, and Yang (2017)); eligibility for World Bank loans on political liberalization (Carnegie and Samii (2017)); and anti-discrimination laws on minority employment (Hahn, Todd, and Van der Klaauw (1999)). We’ve discussed a “sharp” regression discontinuity design in which all units above the threshold were treated and all units below were untreated. In fuzzy regression discontinuity designs, some units above the cutoff remain untreated or some units below take treatment. This setting is analogous to experiments that experience noncompliance and may require instrumental variables approaches to the answer strategy (see Compliance is a Potential Outcome). Geographic regression discontinuity designs use distance to a border as the running variable: units on one side of the border are treated and units on the other are untreated. Keele and Titiunik (2016) use such a design to study whether voters are more likely to turn out when they have the opportunity to vote directly on legislation on so-called ballot initiatives. A complication of this design is how to measure distance to the border in two dimensions. 15.4.4 Exercises Gelman and Imbens (2017) point out that higher order polynomial regression specifications lead to extreme regression weights. One approach to obtaining better estimates is to select a bandwidth, \\(h\\), around the cutoff, and run a linear regression. Declare a sampling procedure that subsets the data to a bandwidth around the threshold, as well as a first order linear regression specification, and analyze how the power, bias, RMSE, and coverage of the design vary as a function of the bandwidth. The rdrobust estimator in the rdrobust package implements a local polynomial estimator that automatically selects a bandwidth for the RD analysis and bias-corrected confidence intervals. Declare another estimator using the rdrobust function and add it to the design. How does the coverage and bias of this estimator compare to the regression approaches declared above? Reduce the number of polynomial terms of the the treatment() and control() functions and assess how the bias of the design changes as the potential outcomes become increasingly linear as a function of the running variable. Redefine the population function so that units with higher potential outcome are more likely to locate just above the cutoff than below it. Assess whether and how this affects the bias of the design. References "],
["process-tracing.html", "15.5 Process tracing", " 15.5 Process tracing "],
["synthetic-controls.html", "15.6 Synthetic controls", " 15.6 Synthetic controls Modeled after the example here: https://www.mitpressjournals.org/doi/abs/10.1162/REST_a_00429?casa_token=o-zWqCima50AAAAA:yiEERZfdhAUoHV0-xBYNjgdljvgfRXrriR8foG7X8nHSUAMFrLcw2vWY8e9pHzmRT24MMAIv9hvKpQ Did the 2007 Legal Arizona Workers Act Reduce the State’s Unauthorized Immigrant Population? Sarah Bohn, Magnus Lofstrom, and Steven Raphael The Review of Economics and Statistics 2014 96:2, 258-269 set up states with time trends and levels that are correlated with a type try three estimators: (1) difference-in-difference; (2) difference in treated period; and (3) difference in treated period weighted by Synth weights # tidy function that takes data and just adds the synthetic control weights to it synth_weights_tidy &lt;- function(data) { dataprep.out &lt;- dataprep( foo = data, predictors = &quot;prop_non_hispanic_below_hs&quot;, predictors.op = &quot;mean&quot;, time.predictors.prior = 1998:2006, dependent = &quot;prop_non_hispanic_below_hs&quot;, unit.variable = &quot;state_number&quot;, time.variable = &quot;year&quot;, treatment.identifier = 4, controls.identifier = c(1:3, 5:50), # states without Arizona time.optimize.ssr = 1998:2006, time.plot = 1998:2009) capture.output(fit &lt;- synth(data.prep.obj = dataprep.out)) tab &lt;- synth.tab(dataprep.res = dataprep.out, synth.res = fit) data %&gt;% left_join(tab$tab.w %&gt;% mutate(synth_weights = w.weights) %&gt;% dplyr::select(synth_weights, unit.numbers), by = c(&quot;state_number&quot; = &quot;unit.numbers&quot;)) %&gt;% mutate(synth_weights = replace(synth_weights, state_number == 4, 1)) } augsynth_tidy &lt;- function(data) { fit &lt;- augsynth(prop_non_hispanic_below_hs ~ legal_worker_act, state, year, t_int = 2007, data = data) res &lt;- summary(fit)$att %&gt;% filter(Time == 2007) %&gt;% select(Estimate, Std.Error) names(res) &lt;- c(&quot;estimate&quot;, &quot;std.error&quot;) res$conf.low &lt;- res$estimate - 1.96 * res$std.error res$conf.high &lt;- res$estimate + 1.96 * res$std.error res } # note need to clean up the range of the data, currently over 1 design &lt;- declare_population( states = add_level( N = 50, state = state.abb, state_number = as.numeric(as.factor(state)), state_shock = runif(N, -.15, .15), border_state = state %in% c(&quot;AZ&quot;, &quot;CA&quot;, &quot;NM&quot;, &quot;TX&quot;), state_shock = ifelse(border_state, .2, state_shock) ), years = add_level( N = 12, nest = FALSE, year = 1998:2009, post_treatment_period = year &gt;= 2007, year_shock = runif(N, -.025, .025), year_trend = year - 1998 ), obs = cross_levels( by = join(states, years), legal_worker_act = 1*(post_treatment_period == TRUE &amp; state == &quot;AZ&quot;), state_year_shock = runif(N, -.025, .025), prop_non_hispanic_below_hs_baseline = 0.4 + state_shock + year_shock + (.01 + .05 * border_state) * year_trend + state_year_shock ) ) + declare_potential_outcomes( prop_non_hispanic_below_hs ~ prop_non_hispanic_below_hs_baseline + 0.25 * legal_worker_act, assignment_variable = legal_worker_act) + declare_estimand( ATE_AZ = mean(prop_non_hispanic_below_hs_legal_worker_act_1 - prop_non_hispanic_below_hs_legal_worker_act_0), subset = legal_worker_act == TRUE) + declare_reveal(prop_non_hispanic_below_hs, legal_worker_act) + declare_step(handler = synth_weights_tidy) + declare_estimator(prop_non_hispanic_below_hs ~ legal_worker_act, subset = year &gt;= 2007, weights = synth_weights, model = lm_robust, label = &quot;synth&quot;) + declare_estimator(prop_non_hispanic_below_hs ~ legal_worker_act, subset = year &gt;= 2007, model = lm_robust, label = &quot;unweighted&quot;) + declare_estimator(prop_non_hispanic_below_hs ~ I(state == &quot;AZ&quot;) + post_treatment_period + legal_worker_act, term = &quot;legal_worker_act&quot;, model = lm_robust, label = &quot;unweighted_did&quot;) + declare_estimator(handler = tidy_estimator(augsynth_tidy), label = &quot;augsynth&quot;) state_data &lt;- draw_data(design) state_data %&gt;% dplyr::select(state, synth_weights) %&gt;% distinct %&gt;% arrange(-synth_weights) %&gt;% head ## state synth_weights ## 1 AZ 1.000 ## 2 TX 0.942 ## 3 NM 0.026 ## 4 CA 0.016 ## 5 AL 0.001 ## 6 AK 0.001 state_data %&gt;% ggplot() + geom_line(aes(year, prop_non_hispanic_below_hs)) + facet_wrap(~ state) This chunk is set to echo = TRUE and eval = do_diagnosis simulations &lt;- simulate_design(design, sims = sims) Right after you do simulations, you want to save the simulations rds. Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. synth_diagnosands &lt;- declare_diagnosands(select = c(&quot;bias&quot;, &quot;coverage&quot;)) diagnosis &lt;- diagnose_design(simulations, diagnosands = synth_diagnosands, bootstrap_sims = b_sims) reshape_diagnosis(diagnosis) ## Design Label Estimand Label Estimator Label Term N Sims ## 1 design ATE_AZ augsynth &lt;NA&gt; 100 ## 2 ## 3 design ATE_AZ synth legal_worker_act 100 ## 4 ## 5 design ATE_AZ unweighted legal_worker_act 100 ## 6 ## 7 design ATE_AZ unweighted_did legal_worker_act 100 ## 8 ## Bias Coverage ## 1 -0.00 0.97 ## 2 (0.00) (0.02) ## 3 0.00 1.00 ## 4 (0.00) (0.00) ## 5 0.66 0.00 ## 6 (0.00) (0.00) ## 7 0.28 0.00 ## 8 (0.00) (0.00) we see that Synth outperforms either method 15.6.1 References "],
["papers-with-multiple-studies.html", "15.7 Papers with multiple studies", " 15.7 Papers with multiple studies You can use the global bib file via rmarkdown cites like this: Imai, King, and Stuart (2008) design &lt;- declare_population(N = 100, u = rnorm(N)) + declare_potential_outcomes(Y ~ Z + u) + declare_assignment(prob = 0.5) + declare_reveal(Y, Z) + declare_estimator(Y ~ Z, model = difference_in_means) This chunk is set to echo = TRUE and eval = do_diagnosis simulations_pilot &lt;- simulate_design(design, sims = sims) Right after you do simulations, you want to save the simulations rds. Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. kable(head(simulations_pilot)) design_label sim_ID estimator_label term estimate std.error statistic p.value conf.low conf.high df outcome design 1 estimator Z 0.9624445 0.1705951 5.641688 0.0000002 0.6234399 1.301449 88.32465 Y design 2 estimator Z 0.9532166 0.2124610 4.486548 0.0000200 0.5315217 1.374911 96.66334 Y design 3 estimator Z 0.9674071 0.2137695 4.525468 0.0000170 0.5431863 1.391628 97.96114 Y design 4 estimator Z 1.0545908 0.1942247 5.429747 0.0000004 0.6690878 1.440094 96.58766 Y design 5 estimator Z 1.0708566 0.1897958 5.642153 0.0000002 0.6942127 1.447501 97.99051 Y design 6 estimator Z 0.7197320 0.1964379 3.663916 0.0004076 0.3297870 1.109677 95.63827 Y References "],
["multi-site-studies.html", "15.8 Multi-site studies", " 15.8 Multi-site studies You can use the global bib file via rmarkdown cites like this: Imai, King, and Stuart (2008) design &lt;- declare_population(N = 100, u = rnorm(N)) + declare_potential_outcomes(Y ~ Z + u) + declare_assignment(prob = 0.5) + declare_reveal(Y, Z) + declare_estimator(Y ~ Z, model = difference_in_means) This chunk is set to echo = TRUE and eval = do_diagnosis simulations_pilot &lt;- simulate_design(design, sims = sims) Right after you do simulations, you want to save the simulations rds. Now all that simulating, saving, and loading is done, and we can use the simulations for whatever you want. kable(head(simulations_pilot)) design_label sim_ID estimator_label term estimate std.error statistic p.value conf.low conf.high df outcome design 1 estimator Z 0.9624445 0.1705951 5.641688 0.0000002 0.6234399 1.301449 88.32465 Y design 2 estimator Z 0.9532166 0.2124610 4.486548 0.0000200 0.5315217 1.374911 96.66334 Y design 3 estimator Z 0.9674071 0.2137695 4.525468 0.0000170 0.5431863 1.391628 97.96114 Y design 4 estimator Z 1.0545908 0.1942247 5.429747 0.0000004 0.6690878 1.440094 96.58766 Y design 5 estimator Z 1.0708566 0.1897958 5.642153 0.0000002 0.6942127 1.447501 97.99051 Y design 6 estimator Z 0.7197320 0.1964379 3.663916 0.0004076 0.3297870 1.109677 95.63827 Y References "],
["putting-designs-to-use.html", "Chapter 16 Putting Designs to Use", " Chapter 16 Putting Designs to Use "],
["before-studies.html", "16.1 Before studies", " 16.1 Before studies 16.1.1 Pre-Analysis Plans 16.1.2 Registered Reports 16.1.3 Standard Operating Procedures 16.1.4 Evaluating and Supporting Research "],
["after-studies.html", "16.2 After studies", " 16.2 After studies 16.2.1 Reconciliation 16.2.2 Replication 16.2.3 Peers: Better scholarly critique 16.2.4 Combining designs Job market papers with multiple studies / three paper paradigm in psych (is it one design targeting same inquiry?) [JC 1p] Multi-site studies – take a design from another study and use it for another one Knowledge accumulation "],
["a-library-of-research-designs-as-objects.html", "16.3 A library of research designs as objects", " 16.3 A library of research designs as objects "],
["references-3.html", "References", " References "]
]
