<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 26 Piloting | Research Design: Declare, Diagnose, Redesign</title>
<meta name="author" content="Graeme Blair, Jasper Cooper, Alexander Coppock, and Macartan Humphreys">
<!-- CSS --><!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.1.9000/tabs.js"></script><script src="libs/bs3compat-0.2.1.9000/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://hypothes.is/embed.js" async></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Research Design: Declare, Diagnose, Redesign</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li class="book-part">Introduction</li>
<li><a class="" href="preamble.html"><span class="header-section-number">1</span> Preamble</a></li>
<li><a class="" href="improving-research-designs.html"><span class="header-section-number">2</span> Improving research designs</a></li>
<li><a class="" href="software-primer.html"><span class="header-section-number">3</span> Software primer</a></li>
<li><a class="" href="part-i-exercises.html"><span class="header-section-number">4</span> Part I Exercises</a></li>
<li class="book-part">Declaration, Diagnosis, Redesign</li>
<li><a class="" href="formalizing-mida.html"><span class="header-section-number">5</span> Formalizing MIDA</a></li>
<li><a class="" href="specifying-the-model.html"><span class="header-section-number">6</span> Specifying the model</a></li>
<li><a class="" href="defining-the-inquiry.html"><span class="header-section-number">7</span> Defining the inquiry</a></li>
<li><a class="" href="crafting-a-data-strategy.html"><span class="header-section-number">8</span> Crafting a data strategy</a></li>
<li><a class="" href="choosing-an-answer-strategy.html"><span class="header-section-number">9</span> Choosing an answer strategy</a></li>
<li><a class="" href="diagnosis.html"><span class="header-section-number">10</span> Diagnosis</a></li>
<li><a class="" href="redesign-1.html"><span class="header-section-number">11</span> Redesign</a></li>
<li><a class="" href="part-ii-exercises.html"><span class="header-section-number">12</span> Part II Exercises</a></li>
<li class="book-part">Design Library</li>
<li><a class="" href="design-library.html"><span class="header-section-number">13</span> Design Library</a></li>
<li><a class="" href="observational-designs-for-descriptive-inference.html"><span class="header-section-number">14</span> Observational designs for descriptive inference</a></li>
<li><a class="" href="experimental-designs-for-descriptive-inference.html"><span class="header-section-number">15</span> Experimental designs for descriptive inference</a></li>
<li><a class="" href="observational-designs-for-causal-inference.html"><span class="header-section-number">16</span> Observational designs for causal inference</a></li>
<li><a class="" href="experimental-designs-for-causal-inference.html"><span class="header-section-number">17</span> Experimental designs for causal inference</a></li>
<li><a class="" href="exercises-6.html"><span class="header-section-number">18</span> Exercises</a></li>
<li><a class="" href="multi-study-designs.html"><span class="header-section-number">19</span> Multi-study designs</a></li>
<li><a class="" href="part-iii-exercises.html"><span class="header-section-number">20</span> Part III Exercises</a></li>
<li class="book-part">Research Design Lifecycle</li>
<li><a class="" href="research-design-lifecycle.html"><span class="header-section-number">21</span> Research Design Lifecycle</a></li>
<li><a class="" href="planning.html"><span class="header-section-number">22</span> Planning</a></li>
<li><a class="" href="ethical-review.html"><span class="header-section-number">23</span> Ethical Review</a></li>
<li><a class="" href="partners.html"><span class="header-section-number">24</span> Partners</a></li>
<li><a class="" href="funding.html"><span class="header-section-number">25</span> Funding</a></li>
<li><a class="active" href="piloting.html"><span class="header-section-number">26</span> Piloting</a></li>
<li><a class="" href="implementation.html"><span class="header-section-number">27</span> Implementation</a></li>
<li><a class="" href="populated-preanalysis-plan.html"><span class="header-section-number">28</span> Populated Preanalysis Plan</a></li>
<li><a class="" href="reconciliation.html"><span class="header-section-number">29</span> Reconciliation</a></li>
<li><a class="" href="writing.html"><span class="header-section-number">30</span> Writing</a></li>
<li><a class="" href="publication.html"><span class="header-section-number">31</span> Publication</a></li>
<li><a class="" href="archiving.html"><span class="header-section-number">32</span> Archiving</a></li>
<li><a class="" href="reanalysis.html"><span class="header-section-number">33</span> Reanalysis</a></li>
<li><a class="" href="replication.html"><span class="header-section-number">34</span> Replication</a></li>
<li><a class="" href="resolving-disputes.html"><span class="header-section-number">35</span> Resolving Disputes</a></li>
<li><a class="" href="synthesis.html"><span class="header-section-number">36</span> Synthesis</a></li>
<li><a class="" href="part-iv-exercises.html"><span class="header-section-number">37</span> Part IV Exercises</a></li>
<li class="book-part">Epilogue</li>
<li><a class="" href="epilogue.html"><span class="header-section-number">38</span> Epilogue</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="glossary.html"><span class="header-section-number">39</span> Glossary</a></li>
<li><a class="" href="references-4.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="piloting" class="section level1">
<h1>
<span class="header-section-number">26</span> Piloting<a class="anchor" aria-label="anchor" href="#piloting"><i class="fas fa-link"></i></a>
</h1>
<!-- make sure to rename the section title below -->
<!-- -- can't learn causal effect -->
<!-- -- what can you learn? about Y0, about M, about se -->
<!-- -- bring in blog post -->
<p>The designs and results of past studies are important guides for selecting M, I, D, and A. Our understanding of the nodes and edges in the causal graph of M, expected effect sizes, the distribution of outcomes, feasible randomization schemes, and many other features are directly selected from past research or chosen based on a literature review of the distribution over past studies. However, researchers face a problem in being guided by past research: the research context and our inquiries often differ in at least subtle ways from any past study. Even when we are replicating a past study, we are collecting data in a different time period and if effects vary over time then aspects of M may differ from the original study. To deal with this, we often run pilot studies. These take many forms: focus groups to learn about features of M or to learn how to ask survey questions; small-scale tests of measurement tools to verify our data collection technology works; up to mini studies with the planned design but on a smaller scale.</p>
<p>Pilot studies are constrained by our time and by money. If we were not constrained, we would run the full study and learn what is wrong with our design and then run a corrected design for the main study. Since we cannot due to our constraints, we run either smaller mini studies or test out only a subset of the elements of our planned design. This places us in a bind: we are running a design smaller or less complete than the study we imagine conducting, and so the properties of the pilot design will not measure up.</p>
<p>MIDA provides a framework for thinking about two aspects of piloting: what can be learned from a pilot? To answer this question, pilot studies should be diagnosed before they are run. Just like for a full study, we can define inquiries about the decisions we would make and the parameter estimates we would draw on in designing the full study.</p>
<p>In Figure XX, we display the results of a diagnosis of a 50-unit pilot study that we are conducting to prepare for a larger main study. We consider two strategies: (1) determining the sample size from a power analysis of the main study, selecting the minimum <span class="math inline">\(N\)</span> such that the study is 80% powered to detect the pilot study’s effect size); (2) setting a fixed <span class="math inline">\(N\)</span> determined by our budget constraint, in this case to 500, and using the standard deviation of units in the treated and control group from the pilot to determine the minimum detectable effect size of our 500-unit main study.</p>
<p>In the left panel is the sampling distribution of effect size estimates, i.e., a histogram of the effect estimates from the pilot. In the design, the standard deviation of the outcome is set to one, so effect estimates are in standard deviation units. The true effect size is set to 0.2. We can see that the sampling distribution has a huge range, from nearly -0.5 to nearly 0.75. The first problem with the sampling distribution is that many estimates, in fact nearly a quarter of them, are negative (the wrong sign!). This might lead us not only to choose the wrong sample size but to choose one-sided tests in the wrong direction. The second is that we have a high likelihood of guessing the effect size is <em>much</em> higher than it really is. If we obtain one of the estimates over 0.75 or even over 0.5, we would choose an <span class="math inline">\(N\)</span> too small to detect the true effect size of 0.2. In short, our estimates of the effect size from our 50-person pilot study are simple too variable to be useful in designing our main study.</p>
<p>However, there is good news: we can learn a lot about the power of our main study from the pilot study, just not from the effect estimates. In the right panel of Figure XX, we estimate the minimum detectable effect size of a 500-unit main study, relying on the estimated standard deviation in the control group and the estimated standard deviation in the treatment group to calculate the estimated standard error of the effect estimate in the main study. We then calculate the minimum detectable effect size using the approximation from <span class="citation">(Gelman and Hill <a href="references-4.html#ref-gelman2006data">2006</a>, pg. 441)</span>, 2.8 times the estimated standard error. We find that our estimates of the MDE for the full study are much more precise, tightly centered around 0.25. Since we don’t know if that is larger or smaller than the true effect size, we then must make an argument based on past studies’ effect sizes to justify whether that minimum size is sufficiently large or whether we should increase the sample size in order to detect even smaller effects. The reason the MDE is more precisely estimated is that the standard deviation of the control group is a much less variable estimate of the true standard deviation of the control potential outcome than the effect size estimate is of the true effect size.</p>
<div class="inline-figure"><img src="book_files/figure-html/unnamed-chunk-494-1.png" width="100%"></div>
<p>By diagnosing our pilot studies in this way, we can learn what decisions can be made with confidence from pilot data and what should be shaped instead by expectations from past studies and qualitative knowledge. Diagnosis can also help us to decide how large a pilot study we need in order to estimate quantities like the MDE of the full study with precision.</p>
<p>Beyond estimating the MDE of studies, other facts that can often be usefully learned from pilot studies take the form of existence proofs. We often wish to study how variation in <span class="math inline">\(D\)</span> (a treatment) affects variation in <span class="math inline">\(Y\)</span> (an outcome), but in the absence of past data from these two variables we may not know even if there is variation in <span class="math inline">\(Y\)</span> to explain. In experimental studies, we can learn whether a treatment <em>can</em> be implemented, and in an observational study we can learn whether there is variation in the treatment variable.</p>
<p>Baseline measurement may often be used instead of a pilot study to learn about some empirical features.</p>
<!-- Diagnosing the pilot study on its own provides stark insights, which amount to: we cannot provide answers to the inquiry in the main study, and should not try to do so. There are also aspects of the logistics of research that within time and financial constraints we simply cannot learn until we run the main study. Science is imperfect, and also iterative, but these mistakes or suboptimal design choices also often lead to discoveries. -->
<!-- -- how does it help to diagnose the design together? the properties of the main study *change* when we do a pilot. This is because if we run the pilot study, we are doing so to make decisions about how to run the main study, and so our *design* of the main study and thus its results may depend on the *results* (and design) of the pilot study.  -->
<!-- In this section, we illustrate several general principles that flow from diagnosing pilot studies.  -->
<!-- Purposes of pilot studies: -->
<!-- Existence proofs: -->
<!-- -- is there variation in Y -->
<!-- -- is there variation in X -->
<!-- -- what are nodes in M -->
<!-- -- what are feasible D's, what are feasible treatments / can you implement the treatment (existence proof) -->
<!-- Harder questions requiring bigger sample sizes: -->
<!-- -- what is the distribution of X (helps select stratification proportions etc.) -->
<!-- -- what is the standard deviation of Y0 -->
<!-- #### Assessing a pilot design -->
<!-- declare pilot itself and diagnose just as if it were the main study -->
<!-- if you can't learn the answer, don't make any decisions based on it -->
<!-- #### Assessing a sequenced design -->
<!-- if you are making decisions about MIDA for main study based on pilot, diagnose the procedure of two studies, think about POs of pilot -->
<!-- #### Pilots and baselines -->
<!-- Designs can be reassessed after baselines and before treatment assignment -- so some of the questions you might do a pilot for can just be answered in a baseline -->
<!-- #### BLOG material -->
<!-- Data collection is expensive, and we often only get one bite at the apple. In response, we often conduct an inexpensive (and small) pilot test to help better design the study. Pilot studies have many virtues, including practicing the logistics of data collection and improving measurement tools. But using pilots to get noisy estimates in order to determine sample sizes for scale up comes with risks. -->
<!-- Pilot studies are often used to get a guess of the average effect size, which is then plugged into power calculators when designing the full study. -->
<!-- The procedure is: -->
<!-- 1. Conduct a small pilot study (say, N = 50) -->
<!-- 2. Obtain an estimate of the effect size (this is noisy, but better than nothing!) -->
<!-- 3. Conduct a power analysis for a larger study (say, N = 500) on the basis of the estimated effect size in the pilot -->
<!-- We show in this post that this procedure turns out to be dangerous: at common true effect sizes found in the social sciences, you are at risk of selecting an underpowered design based on the noisy effect estimate in your pilot study. -->
<!-- A different procedure has better properties: -->
<!-- 1. Conduct a small pilot study (say, N = 50) -->
<!-- 2. Obtain an estimate of the **standard deviation of the outcome variable** (again, this is a noisy estimate but better than nothing!) -->
<!-- 3. Estimate the minimum detectable effect (MDE) for a larger study (say, N = 500), using the estimated standard deviation -->
<!-- We show what happens in each procedure, using DeclareDesign. In each case, we'll think about a decision the researcher wants to make based on the pilot: should I move forward with my planned study, or should I go back to the drawing board? We'll rely on power to make that decision in the first procedure and the MDE in the second procedure. -->
<!-- [omitting code] -->
<!-- For each true effect size, the simulations will give us a distribution of estimated effects that a researcher might use as a basis for power analysis. For example, for a true effect size of 0 the researcher might still estimate an effect of 0.10, and so conduct their power analysis assuming that the true effect is 0.10. For each true effect, we can thus construct a distribution of *power estimates* a researcher might obtain from *estimated* effects. Since we know the true power for the true underlying effect, we can compare the distribution of post-hoc power estimates to the true power one would estimate if one knew the true effect size. -->
<!-- What did we find? In the plot, we show our guesses for the power of the main study based on our pilot effect size estimates.  -->
<!-- At high true effect sizes (top row), we do pretty well. Most of our guesses are above 80\% power, leading us to the correct decision that the study is powered. Indeed we often *underestimate* our power in these cases meaning that we run larger studies than we need to. -->
<!-- However, at low true effect sizes (bottom row) we show we are equally likely to find that the design is in fact powered as underpowered. We are equally likely to guess the power of the design is 90% as 10%. There is a good chance that we will falsely infer that our design is well powered just because we happened to get a high estimate from a noisy pilot. -->
<!-- ### How about estimating the standard deviation of the outcome? -->
<!-- Now, let's look at the second approach. Here, instead of using our pilot study to estimate the effect size for a power calculation, we estimate the **standard deviation of the outcome** and use this to calculate the main study's minimum detectable effect. The decision we want to make is: is this MDE small enough to be able to rule out substantively important effects? -->
<!-- We calculate the minimum detectable effect size using the approximation from [@gelman2006data, pg. 441], 2.8 times the estimated standard error. We estimate the standard error using Equation 3.6 from @gerber2012field.  -->
<!-- In summary, pilot studies can be valuable in planning research for many reasons, but power calculations based on noisy effect size estimates can be misleading. A better approach is to use the pilot to learn about the distribution of outcome variables. The variability of the outcome variable can then be plugged into MDE formulas or even power calculations with, say, the smallest effect size of political, economic, or social importance. -->
<!-- In the same spirit, pilot studies could also be used to learn the strength of the correlation between pre-treatment covariates and the outcome variable. With this knowledge in hand, researchers can develop their expectations about how much precision there is to be gained from covariate control or blocking. -->

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
<!-- start post here, do not edit above -->
</div>
  <div class="chapter-nav">
<div class="prev"><a href="funding.html"><span class="header-section-number">25</span> Funding</a></div>
<div class="next"><a href="implementation.html"><span class="header-section-number">27</span> Implementation</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav"><li><a class="nav-link" href="#piloting"><span class="header-section-number">26</span> Piloting</a></li></ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Research Design: Declare, Diagnose, Redesign</strong>" was written by Graeme Blair, Jasper Cooper, Alexander Coppock, and Macartan Humphreys. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
