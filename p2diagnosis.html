<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 10 Diagnosis | Research Design: Declare, Diagnose, Redesign</title>
<meta name="author" content="Graeme Blair, Jasper Cooper, Alexander Coppock, and Macartan Humphreys">
<!-- CSS --><!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.1.9000/tabs.js"></script><script src="libs/bs3compat-0.2.1.9000/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://hypothes.is/embed.js" async></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Research Design: Declare, Diagnose, Redesign</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li class="book-part">Introduction</li>
<li><a class="" href="preamble.html"><span class="header-section-number">1</span> Preamble</a></li>
<li><a class="" href="improving-research-designs.html"><span class="header-section-number">2</span> Improving research designs</a></li>
<li><a class="" href="software-primer.html"><span class="header-section-number">3</span> Software primer</a></li>
<li><a class="" href="part-i-exercises.html"><span class="header-section-number">4</span> Part I Exercises</a></li>
<li class="book-part">Declaration, Diagnosis, Redesign</li>
<li><a class="" href="formalizing-mida.html"><span class="header-section-number">5</span> Formalizing MIDA</a></li>
<li><a class="" href="specifying-the-model.html"><span class="header-section-number">6</span> Specifying the model</a></li>
<li><a class="" href="defining-the-inquiry.html"><span class="header-section-number">7</span> Defining the inquiry</a></li>
<li><a class="" href="crafting-a-data-strategy.html"><span class="header-section-number">8</span> Crafting a data strategy</a></li>
<li><a class="" href="choosing-an-answer-strategy.html"><span class="header-section-number">9</span> Choosing an answer strategy</a></li>
<li><a class="active" href="p2diagnosis.html"><span class="header-section-number">10</span> Diagnosis</a></li>
<li><a class="" href="redesign-1.html"><span class="header-section-number">11</span> Redesign</a></li>
<li><a class="" href="part-ii-exercises.html"><span class="header-section-number">12</span> Part II Exercises</a></li>
<li class="book-part">Design Library</li>
<li><a class="" href="design-library.html"><span class="header-section-number">13</span> Design Library</a></li>
<li><a class="" href="observational-designs-for-descriptive-inference.html"><span class="header-section-number">14</span> Observational designs for descriptive inference</a></li>
<li><a class="" href="experimental-designs-for-descriptive-inference.html"><span class="header-section-number">15</span> Experimental designs for descriptive inference</a></li>
<li><a class="" href="observational-designs-for-causal-inference.html"><span class="header-section-number">16</span> Observational designs for causal inference</a></li>
<li><a class="" href="experimental-designs-for-causal-inference.html"><span class="header-section-number">17</span> Experimental designs for causal inference</a></li>
<li><a class="" href="multi-study-designs.html"><span class="header-section-number">18</span> Multi-study designs</a></li>
<li><a class="" href="part-iii-exercises.html"><span class="header-section-number">19</span> Part III Exercises</a></li>
<li class="book-part">Research Design Lifecycle</li>
<li><a class="" href="research-design-lifecycle.html"><span class="header-section-number">20</span> Research Design Lifecycle</a></li>
<li><a class="" href="part-iv-exercises.html"><span class="header-section-number">21</span> Part IV Exercises</a></li>
<li class="book-part">Epilogue</li>
<li><a class="" href="epilogue.html"><span class="header-section-number">22</span> Epilogue</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="glossary.html"><span class="header-section-number">23</span> Glossary</a></li>
<li><a class="" href="references-4.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="p2diagnosis" class="section level1">
<h1>
<span class="header-section-number">10</span> Diagnosis<a class="anchor" aria-label="anchor" href="#p2diagnosis"><i class="fas fa-link"></i></a>
</h1>
<!-- make sure to rename the section title below -->
<p>Research design diagnosis is the process of evaluating the properties of a research design. Since “property of a research design” is a cumbersome phrase, we made up the word “diagnosand” to refer to those properties of a research design we would like to diagnose. Many diagnosands are familiar. Power is the probability of obtaining a statistically significant result. Bias is the average deviation of estimates from the true value of the estimand. Other diagnosands are more exotic, like the Type-S error rate, which is the probability the estimate has the incorrect sign, conditional on being statistically significant (Gelman and Carlin 2014).</p>
<!-- snip -->
<p>Research designs are strong when the empirical answer <span class="math inline">\(a^d\)</span> generated by the design is close to the true answer <span class="math inline">\(a^w\)</span>. Since we can never know <span class="math inline">\(a^w\)</span>, we have to assess whether the distribution of <span class="math inline">\(a^d\)</span> over possible realizations of the research design is close to the distribution of <span class="math inline">\(a^m\)</span>, the answer under the model. How can we assess how close the distributions are? The distribution of <span class="math inline">\(a^m\)</span> is easy to simulate. Given a theoretical model, we can easily ask a computer to generate the distribution of the estimand. More often than not, the distribution of <span class="math inline">\(a^M\)</span> is degenerate – in a fixed population, for example, most theoretical models posit that estimands like the Average Treatment Effect have just one value.</p>
<p>The distribution of <span class="math inline">\(a^d\)</span> is trickier to estimate. The <em>actual</em> research design will only be implemented once, so we don’t get to see the distribution of actual answers that <em>could have</em> eventuated from the application of D and A to the world. To solve this problem, we make a small – but extremely consequential – substitution of <span class="math inline">\(m\)</span> for <span class="math inline">\(w\)</span> in the DAG of a research design. Swapping <span class="math inline">\(m\)</span> in for <span class="math inline">\(w\)</span>, we can ask the computer to simulate the distribution of <span class="math inline">\(a^d\)</span> <em>conditional on the model</em>. If the model is wrong, the simulated distribution of <span class="math inline">\(a^d\)</span> will be wrong too – as ever, garbage in, garbage out. Figure YY shows the DAG we use to simulate research designs. When we simulate designs, <span class="math inline">\(d\)</span> is affected by <span class="math inline">\(m\)</span> (a realization of a theoretical model), rather than by <span class="math inline">\(w\)</span>. This makes sense, since the computer simulations can be entirely untethered from reality.</p>
<p>Design diagnosis is the process of simulating both <span class="math inline">\(I(m) = a^m\)</span> and <span class="math inline">\(A(d) = a^d\)</span> over many draws from <span class="math inline">\(M()\)</span> and <span class="math inline">\(D(m)\)</span>, and comparing them. The specific comparisons we make are called “diagnostic statistics.”</p>
<p>A “diagnostic statistic” is a function <span class="math inline">\(g\)</span> of <span class="math inline">\(a^d\)</span>, the answer given the data, of <span class="math inline">\(a^m\)</span>, the answer given the model, or of both answers. This function might be really simple, like the identity function, <span class="math inline">\(g(a^d)=a^d\)</span>, or the difference between the two answers: <span class="math inline">\(g(a^m, a^d) = a^d - a^m\)</span>. Because <span class="math inline">\(a^m\)</span> and <span class="math inline">\(a^d\)</span> are random variables, and any function of random variables is also a random variable, any diagnostic statisitic is also a random variable. A diagnosand is a summary of that random variable.
<!-- If $a^m$ and $a^d$ are vector-valued (e.g., when answers provide a prediction for each observation), one might have a vector of diagnostic statistics, such as the vector of differences between each element of  $a^m$ and $a^d$, or a scalar valued summary, such as a multidimensional distance measure.  In some cases $a^m$ and $a^d$ might have different dimensions; for instance $a^d$ may be the lower and upper bounds on a confidence interval and the diagnostic statistic is "Is $a^m$ within $a^d$?" --></p>
<p><span class="math display">\[
\phi = f(g(a^m, a^d))
\]</span></p>
<p>The <span class="math inline">\(f()\)</span> is a statistical functional that summarizes the random variable. For example, the expectation function <span class="math inline">\(E[X]\)</span> summarizes the random variable <span class="math inline">\(X\)</span> with its expectation, the mean, while the variance function summarizes the expectation of the squared deviation of a random variable from its mean.<!-- Diagnosands are summaries of diagnostic statics.--> We’ll use the Greek letter <span class="math inline">\(\phi\)</span> to describe the idea of a diagnosand in general.</p>
<p>Let’s back up a moment to work through some concrete examples of common diagnosands (see section <a href="p2diagnosis.html#diagnosands">10.3</a> for a more exhaustive list). Consider the diagnosand “bias.” Bias is the average difference between the estimand and the estimate. Under a model that has two potential outcomes, a treated potential outcome and an untreated potential outcome, the inquiry might be the average treatment effect (the difference in the two potential outcomes averaged over units in the population or sample, abbreviated as ATE). Under a single realization <span class="math inline">\(m\)</span> of the model <span class="math inline">\(M\)</span>, the value of the ATE will be a particular number, which we call <span class="math inline">\(a^m\)</span>. If our data strategy is simply to collect data on those who come to be treated versus those who don’t (i.e., we do not use random assignment), and our answer strategy is difference-in-means, our answer <span class="math inline">\(a^d\)</span> could be systematically different from <span class="math inline">\(a^m\)</span>. The diagnostic statistic is the error <span class="math inline">\(a^d - a^m\)</span>; this error is a random variable because each draw of <span class="math inline">\(m\)</span> from <span class="math inline">\(M\)</span> is slightly different. The expectation of this random variable is <span class="math inline">\(E[a^d - a^m]\)</span>, or the value of the bias diagnosand.</p>
<p>Answer strategies commonly rely on measures of uncertainty like <span class="math inline">\(p\)</span>-values, standard errors, and confidence intervals in order to make decisions about how to interpret <span class="math inline">\(a^d\)</span>. Any measure estimated from the data to make a decision about <span class="math inline">\(a^d\)</span> can be used as a diagnostic statistic and summarized as a diagnosand. Like bias, statistical power is an expectation, this time of the diagnostic statistic <span class="math inline">\(\mathbb{1}(p \leq 0.05)\)</span>, an indicator function that equals 1 if the <span class="math inline">\(p\)</span>-value is no greater than 0.05 and 0 otherwise. Power describes how frequently (under beliefs about the model) a research design would return a statistically significant result. A standard error provides another diagnostic statistic whose expectation provides the expected standard error diagnosand, for example. This can be especially informative in comparison with another diagnosand, <span class="math inline">\(\sqrt{\mathbb{V}(a^d)}\)</span>, the actual standard deviation of the estimates generated by the model.</p>
<p>Some diagnosands can be calculated analytically. It can be straightforward to calculate <span class="math inline">\(\sqrt{\mathbb{V}(a^d)}\)</span> if you have a two-arm experiment and are willing to make a lot of simplifying assumptions.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Insert formula 3.4 and cite to FEDAI.&lt;/p&gt;"><sup>19</sup></a> Most diagnosands for even moderately complex designs, however, require Monte Carlo computer simulation. A main purpose of the <code>DeclareDesign</code> software package is making the simulation step easier.</p>
<p>In practice, it is important to diagnose a design under multiple possible <span class="math inline">\(M\)</span>’s, given our fundamental uncertainty about the world <span class="math inline">\(w\)</span>. We do not know the precise distributions of exogenous variables or the exact functional forms of potential outcomes (e.g., we do not know the true effect size). Diagnosis, therefore, should typically involve simulating the properties of a fixed set of inquiries, data strategies, and answer strategies under <em>multiple</em> likely models. A <span class="math inline">\(D\)</span> and <span class="math inline">\(A\)</span> for a given <span class="math inline">\(I\)</span> that provide good diagnosand values under multiple <span class="math inline">\(M\)</span>’s can be said to be robust to multiple models.</p>
<div class="figure">
<span id="fig:unnamed-chunk-105"></span>
<img src="book_files/figure-html/unnamed-chunk-105-1.png" alt="MIDA as a DAG" width="100%"><p class="caption">
Figure 10.1: MIDA as a DAG
</p>
</div>
<!-- snip -->
<div id="estimating-diagnosands-analytically" class="section level2">
<h2>
<span class="header-section-number">10.1</span> Estimating diagnosands analytically<a class="anchor" aria-label="anchor" href="#estimating-diagnosands-analytically"><i class="fas fa-link"></i></a>
</h2>
<p>Diagnosis can be be done with analytic, pencil-and-paper methods. Indeed, research design textbooks often contain many formulas for calculating power under a variety of designs. For example, Gerber and Green include the following power formula:</p>
<p>They write:</p>
<blockquote>
<p>“To illustrate a power analysis, consider a completely randomized experiment where <span class="math inline">\(N&gt;2\)</span> of <span class="math inline">\(N\)</span> units are selected into a binary treatment. The researcher must now make assumptions about the distributions of outcomes for treatment and for control units. In this example, the researcher assumes that the control group has a normally distributed outcome with mean <span class="math inline">\(\mu_c\)</span>, the treatment group has a normally distributed outcome with mean <span class="math inline">\(\mu_t\)</span>, and both group’s outcomes have a standard deviation <span class="math inline">\(\sigma\)</span>. The researcher must also choose <span class="math inline">\(\alpha\)</span>, the desired level of statistical significance (typically 0.05).
Under this scenario, there exists a simple asymptotic approximation for the power of the experiment (assuming that the significance test is two-tailed):
<span class="math display">\[
\beta = \Phi \bigg(\frac{|\mu_t - \mu_c| \sqrt{N}}{2\sigma} - \Phi^{-1} (1 - \frac{\alpha}{2}) \bigg)
\]</span>
where <span class="math inline">\(\beta\)</span> is the statistical power of the experiment, <span class="math inline">\(\Phi(\cdot)\)</span> is the normal cumulative distribution function (CDF), and <span class="math inline">\(\Phi^{-1}(\cdot)\)</span> is the inverse of the normal CDF.”</p>
</blockquote>
<p>This power formula makes <strong>detailed</strong> assumptions about <span class="math inline">\(M\)</span>, <span class="math inline">\(D\)</span>, and <span class="math inline">\(A\)</span>? Under <span class="math inline">\(M\)</span>, it assumes that both potential outcomes are normally distributed with group specific means and a common variance. Under <span class="math inline">\(D\)</span>, it assumes a particular randomization strategy (simple random assignment). Under <span class="math inline">\(A\)</span>, it assumes a particular hypothesis testing approach (equal variance <span class="math inline">\(t\)</span>-test with <span class="math inline">\(N - 2\)</span> degrees of freedom). This set of assumptions may be “close enough” in many research settings, but it can be difficult to understand the specific impacts of different beliefs about <span class="math inline">\(M\)</span>, <span class="math inline">\(D\)</span> and <span class="math inline">\(A\)</span> on the value of the diagnosand. What if instead of being normally distributed, the potential outcomes are measured in 1 - 5 Likert scales? What if the randomization procedure includes blocking? What if we include covariates in our treatment effect estimation approach? Formulas for some large sources of design variation have been derived (such as clustering), but certainly not for every design variant.</p>
<p>Very quickly, hope for analytic design diagnosis fades. The analytic formulas are abstractions – they abstract away from design details and sometimes those design details are important. This problem is not confined to the “power” diagnosand. In randomized experiments, claims about the bias diagnosand are quite general. Many randomized designs are unbiased for the ATE, but not all. Designs that encounter noncompliance, attrition, or some forms of spillover may not be unbiased for the ATE. Even without any of those complications, cluster randomized trials with heteogeneous cluster sizes are not unbiased (joel, imai).</p>
<p>Diagnosands depend on design details, because how you conduct your study matters for its properties. That means design diagnosis must be design-aware. Since designs are so heteogenous and can vary on so many dimensions, computer simulation is only feasible way to diagnose anything beyond the simplest ideal-type designs.</p>
</div>
<div id="estimating-diagnosands-via-simulation" class="section level2">
<h2>
<span class="header-section-number">10.2</span> Estimating diagnosands via simulation<a class="anchor" aria-label="anchor" href="#estimating-diagnosands-via-simulation"><i class="fas fa-link"></i></a>
</h2>
<p>Research design diagnosis usually occurs in a two-step, simulation-based procedure. First we simulate research designs over and cover, collecting “diagnostic statistics” from each run of the simulation. Second, we summarise the distribution of the diganostic statistics in order to estimate the diagnosands.</p>
<p>So we estimate diagnosands by summarizing the distribution of diagnostic statistics – of course this raises the question: what is a diagnostic statistic? We take a draw from the model (<span class="math inline">\(m\)</span>) and calculate the value of the inquiry <span class="math inline">\(I(m) = a^m\)</span>. We take one draw from the data strategy (<span class="math inline">\(D(m) = d\)</span>), and calculates the value of the answer strategy <span class="math inline">\(A(d) = a^d\)</span>. A diagnostic statistic is some function of <span class="math inline">\(a^m\)</span> and <span class="math inline">\(a^d\)</span>.</p>
<p>A simple diagnostic statistic is “error,” or the difference between the estimate and the estimand: <span class="math inline">\(error = a^d - a^m\)</span>. The bias diagnosand is the expectation of the error statistic <span class="math inline">\(E[error]\)</span> over all possible ways the study could have come out.</p>
<p>Usually, we consider many diagnostic statistics at the same time. Here’s a design declaration for a two-arm trial with a balanced (50/50) design. We have 100 subjects and their responses to treatment are drawn from a normal distribution with mean 0.1 and sd 0.1.</p>
<p>One draw of this simulation returns the following:</p>
<p>One draw from the simulation returns the following:</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="right">estimand</th>
<th align="right">estimate</th>
<th align="right">std.error</th>
<th align="right">conf.low</th>
<th align="right">conf.high</th>
<th align="right">p.value</th>
</tr></thead>
<tbody><tr class="odd">
<td align="right">0.10</td>
<td align="right">0.08</td>
<td align="right">0.04</td>
<td align="right">0.003</td>
<td align="right">0.156</td>
<td align="right">0.04</td>
</tr></tbody>
</table></div>
<p>Figure <a href="#simdrawsone"><strong>??</strong></a> shows the information we might obtain from a single run of the simulation. The filled point is the estimate <span class="math inline">\(a^d\)</span>. The open triangle is the estimand <span class="math inline">\(a^m\)</span>. The bell-shaped curve is our normal-approximation based estimate of the sampling distribution. The standard deviation of this estimated distribution is our estimated standard error, which expresses our uncertainty. The confidence interval around the estimate is another expression of our uncertainty: We’re not sure where <span class="math inline">\(a^d\)</span> is, but if things are going according to plan, confidence intervals constructed this way will bracket <span class="math inline">\(a^d\)</span> 95% of the time.</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr>
<th style="text-align:right;">
estimand
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
std.error
</th>
<th style="text-align:right;">
conf.low
</th>
<th style="text-align:right;">
conf.high
</th>
<th style="text-align:right;">
p.value
</th>
</tr></thead>
<tbody><tr>
<td style="text-align:right;">
0.1
</td>
<td style="text-align:right;">
0.091
</td>
<td style="text-align:right;">
0.038
</td>
<td style="text-align:right;">
0.016
</td>
<td style="text-align:right;">
0.167
</td>
<td style="text-align:right;">
0.018
</td>
</tr></tbody>
</table></div>
<div class="figure">
<span id="fig:simdrawsone"></span>
<img src="book_files/figure-html/simdrawsone-1.png" alt="Visualization of one draw from a design diagnosis." width="100%"><p class="caption">
Figure 10.2: Visualization of one draw from a design diagnosis.
</p>
</div>
<!-- We're interested in a few diagnosands. Bias (the expectation of the error statistics) is one, but we also want to know about RMSE (the square root of the expectation of the squared errors), power (the probability the $p$-value is below a threshold), and coverage (the probability the 95% confidence interval brackets the estimand).  -->
<p>From this single draw, we can’t yet estimate diagnosands, but we can estimate diagnostic statistics. The estimate was higher than the estimand in this draw, so the error is 0.10 - 0.08 = 0.02. Likewise, the squared error is <code>(0.10 - 0.08)^2 = 0.0004</code>. The <span class="math inline">\(p\)</span>-value is 0.04, which is just barely lower than the threshold of 0.05, so “statistical significance” diagnostic statistic is equal to <code>TRUE</code>. The confidence interval stretches from 0.003 to 0.156, and the value of the estimand (0.10) is between those bounds, so the “covers” diagnostic statistic is equal to <code>TRUE</code> as well.</p>
<p>Learning the <strong>distribution</strong> of diagnostic statistics is the main barrier to design diagnosis. If we could simply write down the distribution of diagnostic statistics, it would be a straightforward matter to summarize them in order to calculate diagnosands. But the distributions of diagnositic statistics depend on the complex of information in all four parts of a research design: M, I, D, and A. For example, the error statistic depends on both <span class="math inline">\(a^d\)</span> and <span class="math inline">\(a^m\)</span>, so the details of each matter greatly.</p>
<p>To calculate the distributions of the diagnostic statistics, we have to simulate designs not just once, but many many times over. The bias diagnosand is the average error over many runs of the simulation. The statistical power diagnosand is the fraction of runs in which the estimate is significant. The coverage diagnosand is the fraction of runs in which the confidence interval covers the estimand.</p>
<p>This figure visualizes just 10 runs of the simulation (obtained with <code>simulate_design(design)</code>). We can see that in each run, <span class="math inline">\(a^m\)</span> is a little different. This might seem counterintuitive – isn’t the estimand supposed to be a fixed number? Some estimands are fixed, others are stochastic, depending on the specifics of the model. Notice how in the design declaration, we drew the potential outcomes from a distribution rather then having them be fixed numbers. This choice incorportates some of our modeling uncertainty. The treatment effects for each unit are close to 0.1, but we’re not sure how close for each particulat unit. We can also see that some of the draws produce statistical significant estimates (the shaded areas are small and the confidence intervals don’t overlap zero), but not all. We get a sense of the <em>true</em> standard error by seeing how the point estimates bounce around. We get a feel for the difference between the estimates of the standard error and true standard error. Design diagnosis is the process of learning about all the ways the study might come out, not just the one way that it will.</p>
<div class="figure">
<span id="fig:simdrawsten"></span>
<img src="book_files/figure-html/simdrawsten-1.png" alt="Visualization of ten draws from a design diagnosis." width="100%"><p class="caption">
Figure 10.3: Visualization of ten draws from a design diagnosis.
</p>
</div>
<p>This line of code does it all in one. We simulate the design 1000 times to calculate the diagnositic satistics, then we summarise them in terms of bias, the true standard error (the standard deviation of the sampling distribution), RMSE, power, and coverage.</p>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">diagnosis</span> <span class="op">&lt;-</span> <span class="fu">diagnose_design</span><span class="op">(</span><span class="va">design</span>, sims <span class="op">=</span> <span class="fl">1000</span>, 
                             diagnosands <span class="op">=</span> <span class="fu">declare_diagnosands</span><span class="op">(</span>
                               select <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"bias"</span>, <span class="st">"sd_estimate"</span>, <span class="st">"rmse"</span>, <span class="st">"power"</span>, <span class="st">"coverage"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">diagnosis</span></code></pre></div>
<pre><code>## 
## Research design diagnosis based on 100 simulations. Diagnosand estimates with bootstrapped standard errors in parentheses (100 replicates).
## 
##  Design Label Estimand Label Estimator Label Term N Sims   Bias SD Estimate
##        design            ATE       estimator    Z    100   0.00        0.04
##                                                          (0.00)      (0.00)
##    RMSE  Power Coverage
##    0.04   0.62     0.96
##  (0.00) (0.05)   (0.02)</code></pre>
<!-- ## Interactive element -->
<!-- You can have fun building up a design diagnosis run-by-run with this shiny app. Each time you run the design, it adds a new estimate to the sampling distribution so you can learn about the relationship of a single run of the design to the overall diagnosis. -->
<!-- ```{r, echo = FALSE} -->
<!-- knitr::include_app('https://gblair.shinyapps.io/diagnosis/', height = '400px') -->
<!-- ``` -->
<div id="interpreting-diagnosands" class="section level3">
<h3>
<span class="header-section-number">10.2.1</span> Interpreting diagnosands<a class="anchor" aria-label="anchor" href="#interpreting-diagnosands"><i class="fas fa-link"></i></a>
</h3>
<p>Say you decide to simulate the design 1000 times to see if it’s biased. After a few seconds, the computer spits out bias of .02. Unsure whether to be concerned, you simulate the design another 1000 times. Now the computer says the bias is .01. Since each simulation was randomly generated, you should expect to get estimates of bias that vary. Given the random variation in the simulations and how large the estimate and estimand are, maybe it was even quite likely to get .01 or .02 bias even if the true bias is zero. Or maybe your design is biased.</p>
<p>As with frequentist statistics, we have to separate signal from noise. We have an estimate (here, of the bias diagnosand) that varies depending on how each simulation turns out. We want to quantify that variation in order to make a decision about whether the estimated bias is large enough to be significant. The simulation standard error (or, Monte Carlo standard error) is an estimate of the variation created by the simulation procedure. If that estimated variation is high relative to the estimated diagnosand, then we need to increase the number of simulations.
<!-- Following the example from the previous paragraph, if the simulation standard error was .015, then the simulation we ran was consistent with negative, positive, and no bias given the point estimate of .01. However, if we estimate a simulation standard error of .0001, then the .01 or .02 bias estimates give real cause for concern, since they were so unlikely to arise if the true bias were zero.  --></p>
<p><span class="citation">Morris, White, and Crowther (<a href="references-4.html#ref-morrisetal2019">2019</a>)</span> provide a range of helpful formulas to estimate simulation standard errors of diagnosands. Define <span class="math inline">\(a^d_i\)</span> as the point estimate in the <span class="math inline">\(i\)</span>’th simulation, <span class="math inline">\(n_{sim}\)</span> as the total number of simulations, and the average of the point estimates across simulations as <span class="math inline">\(\bar{a^d} = \frac{1}{n_{sim}}\sum a^d_i\)</span>. Then, if the estimand is a constant and <span class="math inline">\(a^d_i\)</span> is independently and normally distributed, the simulation standard error for bias can be calculated as <span class="math inline">\(\sqrt{1/(n_{sim}(n_{sim} - 1)) \sum^{n_{sim}}_{i = 1} (a^d_i - \bar{a^d})^2}\)</span>. Here is how you can do it in code:</p>
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">simulated_estimates</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span><span class="op">(</span><span class="va">diagnosis</span><span class="op">$</span><span class="va">simulations_df</span>, <span class="va">estimate</span><span class="op">)</span>
<span class="va">mean_estimate</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">simulated_estimates</span><span class="op">)</span>
<span class="va">n_sim</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">simulated_estimates</span><span class="op">)</span>
<span class="va">SE_bias</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">1</span><span class="op">/</span><span class="op">(</span><span class="va">n_sim</span> <span class="op">*</span> <span class="op">(</span><span class="va">n_sim</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">simulated_estimates</span> <span class="op">-</span> <span class="va">mean_estimate</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>
<span class="va">SE_bias</span></code></pre></div>
<pre><code>## [1] 0.004152528</code></pre>
<p>Of course, this estimate of the variance relies on assumptions and won’t work as soon as the estimand is not a constant or the estimates are not normally distributed. Rather than using analytic formulas, we recommend using an approach called nonparametric bootstrapping to estimate the simulation standard errors. Nonparametric bootstrapping can be used to estimate the standard error of any diagnosand whose diagnostic statistic is independently and identically distributed, so you don’t have to limit yourself to the classic diagnosands for which we have a formula. Nonparametric bootstrapping is quite simple to do: you randomly resample your <span class="math inline">\(n_{sims}\)</span> diagnostic statistics with replacement a large number of times and re-estimate the diagnosand on each resampled collection of diagnostic statistics. The standard deviation of the resulting distribution of diagnosand estimates gives you an estimate of the simulation standard error. <code>DeclareDesign</code> does nonparametric bootstrapping by default whenever the <code>diagnose_design()</code> function is called. The numbers are reported in the parentheses under the diagnosand estimates in the table above. Here’s the simulation standard error for bias calculated by bootstrap resampling 100 times:</p>
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">diagnosis</span><span class="op">$</span><span class="va">diagnosands_df</span> <span class="op">%&gt;%</span> <span class="fu">pull</span><span class="op">(</span><span class="st">"se(bias)"</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 0.003838199</code></pre>
<p>As you can see, the nonparametric bootstrap gets us very close to the correct answer given by the analytic formula, without making any assumptions about the shape of the distribution of diagnostic statistics.</p>
</div>
<div id="how-many-simulations-to-run" class="section level3">
<h3>
<span class="header-section-number">10.2.2</span> How many simulations to run<a class="anchor" aria-label="anchor" href="#how-many-simulations-to-run"><i class="fas fa-link"></i></a>
</h3>
<p>Unlike increasing sample size in the real world, increasing the precision of your diagnosand estimates is as simple as waiting a little longer while your computer runs a higher number of simulations. However, while running 100 simulations may take a matter of seconds, running 10,000,000 simulations may take a few days. So how do you know when you’ve done enough?</p>
<p>As a rule of thumb, you should be multiplying the simulation standard errors by two in your head in order to make decisions about whether to do more simulations, since this gets you close to the upper and lower bounds of a 95% confidence interval (properly calculated by muliplying the standard error by 1.96). So, if you change your randomization strategy and the power increases from .78 to .82 with a simulation standard error of .015, you should increase the number of simulations: the upper bound on the first estimate <span class="math inline">\((.78 + 0.015*1.96 = .81)\)</span> overlaps with the lower bound on the second <span class="math inline">\((.82 - 0.015*1.96)\)</span>, so the apparent improvement in power may be an artefact of the simulation.</p>
</div>
</div>
<div id="diagnosands" class="section level2">
<h2>
<span class="header-section-number">10.3</span> How to choose among diagnosands<a class="anchor" aria-label="anchor" href="#diagnosands"><i class="fas fa-link"></i></a>
</h2>
<p>A diagnostic statistic is a summary function of <span class="math inline">\(a^m\)</span> and <span class="math inline">\(a^d\)</span>, and a diagnosand is a summary of diagnostic-statistics. As a result, there are a great many to choose from. In Table <a href="p2diagnosis.html#tab:diagnosticstatistics">10.1</a>, we introduce a set of diagnostic statistics (far from complete!), and in Table <a href="p2diagnosis.html#tab:diagnosands">10.2</a> a set of diagnosands including those commonly and less commonly considered.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:diagnosticstatistics">Table 10.1: </span> Diagnostic statistics.</caption>
<colgroup>
<col width="42%">
<col width="57%">
</colgroup>
<thead><tr class="header">
<th>Diagnostic-statistic</th>
<th>Definition</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Estimate</td>
<td><span class="math inline">\(a^d\)</span></td>
</tr>
<tr class="even">
<td>Estimand under the model</td>
<td><span class="math inline">\(a^m\)</span></td>
</tr>
<tr class="odd">
<td>True estimand</td>
<td><span class="math inline">\(a^w\)</span></td>
</tr>
<tr class="even">
<td>
<span class="math inline">\(p\)</span>-value</td>
<td><span class="math inline">\(p \equiv \Pr_{M_0} \{\mid A - a^{m_0}\mid~\geq~\mid a^d - a^{m_0}\mid \}\)</span></td>
</tr>
<tr class="odd">
<td>
<span class="math inline">\(p\)</span>-value is no greater than <span class="math inline">\(\alpha\)</span>
</td>
<td><span class="math inline">\(\mathbf{I}(p \leq \alpha)\)</span></td>
</tr>
<tr class="even">
<td>Confidence interval</td>
<td><span class="math inline">\(\mathrm{CI}_{1-\alpha}\)</span></td>
</tr>
<tr class="odd">
<td>Confidence interval covers the estimand under the model</td>
<td><span class="math inline">\(\mathrm{covers}^{a^m} \equiv \mathbb{I}\{ a^m \in \mathrm{CI}_{1-\alpha} \}\)</span></td>
</tr>
<tr class="even">
<td>Estimated standard error</td>
<td><span class="math inline">\(\widehat\sigma(A)\)</span></td>
</tr>
<tr class="odd">
<td>Cost</td>
<td><span class="math inline">\(\mathrm{cost}\)</span></td>
</tr>
<tr class="even">
<td>Proportion of subjects harmed</td>
<td><span class="math inline">\(\Pr(\mathrm{harm}) \equiv \frac{1}{n} \sum_i \mathrm{harm_i}\)</span></td>
</tr>
</tbody>
</table></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:diagnosands">Table 10.2: </span> Diagnosands.</caption>
<colgroup>
<col width="16%">
<col width="54%">
<col width="29%">
</colgroup>
<thead><tr class="header">
<th>Diagnosand</th>
<th>Description</th>
<th>Definition</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Average estimate</td>
<td></td>
<td><span class="math inline">\(\mathbb{E}(a^d)\)</span></td>
</tr>
<tr class="even">
<td>Average estimand</td>
<td></td>
<td><span class="math inline">\(\mathbb{E}(a^m)\)</span></td>
</tr>
<tr class="odd">
<td>Power</td>
<td>Probability of rejecting null hypothesis of no effect</td>
<td><span class="math inline">\(\mathbb{E}(I(p \leq \alpha))\)</span></td>
</tr>
<tr class="even">
<td>Bias</td>
<td>Expected difference between estimate and estimand</td>
<td><span class="math inline">\(\mathbb{E}(a^d - a^m)\)</span></td>
</tr>
<tr class="odd">
<td>Variance</td>
<td></td>
<td><span class="math inline">\(\mathbb{V}(a^d)\)</span></td>
</tr>
<tr class="even">
<td>True standard error</td>
<td></td>
<td><span class="math inline">\(\sqrt{\mathbb{V}(a^d)}\)</span></td>
</tr>
<tr class="odd">
<td>Average estimated standard error</td>
<td></td>
<td><span class="math inline">\(\widehat\sigma(A)\)</span></td>
</tr>
<tr class="even">
<td>RMSE</td>
<td>Root mean-squared-error</td>
<td><span class="math inline">\(\sqrt{\mathbb{E}(a^d - a^m)}\)</span></td>
</tr>
<tr class="odd">
<td>Coverage</td>
<td>Probability confidence interval overlaps estimand</td>
<td><span class="math inline">\(\Pr(\mathrm{covers}^{a^m})\)</span></td>
</tr>
<tr class="even">
<td>Bias‐eliminated coverage</td>
<td>Probability confidence interval overlaps average estimate (<span class="citation">Morris, White, and Crowther (<a href="references-4.html#ref-morrisetal2019">2019</a>)</span>)</td>
<td><span class="math inline">\(\Pr(\mathrm{covers}^{a^d})\)</span></td>
</tr>
<tr class="odd">
<td>Type S error rate</td>
<td>Probability estimate has incorrect sign, if statistically significant (Gelman and Carlin 2014)</td>
<td><span class="math inline">\(\Pr(\mathrm{sgn}(a^d) \neq \mathrm{sgn}(a^m) \mid p \leq \alpha)\)</span></td>
</tr>
<tr class="even">
<td>Exaggeration ratio</td>
<td>Expected ratio of absolute value of estimate to estimand, if statistically significant (Gelman and Carlin 2014)</td>
<td><span class="math inline">\(\mathbb{E}( a^d / a_m \mid p \leq \alpha)\)</span></td>
</tr>
<tr class="odd">
<td>Type I error</td>
<td></td>
<td><span class="math inline">\(\Pr(p \leq \alpha \mid a^m = a^0)\)</span></td>
</tr>
<tr class="even">
<td>Type II error</td>
<td></td>
<td><span class="math inline">\(\Pr(p \geq \alpha \mid a^m \neq a^0)\)</span></td>
</tr>
<tr class="odd">
<td>Sampling bias</td>
<td>Expected difference between population average treatment effect and sample average treatment effect <span class="math inline">\(Imai, King, and Stuart 2008\)</span>
</td>
<td><span class="math inline">\(\mathbb{E}(a^m_{\mathrm sample} - a^m_{\mathrm population})\)</span></td>
</tr>
<tr class="even">
<td>Maximum possible cost</td>
<td></td>
<td><span class="math inline">\(\max{cost}\)</span></td>
</tr>
<tr class="odd">
<td>Bayesian learning</td>
<td></td>
<td><span class="math inline">\(a^m_{\mathrm post} - a^m_{\mathrm pre}\)</span></td>
</tr>
<tr class="even">
<td>Value for money</td>
<td>Probability that a decision based on estimated effect yields net benefits</td>
<td></td>
</tr>
<tr class="odd">
<td>Success</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Minimum detectable effect (MDE)</td>
<td></td>
<td><span class="math inline">\(\mathrm{argmin}_{a^m_*} \Pr(p \leq \alpha) = 0.8\)</span></td>
</tr>
<tr class="odd">
<td>Robustness</td>
<td>Joint probability of rejecting the null hypothesis across multiple tests</td>
<td></td>
</tr>
<tr class="even">
<td>Maximum proportion of subjects harmed</td>
<td></td>
<td><span class="math inline">\(\min{\Pr(\mathrm{harm})}\)</span></td>
</tr>
</tbody>
</table></div>
<p>Not all of these diagnosands are relevant for every study. For example, in a descriptive study whose goal is to estimate the fraction of people in France who are left-handed, statistical power is irrelevant. A hypothesis test against the null hypothesis that 0 percent of the people in France are left-handed is preposterous. We know for sure that the fraction is not zero, we just don’t know its precise value. A much more important diagnosand for this study would be RMSE (root-mean-squared-error), which is a measure of how well-estimated the estimand is that incorporates both bias and variance.</p>
<p>Often, we need to look at several diagnosands in order to understand what might be going wrong. If your design exhibits “undercoverage” (e.g., your coverage is less than <span class="math inline">\(1 - \alpha\)</span>), that might be because your standard errors are too small (under-estimation of the variance in the sampling distribution) or because your point estimate is biased, or some combination of the two. In really perverse instances, you might have a biased point estimate which, thanks to overly-wide confidence intervals, just happens to get covered 95% of the time. So, when assessing coverage it’s important to look not only at point estimate bias, but you should also check that the average estimated standard error lines up with the true standard error. Alternatively, you could look at the “bias-eliminated coverage,” which assesses coverage purely in terms of confidence interval width, ignoring bias.</p>
<p>Many research design decisions involve trading off bias and variance. In trade-off settings, we may need to accept higher variance in order to decrease bias. Likewise, we may need to accept a bit of bias in order to achieve lower variance. The tradeoff is captured by mean-squared error – which is the average squared distance between <span class="math inline">\(a^d\)</span> and <span class="math inline">\(a^m\)</span>. Of course we would ideally like to have as low a mean-squared error as possible. We would like to achieve low variance and low bias simultaneously.</p>
<p>To illustrate, consider the following three designs as represented by three targets. The inquiry is the bullseye of the target. The data and answer strategies combine to generate a process by which arrows are shot towards the target. On the left, we have a very bad archer: even though the estimates are unbiased in the sense that they hit the bullseye “on average”, very few of the arrows are on target. In the middle, we have the Katniss Everdeen (the heroine of the Hunger Games novels who is good with a bow) of data and answer strategies: they are both on target and low variance. On the right, we have an archer who is very consistent (low variance) but biased. The mean squared error is highest on the left and lowest in the middle.</p>
<div class="inline-figure"><img src="book_files/figure-html/unnamed-chunk-115-1.png" width="100%"></div>
<p>The archery metaphor is common in research design textbooks because it effectively conveys the difference between variance and bias, but it does elide an important point. It really matters <strong>which target</strong> your archer is shooting at. Figure <a href="#fig:archerydual"><strong>??</strong></a> shows a bizarre double-target representing two inquiries. The empirical strategy is unbiased and precise for the left inquiry, but it is clearly biased for the right inquiry. When we are describing the properties of <span class="math inline">\(a^d\)</span>, we have to be clear about which <span class="math inline">\(a^a\)</span> they are associated with.</p>
<div class="inline-figure"><img src="book_files/figure-html/archerydual-1.png" width="100%"></div>
<p>RMSE is an exactly equal weighting of variance and bias. Yet many other weightings of these two diagnosands are possible, and different researchers will vary in their weightings. Those weights may also depend on the research question a researcher is studying, their career stage, the strength of priors, the size of the effects, and other features.</p>
<p>In evaluating a research design diagnosis, what we need to know is a researcher’s weighting of all relevant diagnosands. You can think of this as your utility unction. Your utility function includes how important it is to you to study big questions, to shift beliefs in a research field, to overturn established findings, to obtain unbiased answers, and to get the sign right. Your utility function evaluated for a given design will yield a utility and these can be compared across designs (this is the process of redesign, described in detail in the next section).</p>
<p>We often consider the diagnosand power on its own. This diagnosand is the probability of getting a statistically significant result, which of course depends on many things about your design including, crucially, the unknown magnitude of the parameter to be estimated. You can think of statistical power as the probability of a success, where success is defined as getting a significant results. The conventional power target is 80% power. One could imagine redefining statistical power as “null risk,” or the probability of obtaining a null result. In these terms, the conventional power target is a 20% null risk, or a one in five chance of “failure.” Those odds aren’t great, so we recommend designing studies with lower null risk. But considering power alone is also misleading: no researcher wants to design a study that is 80% powered but which returns highly biased estimates that are 2-3x the true estimate. Another way of saying this is that researchers always carry about power and bias. How much they care about each feature determines the weight of power and bias in their utility function.</p>
<p>Diagnosands need not be about hypothesis testing or even statistical analysis of the data at all. We often tradeoff how much we learn from a research design with its cost in terms of money and our time. We have financial and time budgets that provide hard constraints to our designs, but we also at the margin many researchers wish to select cheaper (or shorter) designs in order to carry out more studies or finish their degree sooner. Time and cost are also diagnostic statistics! We may wish to explore the maximum cost of a study or the maximum amount of time it would take.</p>
<p>Ethical considerations also often enter the process of assessing research designs, if implicitly. We can explicitly incorporate them into our utility function by valuing minimizing harm and maximimizing the degree of informed consent requested of subjects. When collecting, researchers often believe that they face a tradeoff between informing subjects about the subject of the data collection (an ethical consideration, or a requirement of the IRB) on the one hand and the bias that comes from Hawthorne or demand effects. We can incorporate these considerations in a research design diagnosis by specifying diagnostic statistics related to the amount of disclosure about the purposeses of research or the number of subjects harmed in the research.</p>
<!-- Suppose your study would be a success if it produced an estimate that is higher than 4, had a standard error of 1 or less, and yielded statistically significant evidence of treatment effect heterogeneity -- write down a diagnosand that is the probability of all three of those conditions being true at the same time. If a study is a failure (in terms of not having been worth the money and effort *ex post*) when the confidence interval is 20 points wide or wider, then the research team should design to minimize the probability of that occurance. -->
</div>
<div id="diagnosing-with-respect-to-variations-in-m" class="section level2">
<h2>
<span class="header-section-number">10.4</span> Diagnosing with respect to variations in M<a class="anchor" aria-label="anchor" href="#diagnosing-with-respect-to-variations-in-m"><i class="fas fa-link"></i></a>
</h2>
<p>We are always uncertain about M – if we were certain of M (or there was no real dispute about it), there would be no need to conduct new empirical research about it. Research design diagnosis can account for this uncertainty by evaluating the performace of the design under alternative models. We are unsure of the exact value of the intra-cluster correlation of outcomes we will encounter, so we simulate the variance of the estimator under a range of plausible ICC values. We are unsure of the true average treatment effect, so we simulate the power of the study over a range of plausible effect sizes. Uncertainty over model inputs like the means, variances, and covariances in data that will eventually be collect is a major reason to simulate under a range of plausible values.</p>
<div id="estimating-the-minimum-detectable-effect-size" class="section level3">
<h3>
<span class="header-section-number">10.4.1</span> Estimating the minimum detectable effect size<a class="anchor" aria-label="anchor" href="#estimating-the-minimum-detectable-effect-size"><i class="fas fa-link"></i></a>
</h3>
</div>
<div id="adjudicating-between-competing-models" class="section level3">
<h3>
<span class="header-section-number">10.4.2</span> Adjudicating between competing models<a class="anchor" aria-label="anchor" href="#adjudicating-between-competing-models"><i class="fas fa-link"></i></a>
</h3>
<p>We can apply the same principle to <em>competing</em> models. Imagine that you believe <span class="math inline">\(M_1\)</span> is true but that your scholarly rival believes <span class="math inline">\(M_2\)</span>. In the spirit of scientific progress, you design a study together. The design should (A) demonstrate <span class="math inline">\(M_1\)</span> is true if it is true and (B) demonstrate <span class="math inline">\(M_2\)</span> is true if <em>it</em> is true. In order to come agreement about the properties of the design, you will need to simulate the design under both models.</p>
</div>
</div>
<div id="further-reading-5" class="section level2">
<h2>
<span class="header-section-number">10.5</span> Further reading<a class="anchor" aria-label="anchor" href="#further-reading-5"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li>
<span class="citation">Gelman and Carlin (<a href="references-4.html#ref-Gelman2014b">2014</a>)</span> on Type M and Type S errors</li>
<li>
<span class="citation">Herron and Quinn (<a href="references-4.html#ref-herron2016careful">2016</a>)</span> on case selection / sampling bias</li>
<li>
<span class="citation">Baumgartner and Thiem (<a href="references-4.html#ref-Baumgartner2019">2017</a>)</span> and <span class="citation">Rohlfing (<a href="references-4.html#ref-rohlfing2018power">2018</a>)</span> on diagnosands in qualitative research</li>
<li>
<span class="citation">Rubin (<a href="references-4.html#ref-Rubin1984">1984</a>)</span> on diagnosands in Bayesian research</li>
</ul>
<!-- ## Ethics --><!-- - There are ethical diagnosands, e.g., what is the distribution of ethical costs such as loss of autonomy. --><!-- - Cite Tara's paper. Cite Lauren's paper.  --><!-- - Do a diagnosis of how many minutes of subject time are wasted by audit experiments, weigh that against the cost of demonstrating outright racial bias. (cite an audit study that does this calculation) --><!-- ## grab bag --><!-- <!-- example of when there is a mismatch between Am and Ad --><p>–&gt;</p>
<!-- ```{r, echo = FALSE} -->
<!-- ATE <- 0.0 -->
<!-- design <-  -->
<!--   declare_population(N = 1000, -->
<!--                      binary_covariate = rbinom(N, 1, 0.5), -->
<!--                      normal_error = rnorm(N)) + -->
<!--   # crucial step in POs: effects are not heterogeneous -->
<!--   declare_potential_outcomes(Y ~ ATE * Z + normal_error) + -->
<!--   declare_assignment(prob = 0.5) + -->
<!--   declare_estimator(Y ~ Z, subset = (binary_covariate == 0), label = "CATE(0)") +  -->
<!--   declare_estimator(Y ~ Z, subset = (binary_covariate == 1), label = "CATE(1)") + -->
<!--   declare_estimator(Y ~ Z * binary_covariate,  -->
<!--                     model = lm_robust, term = "Z:binary_covariate", label = "Interaction") -->
<!-- ``` -->
<!-- ```{r, echo = FALSE, purl = FALSE} -->
<!-- # note this was rerun a bunch of times to get the right example (one is non sig the other is sig diff and diff-in-CATE is not diff from zero) -->
<!-- # estimates <- draw_estimates(design) -->
<!-- rds_file_path <- paste0(get_dropbox_path("answer_strategy"), "/estimates_diff_in_significance_plot.RDS") -->
<!-- # write_rds(estimates, path = rds_file_path) -->
<!-- estimates <- read_rds(rds_file_path) -->
<!-- ``` -->
<!-- ```{r, echo = FALSE, fig.height = 3} -->
<!-- g1 <- ggplot(data = estimates %>% filter(term == "Z"), aes(estimator_label, estimate)) +  -->
<!--   geom_point() +  -->
<!--   geom_errorbar(aes(x = estimator_label, ymin = conf.low, ymax = conf.high), width = 0.2) +  -->
<!--   ylab("Estimate (95% confidence interval)") + -->
<!--   geom_hline(yintercept = 0, lty = "dashed") + -->
<!--   ggtitle("Visualization A") + -->
<!--   dd_theme() +  -->
<!--   theme(axis.title.x = element_blank()) -->
<!-- g2 <- ggplot(data = estimates, aes(estimator_label, estimate)) +  -->
<!--   geom_point() +  -->
<!--   geom_errorbar(aes(x = estimator_label, ymin = conf.low, ymax = conf.high), width = 0.2) +  -->
<!--   ylab("Estimate (95% confidence interval)") + -->
<!--   geom_hline(yintercept = 0, lty = "dashed") + -->
<!--   ggtitle("Visualization B") + -->
<!--   dd_theme() +  -->
<!--   theme(axis.title.x = element_blank()) -->
<!-- g1 + g2 -->
<!-- ``` -->
<!-- We now demonstrate that the answer strategy on the left is flawed. XXYY describe sims. -->
<!-- ```{r, echo = FALSE} -->
<!-- # sweep across all ATEs from 0 to 0.5 -->
<!-- designs <- redesign(design, ATE = seq(0, 0.5, 0.05)) -->
<!-- ``` -->
<!-- ```{r, echo = FALSE, eval = do_diagnosis & !exists("do_bookdown")} -->
<!-- simulations_one_significant_not_other <- simulate_design(designs, sims = sims) -->
<!-- ``` -->
<!-- ```{r, echo = FALSE, purl = FALSE} -->
<!-- # figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file -->
<!-- rds_file_path <- paste0(get_dropbox_path("answer_strategy"), "/simulations_one_significant_not_other.RDS") -->
<!-- if (do_diagnosis & !exists("do_bookdown")) { -->
<!--   write_rds(simulations_one_significant_not_other, path = rds_file_path) -->
<!-- } -->
<!-- simulations_one_significant_not_other <- read_rds(rds_file_path) -->
<!-- ``` -->
<!-- ```{r, eval = FALSE, echo = FALSE, fig.height = 3.5} -->
<!-- # Summarize simulations --------------------------------------------------- -->
<!-- reshaped_simulations <- -->
<!--   simulations_one_significant_not_other %>% -->
<!--   transmute(ATE, -->
<!--             sim_ID, -->
<!--             estimator_label, -->
<!--             estimate, -->
<!--             conf.high, -->
<!--             conf.low, -->
<!--             significant = p.value < 0.05) %>% -->
<!--   pivot_wider(id_cols = c("ATE", "sim_ID"), names_from = "estimator_label", values_from = c("estimate", "conf.high", "conf.low", "significant")) -->
<!-- # Plot 1 ------------------------------------------------------------------ -->
<!-- gg_df <-  -->
<!--   reshaped_simulations %>% -->
<!--   group_by(ATE) %>% -->
<!--   summarize(`Significant for one group but not the other` = mean(xor(significant_CATE_0, significant_CATE_1)), -->
<!--             `Difference in subgroup effects is significant` = mean(significant_interaction)) %>% -->
<!--   gather(condition, power, -ATE) -->
<!-- ggplot(gg_df, aes(ATE, power, color = condition)) + -->
<!--   geom_point() + -->
<!--   geom_line() + -->
<!--   geom_label(data = (. %>% filter(ATE == 0.2)), -->
<!--              aes(label = condition), -->
<!--              nudge_y = 0.02, -->
<!--              family = "Palatino") + -->
<!--   dd_theme() + -->
<!--   scale_color_manual(values = c("red", "blue")) + -->
<!--   theme(legend.position = "none") + -->
<!--   labs( -->
<!--     x = "True constant effect size", -->
<!--     y = "Probability of result (akin to statistical power)" -->
<!--   ) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- report_lower_p_value <- function(data){ -->
<!--   fit_nocov <- lm_robust(Y ~ Z, data) -->
<!--   fit_cov <- lm_robust(Y ~ Z + X, data) -->
<!--   # select fit with lower p.value on Z -->
<!--   if(fit_cov$p.value[2] < fit_nocov$p.value[2]){ -->
<!--     fit_selected <- fit_cov -->
<!--   } else { -->
<!--     fit_selected <- fit_nocov -->
<!--   } -->
<!--   fit_selected %>% tidy %>% filter(term == "Z") -->
<!-- } -->
<!-- design <- -->
<!--   declare_population(     -->
<!--     N = 100, X = rbinom(N, 1, 0.5), u = rnorm(N) -->
<!--   ) +  -->
<!--   declare_potential_outcomes(Y ~ 0.25 * Z + 10 * X + u) +  -->
<!--   declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +  -->
<!--   declare_assignment(prob = 0.5) +  -->
<!--   declare_reveal(Y, Z) +  -->
<!--   declare_estimator(Y ~ Z, model = lm_robust, label = "nocov", estimand = "ATE") +  -->
<!--   declare_estimator(Y ~ Z, model = lm_robust, label = "cov", estimand = "ATE") +  -->
<!--   declare_estimator( -->
<!--     handler = label_estimator(report_lower_p_value), -->
<!--     label = "select-lower-p-value", -->
<!--     estimand = "ATE")  -->
<!-- diags <- diagnose_design(design, sims = sims) -->
<!-- ``` -->
<!-- ```{r, echo = FALSE} -->
<!-- kable(get_diagnosands(diags)) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- bivariate_correlation_decision <- function(data) { -->
<!--   fit <- lm_robust(y2 ~ y1, data) %>% tidy %>% filter(term == "y1") -->
<!--   tibble(decision = fit$p.value <= 0.05) -->
<!-- } -->
<!-- interacted_correlation_decision <- function(data) { -->
<!--   fit <- lm_robust(y2 ~ y1 + x, data) %>% tidy %>% filter(term == "y1") -->
<!--   tibble(decision = fit$p.value <= 0.05) -->
<!-- } -->
<!-- robustness_check_decision <- function(data) { -->
<!--   main_analysis <- bivariate_correlation_decision(data) -->
<!--   robustness_check <- interacted_correlation_decision(data) -->
<!--   tibble(decision = main_analysis$decision == TRUE & robustness_check$decision == TRUE) -->
<!-- } -->
<!-- robustness_checks_design <-  -->
<!--   declare_population( -->
<!--     N = 100, -->
<!--     x = rnorm(N), -->
<!--     y1 = rnorm(N), -->
<!--     y2 = 0.15 * y1 + 0.01 * x + rnorm(N) -->
<!--   ) + -->
<!--   declare_estimand(y1_y2_are_related = TRUE) +  -->
<!--   declare_estimator(handler = label_estimator(bivariate_correlation_decision), label = "bivariate") +  -->
<!--   declare_estimator(handler = label_estimator(robustness_check_decision), label = "robustness-check") -->
<!-- decision_diagnosis <- declare_diagnosands(correct = mean(decision == estimand), keep_defaults = FALSE) -->
<!-- diag <- diagnose_design(robustness_checks_design, sims = sims, diagnosands = decision_diagnosis) -->
<!-- ``` -->
<!-- We evaluate the two answer strategies in terms of the rate of correctly deciding there is a correlation between `y2` and `y1`. In the main analysis, this means we judge there is a correlation when the p-value is below $0.05$. In our robustness check answer strategy, we decide there is a correlation when both the main analysis and the robustness check return p-values below $0.05$ on the coefficient on `y1`. We see that we are more likely to correctly judge there is a correlation in the simpler analysis strategy. This is because we added an additional criterion to our decision; both criteria, due to random noise, sometimes fail to reject the null of no correlation. Our second answer strategy is more robust in the sense that we have stronger evidence of a correlation when we run the two analyses together. But we are also less likely to decide (correctly) that there is a relationship. The robustness check is conservative. This exercise highlights that the properties of an answer strategy with secondary analyses will be different than the properties of the main analysis alone. If we planned (or conducted) robustness checks, we may wish to know how good the pair of strategies is together. -->
<!--

```r
robustness_checks_design <-
  robustness_checks_design +
  declare_estimator(handler = label_estimator(interacted_correlation_decision), label = "interacted")

robustness_checks_design_dgp2 <- replace_step(
  robustness_checks_design,
  step = 1,
  new_step = 
    declare_population(
      N = 100,
      x = rnorm(N),
      y1 = rnorm(N),
      y2 = 0.15 * y1 + 0.01 * x + 0.05 * y1 * x + rnorm(N)
    )
)

robustness_checks_design_dgp3 <- replace_step(
  robustness_checks_design,
  step = 1,
  new_step = 
    declare_population(
      N = 100,
      x = rnorm(N),
      y1 = 0.15 * x + rnorm(N),
      y2 = 0.15 * x + rnorm(N)
    )
)

robustness_checks_design_dgp3 <- replace_step(
  robustness_checks_design_dgp3, 
  step = 2,
  new_step = declare_estimand(y1_y2_are_related = FALSE)
)

decision_diagnosis <- declare_diagnosands(correct = mean(decision == estimand), keep_defaults = FALSE)

diag <- diagnose_design(
  robustness_checks_design, robustness_checks_design_dgp2, robustness_checks_design_dgp3, 
  sims = sims, diagnosands = decision_diagnosis)
```
-->

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
<!-- start post here, do not edit above -->
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="choosing-an-answer-strategy.html"><span class="header-section-number">9</span> Choosing an answer strategy</a></div>
<div class="next"><a href="redesign-1.html"><span class="header-section-number">11</span> Redesign</a></div>
</div></main><div id="on-this-page-nav" class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#p2diagnosis"><span class="header-section-number">10</span> Diagnosis</a></li>
<li><a class="nav-link" href="#estimating-diagnosands-analytically"><span class="header-section-number">10.1</span> Estimating diagnosands analytically</a></li>
<li>
<a class="nav-link" href="#estimating-diagnosands-via-simulation"><span class="header-section-number">10.2</span> Estimating diagnosands via simulation</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#interpreting-diagnosands"><span class="header-section-number">10.2.1</span> Interpreting diagnosands</a></li>
<li><a class="nav-link" href="#how-many-simulations-to-run"><span class="header-section-number">10.2.2</span> How many simulations to run</a></li>
</ul>
</li>
<li><a class="nav-link" href="#diagnosands"><span class="header-section-number">10.3</span> How to choose among diagnosands</a></li>
<li>
<a class="nav-link" href="#diagnosing-with-respect-to-variations-in-m"><span class="header-section-number">10.4</span> Diagnosing with respect to variations in M</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#estimating-the-minimum-detectable-effect-size"><span class="header-section-number">10.4.1</span> Estimating the minimum detectable effect size</a></li>
<li><a class="nav-link" href="#adjudicating-between-competing-models"><span class="header-section-number">10.4.2</span> Adjudicating between competing models</a></li>
</ul>
</li>
<li><a class="nav-link" href="#further-reading-5"><span class="header-section-number">10.5</span> Further reading</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Research Design: Declare, Diagnose, Redesign</strong>" was written by Graeme Blair, Jasper Cooper, Alexander Coppock, and Macartan Humphreys. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
