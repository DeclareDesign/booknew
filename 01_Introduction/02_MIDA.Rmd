---
title: "MIDA"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

```{r mida, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
library(blockTools)
```

# MIDA

Research designs are procedures we apply to the world in order to learn about it. Selecting strong research designs requires us to understand the properties of those procedures in terms of their ability to generate credible answers to theoretically important questions.  

Our understanding of research designs is rooted in the "MIDA" framework. MIDA stands for "model," "inquiry," "data strategy," and "answer strategy." Loosely speaking, the model is how we imagine the world and the inquiry is the question we want to answer about the world, given how we imagine it. The data strategy includes all the processes by which we obtain information from the world and the answer strategy includes everything we do with that information in order to generate an answer to the question. If you can describe and understand these four parts of a design, you can characterize whether the design is a good one. We think this framework applies to nearly every kind of empirical research strategy: qualitative and quantitative, observational and experimental, descriptive and causal. We hope to convince you that even though the framework is very broad and can help to illuminate many empirical approaches, it is not so flexible as to be meaningless. Understanding the distinctions and connections across these four dimensions can help sharpen and refine our procedures in order to produce better research in the future than we have done in the past.

The first two components of a design -- the model and the inquiry -- are theoretical. The model of the world that is in researchers minds may correspond to the real world more or less well, but the model is certainly not the same thing as the real world. The model simplifies the world into kinds of units, types of measurements, and beliefs about causal relationships. Models can be more or less specific. They are what is represented in the "theory" section of empirical research papers. While it may seem like empirical research is entirely agnostic and model-free, this is not so. In order to understand what the variables in a dataset even mean, researchers have to bring to bear background beliefs about the meanings of and interrelations between pieces of information. A key part of many research design processes is revealing how assumptions about the world are part of researchers' implicit theoretical models of the world.

The inquiry is a question expressed in terms of the model. Because the inquiry can only be expressed in model terms, it is also theoretical. If the model specifies that there are two kinds of people (people who are women and people who are not women) and that all people have heights, then we can ask questions like what is the median height of all people? What is the mean height of people who are women? What is the average difference in heights between those who are women and those who are not? Without concepts like "woman" and "height" that are part of the model, we cannot specify an inquiry.  



By contrast, the second two components of a research design - the data strategy and answer strategy - take place in the real world and are not theoretical. The data strategy describes how the research will obtain new information, so includes choices like case selection, sampling procedures, randomization protocols, questionaire design, participant observation techniques, and the like. The answer strategy is how you process the new information. The answer strategy is more than just the choice of estimator -- it's the full set of choices that map the data into the table and figures in the report. For quantitative designs, these choices include how "raw data" are processed into "clean data," which estimators will be applied, and which results will be presented. For qualitative designs, the answer strategy includes procedures for summarizing interviews, field notes, or original source documents. 


These four components correspond to how we imagine the world; given how we imagine the world, the questions we want answer about it; 




<!--
Strong research designs yield credible answers to important theoretical questions with a minimum of auxilliary assumptions. Weak research designs produce biased, noisy, or irrelevant answers that depend on unverfiable or incredible assumptions. 
--->






Interviews, surveys, case studies, randomized experiments, laboratory games, [etc], are all research designs

All empirical research designs gather and analyze information in order to answer a question about the world, stated in terms of a background model. 


These four features form the basis of the "MIDA" framework that we will rely on to describe designs. 



Formally, we defined a causal \textbf{model}, $M$, of how the world works, following Pearl's definition of a probabilistic causal model \citep{Pearl2009}. A casual model itself is made up of three components: 

1) The variables $X$ and their ranges. The variables in $X$ can themselves be partitioned into  endogenous ($V$) and exogenous ($U$) variables.

2) The functional relationships ($F$) between variables. The functional relationships describe how each variable in the model does or does not causally affect the others. Sometimes these functional relationships are described by potential outcomes functions.

3) A probability distribution over the exogenous variables, $P(U)$. 

Many features of a probabalistic causal model can be encoded in a Directed Acyclic Graph, or DAG. It shows the variables and the presence or absence of functional relationships between variables.  It does not encode the functional forms of those relationships or even the strength of the causal effect. It is difficult to represent heterogeneity on a DAG. The DAG does not show the $P(U)$. 


The inquiry generates the answer-under-the-model, or $a^M$. 






<!-- We make heavy use of a fairly general framework for describing the core elements of a research design. The basic idea is a research design is a specification of a problem and a strategy to answer it. We build on two influential frameworks. @kkv1994 enumerate four components of a research design: a theory, a research question, data, and an approach to using the data. @geddes2003paradigms articulates the links between theory formation, research question formulation, case selection and coding strategies, and strategies for case comparison and inference. In both cases the set of steps are closely aligned to those in the framework we propose. In the exposition, we also employ elements from @Pearl2009 approach to structural modeling, which provides a syntax for mapping design inputs to design outputs as well as the potential outcomes framework as presented, for example, in @Imbens2015, which many political scientists use to clarify their inferential targets. We characterize the design problem at a high level of generality with the central focus being on the relationship between questions and answer strategies.  -->


## Model-Inquiry-Data Strategy-Model Strategy

The specification of a problem requires a description of the world and the question to be asked about the world as described. The answering requires a description of what information is used and how conclusions are reached given the information. 

At its most basic we think of a research design, $\Delta$, as including four elements $<M,I,D,A>$:

*  A \textbf{model}, $M$, of how the world works. In general following Pearl's definition of a probabilistic causal model we will assume that a model contains three core elements. First, a specification of the variables $X$  about which research is being conducted. This includes endogenous and exogenous variables ($V$ and $U$ respectively) and the ranges of these variables. In the formal literature this is sometimes called the \textit{signature} of a model [@halpern2000].   Second, a specification of how each endogenous variable depends on other variables (the ``functional relations'' or, as in @Imbens2015, ``potential outcomes''), $F$. Third, a probability distribution over exogenous variables, $P(U)$.  

* An \textbf{inquiry}, $I$, about the distribution of variables, $X$, perhaps given interventions on some variables.  Using Pearl's notation we can distinguish between questions that ask about the conditional values of variables, such as $\Pr(X_1 | X_2 =1)$ and questions that ask about values that would arise under interventions: $\Pr(X_1 | do(X_2 = 1))$.\footnote{
    The distinction lies in whether the conditional probability is recorded through passive observation or active intervention to manipulate the probabilities of the conditioning distribution. For example, $\Pr(X_1 | X_2 =1)$ might indicate the conditional probability that it is raining, given that Jack has his umbrella, whereas $\Pr(X_1 | do(X_2 =1))$ would indicate the probability with which it would rain, given Jack is made to carry an umbrella.
  } We let $a^M$ denote the answer to $I$ \textit{under the model}. Conditional on the model, $a^M$ is the value of the estimand, the quantity that the researcher wants to learn about.  

* A \textbf{data} strategy, $D$, generates data $d$ on $X$. Data $d$ arises, under model $M$ with probability $P_M(d|D)$. The data strategy includes sampling strategies and assignment strategies, which we denote with $P_S$ and $P_Z$ respectively. Measurement techniques are also a part of data strategies and can be thought of as a selection of observable variables that carry information about unobservable variables.

* An **answer** strategy, $A$, that generates answer $a^A$ using data $d$. 


A key feature of this bare specification is that if $M$, $D$, and $A$ are sufficiently well described, the answer to question $I$ has a distribution $P_M(a^A|D)$. Moreover, one can construct a distribution of comparisons of this answer to the correct answer, under $M$, for example by assessing $P_M(a^M-a^A|D)$. One can also compare this to results under different data or analysis strategies, $P_M(a^M-a^A|D')$ and $P_M(a^M-a^{A'}|D)$, and to answers generated under alternative models, $P_M(a^{M'}-a^{A}|D)$, as long as these possess signatures that are consistent with inquiries and answer strategies.  


*MIDA* captures the analysis-relevant features of a design, but it does not describe substantive elements, such as how theories are derived or interventions are implemented. Yet many other aspects of a design that are not explicitly labeled in these features enter into this framework if they are analytically relevant. For example, logistical details of data collection such as the duration of time between a treatment being administered and endline data collection enter into the model if the longer time until data collection affects subject recall of the treatment. However, information in {\it MIDA} is typically insufficient to assess those substantive elements, an important and separate part of assessing the quality of a research study.


## Diagnosands and diagnosis

The ability to calculate distributions of answers, given a model, opens multiple avenues for assessment and critique. How good is the answer you expect to get from a given strategy? Would you do better, given some desideratum, with a different data strategy? With a different analysis strategy? How good is the strategy if the model is wrong in some way or another? 

To allow for this kind of *diagnosis* of a design, we introduce two further concepts, both functions of research designs. These are quantities that a researcher or a third party could calculate with respect to a design. 

* A {\textbf{Diagnostic Statistic}} is a summary statistic generated from a "run" of a design---that is, the results given a possible realization of variables, given the model and data strategy. A diagnostic statistic may or may not depend on the model as well as realized data. For example the statistic: $e=$ "difference between the estimated and the actual average treatment effect" depends on the model (since the ATE depends on the model's assumptions about potential outcomes). The statistic $s = \mathbb{1}(p \leq 0.05)$, interpreted as "the result is considered statistically significant at the 5\% level",'' does not depend on the model but it does presuppose an answer strategy that reports a $p$ value. 
	
	Diagnostic statistics are governed by probability distributions that arise because both the model and the data generation, given the model, may be stochastic.
	
* A *Diagnosand* is a summary of the distribution of a diagnostic statistic. For example, (expected) \emph{bias} in the estimated treatment effect is  $\mathbb{E}(e)$ and statistical \emph{power} is $\mathbb{E}(s)$. 

To illustrate, consider the following design. A model *M* specifies three variables $X$, $Y$ and $Z$ (all defined on the reals). These form the signature. In additional we assume functional relationships between them that allow for the possibility of confounding (for example, $Y =  bX + Z + \epsilon_Y; X = Z+ \epsilon_X$, with $Z, \epsilon_X, \epsilon_Z$ distributed standard normal). The inquiry $I$ is ``what would be the average effect of a unit increase in $X$ on $Y$ in the population?'' Note that this question depends on the signature of the model, but not the functional equations of the model (the answer provided by the model does of course depend on the functional equations). Consider now a data strategy, $D$, in which data is gathered on $X$ and $Y$ for $n$ randomly selected units. An answer $a^A$, is then generated using ordinary least squares as the answer strategy, $A$. 

We have specified all the components of MIDA. We now ask: How strong is this research design? One way to answer this question is with respect to the diagnosand "expected error." Here the model's functional equations provide an answer, $a^M$ to the inquiry (for any draw of $\beta$), and so the distribution of the expected error, *given the model*, $a^A-a^M$, can be calculated.  

In this example the expected performance of the design may be poor, as measured by this diagnosand, because the data and analysis strategy do not handle the confounding described by the model. In comparison, better performance may be achieved through an alternative data strategy (e.g., where $D'$ randomly assigned $X$ to $n$ units before recording $X$ and $Y$) or an alternative analysis strategy (e.g., $A'$  conditions on $Z$). These design evaluations depend on the model, and so one might reasonably ask how performance would look were the model different (for example if the underlying process involved nonlinearities). 

In all cases, the evaluation of a design depends on the assessment of a diagnosand, and comparing the diagnoses to what could be achieved under alternative designs.

In section X we discuss possible choices of diagnosands and operate a set of these

## What is a *Complete*  Design Declaration?

A declaration of a research design that is in some sense complete is required in order to implement it, communicate its essential features, and to assess its properties. Yet existing definitions make clear that there is no single conception of a complete research design: the [Consolidated Standards of Reporting Trials (CONSORT) Statement](http://www.consort-statement.org) widely used in medicine includes 22 features and other proposals range from nine to 60 components.


We propose a conditional notion of completeness: we say a design is ``diagnosand-complete'' for a given diagnosand if that diagnosand can be calculated from the declared design. Thus a design that is diagnosand complete for one diagnosand may not be for another. Consider for example the diagnosand statistical power.  Power is the probability that a *p*-value is lower than a critical value. Thus, power-completeness requires that the answer strategy return a *p* value.  It does not, however, require a well-defined estimand. In contrast, Bias- or RMSE-completeness does not require a hypothesis test, but does require the specification of an estimand. 

Diagnosand-completeness is a desirable property to the extent that it means a diagnosand can be calculated. How useful this is depends however on how useful the diagnosand is for decision making. Thus evaluating completeness should focus first on whether diagnosands for which completeness holds are indeed useful ones.

This usefulness depends in part on whether the information on which diagnoses are made is *believable*. A design may be bias-complete for instance under the assumptions of a particular spillover structure, for example. Readers may disagree with these assumptions but there are still gains from the declaration as the grounds for claims for unbiasedness are clear and the effects of deviations from model assumptions can be assessed. In practice, different research communities set different standards for what constitutes sufficient information to make such conjectures about the world plausible. 

## Declaration-Diagnosis-Redesign
