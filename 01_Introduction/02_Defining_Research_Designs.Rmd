---
title: "Defining research designs"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```


<!-- start post here, do not edit above -->

```{r definingdesigns, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 1000
b_sims <- 200
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

# Defining research designs

At its heart, a research design is a procedure for generating empirical answers to theoretical questions. Research designs can be strong or weak -- assessing whether a design is strong requires having a clear sense of what the question is and knowing whether the answers a study is likely to deliver are reliable. This book offers a language for describing research designs and an algorithm for selecting among them. In other words, it is a set of tools for weighing and describing the dozens of choices we make in our research activities that together determine whether we can provide useful answers to our questions.

Remarkably, we can use the same basic language to describe research designs whether they target causal or descriptive questions, whether they are focused on theory testing or inductive learning, and whether they use quantitative, qualitative, or mixed methods. The algorithm for selecting a strong design is declare-diagnose-redesign. Once a design is declared in simple enough language that a computer can understand it, its properties can be diagnosed at the click of a button. We can then engage in redesign, or the process of comparing among alternative designs. The same language we use to talk to the computer can be used to talk to others. Reviewers, advisors, students, funders, journalists, and the public -- and yes the computer too -- need to know four basic things to understand your design.

## The four components of research design

Empirical research designs share in common that they all have a model, an inquiry, a data strategy, and an answer strategy. The four together, which we refer to as MIDA, represent both your suppositions about how the world works and the choices you make as the researcher to intervene in and learn about the world.

**Model.** The model comprises speculations about what causes what and how. It includes (possibly quite well-informed) guesses about how important variables are distributed, how things are correlated, the sequences of events. Of course, we don't know the true model---if we did we wouldn't have to do any empirical research. We are uncertain about which of many alternative model possibilities is the true one. 

What's in a model? The model defines a set of units that we wish to study. Often, this set of units is a large population of units we cannot afford to take measurements of, but we can nevertheless define and seek to make inferences about. The units might be all of the citizens in Lagos, Nigeria; police beats in Baltimore; mayors in California; or video hall catchment areas in rural Uganda.

The model includes theoretical beliefs about baseline characteristics that describe each unit and the probability distributions of each characteristic. Finally, the model includes a set of outcome variables that may be functions of baseline characteristics and the effects of interventions. The values that an outcome variable take on depending on the level of an intervention are called *potential* outcomes. A unit's treated potential outcome arises when that unit receives the treatment; the untreated potential outcome arises when it does not. The model includes therefore includes beliefs about causal effects, which are defined with respect to these potential outcomes. The treatment effect of a particular intervention is usually defined as the difference between the treated and untreated potential outcomes, though other definitions such as the ratio of the treated to untreated potential outcome are possible too. 

The role of the model in the design declaration is to provide a setting---or rather, a set of settings---within which a design can be evaluated. We hope that the true, unknown causal model is in the set of models we entertain so that we can correctly imagine what will happen when our design is applied in the real world. 


Defining the model can feel like an odd exercise. Since researchers presumably want to learn about the model, declaring it in advance may seem to beg the question. The confusion can be allayed by thinking of the model as a kind of "reference model", against which you can assess performance. Declaring a reference model is often unavoidable when declaring research designs. In practice, doing so is already familiar to any researcher who has calculated the power of a design, which requires the specification of effect sizes. The seeming arbitrariness of the declared model can be mitigated by assessing the sensitivity of diagnosis to alternative models and strategies (see Section \@ref(p2diagnosis)). Further, researchers can inform their reference models with existing data, such as baseline surveys. Just as power calculators focus attention on minimum detectable effects, design declaration offers a tool to demonstrate design properties and how they change depending on researcher assumptions.

**Inquiry**. The research question -- what we call the inquiry -- is a feature of the model we want to measure. Our theories are rich, so in a single model, there may be many possible inquiries that a researcher could seek to learn about. A model may be a complex dance of ten or more interrelated variables, but an inquiry is something like the average causal effect of a single variable on another, the descriptive distribution of a third variable, or a prediction about the value a single variable will take on some time in the future. Important inquiries distinguish between the alternative models under consideration. If theory A predicts the an inquiry will be positive but theory B predicts it will be negative, and a credible research design leads us to conclude it is positive, then we have ruled out all models that are consistent with theory B.

Inquiries are defined with respect to units, conditions, and features; they are summaries of features of units in or across conditions. Inquiries may be causal, as in the average treatment effect (ATE). The ATE is the average difference in the outcome variable across two conditions: the treatment condition and the control condition. Inquiries can be descriptive, as in a population average of some characteristic. While it may seem that descriptive inquiries do not involve conditions, they do -- the population average is defined with respect to conditions under which we measure characteristics of our study units. So-called Hawthorne effects occur when the very act of observation changes that which is observed, underlining the general point that researchers inevitably learn about specific potential outcomes and we should be cognizant of what those are. Inquiries are functions of variables in the model. We refer to value the inquiry function takes on -- the "true answer to the question" -- as an "estimand." 

Together, the model and data form the theoretical half of a research design. The second half is empirical, and its two components mirror those in the theoretical half.

**Data strategy.** The data strategy is the full set of procedures we use to gather information from the world. This set includes three basic groups of procedures: sampling, assignment, and measurement. All data strategies involve sampling in the sense that no empirical strategy is comprehensive -- some units are sampled into the study and some units aren't. Even research designs (like the census, for example) have a sampling strategy in that they don't sample respondents in different years or different countries. Assignment procedures describe how researchers generate variation in the world. If you ask some subjects one question, but other subjects a different question, you've generated variation on the basis of an assignment procedure. We think of assignment procedures most often when they are randomized, as in a randomized experiment, but many kinds of research designs engage in assignment procedures that are not randomized. Measurement procedures are the ways in which researchers reduce the complex and multidimensional social world into a relatively parsimonious set of data. These data need not be "quantitative data" in the sense of being numbers or values on a pre-defined scale -- qualitative data are data too. Measurement is the vexing but necessary reduction of reality to a few choice representations. Measurement always carries the possibility of measurement error, because this reduction is hard. Sampling, assignment, and measurement are parallel to the three features of inquiries: units, conditions, features.

**Answer strategy.** The answer strategy is how we summarize the data that the data strategy produces. Just like the inquiry summarizes a part of the model, the answer strategy summarizes a part of the data. Complex, multidimensional datasets don't just speak for themselves -- they need to be summarized and explained. Answer strategies are functions that take in data and return answers. For some research designs, this is a literal function like `lm_robust` that estimates an ordinary least squares regression with robust standard errors. For some research designs, the function is embodied by the researchers themselves when they read documents and summarize their meanings in a case study. 

Importantly, the answer strategy is more than the choice of an estimator. It includes the full set of procedures that begin with cleaning the dataset and end with answers in words, tables, and graphs. This includes data cleaning, data transformation, estimation, plotting, and interpretation. Not only do we define our choice of OLS as the estimator, we also specify that we will focus attention on the coefficient estimate from the `Z` variable, assess uncertainty using a confidence interval, construct a coefficient plot in a certain way to visualize the inference. The answer strategy also includes all of the if-then procedures that researchers implicitly or explicitly take depending on initial results and features of the data. In a stepwise regression procedure, the answer strategy is not the final regression that results from iterative model selection, but that procedure itself -- because the answer will reflect features of that change depending on sampling variability. Just like the values of the inquiry, the values of the estimates that result from the answer strategy have a probability distribution, because they are the result of the variables defined in the model (which have probability distributions) and the data strategy (sampling and treatment assignment are defined by probability distributions). 

## Declare-Diagnose-Redesign

### Declaration

Declaring a design entails separating out which parts of your design belong in *M*, *I*, *D*, and *A*. The declaration process can be a challenge because mapping your ideas and excitement about your project into MIDA is not always straightforward. We promise it is a rewarding task. When you can express your research design in terms of these four components you are newly able to think about its properties. 

You can implement the MIDA framework in any software package -- Stata, R, Python, Julia, SPSS, SAS, Mathematica, or many others. We wrote the companion software to the book, `DeclareDesign`, in the R statistical environment because of the availability of other useful tools in R and because it is free, open-source, and high-quality. We have designed the rest of the book so that it can be read even if you do not use R, but you will have to translate the code into your own language of choice. On our Web site, we have  [pointers](https://declaredesign.org/pap) to how you might declare steps in Stata, Python, and Excel. In addition, we link to a ["Design wizard"](https://eos.wzb.eu/ipi/DDWizard/) that lets you declare and diagnose variations of standard designs via a web interface.

### Diagnosis

Once you've declared your design, you can diagnose it. Design diagnosis is the process of simulating your research design in order to understand the range of possible ways the study could turn out. It is in the diagnosis stage that we define the design properties that are most desirable in our research setting. We let computers do the simulations for us because imagining how design choices influence sampling distributions is---to put it mildly---cognitively demanding. 

Diagnosis is an opportunity to write down what would make the study a success. For a long time, researchers have classified studies as successful or not based on statistical significance. Accordingly, statistical power (the probability of a statistically significant result) has been the most front-of-mind statistic when researchers have set to designing studies. As we learn more about the pathologies of relying on statistical significance, we learn that features beyond power are just as, if not more important. For example, the "credibility revolution" throughout social science has trained a laser-like focus on bias. Studies are coming under new criticism for lacking "strong identification," which usually implies that the data and answer strategies could lead to biased answers depending on how incorrect the model is. 

Design diagnosis relies on two further concepts, both functions of research designs. These are quantities that a researcher or a third party could calculate with respect to a design. 

The first is a **diagnostic statistic,** which is a summary statistic generated from a "run" of a design---that is, the results given a possible realization of variables, given the model and data strategy. For example, the statistic $e$ refers to the "difference between the estimated and the actual average treatment effect."  The $e$ statistics depends on the model (since the ATE depends on the model's assumptions about potential outcomes). The statistic $s = \mathbb{1}(p \leq 0.05)$, interpreted as "the result is considered statistically significant at the 5% level," presupposes an answer strategy that reports a $p$-value. Diagnostic statistics are governed by probability distributions that arise because both the model and the data generation, given the model, may be stochastic.

Second, a **diagnosand** is a summary of the distribution of a diagnostic statistic. For example, *bias* is the average value of the $e$ statistic and *power* is is the average value of the $s$ statistic. Other diagnosands include things like root-mean-squared-error (RMSE), Type I, Type II, Type M, and Type S error rates. 

One especially important diagnosand is the "success rate," which is the average value of the "success" diagnostic statistic. As a researcher, *you* get to decide what would make your study a success. What matters most in your research scenario? Is it statistical significance? If so, optimize your design with respect to power. Is what matters most in your research setting with the answer has the correct sign or not? Then diagnose how frequently your answer strategy yields an answer with the same sign as your inquiry. Diagnosis is an opportunity for you to articulate what would make your study a success and then to figure out, through simulation, how often you obtain that success. Often, success is a multidimensional aggregation of multiple diagnosands.

We diagnose studies over the set of causal beliefs encoded in the model, since we want to learn the value of diagnosands under many possible scenarios. A clear example of this is the value of the power diagnosand over many possible values of the true effect size. This idea extends well beyond statistical power. Whatever the set of important diagnosands, we want to ensure that our design performs well across all plausible model possibilities.

Computer simulation is not the only way to do design diagnosis. Designs can be declared in writing or mathematical notation and then diagnosed using analytical formulae. Enormous research design progress has been made with this approach. Methodologists across the social sciences have described diagnosands such as bias, power, and root-mean-squared-error for large classes of designs. Not only can this work provide closed-form solutions for many design properties, it can also yield insights about the pitfalls to watch out for when constructing similar designs. That said, pen-and-paper diagnosis is challenging for the majority of social science research designs, first because many designs have idiosyncratic features that are hard to incorporate and second because the analytic formulae for many diagnosands have not yet been worked out by statisticians, even for very common designs (see Section \@ref(p2diagnosis)).

We are enthusiastic about the ability of diagnosis to evaluate designs based on their ex ante properties, rather than on the ex post results that a design produces. That said, design diagnosis doesn't solve every problem and like any tool, can be misused. We outline three concerns.

The first is the worry that evaluative weight could get placed on essentially meaningless diagnoses. Given that design declaration includes declarations of conjectures about the world it is possible to choose inputs so that a design passes any diagnostic test set for it. For instance, a simulation-based claim to unbiasedness that incorporates all features of a design is still only good with respect to the precise conditions of the simulation (in contrast, analytic results, when available, may extend over general classes of designs). Still worse, simulation parameters might be selected because of their properties. A power analysis, for instance, may be useless if implausible parameters are chosen to raise power artificially. While our framework may encourage more honest declarations, nothing about it enforces honesty. As ever, garbage-in, garbage-out.

Second, we see a risk that research may get evaluated on the basis of a narrow, but perhaps inappropriate set of diagnosands. Statistical power is often invoked as a key design feature -- but well-powered studies that are biased are of little theoretical use. The importance of particular diagnosands can depend on the values of other diagnosands in complex ways, so researchers should take care to evaluate their studies along many dimensions.

Third, emphasis on the statistical properties of a design can obscure the substantive importance of a question being answered or other qualitative features of a design. A similar concern has been raised, for example by @huber2013, regarding the ``identification revolution'' where a focus on identification risks crowding out attention to the importance of questions being addressed. Our framework can help researchers determine whether a particular design answers a question well (or at all), and it also nudges them to make sure that their inquiries are defined clearly and independently of their answer strategies. 

### Redesign

The subtitle of this book is "Declaration, Diagnosis, Redesign" to emphasize three important steps in the formulation of a research design. So far, we've outlined the first two points: declaration and diagnosis. Once your design has been declared, and you have learned to diagnose it with respect to the most important diagnosands, the last step is redesign.

Redesign entails tweaking parameters of the data and answer strategies to understand the implications of each for your most important diagnosands. This can mean a variety of things. Many diagnosands (power, RMSE) depend on the size of the study. We can redesign the study, varying the "sample size" feature of the data strategy to determine how big it needs to be to achieve a target diagnosand: 90% power, say, or an RMSE of 0.02. We could also vary an aspect of the answer strategy, say, the covariates used to adjust a regression model. Sometimes the changes to the data and answer strategies interact: if we use better covariates to increase the precision of the estimates in the answer strategy, we have to collect that information as a part of the data strategy. The redesign question now becomes, is it better to collect pre-treatment information from all subjects or is the money better spent on increasing the total number of subjects? 

The redesign process is mainly about optimizing your research design given ethical, logistical, and financial constraints. If related diagnosands such as total harm to subjects, total researcher hours, or total project cost exceed acceptable levels, the design is not feasible. We want to choose the best design we can among the feasible set. If the designs remaining in the feasible set are underpowered, biased, or are otherwise scientifically inadequate, the project may need to be abandoned -- something that's best learned as soon as possible. 

In our experience, it's during the redesign process that designs become **simpler**. We learn that our experiment has too many arms or that the expected level of heterogeneity is too small to be detected by our design. We learn that in our theoretical excitement, we've built a design with too many bells and too many whistles. Some of the complexity needs to be cut, or the whole design will be a muddle. The upshot of many redesign sessions is that our designs pose fewer inquiries, but obtain better answers.

### Example in words: Encouraging Political Candidacy in Pakistan

We illustrate the MIDA framework with a study of political motivations among office-seekers in Pakistan. @Gulzar2020 conducted an experiment that estimated the effects of two alternative motivations for becoming a politician: helping the community or generating personal benefits. The researchers randomly assigned villages to receive different encouragements to run and measured the rates of running for office, the types of people who chose to run, and the congruence of elected politicians' policy positions with those of the general population. 

The model for this study applies to citizens who are eligible to run for office in study villages. The model describes subjects' individual characteristics and their potential outcomes depending on which motivation treatment they receive. The alternative theories that the model encompasses is that politicians run for office only to help themselves, only to help others, neither, or both. Among theories that include room for both motivations, some claim that intrinsic motivations are more powerful than extrinsic, while other claim the reverse. We define these potential outcomes in terms of subjects' latent probability of running for office which is tightly related to the binary choice to run or not to run.

The study has two inquiries are the average treatment effects of each encouragement, defined as the average difference in potential outcomes between receiving and not receiving each encouragement to run for office. We could imagine a third inquiry that is the difference between these two average treatment effects, but we'll leave that complication to the side for the moment.

The *data strategy* includes three elements. First, we randomly sample 50 citizens who are eligible to stand for election from each village. Next, we assign subjects to a personal benefits encouragement, a prosocial encouragement, or no encouragement (control). All subjects in a village are assigned to the same treatment condition, which is to say that this experiment used cluster random assignment. Lastly, the data strategy measures the decision to run for office by checking whether a subject's name appears on the official candidate lists of released by the Election Commission of Pakistan. In contrast to the continuous latent outcome variable in the model, the outcome variable as measured in the data strategy is binary.

The *answer strategy* is the difference-in-means estimator with standard errors clustered at the village level. The clustering of the errors at the village level is an example of how choices in the answer strategy must respect choices made in the data strategy.

### Example in code: Encouraging Political Candidacy in Pakistan

We are now ready to declare the @Gulzar2020 study in code. In the model, we describe a hierarchical structure with 500 villages each of which is home to 100 citizens who are eligible to run for elected office. Each citizen harbors three potential outcomes, `Y_Z_neutral`, `Y_Z_personal`, and `Y_Z_social`. `Y_Z_neutral` is the citizen's latent probability of standing for election if treated with a neutral appeal; `Y_Z_personal` is the probability if treated with an appeal that emphasizes the personal returns to office, and `Y_Z_social` is the probability if treated with an appeal that underlines the benefits to the community. Our simplified model includes a constant treatment effect of 2 percentage points for the personal appeal and 3 percentage points for the social appeal.^[We use the `pnorm` function to define a latent variable representing the potential to run for office. We then in the measurement section measure our binary outcome, which springs from this latent outcome $Y$.]

```{r}
model <-
  declare_model(
    villages = add_level(N = 500, U_village = rnorm(N, sd = 0.1)),
    citizens = add_level(
      N = 100, 
      U_citizen = rnorm(N),
      potential_outcomes(
        Y ~ pnorm(
          U_citizen + U_village + 
            0.1 * (Z == "personal") + 0.15 * (Z == "social")),
          conditions = list(Z = c("neutral", "personal", "social"))
        )))
```

We have two inquiries, representing the average treatment effects in the population for the personal and social appeals compared to the neutral appeal, defined as the average differences in potential outcomes:

```{r}
inquiry <- declare_inquiry(
  ATE_personal = mean(Y_Z_personal - Y_Z_neutral),
  ATE_social = mean(Y_Z_social - Y_Z_neutral)
)
```

The data strategy consists of three steps: sampling, assignment, and measurement. In sampling, we sample 48 of the eligible citizens from each village into the study.  In assignment, we cluster assign one third of the villages to the neutral condition, one third to the personal appeal, and the remaining third to the social appeal. The measurement step maps the realized (but still latent!) potential outcome to the observed choice to run or not.

```{r}
n_villages <- 192
citizens_per_village <- 48

data_strategy <-
  declare_sampling(
    S_village = cluster_rs(clusters = villages, n = n_villages),
    filter = S_village == 1,
    legacy = FALSE) +
  
  declare_sampling(
    S_citizen = strata_rs(strata = villages, n = citizens_per_village),
    filter = S_citizen == 1,
    legacy = FALSE) +
  
  declare_assignment(
  	Z = cluster_ra(
  	  clusters = villages, 
  	  conditions = c("neutral", "personal", "social")),
  	legacy = FALSE) + 
  
  declare_measurement(
    Y_latent = reveal_outcomes(Y ~ Z),
    Y_observed = rbinom(N, 1, prob = Y_latent)
  )
```

The answer strategy consists of an ordinary least squares regression (as implemented by `lm_robust`) of the outcome on the treatments. The standard errors clustered at the village level in order to account for the clustering in the assignment procedure. The regression will return three coefficients: an intercept and two treatment effect estimates. We ensure that the estimates are mapped to the relevant inquiry by explicitly linking them.

```{r}
answer_strategy <- 
  declare_estimator(Y_observed ~ Z, term = c("Zpersonal", "Zsocial"), 
                    clusters = villages, 
                    model = lm_robust,
                    inquiry = c("ATE_personal", "ATE_social"))
```

When we concatenate all four elements with the `+` we get a design:

```{r}
design <- model + inquiry + data_strategy + answer_strategy
```

For most designs that we declare in the book, we will also present a graphical representation of the design. In Figure \@ref(fig:gulzarkhandag), we visualize the Gulzar-Khan design.

```{r gulzarkhandag, fig.cap = "Simplified DAG for Gulzar Khan study", fig.height = 3, fig.width = 7, echo = FALSE}
dag <- dagify(
  Y ~ Z + X + U,
  Z ~ X
)

nodes <-
  tibble(
    name = c("Y", "Z", "U", "X"),
    label = c("Y", "Z", "U", "X"),
    annotation = c(
      "**Outcome**<br>",
      "**Random assignment**<br>",
      "**Unknown heterogeneity**",
      "**villages**<br>Used for cluster assignment"),
    x = c(5, 1, 5, 1),
    y = c(2.5, 2.5, 4, 4), 
    nudge_direction = c("S", "S", "N", "N"),
    data_strategy = c("unmanipulated", "assignment", "unmanipulated", "unmanipulated"),
    answer_strategy = "uncontrolled"
  )
ggdd_df <- make_dag_df(dag, nodes)

base_dag_plot %+% ggdd_df + coord_fixed(ylim = c(2.05, 4.6), xlim = c(0.25 - epsilon, 5.75 + epsilon))
```

To diagnose the design, we first define a set of diagnosands (see Section \@ref(p2diagnosis)), which are statistical properties of the design. In this case, we select the bias (difference between the estimate and the estimand, which is the PATE); the statistical power of the design; the root mean-squared error, which is a weighted average of the bias and efficiency of the design; and its total cost. 

```{r}
diagnosands <- declare_diagnosands(
  bias = mean(estimate - estimand),
  rmse = sqrt(mean((estimate - estimand)^2)),
  power = mean(p.value <= 0.05),
  cost = mean(10 * n_villages + 1 * n_villages * citizens_per_village)
)
```

We then diagnose the design, which involves simulating the design and again and again, and then calculate the diagnosands based on the simulations data. 

```{r, eval = do_diagnosis & !exists("do_bookdown")}
diagnosis <- diagnose_design(design, diagnosands = diagnosands, sims = sims, bootstrap_sims = b_sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("02_Improving_Research_Designs"), "/simple_design_diagnosis.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnosis, file = rds_file_path)
}
diagnosis <- read_rds(rds_file_path)
```

```{r simpledesigndiagnosis, echo = FALSE}
get_diagnosands(diagnosis) %>% 
  select(inquiry_label, term, bias, rmse, power) %>%
  kable(digits = 3, caption = "Diagnosis of the simplified Gulzar-Khan design.", booktabs = TRUE)
```

The diagnosis reveals that the design is unbiased for both parameters, and that for the social treatment inquiry powered above the standard 80% threshold. However, it is not powered for the personal inquiry. This gives us a sense of what effect sizes the design is powered for, since the only difference in the design between these two inquiries is the assumed true effect size in the model. We could redesign this design to increase the sample size for the personal treatment, or use this diagnosis to better interpret the results of the study.

We redesign across possible combinations of numbers of villages and citizens per village, important design parameters in the control of the researchers: 

```{r, eval = FALSE}
designs <- redesign(design, n_villages = c(192, 295, 397, 500), citizens_per_village = c(25, 50, 75, 100))
diagnosis <- diagnose_design(designs, diagnosands = diagnosands)
```

```{r, echo = FALSE, eval = do_diagnosis & !exists("do_bookdown")}
designs <- redesign(design, n_villages = c(192, 295, 397, 500), citizens_per_village = c(25, 50, 75, 100))
diagnosis <- diagnose_design(designs, diagnosands = diagnosands, sims = sims, bootstrap_sims = b_sims)
```


```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("02_Improving_Research_Designs"), "/redesign_diagnosis.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnosis, file = rds_file_path)
}
diagnosis <- read_rds(rds_file_path)
```

In Figure \@ref(fig:gulzarkhanredesign), we illustrate the results of our redesign exercise across all four diagnosands. On the x-axis of each plot is the number of citizens per village and the y-axis the value of the diagnosand. The plot is faceted by diagnosand, and each line represents a different possible number of villages. The diagnoses are for the social treatment (we could create the same plot, one for each treatment).

What we see is that bias is invariant to these choices; our study will be unbiased, regardless of the number of villages and number of citizens we interview per village. However, our other three diagnosands do change. Power is increasing in the number of citizens per village, and always higher with more villages. We might reject designs with 192 villages with only 25 citizens per village, because they fall below the 80% power threshold (in fact, the number chosen by the researchers, 48, is just over the threshold, suggesting they chose the most cost-effective design in terms of power). Root mean-squared error, a measure capturing both bias and efficiency of the design, is improving (decreasing) in the number of citizens per village and number of villages. Cost is, of course, increasing in both sample size parameters. We can use the cost parameters to make decisions about what sample sizes to choose accounting both for scientific diagnosands of the design (i.e., power) and cost at the same time.

```{r gulzarkhanredesign, echo = FALSE}
gg_df <- diagnosis$diagnosands_df %>%
  filter(inquiry_label == "ATE_social") %>%
  pivot_longer(cols = c(bias, rmse, power, cost), names_to =  "diagnosand")

g_base <-
ggplot(data = NULL, aes(citizens_per_village, value, group = n_villages, color = n_villages)) +
  geom_line() +
  scale_color_gradient(low = dd_light_blue, high = dd_dark_blue, breaks =c(192, 295, 397, 500)) +
  coord_cartesian(xlim = c(0, 100)) + 
  dd_theme() + 
  theme(legend.key.height = unit(1.75, units = "cm"))

g1 <- g_base %+% filter(gg_df, diagnosand == "bias") + labs(x = "Citizens per village", y = "Bias", color = "Number of\nvillages") + scale_y_continuous(limits = c(-0.025, 0.025)) 
g2 <- g_base %+% filter(gg_df, diagnosand == "power") + labs(x = "Citizens per village", y = "Statistical power", color = "Number of\nvillages")  + scale_y_continuous(limits = c(0, 1)) + geom_hline(yintercept = 0.80, color = dd_pink, linetype = "dashed") + annotate("text", x = 75, y = 0.72, label = "Power threshold = 0.8", size = 3, color = dd_pink)
g3 <- g_base %+% filter(gg_df, diagnosand == "rmse") + labs(x = "Citizens per village", y = "Root mean-squared error", color = "Number of\nvillages") + scale_y_continuous(limits = c(0, 0.05))
g4 <- g_base %+% filter(gg_df, diagnosand == "cost") + labs(x = "Citizens per village", y = "Cost", color = "Number of\nvillages") 

wrap_plots(g1, g2, g3, g4, guides = "collect")
```

## Putting designs to use

The two pillars of our approach are the language for describing research designs (MIDA) and the algorithm for selecting high-quality designs (Declare-Diagnose-Redesign). Together, these two ideas can shape research design decisions throughout the lifecycle of a project. The full set of implications is drawn out in Part IV but we emphasize the most important ones here. 

Broadly speaking, the lifecycle of an empirical research project has four phases: Brainstorming, Planning, Realization, and Integration. Planning, Realization, and Integration describe what happens before, during, and after the implementation of a research design. We pre-pend a fourth phase -- Brainstorming -- to reflect the idea that research doesn't just begin with "planning" -- research ideas have to being with some spark of inspiration.

Brainstorming is the process of growing a research idea from a kernel into a skeleton specification of the model, inquiry, data strategy, and answer strategy. The inspiration for a good research project can come from many sources -- frustration with an article you're reading, a golden opportunity with a potential research partner, a conversation with a colleague (or adversary!). The spark of an idea might be some bit of a model, perhaps an inquiry in particular, maybe a portion of a data strategy, or just an itch to apply a new answer strategy you learned about. Wherever that kernel of an idea starts, the point of brainstorming is to develop all parts of M, I, D, and A enough that the design becomes a coherent whole.

After an idea has been fleshed out sufficiently, its time to start planning. Planning entails some or all of the following steps, depending on the design: conducting an ethical review, seeking IRB approval, gathering criticism from colleagues and mentors, running pilot studies, and preparing preanalysis documents. The design as encapsulated by M, I, D, and A will go through many iterations and refinements during this period. Planning is the time when frequent re-application of the Declare-Diagnose-Redesign algorithm will pay the highest dividends. How should you investigate the ethics of a study? By casting the ethical costs and benefits as diagnosands. How should you respond to (good and bad) criticism? By re-interpreting the feedback in terms of M, I, D, and A. How can you convince funders and partners that your research project is worth investing in? By credibly communicating your study's diagnosands -- its statistical power, its unbiasedness, its high chance of success, however the partner  or funder defines it. What belongs in a pre-analysis plan? You guessed it -- a specification of the model, inquiry, data strategy and answer strategy. Iterating and improving design details is the essence of good planning.

Realization is the phase of research in which all those plans are executed. You implement your data strategy in order to gather information from the world. Once that's done, you follow your answer strategy in order to finally generate answers to the inquiry. Of course, that's only if things go exactly according to plan -- which has never once happened once in our own careers. Survey questions don't work as we imagined, partner organizations over-promise and under-deliver, subjects move or become otherwise unreachable. A critic or a reviewer may insist you change your answer strategy -- or may think a different inquiry altogether is the theoretically appropriate one. You may yourself change how you think of the design as you embark on writing up the research project. It is inevitable that some features of M, I, D, and A will change during the realization phase. Some design changes have very bad properties, like sifting through the data ex-post, finding a statistically significant result, then back-fitting an new M and a new I to match the new A. This bad practice goes by the name HARKing -- Hypothesizing After Results are Known (@Kerr1998). Indeed, if we declare and diagnose this actual answer strategy (sifting through data ex-post), we can show through design diagnosis that it is badly biased (away from zero) for any of the inquiries it could end up choosing. Other changes made along the way may help the design quite a bit -- if the planned design did not include covariate adjustment but a friendly critic suggests adjusting for the pre-treatment measure of the outcome, the RMSE diagnosand might drop nicely. The point here is that design changes during the implementation process -- whether necessitated by unforeseen logistical constraints or required by the review process -- can be understood using in terms of M, I, D, and A by reconciling the planned design with the design as implemented.

When that acceptance email finally hits your inbox, you can celebrate a job well done and a design well realized. But the research design lifecycle is not finished -- the design must be integrated into the scientific community. Studies must be archived, along with design information, to prepare for reanalysis. Future scholars may well want to reanalyze your design in order to learn more than is represented in the published article or book. Good reanalysis requires a full understanding of the design as implemented, so archiving design information along with code and data is critical. Not only may your design be reanalyzed, it may also be replicated with fresh data. Ensuring that replication studies answer the same theoretical questions as original studies requires explicit design information without which replicators and original study authors may simply talk past one another.^[For a discussion of the distinctions between different modes of replication, see @Clemens2017.] Indeed, as your study is integrated into the scientific literature and beyond, you should anticipate disagreement over your claims. Resolving disputes is very difficult if parties do not share a common understanding of the research design.^[For example, much of the debate over instrumental variables designs in economics centers on which of two inquiries is the "correct" theoretical target, the average treatment effect or the local average treatment effect (@deaton2009instruments, @imbens2010better).] Finally, you should anticipate that your results will be formally synthesized with others' work via meta-analysis. Meta-analysts need design information in order to be sure they aren't inappropriately mixing together studies that ask importantly different questions or answer them too poorly to be of use.

## Further Reading

TBD

<!-- - @brady2010rethinking -->

<!-- We build on two influential research design frameworks. [@kkv1994, p. 13] enumerate four components of a research design: a theory, a research question, data, and an approach to using the data. @geddes2003paradigms articulates the links between theory formation, research question formulation, case selection and coding strategies, and strategies for case comparison and inference. In both cases, the set of components are closely aligned to those in the framework we propose. -->

<!-- Here follows the introduction from the paper: -->
<!-- As empirical social scientists, we routinely face two research design problems. First, we need to select high-quality designs, given financial and practical constraints. Second, we need to communicate those designs to readers and reviewers. To select strong designs, we often rely on rules of thumb, simple power calculators, or principles from the methodological literature that typically address one component of a design while assuming optimal conditions for others. These relatively informal practices can result in the selection of suboptimal designs, or worse, designs that are simply too weak to deliver useful answers. To convince others of the quality of our designs, we often defend them with references to previous studies that used similar approaches, with power analyses that may rely on assumptions unknown even to ourselves, or with ad hoc simulation code. In cases of dispute over the merits of different approaches, disagreements sometimes fall back on first principles or epistemological debates rather than on demonstrations of the conditions under which one approach does better than another. -->

<!-- In this paper we describe an approach to address these problems. We introduce a framework --- MIDA --- that asks researchers to specify information about their background model (M), their inquiry (I), their data strategy (D), and their answer strategy (A). We then introduce the notion of "diagnosands," or quantitative summaries of design properties. Familiar diagnosands include statistical power, the bias of an estimator with respect to an estimand, or the coverage probability of a procedure for generating confidence intervals.  -->

<!-- Many aspects of design quality can be assessed through design diagnosis, but many cannot. For instance the contribution to an academic literature, relevance to a policy decision, and impact on public debate are unlikely to be quantifiable ex ante. -->

<!-- Using this framework, researchers can declare a research design as a computer code object and then diagnose its statistical properties on the basis of this declaration. A researcher may declare the features of designs in our framework for their own understanding and declaring designs may be useful before or after the research is implemented. Researchers can declare and diagnose their designs with the companion software for this paper, DeclareDesign, but the principles of design declaration and diagnosis do not depend on any particular software implementation. -->

<!-- The formal characterization and diagnosis of designs before implementation can serve many purposes. First, researchers can learn about and improve their inferential strategies. Done at this stage, diagnosis of a design and alternatives can help a researcher select from a range of designs, a process we call "redesign." Later, a researcher may include design declaration and diagnosis as part of a preanalysis plan or in a funding request. At this stage, the full specification of a design serves a communication function and enables third parties to understand a design and an authorâ€™s intentions. Even if declared ex post, formal declaration still has benefits. The characterization can help readers understand the properties of a research project, facilitate transparent replication, and can help guide future (re-)analysis of the study data. -->





<!-- Together, the language and the algorithm help researchers address two main problems.  -->

<!-- First, they have to select high-quality research designs that can be relied upon to generate credible answers to their research questions. Without a way to measure the quality of design, it's very difficult to choose strong ones over weak ones. Second, researchers need to communicate their designs to others. Without a way to describe a design in detail, it's very difficult to explain to other scholars why they are of high quality and why they are the right design for the question.  -->

<!-- Our language for research designs helps with both problems. -->

<!-- -->



<!-- ^[MIDA is of course itself a model, we present it here as a simple four step sequence though as will become clear in applications in some cases some of these steps are barely visible, some repeat, and, within a given project, the order of steps can be complex.]  -->


<!-- An inquiry is a function of the exogenous characteristics of units, of endogenous outcome variables, or both. It may be defined over all units in the population defined by the model, as in the average treatment effect for all units, or it may be defined over a subset of units, as in the conditional average treatment effect for women. Because we defined the distribution of the variables in the model, we can define the probability distribution of inquiries, which are a function of those variables.  -->


<!-- How can we rule out theoretical models? First, we need to enumerate the many models that are possible, at least in theory. The exhaustive enumeration of possible theoretical models is a daunting challenge. We describe below an approach that begins with a "kernel" -- a small portion of a theoretical model from which we grow theoretical possibilities. Suppose we have a list of possibilities in hand. A research question -- what we call an "inquiry" -- refers to a fact that, if known, would rule out some models in favor of others. If we can show through a credible research design that $Z$ causes $Y$, we can rule out all models in which $Z$ does not cause $Y$. The *reason* that we seek to learn about inquiries is to distinguish among theoretical possibilities. -->



<!-- Typically, endogenous outcome variables are random variables, either because they are a function of an exogenous baseline variable for which we defined a probability distributions or because assignment to treatment or control is random as part of the data strategy. -->

<!-- # we should turn this into a picture labeling MIDA -->