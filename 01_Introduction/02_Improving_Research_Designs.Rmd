---
title: "Better Design"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

```{r mida, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

# Improving research designs

This book offers a language for describing research designs and an algorithm for assessing their properties. Together, the language and the algorithm help researchers address two main problems. First, they have to select high quality research designs that can be relied upon to generate credible answers to their research questions. Without a way to measure the quality of design, it's very difficult to choose strong ones over weak ones. Second, researchers need to communicate their designs to others. Without a way to describe a design in detail, it's very difficult to explain to other scholars why they are high quality and why they are the right design for the question. Our language for research design helps with both problems: once a design is "declared" in simple enough language that a computer can understand it, then we can implement our "diagnosis" algorithm for assessing the properties of a design at the click of a button. This makes it easy to compare among alternative designs and pick a strong one. The same language we use to talk to the computer can be used to talk to others: reviewers, advisors, students, funders, journalists, and the public -- and yes the computer too -- need to know four basic things to understand your design.

## The four components of research design

Empirical research designs share in common that they all have a model, an inquiry, a data strategy, and an answer strategy. 

The model is your theory of the system under study. It includes causal beliefs about what causes what and why. It includes beliefs about how important variables are distributed, how things are correlated, the sequences of events. Of course we do not know the true model---if we did we would not have to do any further research. That's not the point. The point is that by  *stipulating*  a model we can assess how our analysis would perform if the model were in fact correct. In practice, the models we describe are *sets* of models, and our hope only is that the world (the real model!) is sufficiently "like" the models in this set.

The model is \textit{not} the research question. The research question -- what we're calling the inquiry -- is a feature of the model we want to measure. Our theories are rich, so in a single model, there may be many possible inquiries that a researcher could seek to learn about. A model may be a complex dance of 10 or more interrelated variables, but an inquiry is something like the average causal effect of a single variable on another, the descriptive distribution of a third variable, or a prediction about the value a single variable will take on some time in the future. 

Together, the model and data form the theoretical half of a research design. The second half is empirical, and its two components mirror those in the theoretical half.

The data strategy is the full set of procedures we use to gather information from the world. This includes three basic sets of procedures: sampling, assignment, and measurement. Sampling refers to the fact that no empirical strategy is comprehensive -- some units are sampled into the study and some units aren't. Even research designs (like the census, for example) have a sampling strategy in that they don't sample respondents in different years or in different countries. Assignment procedures describe how researchers generate variation in the world. If you ask some subjects one question, but other subjects a different question, you've generated variation on the basis of an assignment procedure. We think of assignment procedures most often when they are randomized, as in an randomized experiment, but many kinds of research designs engage in assignment procedures that are not randomized. Measurement procedures are the ways in which researchers reduce the complex and multidimensional social world in to a relatively parsimonious set of data. These data need not be "quantitative data" in the sense of being numbers or values on a pre-defined scale -- qualitative data are data too -- but measurement is the vexing but necessary reduction of reality to a few choice representations. Measurement always carries with the possibility of measurement error, because this reduction is hard. 

The answer strategy is how you summarize the data that the data strategy produces. Just like the inquiry summarizes a part of the model, the answer strategy summarizes the data. Complex, multidimensional datasets don't just speak for themselves -- they need to be summarized and explained. You can think of answer strategy as function that takes in data and returns answers. For some research designs, this is a literal function like `lm_robust` that estimates an ordinary least squares regression with robust standard errors. For some research designs, the function is embodied by the researcher themself when they read case study documents and summarize their meaning. 

The theoretical and empirical halves of research design go hand-in-hand and no description of a research design is complete unless both halves have been communicated. If someone asks, "What's your research design?" and you respond "It's an regression discontinuity design," we've maybe learned what your answer strategy might be, but we don't yet have enough information to decide whether it's a strong design until we learn more about the model and the inquiry and whether the data and answer strategies are indeed well suited for them.

* a *data strategy* to intervene in the world to learn an answer to the inquiry, including interventions that are implemented but also measurement that is collected, and

* an *answer strategy* that defines how we construct an answer to the inquiry from the data that results from implementing our data strategy. 

The four together, which we refer to as MIDA, represent both your suppositions about how the world works and the choices you make as the researcher to intervene in and learn about the world. 

The model defines a set of units, people or neighborhoods or social groups, that we wish to study. Often, this set of units is a large population of units we cannot afford to take measurements of, but we can nevertheless define and make inferences about through sampling and studying a subset of its units. The model then includes a set of baseline characteristics that describe each unit and the probability distributions of each characteristic (i.e., are heights normally distributed, or is there skew that comes from stunting in infants). Finally, the model includes a set of endogenous outcome variables that may be functions of exogenous (pretreatment) characteristics and the effects of interventions. Each endogenous outcome variable has a function that defines the variables that affect what values it takes on. When an outcome depends on an intervention, it will be a potential outcome, where we can define what value the outcome would take on if a unit received the treatment and what outcome that unit would take on if it did not receive the treatment. Typically, endogenous outcome variables are random variables, either because it is a function of an exogenous baseline variable for which we defined a probability distributions or because whether a unit is assigned to treatment or control is randomly assigned as part of the data strategy.

Defining the model can feel like an odd exercise. Since researchers presumably want to learn about the model, declaring it in advance may seem to beg the question. Yet declaring a model is often unavoidable when declaring research designs. In practice, doing so is already familiar to any researcher who has calculated the power of a design, which requires the specification of effect sizes. The seeming arbitrariness of the declared model can be mitigated by assessing the sensitivity of diagnosis to alternative models and strategies (see Section XX). Further, researchers can inform their substantive models with existing data, such as baseline surveys. Just as power calculators focus attention on minimum detectable effects, design declaration offers a tool to demonstrate design properties and how they change depending on researcher assumptions.

The second component of a research design is the inquiry, often known as the estimand. This is a quantity that represents the true answer to the research question we are asking. Inquiries may be causal, as in the average treatment effect, or descriptive, as in the proportion of units who hold a certain characteristic. An inquiry is a function of the exogenous characteristics of units, of endogenous outcome variables, or both. It may be defined over all units in the population defined by the model, as in the average treatment effect for all units, or it may be defined over a subset of units, as in the conditional average treatment effect for women. Because we defined the distribution of the variables in the model, we can define the probability distribution of inquiries, which are a function of those variables. 

The data strategy defines how the researcher, along with research partners, intervenes in the world to generate an answer to the question posed by the inquiry. The researcher must select a sampling strategy for measurement, which could be to take measurements of all units or to first sample a subset of units. It includes the treatments and treatment assignment procedures. And it includes the measurement procedure itself, defining the set of survey questions or administrative data that will be collected from selected units. In short, the data strategy is everything the researcher does to obtain a set of data or observations used to answer the inquiry about the world.

With the data that results from the data strategy, the answer strategy defines a set of procedures the researcher uses to translate the data into an answer to the inquiry. It is not simply the choice of an estimator, such as OLS or logit, but the full set of procedures from receiving the dataset to providing the answer in words, tables, and graphs. This includes data cleaning, data transformation, estimation, plotting, and interpretation. Not only the choice of OLS must be defined, but that we will focus attention on the coefficient estimate from the `Z` variable and assess uncertainty using a confidence interval and construct a coefficient plot in a certain way to visualize the inference. The answer strategy also includes all of the if-then procedures that researchers implicitly or explicitly take depending on initial results and features of the data. In a stepwise regression procedure, the answer strategy is not the final regression that results from iterative model selection, but that procedure itself -- because the answer will depend on features of that change depending on sampling variability. Just like the values of the inquiry, the values of the estimates that result from the answer strategy have a probability distribution, because they are the result of the variables defined in the model (which have probability distributions) and the data strategy (sampling and treatment assignment are defined by probability distributions). 

Declaring a design entails separating out which parts of your idea belong in $M$, $I$, $D$, and $A$. The declaration process can be a challenge because mapping your ideas and excitement about your project into MIDA is not always straightforward. We promise it is a rewarding task. When you can express your research design in terms of these four components you are newly able to think about its properties. 

### Example

We illustrate the application of the MIDA framework to a study of the motivations of political office-seekers in Pakistan. @Gulzar2020 conducted a experiment to test whether prosocial or personal benefits were more important motivations for running for office. They randomly-assigned villages to receive different encouragements to run and measured how this affected the rate of running for office, the types of people who chose to run, and the congruence of their policy positions with the general population. 

The *model* describes the citizens in the villages they conduct the study, and which village they live in. It describes both their individual characteristics and their potential outcomes in response to the possible treatment assignments. The *inquiry* is the average treatment effect between receiving and not receiving an encouragement to run for office. The *data strategy* is (1) randomly sampling 50 citizens per village; (2) random assignment of villages into either an encouragement to run for office (treatment) or no encouragement (control); and (3) measurement via a posttreatment survey. The *answer strategy* is calculating the difference-in-means, with standard errors accounting for the clustering of the treatment assignment by village.

<!-- MIDA captures the analysis-relevant features of a design, but it does not describe substantive elements, such as how theories are derived, how interventions are implemented, or even, qualitatively, how outcomes are measured. Yet many other aspects of a design that are not explicitly labeled in these features enter into this framework if they are analytically relevant. For example, if treatment effects decay, logistical details of data collection (such as the duration of time between a treatment being administered and endline data collection) may enter into the model. Similarly, if a researcher anticipates noncompliance, substantive knowledge of how treatments are taken up can be included in many parts of the design. -->

## Declaring a design in code

You can implement the MIDA framework in any software package. Indeed, a design could be declared in writing or mathematical notation and then diagnosed using analytical formula.^[However, we suggested in Section XX why analytical diagnoses may not be ideal for typical designs in the social sciences: they do not account for the specific features of research designs such as varying numbers of units per cluster and the interaction of choices about a data strategy and an answer strategy.] 

Social scientists use [a number of tools](https://stackoverflow.blog/2017/10/10/impressive-growth-r) for conducting statistical analysis: Stata, R, Python, Julia, SPSS, SAS, Mathematica, and more. Stata and R are most commonly used. We wrote the companion software to the book, `DeclareDesign`, in the R statistical environment because of the availability of other tools for implementing research designs and because it is free-to-use.

To illustrate declaring a design in code, we declare a simplified version of the @Gulzar2020 study. (In Part II we declare the complete design.)

```{r}
# we should turn this into a picture labeling MIDA
simple_design <- 
  
  # M: model
  
  # 50 citizens in each of 100 villages
  declare_population(
    # 100 villages
    villages = add_level(N = 100, N_citizens_per_village = sample(20:100, N, replace = TRUE)),
    
    # 50 citizens in each village
    citizens = add_level(N = N_citizens_per_village, u = rnorm(N))
  ) +
  
  # two potential outcomes, Y_Z_0 and Y_Z_1
  # Y_Z_0 is the control potential outcome (what would happen if the unit is untreated)
  #   it is equal to the unobserved shock 'u'
  # Y_Z_1 is the treated potential outcome 
  #   it is equal to the control potential outcome plus a treatment effect of 0.25
  declare_potential_outcomes(
    Y_Z_0 = u,
    Y_Z_1 = Y_Z_0 + 0.25) +
  
  # I: inquiry
  
  # we are interested in the average treatment effect in the population
  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +
  
  # D: data strategy
  
  # sampling: we randomly sample 50 of the 100 villages in the population
  declare_sampling(n = 50, clusters = villages) +
  
  # assignment: we randomly assign half of the 50 sampled units to treatment (half to control)
  declare_assignment(prob = 0.5, clusters = villages) +
  
  # measurement: construct outcomes from the potential outcomes named Y depending on 
  #   the realized value of their assignment variable named Z
  #   we measure a binary outcome Yobs from the unobserved, latent variable Y
  declare_measurement(
    Yobs = case_when(
      Z == 1 & Y_Z_1 > 0 ~ 1,
      Z == 1 & Y_Z_1 <= 0 ~ 0,
      Z == 0 & Y_Z_0 > 0 ~ 1,
      Z == 0 & Y_Z_0 <= 0 ~ 0)
  ) + 
                        
  declare_reveal(outcome_variables = Y, assignment_variables = Z) +
  
  # A: answer strategy
  
  # calculate the difference-in-means of Y depending on Z 
  # we link this estimator to ATE because this is our estimate of our inquiry
  declare_estimator(Y ~ Z, clusters = villages, model = difference_in_means, estimand = "ATE")
```

For each design that we declare in the book, we will also present a graphical representation of the design. Below, we visualize the simplified form of the Gulzar-Khan design:

```{r,echo=FALSE}

design <-
  declare_population(N = 100,
                     X = rbinom(N, 1, 0.3),
                     U = rnorm(N)) +
  declare_potential_outcomes(Y ~ Z + X + U) +
  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +
  declare_assignment(blocks = X, block_prob = c(0.1, 0.5)) +
  declare_estimator(Y ~ Z, estimand = "ATE", label = "Naive DIM") +
  declare_estimator(Y ~ Z,
                    blocks = X,
                    estimand = "ATE",
                    label = "Blocked DIM")

dag <- dagify(
  Y ~ Z + X + U,
  Z ~ X
)

nodes <-
  tibble(
    name = c("Y", "Z", "U", "X"),
    label = c("Y", "Z", "U", "X"),
    annotation = c(
      "**Outcome**<br>",
      "**Random assignment**<br>",
      "**Unknown heterogeneity**",
      "**villages**<br>Used for cluster assignment"),
    x = c(5, 1, 5, 1),
    y = c(2.5, 2.5, 4, 4), 
    nudge_direction = c("S", "S", "N", "N"),
    answer_strategy = "uncontrolled"
  )
ggdd_df <- make_dag_df(dag, nodes, design)

base_dag_plot %+% ggdd_df
```


With the declared design, you can learn about it in a number of ways: 

* You can "diagnose" or simulate its properties via `diagnose_design(design)`
* You can simulate data from it to explore possible estimation strategies before analyzing the real data with `draw_data(design`)
* You can obtain simulated estimates via `draw_estimates(design)`, and after the data comes in, 
* You can apply the planned design to your data via `get_estimates(design, data = study_data)`. 

We introduce the software in more detail in the next section.

We have designed the rest of the book so that it can be read even if you do not use R, but you will have to translate the code into your own language of choice. On our Web site, we have a [translation](https://declaredesign.org/pap) of parts of the declaration and diagnosis process into Stata, Python, and Excel. In addition, we link to a ["Design wizard"](https://eos.wzb.eu/ipi/DDWizard/) that lets you declare and diagnose variations of standard designs via a web interface.

## Assessing research design quality: design diagnosis

Once you've declared your design, you can diagnose it. Design diagnosis is the process of simulating your research design in order to understand the range of possible ways the study could turn out. It is in the diagnosis stage that we define the design properties that are most desirable in our research setting. We let computers do the simulations for us because imagining how design choices influence sampling distributions is --- to put it lightly --- cognitively demanding. 

Diagnosis is an opportunity to write down what would make the study a success. For a long time, researchers have classified studies as successful or not based on statistical significance. Accordingly, statistical power (the probability of a statistically significant result) has been the most front-of-mind diagnosand when researchers have set to designing studies. As we learn more about the pathologies of relying on the statistical significance, we learn that diagnosands beyond power are just as, if not more important. For example, the "credibility revolution" throughout social science has trained a laser-like focus on the bias diagnosand. Studies are coming under new criticism for lacking "strong identification," which usually implies that the data and answer strategies could lead to biased answers depending on how incorrect the model is. Randomized experimentation promises unbiased answers, at least when the data and answer strategies are implemented well.

Design diagnosis relies on two further concepts, both functions of research designs. These are quantities that a researcher or a third party could calculate with respect to a design. 

The first is a **diagnostic statistic,** which is a summary statistic generated from a "run" of a design---that is, the results given a possible realization of variables, given the model and data strategy. For example the statistic: $e=$ "difference between the estimated and the actual average treatment effect" depends on the model (since the ATE depends on the model's assumptions about potential outcomes). The statistic $s = \mathbb{1}(p \leq 0.05)$, interpreted as "the result is considered statistically significant at the 5% level," does not depend on the model but it does presuppose an answer strategy that reports a $p$-value. Diagnostic statistics are governed by probability distributions that arise because both the model and the data generation, given the model, may be stochastic.

Second, a **diagnosand** is a summary of the distribution of a diagnostic statistic. For example, \emph{bias} is average value of the $e$ statistic and \emph{power} is is the average value of the $s$ statistic. Other diagnosands include things like root-mean-squared-error (RMSE), Type I, Type II, Type M, and Type S error rates. 

One especially important diagnosand is the "success rate," which is the average value of the "success" diagnostic statistic. As a researcher, *you* get to decide what would make your study a success.  What matters most in your research scenario? Is it statistical significance? If so, optimize your design with respect to power. Is what matters most in your research setting with the answer has the correct sign or not? Then diagnose how frequently your answer strategy yields an answer with the same sign as your inquiry. Diagnosis is an opportunity for you to articulate what would make your study a success and then to simulate how often you obtain that success.

To diagnose the design, we first define a set of diagnosands (see Section XX), which are statistical properties of the design. In this case, we select the bias (difference between the estimate and the estimand, which is the PATE); the root mean-squared error; and the statistical power of the design.

```{r}
# Select diagnosands
simple_design_diagnosands <- 
  declare_diagnosands(select = c(bias, rmse, power))
```

We then diagnose the design, which involves simulating the design and again and again, and then calculate the diagnosands based on the simulations data. 

```{r}
# Diagnose the design
simple_design_diagnosis <- 
  diagnose_design(simple_design, diagnosands = simple_design_diagnosands, sims = 500)
```

```{r, echo = FALSE}
get_diagnosands(simple_design_diagnosis) %>% select(estimand_label, estimator_label, bias, rmse, power) %>% kable
```

## Redesign

The subtitle of this book is "Declaration, Diagnosis, Redesign" to emphasize three important steps in the conceptualization of a research design. So far, we've outlined the first two points: declaration and diagnosis. Once your design has been declared, and you have learned to diagnose it with respect to the most important diagnosands, the last step is redesign.

Redesign entails playing with each of the design parameters to understand the implications of each for your most important diagnosands. This can mean a variety of things. Many diagnosands (power, RMSE) depend on the size of the study. We can redesign the study, varying the "sample size" feature of the data strategy to determine how big it needs to be to achieve a target diagnosand: 90% power, say, or an RMSE of 0.02. We could also vary an aspect of the answer strategy, say, the covariates used to adjust a regression model. Sometimes the changes to the data and answer strategies interact: if we use better covariates to increase the precision of the estimates in the answer strategy, we have to collect that information as a part of the data strategy. The redesign question now becomes, is it better to collect pre-treatment information from all subjects or is the money better spent on increasing the total number of subjects? Finally, redesign sometimes means changing the model. That is, sometimes we want to understand whether our design yields the right inferences even when the underlying data generating processes shift beneath our feet. In summary, redesign entails enumerating a set of possible designs given resource and theoretical constraints then picking the best one.

In `DeclareDesign`, the `redesign()` function replaces key inputs to the design to form a new design.

## Putting designs to use

In this book, we are asking that scholars add a new step to their workflow. We want scholars to formally declare and diagnose their research designs both in order to learn about them and to improve them. Much of the work of declaring and diagnosing designs is already part of how social scientists conduct research: grant proposals, IRB protocols, preanalysis plans, and dissertation prospectuses contain design information and justifications for why the design is appropriate for the question. The lack of a common language to describe designs and their properties, however, seriously hampers the utility of these practices for assessing and improving design quality. We hope that the inclusion of a declaration-diagnosis-redesign step to the research process can help address this basic difficulty.

We outline three phases of the scientific process during which the MIDA and declaration-diagnosis-redesign framework can assist study authors, readers, and research funders.

**Making design choices**. The move towards increasing credibility of research in the social sciences places a premium on considering alternative data strategies and analysis strategies at early stages of research projects, not only because it reduces researcher discretion, but more importantly because it can improve the quality of the final research design. While there is nothing new about the idea of determining features such as sampling and estimation strategies ex ante, in practice many designs are finalized late in the research process, after data are collected. Frontloading design decisions is difficult not only because existing tools are rudimentary and often misleading, but because it is not clear in current practice what features of a design must be considered ex ante.

We provide a framework for identifying *which* features affect the assessment of a design's properties, declaring designs and diagnosing their inferential quality, and frontloading design decisions. Declaring the design's features in code enables direct exploration of alternative data and analysis strategies using simulated data; evaluating alternative strategies through diagnosis; and exploring the robustness of a chosen strategy to alternative models. Researchers can undertake each step before study implementation or data collection.

**Communicating design choices**. Bias in published results can arise for many reasons. For example, researchers may deliberately or inadvertently select analysis strategies because they produce statistically significant results. Proposed solutions to reduce this kind of bias focus on various types of preregistration of analysis strategies by researchers [@Rennie2004; @Zarin2008; @Casey2012; @Nosek2015a; @Green2015]. Study registries are now operating in numerous areas of social science, including those hosted by the American Economic Association, Evidence in Governance and Politics, and the Center for Open Science. Bias may also arise from reviewers basing publication recommendations on statistical significance. Results-blind review processes are being introduced in some journals to address this form of bias [e.g. @Findley:2016].

However, the effectiveness of design registries and results-blind review in reducing the scope for either form of publication bias depends on clarity over which elements must be included to describe the design. In practice, some registries rely on checklists and preanalysis plans exhibit great variation, ranging from lists of written hypotheses to all-but-results journal articles. In our view, the solution to this problem does not lie in ever-more-specific questionnaires, but rather in a new way of characterizing designs whose analytic features can be diagnosed through simulation.

The actions to be taken by researchers are described by the data strategy and the answer strategy; these two features of a design are clearly relevant elements of a preregistration document. In order to know which design choices were made ex ante and which were arrived at ex post, researchers need to communicate their data and answer strategies unambiguously. However, assessing whether the data and answer strategies are any good usually requires specifying a model and an inquiry. Design declaration can clarify for researchers and third parties what aspects of a study need to be specified in order to meet standards for effective preregistration. 

Declaration of a design in code also enables a final and infrequently practiced step of the registration process, in which the researcher ``reports and reconciles'' the final with the planned analysis. Identifying how and whether the features of a design diverge between ex ante and ex post declarations highlights deviations from the preanalysis plan. The magnitude of such deviations determines whether results should be considered exploratory or confirmatory. At present, this exercise requires a review of dozens of pages of text, such that differences (or similarities) are not immediately clear even to close readers. Reconciliation of designs declared in code can be conducted automatically, by comparing changes to the code itself (e.g., a move from the use of a stratified sampling function to simple random sampling) and by comparing key variables in the design such as sample sizes.

**Challenging Design Choices.** The independent replication of the results of studies after their publication is an essential component of the shift toward more credible science. Replication --- whether verification, reanalysis of the original data, or reproduction using fresh studies --- provides incentives for researchers to be clear and transparent in their analysis strategies, and can build confidence in findings.^[For a discussion of the distinctions between these different modes of replication, see @Clemens2015.]

In addition to rendering the design more transparent, design declaration can allow for a different approach to the re-analysis and critique of published research. A standard practice for replicators engaging in reanalysis is to propose a range of alternative strategies and assess the robustness of the *data-dependent* estimates to different analyses. The problem with this approach is that, when divergent results are found, third parties do not have clear grounds to decide which results to believe. This issue is compounded by the fact that, in changing the analysis strategy, replicators risk departing from the estimand of the original study, possibly providing different answers to different questions. In the worst case scenario, it can be difficult to determine what is learned both from the original study and from the replication. 

A more coherent strategy facilitated by design simulations would be to use a design declaration to conduct "design replication." In a design replication, a scholar restates the essential design characteristics to learn about what the study *could have* revealed, not just what the original author reports *was* revealed. This helps to answer the question: under what conditions are the results of a study to be believed? By emphasizing abstract properties of the design, design replication provides grounds to support alternative analyses on the basis of the original authors' intentions and not on the basis of the degree of divergence of results. Conversely, it provides authors with grounds to question claims made by their critics. 

## Avoiding declaration and diagnosis pitfalls

Declaring a design is just like writing out a recipe. You *can* cook without writing out a recipe, but when you do, you can think through the whole process start to finish, you can critique the process, and you can modify it. That said, designing high quality research is difficult and comes with many pitfalls, only a subset of which are addressed by the *MIDA* framework. Others we fail to help with entirely and, in some cases, we may even exacerbate them. We outline four concerns.

The first is the worry that evaluative weight could get placed on essentially meaningless diagnoses. Given that design declaration includes declarations of conjectures about the world it is possible to choose inputs so that a design passes any diagnostic test set for it. For instance, a simulation-based claim to unbiasedness that incorporates all features of a design is still only good with respect to the precise conditions of the simulation (in contrast, analytic results, when available, may extend over general classes of designs). Still worse, simulation parameters might be selected because of their properties. A power analysis, for instance,  may be useless if implausible parameters are chosen to raise power artificially. While MIDA may encourage more honest declarations, there is nothing in the framework that enforces them. As ever, garbage-in, garbage-out.

Second, we see a risk that research may get evaluated on the basis of a narrow, but perhaps inappropriate set of diagnosands. Statistical power is often invoked as a key design feature -- but even well-powered studies that are biased away from their targets of interest are of little theoretical use. The appropriateness of the diagnosand depends on the purposes of the study. As MIDA is silent on the question of a study's purpose, it cannot guide researchers or critics to the appropriate set of diagnosands by which to evaluate a design. An advantage of the approach however is that the choice of diagnosands gets highlighted and new diagnosands can be generated in response to substantive concerns.

Third, emphasis on the statistical properties of a design can obscure the substantive importance of a question being answered or other qualitative features of a design. A similar concern has been raised regarding the ``identification revolution'' where a focus on identification risks crowding out attention to the importance of questions being addressed \citep{huber2013}. Our framework can help researchers determine whether a particular design answers a question well (or at all), and it also nudges them to make sure that their questions are defined clearly and \textit{independently of their answer strategies}. It cannot, however, help researchers choose good questions.

Finally, we see a risk that the variation in the suitability of design declaration to different research strategies may be taken as evidence of the relative superiority of different types of research strategies. While we believe that the range of strategies that can be declared and diagnosed is wider than what one might at first think possible, there is no reason to believe that all strong designs can be declared either ex ante or ex post. An advantage of our framework, we hope, is that it can help clarify when a strategy can or cannot be completely declared. When a design cannot be declared, nondeclarability is all the framework provides, and in such cases we urge caution in drawing conclusions about design quality.

*MIDA* captures the analysis-relevant features of a design, but it does not describe substantive elements, such as how theories are derived or interventions are implemented. Yet many other aspects of a design that are not explicitly labeled in these features enter into this framework if they are analytically relevant. For example, logistical details of data collection such as the duration of time between a treatment being administered and endline data collection enter into the model if the longer time until data collection affects subject recall of the treatment. However, information in *MIDA* is typically insufficient to assess those substantive elements, an important and separate part of assessing the quality of a research study.



## Further Reading

- @brady2010rethinking

We build on two influential research design frameworks. [@kkv1994, p. 13] enumerate four components of a research design: a theory, a research question, data, and an approach to using the data. @geddes2003paradigms articulates the links between theory formation, research question formulation, case selection and coding strategies, and strategies for case comparison and inference. In both cases, the set of components are closely aligned to those in the framework we propose.

<!-- Here follows the introduction from the paper: -->
<!-- As empirical social scientists, we routinely face two research design problems. First, we need to select high-quality designs, given financial and practical constraints. Second, we need to communicate those designs to readers and reviewers. To select strong designs, we often rely on rules of thumb, simple power calculators, or principles from the methodological literature that typically address one component of a design while assuming optimal conditions for others. These relatively informal practices can result in the selection of suboptimal designs, or worse, designs that are simply too weak to deliver useful answers. To convince others of the quality of our designs, we often defend them with references to previous studies that used similar approaches, with power analyses that may rely on assumptions unknown even to ourselves, or with ad hoc simulation code. In cases of dispute over the merits of different approaches, disagreements sometimes fall back on first principles or epistemological debates rather than on demonstrations of the conditions under which one approach does better than another. -->

<!-- In this paper we describe an approach to address these problems. We introduce a framework --- MIDA --- that asks researchers to specify information about their background model (M), their inquiry (I), their data strategy (D), and their answer strategy (A). We then introduce the notion of "diagnosands," or quantitative summaries of design properties. Familiar diagnosands include statistical power, the bias of an estimator with respect to an estimand, or the coverage probability of a procedure for generating confidence intervals.  -->

<!-- Many aspects of design quality can be assessed through design diagnosis, but many cannot. For instance the contribution to an academic literature, relevance to a policy decision, and impact on public debate are unlikely to be quantifiable ex ante. -->

<!-- Using this framework, researchers can declare a research design as a computer code object and then diagnose its statistical properties on the basis of this declaration. A researcher may declare the features of designs in our framework for their own understanding and declaring designs may be useful before or after the research is implemented. Researchers can declare and diagnose their designs with the companion software for this paper, DeclareDesign, but the principles of design declaration and diagnosis do not depend on any particular software implementation. -->

<!-- The formal characterization and diagnosis of designs before implementation can serve many purposes. First, researchers can learn about and improve their inferential strategies. Done at this stage, diagnosis of a design and alternatives can help a researcher select from a range of designs, a process we call "redesign." Later, a researcher may include design declaration and diagnosis as part of a preanalysis plan or in a funding request. At this stage, the full specification of a design serves a communication function and enables third parties to understand a design and an authorâ€™s intentions. Even if declared ex-post, formal declaration still has benefits. The characterization can help readers understand the properties of a research project, facilitate transparent replication, and can help guide future (re-)analysis of the study data. -->

