---
title: "Improving research designs"
output: html_document
bibliography: bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```


<!-- start post here, do not edit above -->

```{r mida, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 1000
b_sims <- 200
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

# Improving research designs

Social science is awash in theoretical speculation. Our main scientific problem is that we don't know which (if any) of our theories are correct. The purpose of conducting an empirical research study is to learn the answers to inquiries that, if known, would bring our theoretical models of the world closer to the truth. If a theory predicts that the average causal effect of D on Y is positive, but though good research design we learn that it is negative, we've made scientific progress. Ruling out incorrect theories -- or even just the incorrect portions of otherwise correct theories -- is the main goal of empirical research. 

Research designs are procedures for learning about inquiries. This book offers a language for describing research designs and an algorithm for selecting among them. Remarkably, the same basic structure can be used whether researchers are interested in causal questions or descriptive questions, whether they are focused on theory testing and inductive learning, and whether they use quantitative, qualitative, or mixed methods. Once a design is **declared** in simple enough language that a computer can understand it, then we can implement our **diagnosis** algorithm for assessing the properties of a design at the click of a button. We can then engage in **redesign**, or the process of comparing among alternative designs to select a strong one. The same language we use to talk to the computer can be used to talk to others. Reviewers, advisors, students, funders, journalists, and the public -- and yes the computer too -- need to know four basic things to understand your design.

## The four components of research design

Empirical research designs share in common that they all have a model, an inquiry, a data strategy, and an answer strategy. The four together, which we refer to as MIDA, represent both your suppositions about how the world works and the choices you make as the researcher to intervene in and learn about the world.

The model comprises our uncertain beliefs about what causes what and how. It includes (possibly quite well-informed) guesses about how important variables are distributed, how things are correlated, the sequences of events. Of course, we don't know the true model---if we did we wouldn't have to do any empirical research. We are uncertain about which of many alternative model possibilities is the true one. We hope that the true, unknown causal model is in the set of models we entertain so that we can correctly imagine what will happen when our design is applied in the real world.

The model defines a set of units that we wish to study. Often, this set of units is a large population of units we cannot afford to take measurements of, but we can nevertheless define and seek to make inferences about. The model includes theoretical beliefs about baseline characteristics that describe each unit and the probability distributions of each characteristic. Finally, the model includes a set of outcome variables that may be functions of baseline characteristics and the effects of interventions. The values that an outcome variable take on depending on the level of an intervention are called *potential* outcomes. A unit's treated potential outcome arises when that unit receives the treatment; the untreated potential outcome arises when it does not. The model includes therefore includes beliefs about causal effects, which are defined with respect to these potential outcomes. The treatment effect of a particular intervention is usually defined as the difference between the treated and untreated potential outcomes, though other definitions such as the ratio of the treated to untreated potential outcome are possible too. 

Defining the model can feel like an odd exercise. Since researchers presumably want to learn about the model, declaring it in advance may seem to beg the question. The confusion can be allayed by thinking of the model as a kind of "reference model", against which you can assess performance. Declaring a reference model is often unavoidable when declaring research designs. In practice, doing so is already familiar to any researcher who has calculated the power of a design, which requires the specification of effect sizes. The seeming arbitrariness of the declared model can be mitigated by assessing the sensitivity of diagnosis to alternative models and strategies (see Section \@ref(p2diagnosis)). Further, researchers can inform their reference models with existing data, such as baseline surveys. Just as power calculators focus attention on minimum detectable effects, design declaration offers a tool to demonstrate design properties and how they change depending on researcher assumptions.

:::: {.principles data-latex=""}
::: {.principles-title data-latex=""}
Model principles
:::

**Models should include how you might be right about the world and how you might be wrong.** Ideally, the quality of a research design depends as little as possible on whether our assumptions about how the world works are correct beforehand --- because we are often wrong. Indeed, that is why we are conducting research. There are two senses in which it is useful to describe the world if you are correct and the world if you are wrong: alternative theories, and threats to inference. In order to make theoretical progress, we want to be able to provide evidence about whether our preferred theory is correct or an alternative is. The model must include the model under our theory and the alternative theory in order to assess whether the inquiry, data strategy, and answer strategy are good at distinguishing the two. We also should include threats to inference, which could undermine our ability to falsify our theory or distinguish between competing theories. In order to assess whether our design is strong whether these threats to inference occur or not, they should be defined in the model.

**Models should be mapped to theories.** Our goal in research is to build better theories, and thus to use data to distinguish between competing theories or build new ones. Models are theoretical, and so distinguishing with a research design between two models would yield theoretical progress. But theories in social science are typically simplifications of models, referring often to only one edge or the mechanism linking two nodes. As a result, when we define multiple models that include possible threats to inference, multiple models may be mapped to a single theory. Thus, a last step in setting up a model is providing a mapping between the model parameters and theory. When we then set up an inquiry and a design to measure the value of a parameter, we can update our theory after seeing the data using this mapping. The data tell us which models are ruled out and the mapping which theories are ruled out.
::::

The model is *not* the research question. The research question -- what we call the inquiry -- is a feature of the model we want to measure. Our theories are rich, so in a single model, there may be many possible inquiries that a researcher could seek to learn about. A model may be a complex dance of ten or more interrelated variables, but an inquiry is something like the average causal effect of a single variable on another, the descriptive distribution of a third variable, or a prediction about the value a single variable will take on some time in the future. Important inquiries distinguish between the alternative models under consideration. If theory A predicts the an inquiry will be positive but theory B predicts it will be negative, and a credible research design leads us to conclude it is positive, then we have ruled out all models that are consistent with theory B.

Inquiries are defined with respect to units, conditions, and features; they are summaries of features of units in or across conditions. Inquiries may be causal, as in the average treatment effect (ATE). The ATE is the average difference in the outcome variable across two conditions: the treatment condition and the control condition. Inquiries can be descriptive, as in a population average of some characteristic. While it may seem that descriptive inquiries do not involve conditions, they do -- the population average is defined with respect to conditions under which we measure characteristics of our study units. So-called Hawthorne effects occur when the very act of observation changes that which is observed, underlining the general point that researchers inevitably learn about specific potential outcomes and we should be cognizant of what those are. Inquiries are functions of variables in the model. We refer to value the inquiry function takes on -- the "true answer to the question" -- as an "estimand." 

:::: {.principles data-latex=""}
::: {.principles-title data-latex=""}
Inquiry principles
:::

**Inquiries should not be unidentifiable.** A basic requirement of an inquiry is that, if we had infinite data, we could provide at least a partial answer to the inquiry. (Of course, then we will get to whether we can obtain an answer with finite data.) There are, perhaps suprisingly, many circumstances under which inquiries are not identifiable. If we have a model that says an outcome Y is defined by two parameters a and b and a measured variable X and the outcome function is (a+b)*X, no amount of data can tell us the exact values of a and b. We can estimate, using regression for example, the value of a+b, but there are multiple possible solutions to the values of a and b. There is one variable, but two unknowns. In this setting, we may want to switch our inquiry to the sum a+b. Many other types of inquiries, such as mediation inquiries, are not identifiable. There are some circumstances where we can provide a partial answer to the inquiry, such as learning a range of values within which the parameter lives. These can be useful. At a minimum, our inquiries should not be unidentifiable.

**Inquiries should distinguish among theories.** Our goal is to make theoretical progress: to learn how the world works. For research designs to help us in this goal, inquiries must distinguish among theories. If we learned the answer to the inquiry, we would be more sure that theory A is correct than theory B, or vice versa. Even when we do not have two alternative theories and instead want to falsify our preferred theory, we typically have in mind a null model. The inquiry should distinguish between our preferred theory and the null model. The theories under consideration need not be scientific theories. Bankers, politicians, social workers, and others have mental models of how the world works that scientific research can inform. In these cases, inquiries even when providing practical knowledge rather than scientific should be able to distinguish between two or more competing mental models.

**Inquiries must be defined across all models under consideration.** Because inquiries should help distinguish among alternative models (and thus theories), a good inquiry is *defined* under all of those competing models. If it is undefined, then it cannot help distinguish between the two. Common, simple inquiries such as the average treatment effect are not defined in the presence of spillovers. If our model includes cases with and without spillovers, the average treatment effect cannot be used to distinguish among those models and we cannot assess its performance under the threat to inference from spillovers. We can redefine the average treatment effect, however, as the average difference between a unit being treated and all other units being assigned to control. Diagnosing a design will reveal whether inquiries are defined under alternative models, allowing us to redefine inquiries when needed.
::::

Together, the model and data form the theoretical half of a research design. The second half is empirical, and its two components mirror those in the theoretical half.

The data strategy is the full set of procedures we use to gather information from the world. This set includes three basic groups of procedures: sampling, assignment, and measurement. All data strategies involve sampling in the sense that no empirical strategy is comprehensive -- some units are sampled into the study and some units aren't. Even research designs (like the census, for example) have a sampling strategy in that they don't sample respondents in different years or different countries. Assignment procedures describe how researchers generate variation in the world. If you ask some subjects one question, but other subjects a different question, you've generated variation on the basis of an assignment procedure. We think of assignment procedures most often when they are randomized, as in a randomized experiment, but many kinds of research designs engage in assignment procedures that are not randomized. Measurement procedures are the ways in which researchers reduce the complex and multidimensional social world into a relatively parsimonious set of data. These data need not be "quantitative data" in the sense of being numbers or values on a pre-defined scale -- qualitative data are data too. Measurement is the vexing but necessary reduction of reality to a few choice representations. Measurement always carries the possibility of measurement error, because this reduction is hard. Sampling, assignment, and measurement are deeply parallel to the three features of inquiries: units, conditions, features. 

:::: {.principles data-latex=""}
::: {.principles-title data-latex=""}
Data strategy principles
:::

**Data strategies should sample the units, reveal the treatment conditions, and measure the outcomes that define the inquiry and are required by the answer strategy.** The object of the data strategy is to produce data that can be used to produce an answer to the inquiry. The inquiry is defined by four components: a set of units, which we are interested in; a set of outcomes we wish to learn about; a set of treatment conditions we wish to study those outcomes of those units in; and a summary function that summarizes the outcomes in the conditions for the units. In order to yield unbiased answers to the inquiry, the data strategy should mirror the inquiry in each of the first three aspects (the summary function will be mirrored in the answer strategy). The data strategy should sample from the same set of units in the inquiry (or include all of them); measure the outcomes defined in the inquiry; and do so within one or more treatment conditions defined in the inquiry. 

**Data strategies should rule out threats to validity with sampling, assignment, and measurement procedures.** The model of a research designs should contemplate the possibility of threats to validity such as confounders and measurement biases. When these biases are present and unaddressed, they must be ruled out by assumptions, which could be false. The data strategy can be used to rule out threats by design instead of assumption, by setting when data is collected and randomizing which units are sampled and assigned to treatment conditions. The more assumptions that can be ruled out by the data strategy in this way, the more credible the design (i.e., the more unbiased the results). 
::::

The answer strategy is how we summarize the data that the data strategy produces. Just like the inquiry summarizes a part of the model, the answer strategy summarizes a part of the data. Complex, multidimensional datasets don't just speak for themselves -- they need to be summarized and explained. Answer strategies are functions that take in data and return answers. For some research designs, this is a literal function like `lm_robust` that estimates an ordinary least squares regression with robust standard errors. For some research designs, the function is embodied by the researchers themselves when they read documents and summarize their meanings in a case study. 

Importantly, the answer strategy is more than the choice of an estimator. It includes the full set of procedures that begin with cleaning the dataset and end with answers in words, tables, and graphs. This includes data cleaning, data transformation, estimation, plotting, and interpretation. Not only do we define our choice of OLS as the estimator, we also specify that we will focus attention on the coefficient estimate from the `Z` variable, assess uncertainty using a confidence interval, construct a coefficient plot in a certain way to visualize the inference. The answer strategy also includes all of the if-then procedures that researchers implicitly or explicitly take depending on initial results and features of the data. In a stepwise regression procedure, the answer strategy is not the final regression that results from iterative model selection, but that procedure itself -- because the answer will reflect features of that change depending on sampling variability. Just like the values of the inquiry, the values of the estimates that result from the answer strategy have a probability distribution, because they are the result of the variables defined in the model (which have probability distributions) and the data strategy (sampling and treatment assignment are defined by probability distributions). 

:::: {.principles data-latex=""}
::: {.principles-title data-latex=""}
Answer strategy principles
:::

**Answer strategies should explicitly target inquiries.** To learn whether an analysis provides useful answers, we have to know what it is providing answers *to*. Answer strategies for social science studies are often multifaceted: we want to learn about theory, and each new data collection is expensive so we want to answer multiple questions. To declare a design, we need to know what each question (inquiry) is being asked and then what our strategy or strategies for answering each one are. 

**When data strategies introduce no distortions, answer strategies can be directly analogous to inquiries. When data strategies do introduce distortions, they must be compensated for in the answer strategy.** Our answer strategies are inextricably tied to our data strategies: the answer strategies are only as good as the data provided to them. A principle for developing good data strategies was to sample units, reveal treatment conditions, and measure outcomes defined in our inquiries. If they do so with no distortions, then our answer strategies can be simple: simply applying the same summary function to the realized data as the inquiry applies to the model. When there are distortions, however, the answer strategy plays a more important role: reversing the distortions. 

The theoretical and empirical halves of research design go hand-in-hand and a complete description of a research design generally needs both halves. If we ask, "What's your research design?" and you respond "It's a regression discontinuity design," we've maybe learned what your answer strategy might be, but we don't yet have enough information to decide whether it's a strong design until we learn more about the model, the inquiry, and whether the data and answer strategies are indeed well suited for them. Design declaration is our term for describing a design in terms of its theoretical and empirical halves.
::::

## Declare-Diagnose-Redesign

### Declaration

Declaring a design entails separating out which parts of your design belong in $M$, $I$, $D$, and $A$. The declaration process can be a challenge because mapping your ideas and excitement about your project into MIDA is not always straightforward. We promise it is a rewarding task. When you can express your research design in terms of these four components you are newly able to think about its properties. 

You can implement the MIDA framework in any software package -- Stata, R, Python, Julia, SPSS, SAS, Mathematica, or many others. We wrote the companion software to the book, `DeclareDesign`, in the R statistical environment because of the availability of other useful tools in R and because it is free, open-source, and high-quality. We have designed the rest of the book so that it can be read even if you do not use R, but you will have to translate the code into your own language of choice. On our Web site, we have  [pointers](https://declaredesign.org/pap) to how you might declare steps in Stata, Python, and Excel. In addition, we link to a ["Design wizard"](https://eos.wzb.eu/ipi/DDWizard/) that lets you declare and diagnose variations of standard designs via a web interface.

:::: {.principles data-latex=""}
::: {.principles-title data-latex=""}
Declaration principles
:::

**Designs should be declared before they are implemented.** Research designs can be declared and diagnosed profitably at many stages of the research process, including after they are implemented by the researcher and by others after the research is published if they were not before. However, research designs that are declared before they are implemented can be modified before it is too late. Once data strategies are implemented --- units sampled, treatments assigned, and outcomes measured --- there is no going backwards. The data is already collected. *The process of declaring the design may change the design*. By revealing how the model, inquiry, data strategy, and answer strategy are interconnected, better data strategies and inquiries may be surfaced. Better inquiries, that provide more theoretical leverage in providing information about the model may be identified. Inquiries that cannot be answered may be replaced. Models may be augmented to include threats to inference that may then reveal tweaks to data and answer strategies that improve the design. 

**Designs should be implemented as they are declared.** Once declared, the design should be implemented following the declared data strategy and answer strategy. The properties of the design, learned through diagnosis, may change even with seemingly slight changes to how cases are selected, treatments assigned, and outcome measurement strategies implemented. Similarly, the properties may change with modifications to the answer strategy, so it should be implemented as declared. However, as we detail in Chapter 7, answer strategies can and often should be procedures that involve exploration of the data and data-dependent choices. Such a procedure --- explore the data and select the optimal analysis based on a model selection criteria --- can be declared and assessed compared to other procedures. Often such procedures will be preferred to single preregistered specifications. The principle here is to declare the full procedure, including data-dependent choices, and assess the properties of the procedure as a whole. Then implement the procedure.
::::

### Diagnosis

Once you've declared your design, you can diagnose it. Design diagnosis is the process of simulating your research design in order to understand the range of possible ways the study could turn out. It is in the diagnosis stage that we define the design properties that are most desirable in our research setting. We let computers do the simulations for us because imagining how design choices influence sampling distributions is---to put it mildly---cognitively demanding. 

Diagnosis is an opportunity to write down what would make the study a success. For a long time, researchers have classified studies as successful or not based on statistical significance. Accordingly, statistical power (the probability of a statistically significant result) has been the most front-of-mind statistic when researchers have set to designing studies. As we learn more about the pathologies of relying on statistical significance, we learn that features beyond power are just as, if not more important. For example, the "credibility revolution" throughout social science has trained a laser-like focus on bias. Studies are coming under new criticism for lacking "strong identification," which usually implies that the data and answer strategies could lead to biased answers depending on how incorrect the model is. 

Design diagnosis relies on two further concepts, both functions of research designs. These are quantities that a researcher or a third party could calculate with respect to a design. 

The first is a **diagnostic statistic,** which is a summary statistic generated from a "run" of a design---that is, the results given a possible realization of variables, given the model and data strategy. For example, the statistic $e$ refers to the "difference between the estimated and the actual average treatment effect."  The $e$ statistics depends on the model (since the ATE depends on the model's assumptions about potential outcomes). The statistic $s = \mathbb{1}(p \leq 0.05)$, interpreted as "the result is considered statistically significant at the 5% level," presupposes an answer strategy that reports a $p$-value. Diagnostic statistics are governed by probability distributions that arise because both the model and the data generation, given the model, may be stochastic.

Second, a **diagnosand** is a summary of the distribution of a diagnostic statistic. For example, *bias* is the average value of the $e$ statistic and *power* is is the average value of the $s$ statistic. Other diagnosands include things like root-mean-squared-error (RMSE), Type I, Type II, Type M, and Type S error rates. 

One especially important diagnosand is the "success rate," which is the average value of the "success" diagnostic statistic. As a researcher, *you* get to decide what would make your study a success. What matters most in your research scenario? Is it statistical significance? If so, optimize your design with respect to power. Is what matters most in your research setting with the answer has the correct sign or not? Then diagnose how frequently your answer strategy yields an answer with the same sign as your inquiry. Diagnosis is an opportunity for you to articulate what would make your study a success and then to figure out, through simulation, how often you obtain that success. Often, success is a multidimensional aggregation of multiple diagnosands.

We diagnose studies over the set of causal beliefs encoded in the model, since we want to learn the value of diagnosands under many possible scenarios. A clear example of this is the value of the power diagnosand over many possible values of the true effect size. This idea extends well beyond statistical power. Whatever the set of important diagnosands, we want to ensure that our design performs well across all plausible model possibilities.

Computer simulation is not the only way to do design diagnosis. Designs can be declared in writing or mathematical notation and then diagnosed using analytical formulae. Enormous research design progress has been made with this approach. Methodologists across the social sciences have described diagnosands such as bias, power, and root-mean-squared-error for large classes of designs. Not only can this work provide closed-form solutions for many design properties, it can also yield insights about the pitfalls to watch out for when constructing similar designs. That said, pen-and-paper diagnosis is challenging for the majority of social science research designs, first because many designs have idiosyncratic features that are hard to incorporate and second because the analytic formulae for many diagnosands have not yet been worked out by statisticians, even for very common designs (see Section \@ref(p2diagnosis)).

We are enthusiastic about the ability of diagnosis to evaluate designs based on their ex ante properties, rather than on the ex post results that a design produces. That said, design diagnosis doesn't solve every problem and like any tool, can be misused. We outline three concerns.

The first is the worry that evaluative weight could get placed on essentially meaningless diagnoses. Given that design declaration includes declarations of conjectures about the world it is possible to choose inputs so that a design passes any diagnostic test set for it. For instance, a simulation-based claim to unbiasedness that incorporates all features of a design is still only good with respect to the precise conditions of the simulation (in contrast, analytic results, when available, may extend over general classes of designs). Still worse, simulation parameters might be selected because of their properties. A power analysis, for instance, may be useless if implausible parameters are chosen to raise power artificially. While our framework may encourage more honest declarations, nothing about it enforces honesty. As ever, garbage-in, garbage-out.

Second, we see a risk that research may get evaluated on the basis of a narrow, but perhaps inappropriate set of diagnosands. Statistical power is often invoked as a key design feature -- but well-powered studies that are biased are of little theoretical use. The importance of particular diagnosands can depend on the values of other diagnosands in complex ways, so researchers should take care to evaluate their studies along many dimensions.

Third, emphasis on the statistical properties of a design can obscure the substantive importance of a question being answered or other qualitative features of a design. A similar concern has been raised, for example by @huber2013, regarding the ``identification revolution'' where a focus on identification risks crowding out attention to the importance of questions being addressed. Our framework can help researchers determine whether a particular design answers a question well (or at all), and it also nudges them to make sure that their inquiries are defined clearly and independently of their answer strategies. 

:::: {.principles data-latex=""}
::: {.principles-title data-latex=""}
Diagnosis principles
:::

**Ethical, logistical, and scientific goals should be encoded in diagnosands.** We have multiple goals in producing research: we wish to contribute the most we can to scientific knowledge; working within our own financial, time, and logistical constraints; and doing so in an ethical manner. Each of these goals should be specified as a function of an implementation of the design. The cost is straightforward, a function translating the number of units and the amount of time it took to collect and analyze data about them into a financial value. Scientific goals may be represented in a number of ways, such as the root mean-squared error or statistical power or most directly the amount of learning between before and after the study was conducted. Ethical goals may also be translated into functions, though they need to not be quantitative. A quantitative ethical diagnosand might be the number of minutes of time taken from participants of the study. Whether any participants were harmed in the study would be a qualitative diagnosand. 

**Design declarations should be diagnosand complete.** With diagnosands defined, a design should be declared such that each diagnosand can be calculated. We must be able to assess a design in terms of each of our goals, and the process of declaring and attempting to diagnose the design may reveal that we have not declared the design in sufficient detail to do so. When we discover this, we simply need add further details to the design until the diagnosands can all be calculated.

**Designs should be diagnosed over the declared model possibilities.** Research design diagnosis is useful only if the design is assessed not only under conditions favorable to the researcher, but those unfavorable to the researcher. Designs should provide useful answers to inquiries in both sets of circumstances. Diagnosis under both sets can reveal if that is the case, or if changes to the data and answer strategy are needed. Even when there are circumstances under which the design performs poorly, understanding when that is the case aids interpretation of the results and focuses reanalysis debates substantively on which models are plausible.
::::

### Redesign

The subtitle of this book is "Declaration, Diagnosis, Redesign" to emphasize three important steps in the formulation of a research design. So far, we've outlined the first two points: declaration and diagnosis. Once your design has been declared, and you have learned to diagnose it with respect to the most important diagnosands, the last step is redesign.

Redesign entails tweaking parameters of the data and answer strategies to understand the implications of each for your most important diagnosands. This can mean a variety of things. Many diagnosands (power, RMSE) depend on the size of the study. We can redesign the study, varying the "sample size" feature of the data strategy to determine how big it needs to be to achieve a target diagnosand: 90% power, say, or an RMSE of 0.02. We could also vary an aspect of the answer strategy, say, the covariates used to adjust a regression model. Sometimes the changes to the data and answer strategies interact: if we use better covariates to increase the precision of the estimates in the answer strategy, we have to collect that information as a part of the data strategy. The redesign question now becomes, is it better to collect pre-treatment information from all subjects or is the money better spent on increasing the total number of subjects? 

The redesign process is mainly about optimizing your research design given ethical, logistical, and financial constraints. If related diagnosands such as total harm to subjects, total researcher hours, or total project cost exceed acceptable levels, the design is not feasible. We want to choose the best design we can among the feasible set. If the designs remaining in the feasible set are underpowered, biased, or are otherwise scientifically inadequate, the project may need to be abandoned -- something that's best learned as soon as possible. 

In our experience, it's during the redesign process that designs become **simpler**. We learn that our experiment has too many arms or that the expected level of heterogeneity is too small to be detected by our design. We learn that in our theoretical excitement, we've built a design with too many bells and too many whistles. Some of the complexity needs to be cut, or the whole design will be a muddle. The upshot of many redesign sessions is that our designs pose fewer inquiries, but obtain better answers.

:::: {.principles data-latex=""}
::: {.principles-title data-latex=""}
Redesign principles
:::

**Designs should be redesigned with respect to feasible data and answer strategies.** The redesign stage is centered on identifying all of the feasible data and answer strategies and then diagnosing each feasible design. The feasible set may include many minor variations in sampling and assignment probabilities and sample sizes or it may include several very different designs that could be accomplished within ethical and logistical constraints. The diagnosis should be conducted across all feasible combinations of the different dimensions of the designs, because it is difficult to predict how features in different parts of the data and answer strategies will interact.

**Optimization over diagnosand tradeoffs should guide the selection of a particular design among the set of feasible designs.** The reason to define the diagnosands for each of our research goals is that we can then compare a set of designs in terms of how they meet our goals. A diagnosis of five designs across ethical, logistical, and scientific diagnosands provides us with our own multidimensional value statement of each design. We then should select the optimal design, subject to feasibility. Putting this into practice forces us to provide a weighting scheme between ethical, logistical, and scientific values. Those weighting schemes may be that either the study is ethical and we do it or it is not ethical and we do not do it. Or there may be tradeoffs we navigate betwen the amount of time taken up of subjects and the scientific value of a larger sample that imply choosing a middle ground.

**Unforeseen implementation complications should be addressed through redesign.** The flip side of the principle that you should implement the design you declared is that if, for unforeseen reasons, the design must be changed, we should redesign our declaration and assess the properties of the new design. There are three reasons for doing this, with both immediate and longterm consequences. Most immediately, when unforeseen changes to budgets, time availability, or  other logistical constraints appear or when something goes wrong in implementation, we should ask ourselves two questions: given the stage we are at, what is the best design, even if different than originally envisaged? And is it worth implementing this new, modified design? Declaring the new design, diagnosing it, and finding the new optimal design subject to constraints will answer the first question. Then deciding whether the value of the new design, assessed through the diagnosands, is worth its cost. The new design may have a different ethical status, cost, and scientific value and thus sometimes it may not be worth continuing given the new reality. Redesign and diagnosis help navigate these two decisions.
::::

### Example in words: Encouraging Political Candidacy in Pakistan

We illustrate the MIDA framework with a study of political motivations among office-seekers in Pakistan. @Gulzar2020 conducted an experiment that estimated the effects of two alternative motivations for becoming a politician: helping the community or generating personal benefits. The researchers randomly assigned villages to receive different encouragements to run and measured the rates of running for office, the types of people who chose to run, and the congruence of elected politicians' policy positions with those of the general population. 

The model for this study applies to citizens who are eligible to run for office in study villages. The model describes subjects' individual characteristics and their potential outcomes depending on which motivation treatment they receive. The alternative theories that the model encompasses is that politicians run for office only to help themselves, only to help others, neither, or both. Among theories that include room for both motivations, some claim that intrinsic motivations are more powerful than extrinsic, while other claim the reverse. We define these potential outcomes in terms of subjects' latent probability of running for office which is tightly related to the binary choice to run or not to run.

The study has two inquiries are the average treatment effects of each encouragement, defined as the average difference in potential outcomes between receiving and not receiving each encouragement to run for office. We could imagine a third inquiry that is the difference between these two average treatment effects, but we'll leave that complication to the side for the moment.

The *data strategy* includes three elements. First, we randomly sample 50 citizens who are eligible to stand for election from each village. Next, we assign subjects to a personal benefits encouragement, a prosocial encouragement, or no encouragement (control). All subjects in a village are assigned to the same treatment condition, which is to say that this experiment used cluster random assignment. Lastly, the data strategy measures the decision to run for office by checking whether a subject's name appears on the official candidate lists of released by the Election Commission of Pakistan. In contrast to the continuous latent outcome variable in the model, the outcome variable as measured in the data strategy is binary.

The *answer strategy* is the difference-in-means estimator with standard errors clustered at the village level. The clustering of the errors at the village level is an example of how choices in the answer strategy must respect choices made in the data strategy.

### Example in code: Encouraging Political Candidacy in Pakistan

We are now ready to delare the @Gulzar2020 study in code. In the model, we describe a hierarchical structure with 192 villages each of which is home to 100 citizens who are eligible to run for elected office. Each citizen harbors three potential outcomes, `Y_Z_neutral`, `Y_Z_personal`, and `Y_Z_social`. `Y_Z_neutral` is the citizen's latent probability of standing for election if treated with a neutral appeal; `Y_Z_personal` is the probability if treated with an appeal that emphasizes the personal returns to office, and `Y_Z_social` is the probability if treated with an appeal that underlines the benefits to the community . Our simplified model includes a constant treatment effect of 2 percentage points for the personal appeal and 3 percentage points for the social appeal.

```{r}
model <-
  declare_model(
    villages = add_level(N = 500, U_village = rnorm(N, sd = 0.1)),
    citizens = add_level(
      N = 100, 
      U_citizen = rnorm(N),
      potential_outcomes(
        Y ~ pnorm(
          U_citizen + U_village + 
            0.1 * (Z == "personal") + 0.15 * (Z == "social")),
          conditions = list(Z = c("neutral", "personal", "social"))
        )))
```

Our inquiry is the average treatment effects in the population, defined as the average differences in potential outcomes:
  

```{r}
inquiry <- declare_inquiry(
  ATE_personal = mean(Y_Z_personal - Y_Z_neutral),
  ATE_social = mean(Y_Z_social - Y_Z_neutral)
)
```

The data strategy consists of three steps: sampling, assignment, and measurement. In sampling, we sample 48 of the eligible citizens from each village into the study.  In assignment, we cluster assign one third of the villages to the neutral condition, one third to the personal appeal, and the remaining third to the social appeal. The measurement step maps the realized (but still latent!) potential outcome to the observed choice to run or not.
  

```{r}
n_villages <- 192
citizens_per_village <- 48

data_strategy <-
  declare_sampling(
    S_village = cluster_rs(clusters = villages, n = n_villages),
    filter = S_village == 1,
    legacy = FALSE) +
  
  declare_sampling(
    S_citizen = strata_rs(strata = villages, n = citizens_per_village),
    filter = S_citizen == 1,
    legacy = FALSE) +
  
  declare_assignment(
  	Z = cluster_ra(
  	  clusters = villages, 
  	  conditions = c("neutral", "personal", "social")),
  	legacy = FALSE) + 
  
  declare_measurement(
    Y_latent = reveal_outcomes(Y ~ Z),
    Y_observed = rbinom(N, 1, prob = Y_latent)
  )
```

The answer strategy consists of an ordinary least squares regression (as implemented by `lm_robust`) of the outcome on the treatments. The standard errors clustered at the village level in order to account for the clustering in the assignment procedure. The regression will return three coefficients: an intercept and two treatment effect estimates. We ensure that the estimates are mapped to the relevant inquiry by explicitly linking them.

```{r}
answer_strategy <- 
  declare_estimator(Y_observed ~ Z, term = c("Zpersonal", "Zsocial"), 
                    clusters = villages, 
                    model = lm_robust,
                    inquiry = c("ATE_personal", "ATE_social"))
```

When we concatenate all four aspects we get a design:

```{r}
design <- model + inquiry + data_strategy + answer_strategy
```

For most designs that we declare in the book, we will also present a graphical representation of the design. In Figure \@ref(fig:gulzarkhandag), we visualize the simplified form of the Gulzar-Khan design.

```{r gulzarkhandag, fig.cap = "Simplified DAG for Gulzar Khan study", fig.height = 3, fig.width = 7, echo = FALSE}
dag <- dagify(
  Y ~ Z + X + U,
  Z ~ X
)

nodes <-
  tibble(
    name = c("Y", "Z", "U", "X"),
    label = c("Y", "Z", "U", "X"),
    annotation = c(
      "**Outcome**<br>",
      "**Random assignment**<br>",
      "**Unknown heterogeneity**",
      "**villages**<br>Used for cluster assignment"),
    x = c(5, 1, 5, 1),
    y = c(2.5, 2.5, 4, 4), 
    nudge_direction = c("S", "S", "N", "N"),
    data_strategy = c("unmanipulated", "assignment", "unmanipulated", "unmanipulated"),
    answer_strategy = "uncontrolled"
  )
ggdd_df <- make_dag_df(dag, nodes)

base_dag_plot %+% ggdd_df + coord_fixed(ylim = c(2.05, 4.6), xlim = c(0.25 - epsilon, 5.75 + epsilon))
```

To diagnose the design, we first define a set of diagnosands (see Section \@ref(p2diagnosis)), which are statistical properties of the design. In this case, we select the bias (difference between the estimate and the estimand, which is the PATE); the root mean-squared error; and the statistical power of the design.

```{r}
diagnosands <- declare_diagnosands(
  bias = mean(estimate - estimand),
  rmse = sqrt(mean((estimate - estimand)^2)),
  power = mean(p.value <= 0.05),
  cost = mean(10 * n_villages + 1 * n_villages * citizens_per_village)
)
```

We then diagnose the design, which involves simulating the design and again and again, and then calculate the diagnosands based on the simulations data. 

```{r, eval = do_diagnosis & !exists("do_bookdown")}
diagnosis <- diagnose_design(design, diagnosands = diagnosands, sims = sims, bootstrap_sims = b_sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("02_Improving_Research_Designs"), "/simple_design_diagnosis.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnosis, file = rds_file_path)
}
diagnosis <- read_rds(rds_file_path)
```

```{r simpledesigndiagnosis, echo = FALSE}
get_diagnosands(diagnosis) %>% 
  select(inquiry_label, estimator_label, bias, rmse, power) %>%
  kable(digits = 3, caption = "Diagnosis of the simplified Gulzar-Khan design.", booktabs = TRUE)
```

We redesign across possible combinations of numbers of villages and citizens per village:

```{r, eval = FALSE}
designs <- redesign(design, n_villages = c(192, 295, 397, 500), citizens_per_village = c(25, 50, 75, 100))
diagnosis <- diagnose_design(designs, diagnosands = diagnosands)
```

```{r, echo = FALSE, eval = do_diagnosis & !exists("do_bookdown")}
designs <- redesign(design, n_villages = c(192, 295, 397, 500), citizens_per_village = c(25, 50, 75, 100))
diagnosis <- diagnose_design(designs, diagnosands = diagnosands, sims = sims, bootstrap_sims = b_sims)
```


```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("02_Improving_Research_Designs"), "/redesign_diagnosis.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnosis, file = rds_file_path)
}
diagnosis <- read_rds(rds_file_path)
```


```{r, echo = FALSE}
gg_df <- diagnosis$diagnosands_df %>%
  filter(inquiry_label == "ATE_social") %>%
  pivot_longer(cols = c(bias, rmse, power, cost), names_to =  "diagnosand")

g_base <-
ggplot(data = NULL, aes(citizens_per_village, value, group = n_villages, color = n_villages)) +
  geom_line() +
  scale_color_gradient(low = dd_light_blue, high = dd_dark_blue, breaks =c(192, 295, 397, 500)) +
  coord_cartesian(xlim = c(0, 100)) + 
  dd_theme() + 
  theme(legend.key.height = unit(1.75, units = "cm"))

g1 <- g_base %+% filter(gg_df, diagnosand == "bias") + labs(x = "Citizens per village", y = "Bias", color = "Number of\nvillages") + scale_y_continuous(limits = c(-0.025, 0.025)) 
g2 <- g_base %+% filter(gg_df, diagnosand == "power") + labs(x = "Citizens per village", y = "Statistical power", color = "Number of\nvillages")  + scale_y_continuous(limits = c(0, 1)) + geom_hline(yintercept = 0.80, color = dd_pink, linetype = "dashed") + annotate("text", x = 82, y = 0.825, label = "Power threshold = 0.8", size = 3, color = dd_pink)
g3 <- g_base %+% filter(gg_df, diagnosand == "rmse") + labs(x = "Citizens per village", y = "Root mean-squared error", color = "Number of\nvillages") + scale_y_continuous(limits = c(0, 0.05))
g4 <- g_base %+% filter(gg_df, diagnosand == "cost") + labs(x = "Citizens per village", y = "Cost", color = "Number of\nvillages") 

wrap_plots(g1, g2, g3, g4, guides = "collect")
```

[GLOSS DIAGNOSIS; DO A REDESIGN]


## Putting designs to use

The two pillars of our approach are the language for describing research designs (MIDA) and the algorithm for selecting high-quality designs (Declare-Diagnose-Redesign). Together, these two ideas can shape research design decisions throughout the lifecycle of a project. The full set of implications is drawn out in Part IV but we emphasize the most important ones here. 

Broadly speaking, the lifecycle of an empirical research project has four phases: Brainstorming, Planning, Realization, and Integration. Planning, Realization, and Integration describe what happens before, during, and after the implementation of a research design. We pre-pend a fourth phase -- Brainstorming -- to reflect the idea that research doesn't just begin with "planning" -- research ideas have to being with some spark of inspiration.

Brainstorming is the process of growing a research idea from a kernel into a skeleton specification of the model, inquiry, data strategy, and answer strategy. The inspiration for a good research project can come from many sources -- frustration with an article you're reading, a golden opportunity with a potential research partner, a conversation with a colleague (or adversary!). The spark of an idea might be some bit of a model, perhaps an inquiry in particular, maybe a portion of a data strategy, or just an itch to apply a new answer strategy you learned about. Wherever that kernel of an idea starts, the point of brainstorming is to develop all parts of M, I, D, and A enough that the design becomes a coherent whole.

After an idea has been fleshed out sufficiently, its time to start planning. Planning entails some or all of the following steps, depending on the design: conducting an ethical review, seeking IRB approval, gathering criticism from colleagues and mentors, running pilot studies, and preparing preanalysis documents. The design as encapsulated by M, I, D, and A will go through many iterations and refinements during this period. Planning is the time when frequent re-application of the Declare-Diagnose-Redesign algorithm will pay the highest dividends. How should you investigate the ethics of a study? By casting the ethical costs and benefits as diagnosands. How should you respond to (good and bad) criticism? By re-interpreting the feedback in terms of M, I, D, and A. How can you convince funders and partners that your research project is worth investing in? By credibly communicating your study's diagnosands -- its statistical power, its unbiasedness, its high chance of success, however the partner  or funder defines it. What belongs in a pre-analysis plan? You guessed it -- a specification of the model, inquiry, data strategy and answer strategy. Iterating and improving design details is the essence of good planning.

Realization is the phase of research in which all those plans are executed. You implement your data strategy in order to gather information from the world. Once that's done, you follow your answer strategy in order to finally generate answers to the inquiry. Of course, that's only if things go exactly according to plan -- which has never once happened once in our own careers. Survey questions don't work as we imagined, partner organizations over-promise and under-deliver, subjects move or become otherwise unreachable. A critic or a reviewer may insist you change your answer strategy -- or may think a different inquiry altogether is the theoretically appropriate one. You may yourself change how you think of the design as you embark on writing up the research project. It is inevitable that some features of M, I, D, and A will change during the realization phase. Some design changes have very bad properties, like sifting through the data ex-post, finding a statistically significant result, then back-fitting an new M and a new I to match the new A. This bad practice goes by the name HARKing -- Hypothesizing After Results are Known (@Kerr1998). Indeed, if we declare and diagnose this actual answer strategy (sifting through data ex-post), we can show through design diagnosis that it is badly biased (away from zero) for any of the inquiries it could end up choosing. Other changes made along the way may help the design quite a bit -- if the planned design did not include covariate adjustment but a friendly critic suggests adjusting for the pre-treatment measure of the outcome, the RMSE diagnosand might drop nicely. The point here is that design changes during the implementation process -- whether necessitated by unforeseen logistical constraints or required by the review process -- can be understood using in terms of M, I, D, and A by reconciling the planned design with the design as implemented.

When that acceptance email finally hits your inbox, you can celebrate a job well done and a design well realized. But the research design lifecycle is not finished -- the design must be integrated into the scientific community. Studies must be archived, along with design information, to prepare for reanalysis. Future scholars may well want to reanalyze your design in order to learn more than is represented in the published article or book. Good reanalysis requires a full understanding of the design as implemented, so archiving design information along with code and data is critical. Not only may your design be reanalyzed, it may also be replicated with fresh data. Ensuring that replication studies answer the same theoretical questions as original studies requires explicit design information without which replicators and original study authors may simply talk past one another.^[For a discussion of the distinctions between different modes of replication, see @Clemens2017.] Indeed, as your study is integrated into the scientific literature and beyond, you should anticipate disagreement over your claims. Resolving disputes is very difficult if parties do not share a common understanding of the research design.^[For example, much of the debate over instrumental variables designs in economics centers on which of two inquiries is the "correct" theoretical target, the average treatment effect or the local average treatment effect (@deaton2009instruments, @imbens2010better).] Finally, you should anticipate that your results will be formally synthesized with others' work via meta-analysis. Meta-analysts need design information in order to be sure they aren't inappropriately mixing together studies that ask importantly different questions or answer them too poorly to be of use.





## Further Reading

- @brady2010rethinking

<!-- We build on two influential research design frameworks. [@kkv1994, p. 13] enumerate four components of a research design: a theory, a research question, data, and an approach to using the data. @geddes2003paradigms articulates the links between theory formation, research question formulation, case selection and coding strategies, and strategies for case comparison and inference. In both cases, the set of components are closely aligned to those in the framework we propose. -->

<!-- Here follows the introduction from the paper: -->
<!-- As empirical social scientists, we routinely face two research design problems. First, we need to select high-quality designs, given financial and practical constraints. Second, we need to communicate those designs to readers and reviewers. To select strong designs, we often rely on rules of thumb, simple power calculators, or principles from the methodological literature that typically address one component of a design while assuming optimal conditions for others. These relatively informal practices can result in the selection of suboptimal designs, or worse, designs that are simply too weak to deliver useful answers. To convince others of the quality of our designs, we often defend them with references to previous studies that used similar approaches, with power analyses that may rely on assumptions unknown even to ourselves, or with ad hoc simulation code. In cases of dispute over the merits of different approaches, disagreements sometimes fall back on first principles or epistemological debates rather than on demonstrations of the conditions under which one approach does better than another. -->

<!-- In this paper we describe an approach to address these problems. We introduce a framework --- MIDA --- that asks researchers to specify information about their background model (M), their inquiry (I), their data strategy (D), and their answer strategy (A). We then introduce the notion of "diagnosands," or quantitative summaries of design properties. Familiar diagnosands include statistical power, the bias of an estimator with respect to an estimand, or the coverage probability of a procedure for generating confidence intervals.  -->

<!-- Many aspects of design quality can be assessed through design diagnosis, but many cannot. For instance the contribution to an academic literature, relevance to a policy decision, and impact on public debate are unlikely to be quantifiable ex ante. -->

<!-- Using this framework, researchers can declare a research design as a computer code object and then diagnose its statistical properties on the basis of this declaration. A researcher may declare the features of designs in our framework for their own understanding and declaring designs may be useful before or after the research is implemented. Researchers can declare and diagnose their designs with the companion software for this paper, DeclareDesign, but the principles of design declaration and diagnosis do not depend on any particular software implementation. -->

<!-- The formal characterization and diagnosis of designs before implementation can serve many purposes. First, researchers can learn about and improve their inferential strategies. Done at this stage, diagnosis of a design and alternatives can help a researcher select from a range of designs, a process we call "redesign." Later, a researcher may include design declaration and diagnosis as part of a preanalysis plan or in a funding request. At this stage, the full specification of a design serves a communication function and enables third parties to understand a design and an authors intentions. Even if declared ex post, formal declaration still has benefits. The characterization can help readers understand the properties of a research project, facilitate transparent replication, and can help guide future (re-)analysis of the study data. -->





<!-- Together, the language and the algorithm help researchers address two main problems.  -->

<!-- First, they have to select high-quality research designs that can be relied upon to generate credible answers to their research questions. Without a way to measure the quality of design, it's very difficult to choose strong ones over weak ones. Second, researchers need to communicate their designs to others. Without a way to describe a design in detail, it's very difficult to explain to other scholars why they are of high quality and why they are the right design for the question.  -->

<!-- Our language for research designs helps with both problems. -->

<!-- -->



<!-- ^[MIDA is of course itself a model, we present it here as a simple four step sequence though as will become clear in applications in some cases some of these steps are barely visible, some repeat, and, within a given project, the order of steps can be complex.]  -->


<!-- An inquiry is a function of the exogenous characteristics of units, of endogenous outcome variables, or both. It may be defined over all units in the population defined by the model, as in the average treatment effect for all units, or it may be defined over a subset of units, as in the conditional average treatment effect for women. Because we defined the distribution of the variables in the model, we can define the probability distribution of inquiries, which are a function of those variables.  -->


<!-- How can we rule out theoretical models? First, we need to enumerate the many models that are possible, at least in theory. The exhaustive enumeration of possible theoretical models is a daunting challenge. We describe below an approach that begins with a "kernel" -- a small portion of a theoretical model from which we grow theoretical possibilities. Suppose we have a list of possibilities in hand. A research question -- what we call an "inquiry" -- refers to a fact that, if known, would rule out some models in favor of others. If we can show through a credible research design that $Z$ causes $Y$, we can rule out all models in which $Z$ does not cause $Y$. The *reason* that we seek to learn about inquiries is to distinguish among theoretical possibilities. -->



<!-- Typically, endogenous outcome variables are random variables, either because they are a function of an exogenous baseline variable for which we defined a probability distributions or because assignment to treatment or control is random as part of the data strategy. -->

<!-- # we should turn this into a picture labeling MIDA -->