---
title: "Principles of research design"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```


<!-- start post here, do not edit above -->

```{r designprinciples, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 1000
b_sims <- 200
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

# Research design principles


:::: {.principles data-latex=""}
::: {.principles-title data-latex=""}
Model principles
:::

**Models should include how you might be right about the world and how you might be wrong.** Ideally, the quality of a research design depends as little as possible on whether our assumptions about how the world works are correct beforehand --- because we are often wrong. Indeed, that is why we are conducting research. There are two senses in which it is useful to describe the world if you are correct and the world if you are wrong: alternative theories, and threats to inference. In order to make theoretical progress, we want to be able to provide evidence about whether our preferred theory is correct or an alternative is. The model must include the model under our theory and the alternative theory in order to assess whether the inquiry, data strategy, and answer strategy are good at distinguishing the two. We also should include threats to inference, which could undermine our ability to falsify our theory or distinguish between competing theories. In order to assess whether our design is strong whether these threats to inference occur or not, they should be defined in the model.

**Models should admit arbitrary inquiries over causal quantities.**
Define on profiles / SUTVA example [to be written]

<!-- **Models should be mapped to theories.** Our goal in research is to build better theories, and thus to use data to distinguish between competing theories or build new ones. Models are theoretical, and so distinguishing with a research design between two models would yield theoretical progress. But theories in social science are typically simplifications of models, referring often to only one edge or the mechanism linking two nodes. As a result, when we define multiple models that include possible threats to inference, multiple models may be mapped to a single theory. Thus, a last step in setting up a model is providing a mapping between the model parameters and theory. When we then set up an inquiry and a design to measure the value of a parameter, we can update our theory after seeing the data using this mapping. The data tell us which models are ruled out and the mapping which theories are ruled out. -->
::::

:::: {.principles data-latex=""}
::: {.principles-title data-latex=""}
Inquiry principles
:::

**Inquiries must be defined across all models under consideration.** Because inquiries should help distinguish among alternative models (and thus theories), a good inquiry is *defined* under all of those competing models. If it is undefined, then it cannot help distinguish between the two. Common, simple inquiries such as the average treatment effect are not defined in the presence of spillovers. If our model includes cases with and without spillovers, the average treatment effect cannot be used to distinguish among those models and we cannot assess its performance under the threat to inference from spillovers. We can redefine the average treatment effect, however, as the average difference between a unit being treated and all other units being assigned to control. Diagnosing a design will reveal whether inquiries are defined under alternative models, allowing us to redefine inquiries when needed.

**Inquiries should be at least partially identifiable.** There is a little point in asking unanswerable questions. We think of a question as being at least partly answerable if there are at least two different sets of data you might observe that would lead you to make two different inferences. In the best case one might imagine that with infinite data you can pinpoint the value of an inquiry. In this case we say that the model, or estimand, is identified. But short of that it might be possible to close in on values. There are, perhaps suprisingly, many circumstances under which inquiries are not even partially identifiable. If we have a model that says an outcome $Y$ is defined by the structural equation $Y=(a+b)X$, for scalars $a$ and $b$, no amount of data can tell us the exact values of $a$ and $b$. Indeed without boundary constraints no amount of data can even narrow down the ranges of $a$ and $b$. The basic problem is that for any value of $a$ we can choose a $b$ that keeps the sum of $a+b$ constant. In this setting, we may want to switch our inquiry to the sum $a+b$. Many other types of inquiries, such as mediation inquiries, are not identifiable. There are some circumstances where we can provide a partial answer to the inquiry, such as learning a range of values within which the parameter lives. These can be useful. At a minimum, our inquiries should be at least partially identifiable.

**Define inquiries as summaries of (potential) outcomes.**
This third principle should help with the last two. If an inquiry is defined in terms of potential outcomes then its definition depends only on what's called the "signature" of the model---the set of nodes and their ranges and not specific assumptions about how nodes relate to each other---the structural equations. 

**Inquiries should be informative about theories.** Although the MIDA frameworks is centered around questions and answers, very often our questions are themselves only stepping stones towards making greater theoretical progress: to learn how the world works. For research designs to help us in this goal, inquiries must distinguish among theories. If we learned the answer to the inquiry, we would be more sure that theory $A$ is correct than theory $B$, or vice versa. Even when we do not have two alternative theories and instead want to falsify our preferred theory, we typically have in mind a null model. The inquiry should distinguish between our preferred theory and the null model. The theories under consideration need not be scientific theories. Bankers, politicians, social workers, and others have mental models of how the world works that scientific research can inform. In these cases, inquiries even when providing practical knowledge rather than scientific should be able to distinguish between two or more competing mental models.

::::



:::: {.principles data-latex=""}
::: {.principles-title data-latex=""}
Data strategy principles
:::

**Data strategies should rule out threats to validity with sampling, assignment, and measurement procedures.** The model of a research designs should contemplate the possibility of threats to validity such as confounders and measurement biases. When these biases are present and unaddressed, they must be ruled out by assumptions, which could be false. The data strategy can be used to rule out threats by design instead of assumption, by setting when data is collected and randomizing which units are sampled and assigned to treatment conditions. The more assumptions that can be ruled out by the data strategy in this way, the more credible the design (i.e., the more unbiased the results). 
::::




:::: {.principles data-latex=""}
::: {.principles-title data-latex=""}
Answer strategy principles
:::

**Answer strategies should explicitly target inquiries.** To learn whether an analysis provides useful answers, we have to know what it is providing answers *to*. Answer strategies for social science studies are often multifaceted: we want to learn about theory, and each new data collection is expensive so we want to answer multiple questions. To declare a design, we need to know what each question (inquiry) is being asked and then what our strategy or strategies for answering each one are. 

**When data strategies introduce no distortions, answer strategies can be directly analogous to inquiries. When data strategies do introduce distortions, they must be compensated for in the answer strategy.** Our answer strategies are inextricably tied to our data strategies: the answer strategies are only as good as the data provided to them. A principle for developing good data strategies was to sample units, reveal treatment conditions, and measure outcomes defined in our inquiries. If they do so with no distortions, then our answer strategies can be simple: simply applying the same summary function to the realized data as the inquiry applies to the model. When there are distortions, however, the answer strategy plays a more important role: reversing the distortions. 

The theoretical and empirical halves of research design go hand-in-hand and a complete description of a research design generally needs both halves. If we ask, "What's your research design?" and you respond "It's a regression discontinuity design," we've maybe learned what your answer strategy might be, but we don't yet have enough information to decide whether it's a strong design until we learn more about the model, the inquiry, and whether the data and answer strategies are indeed well suited for them. Design declaration is our term for describing a design in terms of its theoretical and empirical halves.

**Answer strategies should anticipate implementation problems in data strategies.** Missing data, archival documents that cannot be traced, noncompliance with treatment assignments, spillovers, and difficulties recontacting subjects in followup surveys are just some of the common problems empirical researchers face. Answer strategies should be developed that anticipate these problems, and account for them. Ideally, answer strategies include if-then plans for handling each likely implementation problem. Mitigation plans may include partial identification or sensitivity analyses.
::::



:::: {.principles data-latex=""}
::: {.principles-title data-latex=""}
Declaration principles
:::

**Designs should be declared before they are implemented.** Research designs can be declared and diagnosed profitably at many stages of the research process, including after they are implemented by the researcher and by others after the research is published if they were not before. However, research designs that are declared before they are implemented can be modified before it is too late. Once data strategies are implemented --- units sampled, treatments assigned, and outcomes measured --- there is no going backwards. The data is already collected. *The process of declaring the design may change the design*. By revealing how the model, inquiry, data strategy, and answer strategy are interconnected, better data strategies and inquiries may be surfaced. Better inquiries, that provide more theoretical leverage in providing information about the model may be identified. Inquiries that cannot be answered may be replaced. Models may be augmented to include threats to inference that may then reveal tweaks to data and answer strategies that improve the design. 

**Designs should be implemented as they are declared.** Once declared, the design should be implemented following the declared data strategy and answer strategy. The properties of the design, learned through diagnosis, may change even with seemingly slight changes to how cases are selected, treatments assigned, and outcome measurement strategies implemented. Similarly, the properties may change with modifications to the answer strategy, so it should be implemented as declared. However, as we detail in Chapter 7, answer strategies can and often should be procedures that involve exploration of the data and data-dependent choices. Such a procedure --- explore the data and select the optimal analysis based on a model selection criteria --- can be declared and assessed compared to other procedures. Often such procedures will be preferred to single preregistered specifications. The principle here is to declare the full procedure, including data-dependent choices, and assess the properties of the procedure as a whole. Then implement the procedure.
::::


:::: {.principles data-latex=""}
::: {.principles-title data-latex=""}
Diagnosis principles
:::

**Ethical, logistical, and scientific goals should be encoded in diagnosands.** We have multiple goals in producing research: we wish to contribute the most we can to scientific knowledge; working within our own financial, time, and logistical constraints; and doing so in an ethical manner. Each of these goals should be specified as a function of an implementation of the design. The cost is straightforward, a function translating the number of units and the amount of time it took to collect and analyze data about them into a financial value. Scientific goals may be represented in a number of ways, such as the root mean-squared error or statistical power or most directly the amount of learning between before and after the study was conducted. Ethical goals may also be translated into functions, though they need to not be quantitative. A quantitative ethical diagnosand might be the number of minutes of time taken from participants of the study. Whether any participants were harmed in the study would be a qualitative diagnosand. 

**Design declarations should be diagnosand complete.** With diagnosands defined, a design should be declared such that each diagnosand can be calculated. We must be able to assess a design in terms of each of our goals, and the process of declaring and attempting to diagnose the design may reveal that we have not declared the design in sufficient detail to do so. When we discover this, we simply need add further details to the design until the diagnosands can all be calculated.

**Designs should be diagnosed over the declared model possibilities.** Research design diagnosis is useful only if the design is assessed not only under conditions favorable to the researcher, but those unfavorable to the researcher. Designs should provide useful answers to inquiries in both sets of circumstances. Diagnosis under both sets can reveal if that is the case, or if changes to the data and answer strategy are needed. Even when there are circumstances under which the design performs poorly, understanding when that is the case aids interpretation of the results and focuses reanalysis debates substantively on which models are plausible.
::::


:::: {.principles data-latex=""}
::: {.principles-title data-latex=""}
Redesign principles
:::

**Designs should be redesigned with respect to all feasible data and answer strategies.** The redesign stage is centered on identifying all of the feasible data and answer strategies and then diagnosing each feasible design. The feasible set may include many minor variations in sampling and assignment probabilities and sample sizes or it may include several very different designs that could be accomplished within ethical and logistical constraints. The diagnosis should be conducted across all feasible combinations of the different dimensions of the designs, because it is difficult to predict how features in different parts of the data and answer strategies will interact.

**Optimization over diagnosand tradeoffs should guide the selection of a particular design among the set of feasible designs.** The reason to define the diagnosands for each of our research goals is that we can then compare a set of designs in terms of how they meet our goals. A diagnosis of five designs across ethical, logistical, and scientific diagnosands provides us with our own multidimensional value statement of each design. We then should select the optimal design, subject to feasibility. Putting this into practice forces us to provide a weighting scheme between ethical, logistical, and scientific values. Those weighting schemes may be that either the study is ethical and we do it or it is not ethical and we do not do it. Or there may be tradeoffs we navigate betwen the amount of time taken up of subjects and the scientific value of a larger sample that imply choosing a middle ground.

**Unforeseen implementation complications should be addressed through redesign.** The flip side of the principle that you should implement the design you declared is that if, for unforeseen reasons, the design must be changed, we should redesign our declaration and assess the properties of the new design. There are three reasons for doing this, with both immediate and longterm consequences. Most immediately, when unforeseen changes to budgets, time availability, or  other logistical constraints appear or when something goes wrong in implementation, we should ask ourselves two questions: given the stage we are at, what is the best design, even if different than originally envisaged? And is it worth implementing this new, modified design? Declaring the new design, diagnosing it, and finding the new optimal design subject to constraints will answer the first question. Then deciding whether the value of the new design, assessed through the diagnosands, is worth its cost. The new design may have a different ethical status, cost, and scientific value and thus sometimes it may not be worth continuing given the new reality. Redesign and diagnosis help navigate these two decisions.
::::

