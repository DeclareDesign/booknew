---
title: "Principles of research design"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```


<!-- start post here, do not edit above -->

```{r designprinciples, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 1000
b_sims <- 200
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

# Research design principles

The Declare-Diagnose-Redesign framework suggests a set of twelve principles that can guide the design process. Not all principles will be equally important in all cases but we think all are worth giving consideration when developing and assessing a design. 

We state the principles and then describe each in detail.

:::: {.principles data-latex=""}
::: {.principles-title data-latex=""}
Design principles
:::

1. Design early

2. Select answerable inquiries

3. Render inquiries insensitive to models

4. Seek M:I, D:A parallelism

5. Make M large

6. Find your limits

7. Bake in implementation fails

8. Specify your criteria for a good design

9. Evaluate holistically

10. Redesign over D and A

11. Redesign often

12. Design to share

::::


## Design early

Research designs can be declared and diagnosed profitably at many stages of the research process, including after implementation or even publication. However, designing early yields big gains. You learn about the properties of a design when there is still time to improve them. Once data strategies are implemented --- units sampled, treatments assigned, and outcomes measured --- there is no going backwards. But when you are at the poing of analysis you might well wish you could go backwards and gather data differently, or even pose your research question differently.

This is the point of early design. The process of declaring the design may change the design. By revealing how the model, inquiry, data strategy, and answer strategy are interconnected, improvements to each element may be surfaced. If the answer strategy and inquiry are mismatched, the designer faces a choice to change one or the other. If the units sampled in the data strategy are in disharmony with the theoretical model, alternative participants might be sourced. Models may reveal assumptions that require defense through additional data collection.  Better inquiries, with more theoretical leverage over the model, may be identified. Inquiries that cannot be answered may be replaced.

Designing early does not mean being inflexible. You can change a design to reflect new knowledge gained during piloting or implementation. But the fact that designs can change later does not take away from the gains from thinking ahead. 

$\rightarrow$ In section `X` we give examples of how to design in advance when you don't yet know.

## Select answerable inquiries

This principle has two components.

First, *you should have an inquiry*. Oddly, it is possible to carry out data analysis --- for instance, running a regression of $Y$ on $X$ --- and getting something that looks like an answer without specifying any question in particular. The diagnosis process is generally geared to seeing whether you have a good answer. Assessing that in turn requires a well defined inquiry. So the process of declaration and diagnosis helps make sure you have a question.

Second, the question you have should be in principle answerable.  That's trickier than it sounds. We can think of being answerable in theory and in practice. 

A question is answerable "in theory" if you can write down a model such that, if that model were the true model, and you knew the model, you could answer the question. You can describe the question in terms of the model. Simple sounding questions: "Did Germany cause the Second World War?" "or "did New Zealand do well against Covid-19 because Prime Minister Jacinda Ardern was a woman?" can turn out to be difficult to ask and answer even if you have a well defined model. This, we think, can give a hint to when a question is poorly posed. Make sure that you can describe *some* world such that if you fully understood the world you would have a precise answer to your question. In the MIDA framework we might think of a model as being answerable in theory if you get an answer when you apply $I$ to $M$. 

We can think of a question as being answerable *in practice* if there is possible data lets you hone in on good answers. This relates to the idea of "identification" in statistics. A question is at least partly answerable if there are at least two different sets of data you might observe that would lead you to make two different inferences.  In the best case, one might imagine that you have lots of data and each possible data pattern you see is consistent with only one possible answer. You might then say that your model, or inquiry, is identifed. Failing that you might imagine that different data patterns at least let you rule out some answers even though you can't be sure of the right answer. In this case we have "partial identification." 
<!-- In the MIDA framework we might think of a question as being answerable in practice if there  is a $D$ and $A$ that produce answers that vary as $M$ varies.  --> Perhaps surprisingly, some inquiries might not even be partially identifiable. For instance if we have a model that says an outcome $Y$ is defined by the structural equation $Y=(a+b)X$, for scalars $a$ and $b$, no amount of data can tell us the exact values of $a$ and $b$. Indeed without boundary constraints no amount of data can even narrow down the ranges of $a$ and $b$. The basic problem is that for any value of $a$ we can choose a $b$ that keeps the sum of $a+b$ constant. In this setting, even though there is an answer to our inquiry ($a$) in theory it is not one we can ever answer in practice. Many other types of inquiries, such as mediation inquiries, are not identifiable. There are some circumstances in which we can provide a partial answer to the inquiry, such as learning a range of values within which the parameter lives. These can be useful. At a minimum, we urge you to pose inquiries that are at least partially answerable with possible data.

$\rightarrow$ In section `X` we give an example of a design that sought to answer a question that turned out to be unanswerable.  

## Render inquiries insensitive to models

Sometimes researchers write down a model and then point to some  model-specific quantity that they want to lean about. The problem with this approach however is that if you, or someone else, worry that  the model might be wrong and want to assess how the design performs under a different model you might find that you can't do it because you don't  have a well defined inquiry anymore. 

Here are two quite common examples where a problem like this arises. First, imagine a researcher assumes that $Y= \beta_0+\beta_1 X$ and says they want to learn about $\beta_1$.  A second researcher thinks the true model is $Y= \gamma_0 + \gamma_1 X +\gamma_2 X^2$. What then is researcher 1's question under researcher 2's model? For a second example researcher 1 is interested in the average treatment effect, which they describe as  $E(Y(1)-Y(0))$. Researcher 2 wonders how well researcher 1's strategy works if there are spillovers, but then realizes that the estimand,   $E(Y(1)-Y(0))$, presupposed no spillovers. Howe then to assess whether the strategy works well?

One approach is to require that readers just accept that our model is correct and so the question is well defined. But this has the distinct disadvantage of preventing us from even *asking* how the design  would  perform if the model is wrong. A better approach is to define the inquiry in a way that requires as few assumptions as possible about the world.  For instance, for example 1, instead of $\beta$ we could say that we are interested in the average effect on $Y$ of a unit change in $X$. Similarly, for example 2,  we might redefine the average treatment effect as the average difference between a unit being the only unit treated  and a situation where all other units being assigned to control. 

In practice we encourage you to define inquiries as summaries of the values (or potential values) of  nodes of a model. If you do that then the definition depends only on what's called the "signature" of the model---the set of nodes and their ranges---and not specific assumptions about how nodes relate to each other---the structural equations.

$\rightarrow$ In section `X` we show an example of a design that seeks to estimate deep parameters from a structural model but using an approach where the question does  not depend on the model being correct. 


## Seek M:I, D:A parallelism


A useful feature that you can see from the MIDA formalism is that when data strategies introduce no distortions, answer strategies can be directly analogous to inquiries. If the data is "like" the distributions generated by a model then if $A$ is like $I$ the estimate will be like the estimand. 

We call this the plug-in principle: the estimator can be like the inquiry. For example if you are interested in estimating the mean or the variance of a population you use the mean and variance of a sample [@van2000asymptotic].  

The principle does not always hold but there is an interesting generalization. When data strategies do introduce distortions, the answer strategy should compensate for them to restore parallelism.  We restore parallelism then by seeking an $A$ such that $A$ after $D$ approximates $I$. This idea underpins the maxim "analyze as you randomize" [@fisher1937design]: if your data is not a simple draw from the population you care about you need to account for that fact and the data strategy itself gives guidance for how to do that. 

$\rightarrow$ We show plug-in principles in action in section `X`.



## Make M large

Broadly there are two ways to be sure that a data and answer strategy return the right answer under some class of models. One is a form of shooting fish in a barrel: you narrow down the set of models so much that your Data and Answer strategy are sure to work within the set. In the extreme case you entertain models in which the answer is "4" and your answer strategy is "4". For a less trivial example you assume that there are no confounders and interpret correlation as causation, refusing to contemplate alternatives. People do that. 

The second approach is to design D and A specifically to ensure that D and A perform well even under a wide class of models---including models that you might have never considered.  This is the principle behind  "design based inference" approaches or behind use of "doubly robust" estimators. The core idea is that your design gets stronger if it continues to perform as well as your assumptions on $M$ get weaker. 


## Find your limits

A kind of corollary of Principle 5 is that while you should try to make $M$ large you have to know where the edges are. It's good to learn from design declaration and diagnosis that given some model and some strategy faithfully implemented, we get a reliable answer with high probability. But designs can also be used to get clarity over when a design will *not* produce a good answer, even when you have built in failures of implementation. Your design might assume for instance that  one variable is not affected by another variable and the validity of your answer might depend on the extent to which this is true. A design that contains a set of models that include violations of this assumption can be used to assess the extent to which the assumption matters, how bad a violation has to be to produce misleading results of consequence, and what types of assumptions are critical for inference and which ones are not. In short, seek to construct a model set do you can understand the worlds for which your design works and the worlds in which you run into problems.

Research design diagnosis is useful only if the design is assessed not only under conditions favorable to the researcher, but those unfavorable to the researcher. Designs should provide useful answers to inquiries in both sets of circumstances. Diagnosis under both sets can reveal if that is the case, or if changes to the data and answer strategy are needed. Even when there are circumstances under which the design performs poorly, understanding when that is the case aids interpretation of the results and focuses reanalysis debates substantively on which models are plausible.


$\rightarrow$ Section `X` describes a design defined over a *set* of models; diagnosis reveals that the design performs excellently in subsets of the set but fails in other subsets.  



## Bake in implementation failures

A common approach to research planning is to write down the ideal design, then go implement it, encounter problems, and seek fixes. Missing data, archival documents that cannot be traced, noncompliance with treatment assignments, evidence of spillovers, and difficulties recontacting subjects in followup surveys are just some of the common problems empirical researchers face. Insofar as these are predictable problems it can be useful to think of them as *parts* of your design not *deviations* from your design. Answer strategies can be developed that anticipate these problems, and account for them, including if-then plans for handling each likely implementation problem. More fundamentally, the deviations themselves can be included in your model so that you can diagnose the properties of different strategies, in advance, given risks of different kinds.  

$\rightarrow$ Section `X` gives an example of a design that incorporates risks of non-random attrition. 

## Specify your criteria for a good design

In evaluating designs, researchers often focus on quite narrow criteria, or what we call diagnosands, and consider them in isolation. Is the estimator unbiased? Do I have statistical power? But the evaluation of a design often requires balancing multiple criteria:  scientific precision, logistical constraints, policy goals, as well ethical considerations. Each of these goals can  be specified as a function of an implementation of the design. The cost is straightforward, a function translating the number of units and the amount of time it took to collect and analyze data about them into a financial value. Scientific goals may be represented in a number of ways, such as the root mean-squared error or statistical power or most directly the amount of learning between before and after the study was conducted. Ethical goals may also be translated into functions. An ethical diagnosand might be the number of minutes of time taken from participants of the study or whether any participants were harmed in the study.

A diagnosis of five designs across ethical, logistical, and scientific diagnosands provides us with our own multidimensional value statement of each design. We then should select the optimal design, subject to feasibility. Putting this into practice forces us to provide a weighting scheme between ethical, logistical, and scientific values. Those weighting schemes may be that either the study is ethical and we do it or it is not ethical and we do not do it. Or there may be tradeoffs we navigate between the amount of time taken up of subjects and the scientific value of a larger sample that imply choosing a middle ground.

$\rightarrow$ Section `X` walks through the selection and combination of "diagnosands." 

## Evaluate holistically

Too often, researchers evaluate *parts* of their design in isolation: is this a good question? Is this a good estimator? What's the best way to sample? However, evaluation of a design requires knowing how the parts fit together. If we ask, "What's your research design?" and you respond "It's a regression discontinuity design," we've learned what your answer strategy might be, but we don't have enough information to decide whether it's a strong design until we learn about the model, inquiry, data strategy, and other parts of the answer strategy. 

In practice we do this by declaring the entire design, and asking how it performs, from start to finish, with respect to specified diagnosands. To be able to do this we require a sufficiently complete design declaration. Indeed the ability to run through a design to the point where a diagnosis can be undertaken is, we think, a good indicator of an adequately declared design. In section X we discuss the idea of  "diagnosand-complete" designs in more detail.   

$\rightarrow$ Section `X` revisits a common debate "should I use OLS or ordered probit?" and show how the question is poorly posed without clarity on other aspects of a design

## Redesign over D and A

You may well feel that you are done once you fully declare a design that promises to answer the questions you care about. That is the point however when you can most easily think through the performance of your design relative to alternatives. The redesign stage is centered on identifying alternative feasible data and answer strategies and then diagnosing each feasible design. The feasible set may include variations in A --- variations in sampling,  assignment probabilities and sample sizes. This is where researchers often fine tune designs. But it can also include variations in A, by varying estimation or inferential procedures.  

Optimization can then be conduced by diagnosing across multiple feasible combinations of the different dimensions of the designs and selecting best performing combinations. Again, design evaluation depends on how the parts of a design related to each other so optimization is best done varying multiple criteria jointly rather than one at a time.

$\rightarrow$ Section `X` describes formal optimization procedures



## Redesign often

We emphasized designing early and we often have an image of designing first and implementing later. But in practice as you implement your design, it might change. Sample sizes change, measures change, estimation strategies change. Sometimes even your questions change. 

These features might not have been part of your original design but they are part of your design once they happen. In such cases you can redesign, assess the properties of the updated design, and compare it to the old design. There are three reasons for doing this, with both immediate and longterm consequences. Most immediately, when unforeseen changes to budgets, time availability, or  other logistical constraints appear or when something goes wrong in implementation, we should ask ourselves two questions: given the stage we are at, what is the best design, even if different than originally envisioned? And is it worth implementing this new, modified design? Declaring the new design, diagnosing it, and finding the new optimal design subject to constraints will answer the first question. Then deciding whether the value of the new design, assessed through the diagnosands, is worth its cost. The new design may have a different ethical status, cost, and scientific value and thus sometimes it may not be worth continuing given the new reality. Redesign and diagnosis help navigate these two decisions.

$\rightarrow$ Section `X` describes "design reconciliation" and useful ways to compare designs as planned and designs as executed.


## Design to share

We emphasize design as a process to improve your work. But it is also a process for communicating your research, justifying your decisions, and contributing to the work of others. Formalizing design declaration makes this sharing easier. By coding up a design as an interrogable object that can be run, diagnosed, and varied, you let other researchers see, understand, and question the logic of your research. We urge you to keep this sharing function in mind as you write code, explore alternatives, and optimize over designs. A design that is hard coded to capture your final decisions might break when researchers try to modify parts. Alternatively designs can be created specifically to make it easier to  explore neighboring designs, let others see why you chose the design you chose, and give them a leg up in their own work. In our ideal world when you create a design you contribute it to a design library so others can check it out and build on your good work.      

$\rightarrow$ Section `X` describes how to prepare your design for the "integration" stage of a research life cycle.
