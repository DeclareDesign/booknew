---
title: "Principles of research design"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```


<!-- start post here, do not edit above -->

```{r designprinciples, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 1000
b_sims <- 200
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

# Research design principles

The MIDA framework suggests a set of ten principles that can guide the design process. Not all principles will be equally important in all cases but all are, we think, worth giving consideration when developing and assessing a design. 

We state the principles next and then describe in more detail what we mean by each.

:::: {.principles data-latex=""}
::: {.principles-title data-latex=""}
Model principles
:::

1. Design early.

2. Select answerable inquiries.

3. Render inquiries insensitive to models.

4. Seek M:I, D:A parallelism. 

5. Bake in implementation fails.

6. Specify your criteria for a good design.

7. Evaluate in wholes.

8. Optimize (over D and A).

9. Know where it breaks.

10. Keep your design alive.

11. Design to share.

::::


## Design early.

Research designs can be declared and diagnosed profitably at many stages of the research process, including after they are implemented by the researcher and by others after the research is published if they were not before. However, there is an obvious gain from designing early: you learn about the properties of a design when there is still time to improve them. Once data strategies are implemented --- units sampled, treatments assigned, and outcomes measured --- there is no going backwards. The data is already collected. *The process of declaring the design may change the design*. By revealing how the model, inquiry, data strategy, and answer strategy are interconnected, better data strategies and inquiries may be surfaced. Better inquiries, that provide more theoretical leverage in providing information about the model may be identified. Inquiries that cannot be answered may be replaced. Models may be augmented to include threats to inference that may then reveal tweaks to data and answer strategies that improve the design. 

Designing early does not mean being inflexible. You can change a design to reflect new knowledge. But the fact that designs can change does take away from the gains from thinking ahead. 

$\rightarrow$ In section `X` we give examples of how to design in advance when you don't yet know 

## Select answerable inquiries.

Our principle on answerable inquiries has two parts. 

First *you should have an inquiry*. Oddly it is possible to do estimation---for instance running a regression on $Y$ on $X$---and getting something that looks like an answer without specifying a question you are trying to answer.^[Loonquawl and Phouchg's problem.] 

Second, the question you have should be in principle answerable.  We can think of "model-answerable" inquiries and "data-answerable" inquiries. 

The idea of Model-answerability is that you can write down a model such that, if that model were the true model, and you knew the model, you could answer the question. You can describe the question in terms of the model. Simple sounding questions: "Did Germany cause the second world war?" "or "did Jacinda Ardern do well against Covid-19 because she is a woman?" can turn out to be difficult to ask and answer within a model which, we think, can give a hint to whan a question is poorly posed. Make sure that you can describe some world in which if you fully understood the world you would have a precise answer to your question.   

Data-answerability relates to the idea of "identiifcation." In statistics we think of a question as being at least partly answerable if there are at least two different sets of data you might observe that would lead you to make two different inferences. In the best case one might imagine that there is posisble data that would give you a rprecise answer. In this case we can think of a model that is identified, or at least partially identified. There are, perhaps suprisingly, many circumstances under which inquiries are not even partially identifiable. If we have a model that says an outcome $Y$ is defined by the structural equation $Y=(a+b)X$, for scalars $a$ and $b$, no amount of data can tell us the exact values of $a$ and $b$. Indeed without boundary constraints no amount of data can even narrow down the ranges of $a$ and $b$. The basic problem is that for any value of $a$ we can choose a $b$ that keeps the sum of $a+b$ constant. In this setting, even though there is an answer to our inquiry ($a$)  it is not one we can ever answer with data. We may want to switch our inquiry to the sum $a+b$. Many other types of inquiries, such as mediation inquiries, are not identifiable. There are some circumstances where we can provide a partial answer to the inquiry, such as learning a range of values within which the parameter lives. These can be useful. At a minimum, our inquiries should be at least partially answerable with possible data. 


$\rightarrow$ In section `X` we give an example of a design that sought to answer a question that turned out to be unanswerable.  

## Render inquiries insensitive to models.

Sometimes researchers write down a model and then point to some  model-specific quantity that they want to lean about. For example you assume that $Y= \alpha+\beta X$ and you want to learn about $\beta$. The problem with this approach however is that it is not very clear what the *question* is (and similarly, what the estimand is) if you have the wrong model. For instance what is the reearcher asking if in fact the "true" model is $Y= X - X^2$? For a second example it is unclear what the average treatment effect inquiry $E(Y(1)-Y(0))$ is if our model allows for spillovers. One approach is to require that readers assume our model is true and so the question is well defined. But this has the distinct disadvantage of preventing us from even *asking* how the design  woudl perform if the model is wrong. A better approach is to define the inquiry in a way that requires as few assumptions as possible about the world.  For instance instance instead of $\beta$ we could say that we are interested in the average effect on $Y$ of a unit change in $X$. Similarly we can redefine the average treatment effect, however, as the average difference between a unit being the only unit treated  and a situation where all other units being assigned to control. 

In practice we encourage you to define inquiries as summaries terms of the values (or potential values) of  nodes of a model. If you do that then the definition depends only on what's called the "signature" of the model---the set of nodes and their ranges---and not specific assumptions about how nodes relate to each other---the structural equations.


$\rightarrow$ In section `X` we show an example of a design that seeks to estimate deep parameters from a structural model but using an approach where the question does  not depend on the model being correct. 


## Seek M:I, D:A parallelism. 

A useful feature that you can see from the MIDA formalism is that when data strategies introduce no distortions, answer strategies can be directly analogous to inquiries. If teh data $D$ is like the distributions generated by $M$ then if $A=I$, $A(d)$ will be like $I(m)$. We call this the plug-in principle: the estimator can be like the inquiry. The principle does not always hold but there is an interesting generalization. When data strategies do introduce distortions, the answer strategy should compensate for them to restore parallelism. If we define inquiries in terms of potential outcomes, then our estimand can be written $I(X(m))$, where $X(m)$ denotes values of nodes produced under model $m$. We get to observe $d=D(m)$  and our answer is $A(D(m))$. Ideally we  want $A(D(m)) = I(X(m))$. We preserve parallelism then by seeking an $A$ such that $A$ after $D$ approximates $I$ after $X$.


$\rightarrow$ We show plug-in principles in action in section `X`.


## Bake in implementation fails.

A common  approach is write down the ideal design, then go to implement, encounter problems, and seek fixes. Missing data, archival documents that cannot be traced, noncompliance with treatment assignments, evidence of spillovers, and difficulties recontacting subjects in followup surveys are just some of the common problems empirical researchers face. Insofar as these are predictable problems it can be useful to think of them as *parts* of your design not *deviations* from your design. Answer strategies can be developed that anticipate these problems, and account for them, including if-then plans for handling each likely implementation problem. More fundamentally the deviations themselves can be included in your `M` so that you can diagnose the properties of different strategies, in advance, given risks of different kinds.  

$\rightarrow$ Section `X` gives an example of a design that incorporates risks of non-random attrition. 

## Specify your criteria for a good design.

In evaluating designs, researchers often  focus on quite narrow criteria. Is the estimator unbiased? Do I have statistical power? But the evaluation of a design often requires balancing multiple criteria:  scientific goals, logistic constraints, policy implications, as well ethical considerations. We wish to contribute the most we can to scientific knowledge; working within our own financial, time, and logistical constraints; and doing so in an ethical manner. Each of these goals can  be specified as a function of an implementation of the design. The cost is straightforward, a function translating the number of units and the amount of time it took to collect and analyze data about them into a financial value. Scientific goals may be represented in a number of ways, such as the root mean-squared error or statistical power or most directly the amount of learning between before and after the study was conducted. Ethical goals may also be translated into functions, though they need to not be quantitative. A quantitative ethical diagnosand might be the number of minutes of time taken from participants of the study. Whether any participants were harmed in the study would be a qualitative diagnosand. 

$\rightarrow$ Section `X` walks through the selection and cmbination of "diagnosands." 

## Evaluate in wholes.

Commonly researchers evaluate *parts* of their design: is this a good question? Is this a good estimator? What's the best way to sample?  There can be merit in this, but a proper evaluation of a design requires knowing how the parts fit together. If we ask, "What's your research design?" and you respond "It's a regression discontinuity design," we've maybe learned what your answer strategy might be, but we don't yet have enough information to decide whether it's a strong design until we learn more about the model, the inquiry, and whether the data and answer strategies are indeed well suited for them. In practice we do this by running through the entire design, and asking how it performs, from start to finish, with respect to specified diagnosands.^[This presupposes of course that we *can* run through a design from start to finish, that the design is whole enough to let us do that.  See section X on  "diagnosand-complete" designs.]   

$\rightarrow$ Section `X` revisits a common debate "should I use OLS or ordered probit?" and show how the question is poorly posed without clarity on other aspects of a design

## Optimize (over D and A).

You may well feel that you are done once you fully declare a design that promises to answer the questions you care about. That is the point however when you can most easily think through the performance of your design relative to alternatives. The redesign stage is centered on identifying all of the feasible data and answer strategies and then diagnosing each feasible design. The feasible set may include many minor variations in sampling and assignment probabilities and sample sizes or it may include several very different designs that could be accomplished within ethical and logistical constraints. The diagnosis can be conducted across multiple feasible combinations of the different dimensions of the designs which address the problem that it is difficult to predict how features in different parts of the data and answer strategies will interact.

A diagnosis of five designs across ethical, logistical, and scientific diagnosands provides us with our own multidimensional value statement of each design. We then should select the optimal design, subject to feasibility. Putting this into practice forces us to provide a weighting scheme between ethical, logistical, and scientific values. Those weighting schemes may be that either the study is ethical and we do it or it is not ethical and we do not do it. Or there may be tradeoffs we navigate between the amount of time taken up of subjects and the scientific value of a larger sample that imply choosing a middle ground.

$\rightarrow$ Section `X` describes formal optimization procedures


## Know where it breaks.

Design declaration and diagnosis can usefully be used to show a set of conditions under which a design works: Given some model and some strategy faithfully implemented, we get a reliable answer with high probability. But designs can also be used to get clarity over when a design will *not* produce a good answer: even when you have built in failures of implementation. Your design might assume for instance that  one variable is not affected by another variable and the validity of your answer might depend on the extent to which this is true. A design that contains a set of models that include violations of this assumption can be used to assees the extent to which the assumption matters, how bad a violation has to be to produce misleading results of consequence, and what types of assumptions are critical for inference and which ones are not. In short, seek to construct a model set do you can understand the  worlds for which your design works and the worlds in which you run into problems.

Research design diagnosis is useful only if the design is assessed not only under conditions favorable to the researcher, but those unfavorable to the researcher. Designs should provide useful answers to inquiries in both sets of circumstances. Diagnosis under both sets can reveal if that is the case, or if changes to the data and answer strategy are needed. Even when there are circumstances under which the design performs poorly, understanding when that is the case aids interpretation of the results and focuses reanalysis debates substantively on which models are plausible.


$\rightarrow$ Section `X` describes a design defined over a *set* of models; diagnosis reveals that the design performs excellently in subsets of the set but fails in other subsets.  


## Keep your design alive

We emphasized designing early and we often have an image of designing first and implementing later. But in practice as you implement your design might change. Sample sizes change, measures change, estimation strategies change. Sometimes even your questions change. 

These features mightn't have been part of your original design but they are part of your design once they happen. In such cases you can redesign, assess the properties of the updated design, and compare it to the old design. There are three reasons for doing this, with both immediate and longterm consequences. Most immediately, when unforeseen changes to budgets, time availability, or  other logistical constraints appear or when something goes wrong in implementation, we should ask ourselves two questions: given the stage we are at, what is the best design, even if different than originally envisaged? And is it worth implementing this new, modified design? Declaring the new design, diagnosing it, and finding the new optimal design subject to constraints will answer the first question. Then deciding whether the value of the new design, assessed through the diagnosands, is worth its cost. The new design may have a different ethical status, cost, and scientific value and thus sometimes it may not be worth continuing given the new reality. Redesign and diagnosis help navigate these two decisions.


$\rightarrow$ Section `X` describes "design reconciliation" and useful ways to compare designs as planned and designs as executed.


## Design to share.

We emphasize design as a process to improve your work. But it is also a process for communicating your research, justifying your decisions, and contributing to the work of others. 

$\rightarrow$ Section `X` describes how to prepare your design for the "integration" stage of a research life cycle.
