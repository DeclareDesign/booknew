---
title: "Defining the inquiry"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

<!-- make sure to rename the section title below -->

```{r defining_the_inquiry, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

## Defining the Inquiry

Social scientists are picking research questions it can be hard to know where to start.

Theory is imaginary. We have to to spend time imagining it. That's what theorists do (with different formalizations). The imagine models open parentheses M). Models are complex, there are many things to know about them. There are many inquiries.

Data are real. We have to spend time making them real. That's what empiricists do (with many different measurement procedures). They make data real by creating them. Data come in many forms, there are many things to know about creating data. There are many ways to summarize them.


Empiricists have to be concerned about research design. We have to develop empirical strategies to demonstrate I.  We have to learn a lot about how to select the date of strategy and the answer strategy in a way that demonstrates an inquiry about a model that anybody cares about. So empiricists have to learn about models and inquiries (theory) as well as about data strategies and answer strategies (empirics).


How should you pick research questions?

You should study M to understand which Is are worth knowing.

You should study D and A in order to learn how to demonstrate I.

It's a difficult mapping but we think to get started it's helpful to add a minimum write down your model your inquiry your data strategy in your answer strategy of any Consol model that interests you to get started learning how to map. I(M) to A(D).  Then return to theory to find new important inquiries, then write down a new model inquiry data strategy and answer strategy. This process is how we make progress on M, that is, how we make M truer.






## OLD BELOW
This strategy makes some sense. Since there are so many things we *don't know* about the social world, researchers are positively overbrimming with interesting theories to test. Then again, most of us are a little bit bewildered when it comes to actually formulating a research question that would simultaneously be worth knowing the answer to and that would be feasible to answer. This section considers and critiques some common research question formulation advice. Perhaps unsurprising, our overarching perspective is that we can't compromise on research design -- scientific conclusions depend essentially on strong research designs. That said, choosing research questions is much more art than science. Moreover, the choice of what to study is not valueless in the sense that scholars' research agendas are reflections of their values and their politics. While scientists seek to reveal objective truths, we also have to choose **which** truths to investigate, and that choice is value-laden.

## The false choice between theory-first and method-first approaches.

Some research questions are boring and trivial -- no doubt you have encountered such research questions when reading bad research. Figuring our the best way to answer unimportant questions can be a stimulating intellectual puzzle, but the answers to those questions themselves will not advance scientific knowledge. Some research questions are exciting and vital -- those are the ones we want to answer using credible research designs. Giving weak or noncredible answers to important research questions is *dangerous* and unethical. 

Sometimes scholarly debate about research design treat these two desiderata (good questions and strong designs) as forming a one-dimensional spectrum. To the left we have boring studies done well and to the right we have interesting studies with weak empirics. This "spectrum" view manufactures a tradeoff between the quality of the question and the strength of the research design. It tricks students into thinking that they have to make compromises on the research design (i.e., control for enough things and hope for the best) in order to do great things. 

The reality is that research is hard and sometimes it is so hard that we must abandon them. Good questions might be too damn hard to answer and we must deliberately walk away from them even though they are interesting. Bad questions aren't worth the effort, even if they are feasible -- walk away from them too. Both interesting questions and strong research designs to answer them are required.

Advice about how to solve this dilemma often takes one of two forms: theory-first approaches and method-first approachs. Under the theory-driven approach, you read a lot of others' work in order to formulate a good research question; you then search among the available methods to find the "right" method for the question. This approach is risky because it is very hard to abandon a good research questions, even if none of the available methods is good enough. The worst case scenario for the theory-first is a research process that takes forever, scholars falling prey to the sunk cost fallacy and falling to abandon a hopeless project, and a final research project that relies on heroic assumptions and makes no real progress on the interesting research question.  

The method-first approach involves learning a method well first, then looking for interesting projects to apply it to later. This frequently means *practicing* the method for the method's sake. To become an expert interviewer, you must conduct many interviews. To become an expert experimentalist, you must randomize many treatments. To become an expert observational data analyst, you must download many datasets and develop a sense for when inferences are reliable and when they are not. The worst-case scenario for the method-first approach is that scholars never stop *practicing* research to actually finally *conduct* research on important topics. What good is it to be a top-flight archivist if those skills are never applied to good questions.

Obviously, a mix of the two approaches is required, and most of variation in advice given to young scholars isn't just one or the other, but the relative weight given to the two. As a team of authors, we disagree amongst ourselves about the right weights. That said, the downsides to boring but well-done research don't seem as bad to us as the downsides of interesting but poorly-done research. Bad studies on important questions are used by disingenous and bad faith actors to push for policies that are bad for society. The ethical stakes are higher for important questions than they are for unimportant questions. For that reason alone, we think it is vitally important that researchers master their methods so as to assiduously avoid making unknowably incorrect claims.

People who are looking in the forest for mushrooms often don't see a mushroom for a long period of time. After a while, they acclimate. They get their ``eyes on,'' and successful finds seem to be around every bend. In this analogy, the theory building process is going for a walk in the forest and methods training is learning to spot mushrooms -- you need get your eyes on answerable research questions worth answering.

In our experience, that means that the most important thing for young scholars to do is practice. Practice both searching for interesting questions and practice research design. Practicing research design often involves honing one's skills on less than interesting questions.￼ Looking for interesting questions entails reading other scholars' work carefully and generously. Even when their research designs are not strong, their questions might nevertheless be interesting. You may learn from reading bad work on interesting questions that some research questions are impossible to answer. That's OK -- now you’ve learned about the constraints on what we can learn. Operating within the constraints of what is knowable means you can anticipate when a poor research design on an interesting question is unlikely to lead to progress.

## Novelty
Another route for finding research questions is to try to determine what is unknown to the research community, then to seek to fill that gap. This approach can be quite generative, but it is not without pitfalls. Establishing primacy -- that a finding is "new" and that the knowledge generated by the study was previously unknown -- is hard. It's also risky, since peer reviewers have access to Google Scholar. False primacy claims are especially damaging during the review process because they are easily refuted and demonstrate ignorance of the published record.

What does it mean for a research finding to be novel? There are two main cases. First, a finding is new if the empirical pattern was predicted by an earlier theory but had yet to be confirmed. That is, there was a theoretical prediction that had not yet found its empirical verification. The empirical finding is novel in that it confirms the previously unconfirmed theoretical prediction. Second, an empirical finding is also novel if the empirical reality was *not* predicted by the theory.
If the empirical funding is trustworthy, it means that the original theory is incomplete or incorrect in a particular way. 
Findings that contradict theoretical predictions are also novel. Empirical findings are not novel when the theoretical predictions that pertain to the finding have already been well-documented.  

This puts the advance of science in a funny position. Because we are worried that many of the advances made at the forefront of the scientific endeavor are incorrect or spurious, we really value replication of novel results. When an empirical claim in favor of a theoretical proposition is made in Place A at Time 1, we immediately want to know if it holds at Place B at Time 2 as well.

## Mixing methods

A common piece of advice for young scholars who are interested in a topic is to mix methods. The thinking goes like this: I have an important question but none of my methods perfectly answers the question. I'll take a multifaceted approach that gets that many different aspects of the question which I hope to aggregate into a whole.￼￼

We caution against this approach. Mastering one method is difficult; mastering three is very difficult.￼
Even when mastery of the methods is not an obstacle, the problem that often occurs is that the three research designs actually address *different* research questions. That is, not only do the three research designs use different data strategies and answer strategies, they also employ different models and inquiries. As a result, mixed methods work often gives three incomplete answers to three related, but ultimately different, research questions. 



```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
library(dagitty)
```

## Example section

## Connections to MIDA

## Defining the Inquiry in DeclareDesign

<!-- -->

### Classes of estimands 

A well defined research design usually requires a well defined question and the quality of a design can often be assessed in terms of how well the question can be answered. 

In all that follows we will make use of the concept of an *estimand*, which we take to be quantity that you seek to estimate, it is the correct answer to the question you are asking.

It's October in an election year. Your inquiry is: "how many voters will vote Democrat in November?" The true answer is 66,221,143. This true answer is your estimand, you seek to estimate this number now, even though the election has not happened yet. On the basis of a survey your best guess is 65, 112, 114. This is your estimate for this estimand.

In this case the estimand is a number and one that will eventually be revealed, letting you assess how well your estimate measures up against your estimand. But in social science inequiry estimands can take many different forms. 

We describe eight families of estimand. These different families  reflect different social scientific orientations and often different 

It turns out that many estimands can be thought of as summaries of potential outcomes. TO describe these we will imagine a  simple model, in which $Y$ is thought to depend on $X$ and $M$ and $M$ in turn is thought to depend on $X$. We represent this barebones model using a DAG in figure \@ref(XMYgraph).

```{r XMYgraph, echo = FALSE, fig.caption = "X affects Y directly as well as indirectly via M"}

plot(dagitty::graphLayout(dagitty("dag{ 
  X -> Y ;
  X -> M ;
  M -> Y
 }")))
```

ADD NUMBERS WITH POTENTIAL OUTCOMES

USE ONLY PO NOTATION

### Descriptive estimands

Descriptive estimands can also require inference, not simply measurement. 

Simplest case level estimand:

$$X = 1$$
"Yes"

An example of a descriptive estimand is:

$$E_{i\in N}(Y)$$

$$E_{i\in N}(Y | X=1)$$

This descriptive estimand is to be distinguished from the counterfactual estimand:


$$\Pr(Y=1 | X \leftarrow 1)$$

where $\leftarrow$ is interpreted to mean that $X$ is "set" to the indicated value. 

* Moments; covariance 

### Simple causal estimands

The simplest causal estimand is the outcome that a unit (or group) would have under a possibly counterfactual condition. The expected **potential outcome**:

$$\Pr(Y=1 | X \leftarrow 1)$$


The **average treatment effect** is a summary of such potential outcomes across two conditions.

$$\Pr(Y=1 | X \leftarrow 1) - \Pr(Y=1 | X \leftarrow 0)$$

Defined over a population *N*, the average treatment effect is written using potential outcomes notation as:

$$E_{i\in N}(Y(X=1) - Y(X=0))$$

These simple estimands might condition on observational quantities, giving rise to the **conditional average treatment effect**:

$$\Pr(Y=1 | X \leftarrow 1, M = 1) - \Pr(Y=1 | X \leftarrow 0, M = 1)$$

or, perhaps,  **controlled conditional average treatment effects**:

$$\Pr(Y=1 | X \leftarrow 1, M \leftarrow 1) - \Pr(Y=1 | X \leftarrow 0, M \leftarrow 1)$$

or, perhaps *differences* in effects:


**Causes of effects** estimand:


### Local estimands / EStiamnds over latent classes, principal strata

Complier average is also a CATE
RDD is also a CATE
Estimands as summaries

### Study dependent estimands
Expected ATT vs realized ATT

### Complex counterfactuals


### Parametric estimands

Dimensionality reduction
Model parameters

### Vector-valued inquiries (set of predictions for N units), 

The diagnostic statistic is ith respect to the vector

e.g. the CEF

QCA estimand.

### Models as estimands



### Selecting estimands

### You are responsible for your estimand

### Estimands to purpose. Choice of inquiry is not valueless


### Estimand scope:What is the set of units which you want to learn the answer about? Know what ATE averages over

Implication of estimand definition for analysis

### Unknown estimands: Inquiries for discovery



## Auxilliary Inquiries

## Connections to MIDA

## How you already specify inquiries in current practice


- these inquiries are not the main substantive focus of the design, but they are features of the model that have observable implications and can be checked. 

- For models that include mediating roles for a variable, need to demonstrate that Z affects M at a minimum.

- Balance tables demonstrate a required assumption that Z is orthogonal to pre-treatment characteristics

- A full DAG specification can enumerate the full set of conditional independencies that are implied by the model; these can be checked and verified.


