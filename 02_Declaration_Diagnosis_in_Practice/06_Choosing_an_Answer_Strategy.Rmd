---
title: "Choosing an answer strategy"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# Choosing an answer strategy

<!-- make sure to rename the section title below -->

```{r choosing_an_answer_strategy, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 10
b_sims <- 10
```

```{r, echo = FALSE}
tau <- .10
N <- 8
N_sampled <- 4
population <- declare_population(N = N, e = runif(N)) 
potential_outcomes <- declare_potential_outcomes(
  Y_Z_0 = .5 < e, Y_Z_1 = .5 < e + tau)
estimand <- declare_estimand(PATE = mean(Y_Z_1 - Y_Z_0))
sampling <- declare_sampling(n = N_sampled)
assignment <- declare_assignment(prob = .5)
reveal_outcomes <- declare_reveal(Y, Z)
estimator <- declare_estimator(Y ~ Z, label = "DiM", estimand = "PATE")
simple_design <- population + potential_outcomes + estimand + 
  sampling + assignment + reveal_outcomes + estimator
simple_design_data <- draw_data(simple_design)
```

## What belongs in an answer strategy

### estimate-estimator pairs 

- once you have the data, you need to have a procedure to develop an answer or a decision from it. it should be quantitative or qualitative. 

- you will connect an estimator to an estimand, and the estimator designed to produces estimates of the estimand. distinguish estimate/estimator, using the notation from the paper (am Am etc.).

- a function to produce an estimate and measure(s) of uncertainty of the estimate.

- may be as simple as a mean or difference-in-means, as in our simple design:

```{r}
estimates_df <- difference_in_means(Y ~ Z, data = simple_design_data)
kable(tidy(estimates_df))
```

- in this case, there is a single statistic (the average difference between outcomes in treated and controlled) that represents the *estimate*. this is our guess of the estimand, the average treatment effect. 

### measures of uncertainty 

- in addition, we have several statistics that assess the *uncertainty of the estimate*, here the standard error and a frequentist confidence interval. the answer strategy is not just how you get to the answer, but how sure you are of it. 

- we often also have statistics related to *hypothesis testing*, here a test statistics and p-value under the null hypothesis of a zero average treatment effect. our "answer" may either be the estimate of the average treatment effect, or in some cases the decision, is there a non-zero average treatment effect. 

- your answer strategy is the full set of steps from first seeing the data until the estimate of the estimand you present in the paper, which is usually more than just the estimate, its uncertainty measure, and associated hypothesis test. 


### procedures 

- *procedures*, if any, by which you explore the data and determine a final set of estimates are part of the answer strategy. for example, we sometimes find that the model we planned to run to analyze the data cannot be estimated. in these cases, there is an iterative estimation procedure in which a first model is run, changes to the specification are made, and a second or third model is presented as the result. that full set of steps -- a decision tree, depending on what is estimable -- is the answer strategy and we can evaluate whether it is a good one not only under the realized data but under other possible realizations where the decision *tree* would be the same but the decisions different.

- *procedures where you run two procedures and pick the best fit or preferred on some dimensions* 
  - show example of a procedure of this form (model selection?) where the coverage is off if you don't account for the multi step

```{r}
# haven't gotten something good here. 

test_sample_estimator <-
  function(data){
    r_sq_full <- lm_robust(Y ~ Z + X, data = data, subset = train == 1)$r.squared
    r_sq_short <- lm_robust(Y ~ Z, data = data, subset = train == 1)$r.squared
    if(r_sq_full > r_sq_short) {
      lm_robust(Y ~ Z + X, subset = train == 0, data = data) %>% 
        tidy %>% filter(term == "Z") %>% mutate(which = "full")
    } else {
      lm_robust(Y ~ Z, subset = train == 0, data = data) %>% 
        tidy %>% filter(term == "Z") %>% mutate(which = "short")
    }
  }

design <-
  declare_population(
    N = 100, X = rbinom(N, 1, 0.5), u = rnorm(N), train = rep(0:1, each = 50)
  ) + 
  declare_potential_outcomes(Y ~ Z + u) + 
  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + 
  declare_assignment(prob = 0.5) + 
  declare_reveal(Y, Z) + 
  declare_estimator(Y ~ Z, model = lm_robust, label = "one-step", estimand = "ATE") + 
  declare_estimator(
    handler = tidy_estimator(test_sample_estimator),
    estimand = "ATE") 

dg <- diagnose_design(design, sims = 5000)
```

- *procedures for testing assumptions of identification strategy before running analysis*, such as falsification or placebo tests. in these tests, you run a test and only analyze the data using the analysis strategy you proposed if it passes the test indicating a failure to reject the null of no violation of the assumptions.
  - show example of RDD where the method is *biased* if you don't use the assumption test but the procedure is *unbiased*

```{r}

```

- precommittment is part of the answer strategy

### robustness checks

- *robustness checks* are part of the answer strategy. often, a single estimator is presented as the main analysis but then a series of alternative specifications are displayed in an appendix (such as including or excluding covariates and their interactions, different subsets of the data, or alternative statistical models). the purpose is to provide readers with evidence about how dependent the main results are on the specification, data subset, and statistical model used. when this is the case, the decision a reader makes based on their inferences about the estimand from the estimate depend not only on the main estimate but also the robustness checks. as a result, we want to assess the properties of the two together. 
- distinguish this from changes to the model where we do robustnesss vis a vis a fixed answer and data strategy. i.e. two notions of "robustness". one is fix I D A and change M, is this "design" robust to changes in M. the other is, within a given run, is the estimate "robust" to changing the estimation procedure, so this is a diagnostic statistic. note I must be defined across these changes in M.

### presentation of results

- *how you present the estimates* --- graphically, in tables, and in text --- are all parts of the answer strategy. this is because the inferences readers make about the estimand from your paper do not just come from the numerical estimate. in some cases, the number may not even be presented exactly, and instead a graphic of the estimate and its confidence interval is what readers rely on.

    - lots of advice to present graphically (cite), what are implications of that? the decisions made from your results by readers are not just a function of numerical estimates but how they are presented. 
    
We explore this by comparing two possible graphical displays of conditional avareage treatment effects in an experiment. A common presentational format is to present the average treatment effect in one group and then the other along with confidence intervals. Inferences are made --- either by the author, or by readers --- as a function of whether one is significant and not the other. If that is true, the inference is that there is a difference in CATEs. An alternative is to present the estimated difference along with the two effects. The inferences can then directly be based on whether the confidence interval of the difference crosses zero. We illustrate these two visual answer strategies below:
    
```{r, echo = FALSE}
ATE <- 0.0

design <- 
  declare_population(N = 1000,
                     binary_covariate = rbinom(N, 1, 0.5),
                     normal_error = rnorm(N)) +
  # crucial step in POs: effects are not heterogeneous
  declare_potential_outcomes(Y ~ ATE * Z + normal_error) +
  declare_assignment(prob = 0.5) +
  declare_estimator(Y ~ Z, subset = (binary_covariate == 0), label = "CATE_0") + 
  declare_estimator(Y ~ Z, subset = (binary_covariate == 1), label = "CATE_1") +
  declare_estimator(Y ~ Z * binary_covariate, 
                    model = lm_robust, term = "Z:binary_covariate", label = "interaction")
```

```{r, echo = FALSE, fig.height = 3}
estimates <- draw_estimates(design)

g1 <- ggplot(data = estimates %>% filter(term == "Z"), aes(estimator_label, estimate)) + 
  geom_point() + 
  geom_errorbar(aes(x = estimator_label, ymin = conf.low, ymax = conf.high), width = 0.2) + 
  ylab("Estimate (95% confidence interval)") +
  geom_hline(yintercept = 0, lty = "dashed") +
  ggtitle("Visualization A") +
  dd_theme() + 
  theme(axis.title.x = element_blank())

g2 <- ggplot(data = estimates, aes(estimator_label, estimate)) + 
  geom_point() + 
  geom_errorbar(aes(x = estimator_label, ymin = conf.low, ymax = conf.high), width = 0.2) + 
  ylab("Estimate (95% confidence interval)") +
  geom_hline(yintercept = 0, lty = "dashed") +
  ggtitle("Visualization B") +
  dd_theme() + 
  theme(axis.title.x = element_blank())

grid.arrange(g1, g2, nrow = 1)
```

We now demonstrate that the answer strategy on the left is flawed. XXYY describe sims.

```{r, echo = FALSE}
# sweep across all ATEs from 0 to 0.5
designs <- redesign(design, ATE = seq(0, 0.5, 0.05))
```

```{r, echo = FALSE, eval = do_diagnosis & !exists("do_bookdown")}
simulations_one_significant_not_other <- simulate_design(designs, sims = sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("answer_strategy"), "/simulations_one_significant_not_other.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(simulations_one_significant_not_other, path = rds_file_path)
}
simulations_one_significant_not_other <- read_rds(rds_file_path)
```

```{r, echo = FALSE, fig.height = 3}
# Summarize simulations ---------------------------------------------------

reshaped_simulations <-
  simulations_one_significant_not_other %>%
  transmute(ATE,
            sim_ID,
            estimator_label,
            estimate,
            conf.high,
            conf.low,
            significant = p.value < 0.05) %>%
  pivot_wider(id_cols = c("ATE", "sim_ID"), names_from = "estimator_label", values_from = c("estimate", "conf.high", "conf.low", "significant"))


# Plot 1 ------------------------------------------------------------------

gg_df <- 
  reshaped_simulations %>%
  group_by(ATE) %>%
  summarize(`Significant for one group but not the other` = mean(xor(significant_CATE_0, significant_CATE_1)),
            `Difference in subgroup effects is significant` = mean(significant_interaction)) %>%
  gather(condition, power, -ATE)

ggplot(gg_df, aes(ATE, power, color = condition)) +
  geom_point() +
  geom_line() +
  geom_label(data = (. %>% filter(ATE == 0.2)),
             aes(label = condition),
             nudge_y = 0.02,
             family = "Palatino") +
  dd_theme() +
  scale_color_manual(values = c("red", "blue")) +
  theme(legend.position = "none") +
  labs(
    x = "True constant effect size",
    y = "Probability of result (akin to statistical power)"
  )
```

### multiple comparisons

- your answer strategy should take into account *how many statistical tests* you are conducting, not just focus on the estimate-estimand pair. when you present the results from many null hypothesis  tests, the rate of falsely rejecting at least one of those tests even when all are true goes up, due to the multiple comparisons problem. if you plan to adjust for this problem, those adjustments are part of your answer strategy, because they will typically adjust the p-values you report and the decisions readers make with them.

- as this seection has highlighted, the answer strategy is intimately connected with the data strategy. people often think of their entire research design as the answer strategy. but they can't be separated.

<!-- ## what are the properties of a good answer strategy -->

<!-- - typically, we want to find an estimation strategy that, given the model and data strategy, produces estimates that have as close as possible as often as possible to the estimand. there are several ways of thinking about how close they are and how often. *bias, MSE.* as we discuss in section XX (diagnosis), you should select the set of diagnosands to purpose, depending on why you are conducting the research and what decisions you expect readers to make in response to it. -->

<!-- - we often think about increasing the power of a design by changing our sample size or other aspects of the data strategy. but there is often significant room to grow power through the answer strategy alone, holding the data strategy constant. in an experiment, controlling for pretreatment covariates that are very predictive of the outcome often yields significant power gains, for example.  -->

<!-- - best in the set (among set of simple linear models, what is the best model) -->

### what you will do when data goes sideways

- to compare answer strategies, you can imagine the estimators that are possible *if things go well* as well as *if things go wrong*, when there is missing data or there are outliers in variables. a good answer strategy (which might be a single estimator, or a procedure if-this-then-that) can handle both states of the world. 

- *procedures for addressing deviations from expected analyses* are part of the answer strategy. whether a study has a PAP or not, we often have a way we expect to analyze the data if things go well. when they do not -- because data are missing, there is noncompliance to an intervention, or the study is suspended for example -- the answers will change. these procedures determine the answer the study provides (or in some cases does not), so are part of the answer strategy. *standard operating procedures* (lin and green) are documents that systematize these procedures in advange.

- point is about what you do not the model aspects of this

- properties of imputation procedure

- standard operating procedures

## How to select an answer strategy

- introduce classes of estimators: qual/quant, frequentist/bayesian, design based and model based (logit probit etc.)

- talk about issue of model-based vs design based, as separated from the model you assume in M. in model based you run a procedure that assumes a dgp, which may or may not be connected to the M.

- your data strategy should shape your answer strategy (analyse as you randomize)
  - assignment strategies (blocks and/or clusters, heterogeneous assignment probabilities, etc.)
  - sampling strategies (strata and/or clusters, heterogeneous sampling probabilities, etc.)
  
- this is true not just for experiments but for surveys (how did you sample), natural experiments (how did nature assign the treatment), and other designs

- you can *select an answer strategy in advance*, by simulating data. when estimators are selected with the data in hand, choices are often made in response to the realized data through examining model fit statistics that appear ideal in the context of this data, but are not ideal from the perspective of other data that could have been collected. we want answer strategies that perform well no matter how the data turn out. 
  - issues are: overfitting, selecting a suboptimal estimator from design perspective that passes a model fit statistic (show this is worse in a simple example)
