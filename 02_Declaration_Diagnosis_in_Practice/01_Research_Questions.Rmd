---
title: "Research questions"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# Research questions

<!-- make sure to rename the section title below -->

```{r research_questions, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```


For most of this book, we're going to imagine that you have already selected a research question and now the main goal is to tailor your design to that research question. In fact that's where most books on research methodology begin. They presuppose that you already have something interesting that you want to learn and now the question is, what's the best way to learn it.

This strategy makes some sense. Since there are so many things we *don't know* about the social world, researchers are positively overbrimming with interesting theories to test. Then again, most of us are a little bit bewildered when it comes to actually formulating a research question that would simultaneously be worth knowing the answer to and that would be feasible to answer. This section considers and critiques some common research question formulation advice. Perhaps unsurprising, our overarching perspective is that we can't compromise on research design -- scientific conclusions depend essentially on strong research designs. That said, choosing research questions is much more art than science. Moreover, the choice of what to study is not valueless in the sense that scholars' research agendas are reflections of their values and their politics. While scientists seek to reveal objective truths, we also have to choose **which** truths to investigate, and that choice is value-laden.

## The false choice between theory-first and method-first approaches.

Some research questions are boring and trivial -- no doubt you have encountered such research questions when reading bad research. Figuring our the best way to answer unimportant questions can be a stimulating intellectual puzzle, but the answers to those questions themselves will not advance scientific knowledge. Some research questions are exciting and vital -- those are the ones we want to answer using credible research designs. Giving weak or noncredible answers to important research questions is *dangerous* and unethical. 

Sometimes scholarly debate about research design treat these two desiderata (good questions and strong designs) as forming a one-dimensional spectrum. To the left we have boring studies done well and to the right we have interesting studies with weak empirics. This "spectrum" view manufactures a tradeoff between the quality of the question and the strength of the research design. It tricks students into thinking that they have to make compromises on the research design (i.e., control for enough things and hope for the best) in order to do great things. 

The reality is that research is hard and sometimes it is so hard that we must abandon them. Good questions might be too damn hard to answer and we must deliberately walk away from them even though they are interesting. Bad questions aren't worth the effort, even if they are feasible -- walk away from them too. Both interesting questions and strong research designs to answer them are required.

Advice about how to solve this dilemma often takes one of two forms: theory-first approaches and method-first approachs. Under the theory-driven approach, you read a lot of others' work in order to formulate a good research question; you then search among the available methods to find the "right" method for the question. This approach is risky because it is very hard to abandon a good research questions, even if none of the available methods is good enough. The worst case scenario for the theory-first is a research process that takes forever, scholars falling prey to the sunk cost fallacy and falling to abandon a hopeless project, and a final research project that relies on heroic assumptions and makes no real progress on the interesting research question.  

The method-first approach involves learning a method well first, then looking for interesting projects to apply it to later. This frequently means *practicing* the method for the method's sake. To become an expert interviewer, you must conduct many interviews. To become an expert experimentalist, you must randomize many treatments. To become an expert observational data analyst, you must download many datasets and develop a sense for when inferences are reliable and when they are not. The worst-case scenario for the method-first approach is that scholars never stop *practicing* research to actually finally *conduct* research on important topics. What good is it to be a top-flight archivist if those skills are never applied to good questions.

Obviously, a mix of the two approaches is required, and most of variation in advice given to young scholars isn't just one or the other, but the relative weight given to the two. As a team of authors, we disagree amongst ourselves about the right weights. That said, the downsides to boring but well-done research don't seem as bad to us as the downsides of interesting but poorly-done research. Bad studies on important questions are used by disingenous and bad faith actors to push for policies that are bad for society. The ethical stakes are higher for important questions than they are for unimportant questions. For that reason alone, we think it is vitally important that researchers master their methods so as to assiduously avoid making unknowably incorrect claims.

People who are looking in the forest for mushrooms often don't see a mushroom for a long period of time. After a while, they acclimate. They get their ``eyes on,'' and successful finds seem to be around every bend. In this analogy, the theory building process is going for a walk in the forest and methods training is learning to spot mushrooms -- you need get your eyes on answerable research questions worth answering.

In our experience, that means that the most important thing for young scholars to do is practice. Practice both searching for interesting questions and practice research design. Practicing research design often involves honing one's skills on less than interesting questions.￼ Looking for interesting questions entails reading other scholars' work carefully and generously. Even when their research designs are not strong, their questions might nevertheless be interesting. You may learn from reading bad work on interesting questions that some research questions are impossible to answer. That's OK -- now you’ve learned about the constraints on what we can learn. Operating within the constraints of what is knowable means you can anticipate when a poor research design on an interesting question is unlikely to lead to progress.

## Novelty
Another route for finding research questions is to try to determine what is unknown to the research community, then to seek to fill that gap. This approach can be quite generative, but it is not without pitfalls. Establishing primacy -- that a finding is "new" and that the knowledge generated by the study was previously unknown -- is hard. It's also risky, since peer reviewers have access to Google Scholar. False primacy claims are especially damaging during the review process because they are easily refuted and demonstrate ignorance of the published record.

What does it mean for a research finding to be novel? There are two main cases. First, a finding is new if the empirical pattern was predicted by an earlier theory but had yet to be confirmed. That is, there was a theoretical prediction that had not yet found its empirical verification. The empirical finding is novel in that it confirms the previously unconfirmed theoretical prediction. Second, an empirical finding is also novel if the empirical reality was *not* predicted by the theory.
If the empirical funding is trustworthy, it means that the original theory is incomplete or incorrect in a particular way. 
Findings that contradict theoretical predictions are also novel. Empirical findings are not novel when the theoretical predictions that pertain to the finding have already been well-documented.  

This puts the advance of science in a funny position. Because we are worried that many of the advances made at the forefront of the scientific endeavor are incorrect or spurious, we really value replication of novel results. When an empirical claim in favor of a theoretical proposition is made in Place A at Time 1, we immediately want to know if it holds at Place B at Time 2 as well.

## Mixing methods

A common piece of advice for young scholars who are interested in a topic is to mix methods. The thinking goes like this: I have an important question but none of my methods perfectly answers the question. I'll take a multifaceted approach that gets that many different aspects of the question which I hope to aggregate into a whole.￼￼

We caution against this approach. Mastering one method is difficult; mastering three is very difficult.￼
Even when mastery of the methods is not an obstacle, the problem that often occurs is that the three research designs actually address *different* research questions. That is, not only do the three research designs use different data strategies and answer strategies, they also employ different models and inquiries. As a result, mixed methods work often gives three incomplete answers to three related, but ultimately different, research questions. 
