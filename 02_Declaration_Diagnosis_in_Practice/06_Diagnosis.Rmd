---
title: "Diagnosis"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# Diagnosis

<!-- make sure to rename the section title below -->

```{r diagnosis, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
library(ggforce)
```

## Diagnosing a design

Research diagnosis is two steps: simulating running a design over and over, and a quantitative summary of runs of the design. 

Diagnosis is the process of evaluating the properties of a research design. Since "property of a research design" is a cumbersome phrase, we made up the word "diagnosand" to refer to those properties of a research design we would like to diagnose. Many diagnosands are familiar. Power is the probability of obtaining a statistically significant result. Bias is the average deviation of estimates from the true value of the estimand. Others are more exotic, like the Type-S error rate, which is the probability the estimate has the incorrect sign, conditional on being statistically significant (Gelman and Carlin 2014).

Not every diagnosand is relevant for every study. For example, in a descriptive study whose goal is to estimate the fraction of people in France who are left-handed, statistical power is irrelevant. A hypothesis test against the null hypothesis that 0 percent of the people in France are left-handed is preposterous. We know for sure that the fraction is not zero, we just don't know its precise value. A much more important diagnosand for this study would be RMSE (root-mean-squared-error), which is a measure of how well-measured the estimand that incorporates both bias and variance.

How should you choose your diagnosands? In our experience, writing out what would make the study either a success or a failure helps tremendously. If a successful study would yield an estimate that is higher than 4, has a standard error of 1 or less, and in which we find statistically significant evidence of treatmnet effect heterogeneity -- write down a diagnosand that is the probability of all three of those conditions being true at the same time. If a study is a failure (in terms of not having been worth the money and effort *ex post*) when the confidence interval is 20 points wide or wider, then the research team should design to minimize the probability of that occurance.

## Estimating diagnosands analytically

Research design diagnosis can be done analytically. Indeed, research design textbooks often contain many formulas for calculating power under a variety of designs. For example, Gerber and Green include the following power formula:

They write: 

> "To illustrate a power analysis, consider a completely randomized experiment where $N>2$ of $N$ units are selected into a binary treatment. The researcher must now make assumptions about the distributions of outcomes for treatment and for control units. In this example, the researcher assumes that the control group has a normally distributed outcome with mean $\mu_c$, the treatment group has a normally distributed outcome with mean $\mu_t$, and both group's outcomes have a standard deviation $\sigma$. The researcher must also choose $\alpha$, the desired level of statistical significance (typically 0.05).
Under this scenario, there exists a simple asymptotic approximation for the power of the experiment (assuming that the significance test is two-tailed):
> $$
\beta = \Phi \bigg(\frac{|\mu_t - \mu_c| \sqrt{N}}{2\sigma} - \Phi^{-1} (1 - \frac{\alpha}{2}) \bigg)
> $$
where $\beta$ is the statistical power of the experiment, $\Phi(\cdot)$ is the normal cumulative distribution function (CDF), and $\Phi^{-1}(\cdot)$ is the inverse of the normal CDF."

See how this power formula makes specific assumptions about $M$, $D$, and $A$? Under $M$, it assumes that the potential outcomes are both normally distributed with group specific means and a common variance. Under $D$, it assumes a particular randomization strategy. Under $A$, it assumes a particular hypothesis testing approach (equal variance t-test with $N - 2$ degrees of freedom). This set of assumptions may be "close enough" in many research settings, but it can be difficult to understand the specific impacts of different beliefs about $M$, $D$ and $A$ on the value of the diagnosand. What if instead of being normally distributed, the potential outcomes are measured in 1 - 5 Likert scales? What if the randomization procedure includes blocking? What if we include covariates in our treatment effect estimation approach? Formulas for some large sources of design variation have been derived (such as clustering), but certainly not for every design variant.

Very quickly, hope for analytic design diagnosis fades. The analytic formulas are abstractions -- they abstract away from design details and sometimes those design details are important. This problem is not confined to the "power" diagnosand. In randomized experiments, claims about the bias diagnosand are quite general.  Many randomized designs are unbiased for the ATE, but not all. Designs that encounter noncompliance, attrition, or some forms of spillover may not be unbiased for the ATE. Even without any of those complications, cluster randomized trials with heteogeneous cluster sizes are not unbiased (joel, imai). 

Diagnosands depend on design details, because how you conduct you conduct your study matters for its properties. That means design diagnosis must be design-aware. Since designs are so heteogenous and can vary on so many dimensions, computer simulation is only feasible way to characterize any study but the most ideal-type designs (assuming normality and ignoring heterogeneity) in the simplest of terms (power, bias, variance, but nothing further).


## Estimating diagnosands via simulation

Our main trouble is learning the distribution of diagnostic statistics -- if we could simply write down the distribution of diagnostic statistics, it would be a straightforward matter to summarize them in order to calculate diagnosands. 

Diagnosands can be estimated with higher levels of precision by increasing $m$. However, simulations are often computationally expensive. In order to assess whether researchers have conducted enough simulations to be confident in their diagnosand estimates, we recommend estimating the sampling distributions of the diagnosands via the nonparametric bootstrap.^[
In their paper on simulating clinical trials through Monte Carlo, @morrisetal2019 provide helpful analytic formula for deriving Monte Carlo standard errors for several diagnosands ("performance measures"). In the companion software, we adopt a non-parametric bootstrap approach that is able to calculate standard errors for any user-provided diagnosand.] With the estimated diagnosand and its standard error, we can characterize our uncertainty about whether the range of likely values of the diagnosand compare favorably to reference values such as statistical power of $0.8$.^[This procedure depends on the researcher choosing a "good" diagnosand estimator. In nearly all cases, diagnosands will be features of the distribution of a diagnostic statistic that, given i.i.d. sampling, can be consistently estimated via plug-in estimation (for example taking sample means). Our simulation procedure, by construction, yields i.i.d. draws of the diagnostic statistic.]


- Graphic of simulations (of multiple runs)


- must do a TYPE S error rate histogram with three areas. Sig with wrong sign, not sig, sig with right sign. PLEASE NOTE: some of the "power" of a study comes from the fraction of simulations with Significant estimates with the wrong sign.  That's just crazy!





```{r, echo = FALSE}
knitr::include_app('https://ushintsho.shinyapps.io/diagnosis/', height = '400px')
```


```{r, echo = FALSE, fig.width = 6.5, fig.height = 3}
n <- 50

points_df <- tibble(rho = sqrt(runif(n)),
                    theta = runif(n, 0, 2*pi),
                    x = rho * cos(theta),
                    y = rho * sin(theta)
)

summary_df <- points_df %>% 
  summarize(
    bias = round(mean(sqrt(x^2 + y^2)), 2),
    mse = round(mean(x^2 + y^2), 2),
    var = round(mean((x - mean(x))^2 + (y - mean(y))^2), 3)
  )

unbiased_lowprecision <- ggplot() + 
  geom_circle(
    data = tibble(
      x0 = rep(0, 5),
      y0 = rep(0, 5),
      r = rev(seq(0.1, 1, length.out = 5))
    ), 
    aes(x0 = x0, y0 = y0, r = r, fill = fct_rev(as.factor(r))),
    col = gray(0.25), lwd = 0.25, alpha = 0.5) +
  geom_point(data = points_df, aes(x, y), size = 0.5) + 
  scale_fill_manual(values = rev(c("yellow", "red", "cyan", "black", "white"))) + 
  coord_fixed() + 
  dd_theme() + 
  xlab("") + ylab("") + 
  ggtitle("Unbiased, imprecise", subtitle = glue::glue("Variance = {summary_df$var}\nBias = {summary_df$bias}\nRMSE = {summary_df$mse}")) + 
  theme(
    legend.position = "none",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank()
  )

n <- 50

points_df <- tibble(
  rho = sqrt(runif(n, min = 0, max = 0.015)),
  theta = runif(n, 0, 2*pi),
  x = rho * cos(theta),
  y = rho * sin(theta)
)

summary_df <- points_df %>% 
  summarize(
    bias = round(mean(sqrt(x^2 + y^2)), 2),
    mse = round(mean(x^2 + y^2), 2),
    var = round(mean((x - mean(x))^2 + (y - mean(y))^2), 3)
  )

unbiased_highprecision <- ggplot() + 
  geom_circle(
    data = tibble(
      x0 = rep(0, 5),
      y0 = rep(0, 5),
      r = rev(seq(0.1, 1, length.out = 5))
    ), 
    aes(x0 = x0, y0 = y0, r = r, fill = fct_rev(as.factor(r))),
    col = gray(0.25), lwd = 0.25, alpha = 0.5) +
  geom_point(data = points_df, aes(x, y), size = 0.5) + 
  scale_fill_manual(values = rev(c("yellow", "red", "cyan", "black", "white"))) + 
  coord_fixed() + 
  dd_theme() + 
  xlab("") + ylab("") + 
  ggtitle("Unbiased, imprecise", subtitle = glue::glue("Variance = {summary_df$var}\nBias = {summary_df$bias}\nRMSE = {summary_df$mse}")) + 
  theme(
    legend.position = "none",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank()
  )

n <- 50

points_df <- tibble(
  rho = sqrt(runif(n, min = 0, max = 0.015)),
  theta = runif(n, 0, 2*pi),
  x = rho * cos(theta) + 0.5,
  y = rho * sin(theta) - 0.2
)

summary_df <- points_df %>% 
  summarize(
    bias = round(mean(sqrt(x^2 + y^2)), 2),
    mse = round(mean(x^2 + y^2), 2),
    var = round(mean((x - mean(x))^2 + (y - mean(y))^2), 3)
  )

biased_highprecision <- ggplot() + 
  geom_circle(
    data = tibble(
      x0 = rep(0, 5),
      y0 = rep(0, 5),
      r = rev(seq(0.1, 1, length.out = 5))
    ), 
    aes(x0 = x0, y0 = y0, r = r, fill = fct_rev(as.factor(r))),
    col = gray(0.25), lwd = 0.25, alpha = 0.5) +
  geom_point(data = points_df, aes(x, y), size = 0.5) + 
  scale_fill_manual(values = rev(c("yellow", "red", "cyan", "black", "white"))) + 
  coord_fixed() + 
  dd_theme() + 
  xlab("") + ylab("") + 
  ggtitle("Unbiased, imprecise", subtitle = glue::glue("Variance = {summary_df$var}\nBias = {summary_df$bias}\nRMSE = {summary_df$mse}")) + 
  theme(
    legend.position = "none",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank()
  )

library(patchwork)

unbiased_lowprecision + unbiased_highprecision + biased_highprecision
```


```{r}
n <- 50

points_df <- tibble(
  rho = sqrt(runif(n, min = 0, max = 0.015)),
  theta = runif(n, 0, 2*pi),
  x = rho * cos(theta),
  y = rho * sin(theta)
)

summary_df <- points_df %>% 
  summarize(
    bias_1 = round(mean(sqrt(x^2 + y^2)), 2),
    mse_1 = round(mean(x^2 + y^2), 2),
    var_1 = round(mean((x - mean(x))^2 + (y - mean(y))^2), 3),
    
    bias_2 = round(mean(sqrt((x-0.25)^2 + y^2)), 2),
    mse_2 = round(mean((x-0.25)^2 + y^2), 2),
    var_2 = round(mean(((x-0.25) - mean(x-0.25))^2 + (y - mean(y))^2), 3)
  )

circle_df <- 
  bind_rows(
    "left" = tibble(
      x0 = rep(0, 5),
      y0 = rep(0, 5),
      r = rev(seq(0.1, 1, length.out = 5))
    ),
    "right" = tibble(
      x0 = rep(0.35, 5),
      y0 = rep(0, 5),
      r = rev(seq(0.1, 1, length.out = 5))
    ),
    .id = "side"
  ) %>% 
  arrange(desc(r), side)

dual_estimands <- ggplot() + 
  geom_circle(
    data = circle_df, 
    aes(x0 = x0, y0 = y0, r = r, fill = fct_rev(as.factor(r))),
    col = gray(0.25), lwd = 0.25) +
  geom_point(data = points_df, aes(x, y), size = 0.5) + 
  scale_fill_manual(values = rev(c("yellow", "red", "cyan", "black", "white"))) + 
  coord_fixed() + 
  dd_theme() + 
  xlab("") + ylab("") + 
  ggtitle("Diagnosands depend on the estimand", subtitle = glue::glue("Variance = {summary_df$var_1} (left), {summary_df$var_2} (right)\nBias = {summary_df$bias_1} (left), {summary_df$bias_2} (right)\nRMSE = {summary_df$mse_1} (left), {summary_df$mse_2} (right)")) + 
  theme(
    legend.position = "none",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank()
  )
dual_estimands
```



## Four large classes of diagnosands:

- Type $M$ error rate
- Type $I$ error rate
- Type $D$ error rate
- Type $A$ error rate

You can get things wrong if anything in your research design is wrong!






