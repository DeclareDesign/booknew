---
title: "Diagnosis"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# Diagnosis

<!-- make sure to rename the section title below -->

```{r diagnosis, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
library(ggforce)
```

## Diagnosing a design

Research diagnosis is two steps: simulating running a design over and over, and a quantitative summary of runs of the design. 

Diagnosis is the process of evaluating the properties of a research design. Since "property of a research design" is a cumbersome phrase, we made up the word "diagnosand" to refer to those properties of a research design we would like to diagnose. Many diagnosands are familiar. Power is the probability of obtaining a statistically significant result. Bias is the average deviation of estimates from the true value of the estimand. Others are more exotic, like the Type-S error rate, which is the probability the estimate has the incorrect sign, conditional on being statistically significant (Gelman and Carlin 2014).

Not every diagnosand is relevant for every study. For example, in a descriptive study whose goal is to estimate the fraction of people in France who are left-handed, statistical power is irrelevant. A hypothesis test against the null hypothesis that 0 percent of the people in France are left-handed is preposterous. We know for sure that the fraction is not zero, we just don't know its precise value. A much more important diagnosand for this study would be RMSE (root-mean-squared-error), which is a measure of how well-measured the estimand that incorporates both bias and variance.

How should you choose your diagnosands? In our experience, writing out what would make the study either a success or a failure helps tremendously. If a successful study would yield an estimate that is higher than 4, has a standard error of 1 or less, and in which we find statistically significant evidence of treatmnet effect heterogeneity -- write down a diagnosand that is the probability of all three of those conditions being true at the same time. If a study is a failure (in terms of not having been worth the money and effort *ex post*) when the confidence interval is 20 points wide or wider, then the research team should design to minimize the probability of that occurance.

# Returning to the formalization


The model $M$ produces a realized state of the world $m$.
The inquiry $I$ is a question about the model $M$ that produces an answer $a^M$. It is calculated on the realized state of the world $m$. In other words, it is the value of estimand.

$$
a^M = I(m)
$$


The data strategy $D$ uses the realized state of the world $m$ to produce a dataset $d$. 

$$
d = D(m)
$$

The answer strategy $A$ uses the realized data $d$ to produce an estimate $a^A$. 

$$
a^A = A(d)
$$

A "diagnostic statistic" is (usually) a scalar function $g()$ of both $a^M$ and $a^A$, though some diagnostic statistics may be a function of just one or the other.  [CAN DIAGNOSTIC STATISTICS BE VECTOR VALUED?] 


$$
\Gamma = g(a^M, a^A)
$$

Because $a^M$ and $a^A$ are random variables, and any function of random variables is also a random variable, $\Gamma$ is itself a random variable. A diagnosand is a summary of that random variable. Just like we summarise random variables with statistical functionals like the expectation and variance operators, a diagnosand is a statistical functional of the diagnostic statistic random variable $\Gamma$. 

$$
\phi = f(\Gamma)
$$

Since $f()$ is a statistical functional (like $E[]$ or $V[]$), it is itself *not* a random variable. It is a description of the distribution of the random variable $\Gamma$. We'll use the greek letter $\phi$ to describe the idea of a diagnosand in general.

Let's back up a moment to work through a concrete example of a diagnosand. Consider the diagnosand "bias". Bias is the average difference between the estimand and the estimate. Under a model that has two potential outcomes, a treated potential outcome and an untreated potential outcome, the inquiry might be the ATE. Under a single realization $m$ of the model $M$, the value of the ATE will be a particular number, which we call $a^M$. If our data strategy is simply to collect data on those who come to be treated versus those who don't (i.e., we do not use random assignment), and our answer strategy is difference-in-means, our answer $a^A$ could be systematically different from $a^M$. The diagnostic statistic is the error $a^A - a^M$; this error is a random variable because each draw of $m$ from $M$ is slightly different. The expectation of this random variable is $E[a^A - a^M]$, which is bias. 

Similarly, the common diagnosand power is also an expectation. It is the expectation of the diagnostic statistic $\mathbb{1}p <= 0.05$, which is an indicator function that equals 1 if the $p$-value is below 0.05 and 0 otherwise. 

## Estimating diagnosands analytically

Research design diagnosis can be done analytically. Indeed, research design textbooks often contain many formulas for calculating power under a variety of designs. For example, Gerber and Green include the following power formula:

They write: 

> "To illustrate a power analysis, consider a completely randomized experiment where $N>2$ of $N$ units are selected into a binary treatment. The researcher must now make assumptions about the distributions of outcomes for treatment and for control units. In this example, the researcher assumes that the control group has a normally distributed outcome with mean $\mu_c$, the treatment group has a normally distributed outcome with mean $\mu_t$, and both group's outcomes have a standard deviation $\sigma$. The researcher must also choose $\alpha$, the desired level of statistical significance (typically 0.05).
Under this scenario, there exists a simple asymptotic approximation for the power of the experiment (assuming that the significance test is two-tailed):
> $$
\beta = \Phi \bigg(\frac{|\mu_t - \mu_c| \sqrt{N}}{2\sigma} - \Phi^{-1} (1 - \frac{\alpha}{2}) \bigg)
> $$
where $\beta$ is the statistical power of the experiment, $\Phi(\cdot)$ is the normal cumulative distribution function (CDF), and $\Phi^{-1}(\cdot)$ is the inverse of the normal CDF."

See how this power formula makes specific assumptions about $M$, $D$, and $A$? Under $M$, it assumes that the potential outcomes are both normally distributed with group specific means and a common variance. Under $D$, it assumes a particular randomization strategy. Under $A$, it assumes a particular hypothesis testing approach (equal variance t-test with $N - 2$ degrees of freedom). This set of assumptions may be "close enough" in many research settings, but it can be difficult to understand the specific impacts of different beliefs about $M$, $D$ and $A$ on the value of the diagnosand. What if instead of being normally distributed, the potential outcomes are measured in 1 - 5 Likert scales? What if the randomization procedure includes blocking? What if we include covariates in our treatment effect estimation approach? Formulas for some large sources of design variation have been derived (such as clustering), but certainly not for every design variant.

Very quickly, hope for analytic design diagnosis fades. The analytic formulas are abstractions -- they abstract away from design details and sometimes those design details are important. This problem is not confined to the "power" diagnosand. In randomized experiments, claims about the bias diagnosand are quite general.  Many randomized designs are unbiased for the ATE, but not all. Designs that encounter noncompliance, attrition, or some forms of spillover may not be unbiased for the ATE. Even without any of those complications, cluster randomized trials with heteogeneous cluster sizes are not unbiased (joel, imai). 

Diagnosands depend on design details, because how you conduct you conduct your study matters for its properties. That means design diagnosis must be design-aware. Since designs are so heteogenous and can vary on so many dimensions, computer simulation is only feasible way to characterize any study but the most ideal-type designs (assuming normality and ignoring heterogeneity) in the simplest of terms (power, bias, variance, but nothing further).


## Estimating diagnosands via simulation

Our main trouble is learning the distribution of diagnostic statistics -- if we could simply write down the distribution of diagnostic statistics, it would be a straightforward matter to summarize them in order to calculate diagnosands. 




- Graphic of simulations (of multiple runs)




```{r, echo = FALSE}
knitr::include_app('https://ushintsho.shinyapps.io/diagnosis/', height = '400px')
```


```{r, echo = FALSE, fig.width = 6.5, fig.height = 3}
n <- 50

points_df <- tibble(rho = sqrt(runif(n)),
                    theta = runif(n, 0, 2*pi),
                    x = rho * cos(theta),
                    y = rho * sin(theta)
)

summary_df <- points_df %>% 
  summarize(
    bias = round(mean(sqrt(x^2 + y^2)), 2),
    mse = round(mean(x^2 + y^2), 2),
    var = round(mean((x - mean(x))^2 + (y - mean(y))^2), 3)
  )

unbiased_lowprecision <- ggplot() + 
  geom_circle(
    data = tibble(
      x0 = rep(0, 5),
      y0 = rep(0, 5),
      r = rev(seq(0.1, 1, length.out = 5))
    ), 
    aes(x0 = x0, y0 = y0, r = r, fill = fct_rev(as.factor(r))),
    col = gray(0.25), lwd = 0.25, alpha = 0.5) +
  geom_point(data = points_df, aes(x, y), size = 0.5) + 
  scale_fill_manual(values = rev(c("yellow", "red", "cyan", "black", "white"))) + 
  coord_fixed() + 
  dd_theme() + 
  xlab("") + ylab("") + 
  ggtitle("Unbiased, imprecise", subtitle = glue::glue("Variance = {summary_df$var}\nBias = {summary_df$bias}\nRMSE = {summary_df$mse}")) + 
  theme(
    legend.position = "none",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank()
  )

n <- 50

points_df <- tibble(
  rho = sqrt(runif(n, min = 0, max = 0.015)),
  theta = runif(n, 0, 2*pi),
  x = rho * cos(theta),
  y = rho * sin(theta)
)

summary_df <- points_df %>% 
  summarize(
    bias = round(mean(sqrt(x^2 + y^2)), 2),
    mse = round(mean(x^2 + y^2), 2),
    var = round(mean((x - mean(x))^2 + (y - mean(y))^2), 3)
  )

unbiased_highprecision <- ggplot() + 
  geom_circle(
    data = tibble(
      x0 = rep(0, 5),
      y0 = rep(0, 5),
      r = rev(seq(0.1, 1, length.out = 5))
    ), 
    aes(x0 = x0, y0 = y0, r = r, fill = fct_rev(as.factor(r))),
    col = gray(0.25), lwd = 0.25, alpha = 0.5) +
  geom_point(data = points_df, aes(x, y), size = 0.5) + 
  scale_fill_manual(values = rev(c("yellow", "red", "cyan", "black", "white"))) + 
  coord_fixed() + 
  dd_theme() + 
  xlab("") + ylab("") + 
  ggtitle("Unbiased, imprecise", subtitle = glue::glue("Variance = {summary_df$var}\nBias = {summary_df$bias}\nRMSE = {summary_df$mse}")) + 
  theme(
    legend.position = "none",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank()
  )

n <- 50

points_df <- tibble(
  rho = sqrt(runif(n, min = 0, max = 0.015)),
  theta = runif(n, 0, 2*pi),
  x = rho * cos(theta) + 0.5,
  y = rho * sin(theta) - 0.2
)

summary_df <- points_df %>% 
  summarize(
    bias = round(mean(sqrt(x^2 + y^2)), 2),
    mse = round(mean(x^2 + y^2), 2),
    var = round(mean((x - mean(x))^2 + (y - mean(y))^2), 3)
  )

biased_highprecision <- ggplot() + 
  geom_circle(
    data = tibble(
      x0 = rep(0, 5),
      y0 = rep(0, 5),
      r = rev(seq(0.1, 1, length.out = 5))
    ), 
    aes(x0 = x0, y0 = y0, r = r, fill = fct_rev(as.factor(r))),
    col = gray(0.25), lwd = 0.25, alpha = 0.5) +
  geom_point(data = points_df, aes(x, y), size = 0.5) + 
  scale_fill_manual(values = rev(c("yellow", "red", "cyan", "black", "white"))) + 
  coord_fixed() + 
  dd_theme() + 
  xlab("") + ylab("") + 
  ggtitle("Unbiased, imprecise", subtitle = glue::glue("Variance = {summary_df$var}\nBias = {summary_df$bias}\nRMSE = {summary_df$mse}")) + 
  theme(
    legend.position = "none",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank()
  )

library(patchwork)

unbiased_lowprecision + unbiased_highprecision + biased_highprecision
```


```{r}
n <- 50

points_df <- tibble(
  rho = sqrt(runif(n, min = 0, max = 0.015)),
  theta = runif(n, 0, 2*pi),
  x = rho * cos(theta) + 0.5,
  y = rho * sin(theta) - 0.2
)

summary_df <- points_df %>% 
  summarize(
    bias = round(mean(sqrt(x^2 + y^2)), 2),
    mse = round(mean(x^2 + y^2), 2),
    var = round(mean((x - mean(x))^2 + (y - mean(y))^2), 3)
  )

dual_estimands <- ggplot() + 
  geom_circle(
    data = tibble(
      x0 = rep(0, 5),
      y0 = rep(0, 5),
      r = rev(seq(0.1, 1, length.out = 5))
    ), 
    aes(x0 = x0, y0 = y0, r = r, fill = fct_rev(as.factor(r))),
    col = gray(0.25), lwd = 0.25) +
  geom_circle(
    data = tibble(
      x0 = rep(0.05, 5),
      y0 = rep(0.0, 5),
      r = rev(seq(0.1, 1, length.out = 5))
    ), 
    aes(x0 = x0, y0 = y0, r = r, fill = fct_rev(as.factor(r))),
    col = gray(0.25), lwd = 0.25) + 
  geom_point(data = points_df, aes(x, y), size = 0.5) + 
  scale_fill_manual(values = rev(c("yellow", "red", "cyan", "black", "white"))) + 
  coord_fixed() + 
  dd_theme() + 
  xlab("") + ylab("") + 
  ggtitle("Unbiased, imprecise", subtitle = glue::glue("Variance = {summary_df$var}\nBias = {summary_df$bias}\nRMSE = {summary_df$mse}")) + 
  theme(
    legend.position = "none",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank()
  )
dual_estimands
```
