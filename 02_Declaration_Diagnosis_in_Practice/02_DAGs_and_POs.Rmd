---
title: "Directed Acyclic Graphs and Potential Outcomes"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# Directed Acyclic Graphs and Potential Outcomes

<!-- make sure to rename the section title below -->

```{r dags_pos, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
library(dagitty)
library(ggdag)
library(ggraph)
```

We make use of two different formal languages for describing causal models: DAGs and potential outcomes. DAGs are "directed acyclic graphs," where each node on a graph is a variable and the edges that connect them represent causal effects. DAGs emphasize a mechanical notion of causality: when the exposure variable changes, the outcome variable changes as a result. By contrast, the potential outcomes formalization emphasizes a counterfactual notion of causality. $Y_i(0)$ is the outcome for unit $i$ when the exposure variable is set to zero and $Y_i(1)$ is the outcome when it is set to one. The difference between them is the effect of the treatment on the outcome for unit $i$. Since at most only one potential outcome can ever be revealed, at least one of the two potential outcomes is necessarily counterfactual. Unrevealed potential outcomes are counter-to-facts; they do not exist. Nevertheless, the potential outcomes model is useful for thinking about what would have happened had things been different.

Despite what you may have inferred from the sometimes heated disagreements between scholars who prefer one formalization to the other, DAGs and potential outcomes are compatible systems for thinking about causality. A theorem in one language is a theorem in the other (CITE???). We use both languages because they are useful for expressing different facets of research design. We'll take the example of heterogeneous treatment effects to show how the two languages convey different aspects of the research setting.

The DAG below has just four variables. $Y$ is the outcome variable. It is affected by all of the other variables: $Z$, the treatment variable, $X$, a pre-treatment covariate, and $U$, an unobserved source of variation in both $X$ and $Y$. Possibly the most important thing that a DAG can teach us is which research questions are even answerable in a given seeting. Here, the causal relationship between $X$ and $Y$ is confounded. Since we have no information about what is in $U$, we can't learn about the effects of $X$ on $Y$. However, the treatment variable $Z$ has no edges leading in to it, which represents the idea that $Z$ is randomly assigned or otherwise exogenous. The DAG shows that even though we can't learn about the causal effects of $X$ on $Y$, we *can* learn about average causal effects of Z on Y. We could also draw descriptive inferences about the distributions of the observed variables like their ranges and averages or their variances and covariances.

```{r, echo = FALSE}
a_blue <- "#0072B2"
a_gray <- "grey80"

dag <-
  dagify(Y ~ X + Z + U,
         X ~ U,
         latent = "U")

gg_df <-
  tidy_dagitty(dag,
               layout = "manual",
               x = c(1, 0, -1, 1),
               y = c(1, 1, 0, 0))

gg_df <-
  gg_df %>%
  mutate(
    color = case_when(
      name == "U" ~ a_gray,
      name == "X" ~ a_blue,
      name == "Y" ~ a_blue,
      name == "Z" ~ a_blue
    )
  )


g <- 
ggplot(gg_df, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_node(aes(color = color)) +
  scale_color_identity() +
  geom_dag_text(color = "black", family = "Helvetica") +
  geom_dag_edges() +
  theme_dag()

g
```

This DAG conveys beliefs about whether *whether* two variables are causally related, but it does not encode beliefs about *how* they are related. The DAG doesn't show whether the effects are positive or negative, large or small, constant or heterogeneous. For example, we can't read from the DAG whether the average effect of $Z$ should be higher or lower depending on whether $X$ is set to 0 or 1. This is no criticism of DAGs -- they just don't encode all of our causal beliefs about a system.

A potential outcomes representation of this same system can fill in some of the details. More specifically, we will make the two potential outcomes of $Y$ with respect to $Z$ explicit, since we are focused on the effects of $Z$ on $Y$. We will skip writing out the potential outcomes of $X$ and $Y$ with respect to $U$ to reduce complexity, though of course the causal process by which $U$ affects its descendant variables could also be represented in potential outcomes. 

The untreated potential outcome is written $Y_i(Z = 0)$ or $Y_i(0)$ for short. Likewise, $Y_i(Z = 1)$ (or $Y_i(1)$) is the treated potential outcome. Notice that the *potential* outcomes don't appear on the DAG -- only the revealed outcome $Y_i$ does. That's because the revealed $Y_i$ is the output of a causal process that switches which potential outcome is revealed depending on the level of the treatment variable. The so-called "switching equation" can be written like this:

$$
Y_i = \begin{cases} 
      Y_i(0) & Z = control \\
      Y_i(1) & Z = treatment \\
   \end{cases}
$$

Sometimes the switching equation is written in a more compact algebraic form: $Y_i = Z * Y_i(1) + (1 - Z) * Y_i(0)$. This expression works because in the treatment condition, Z is equal to 1, so the second half of the expression is zeroed out, and $Y_i = Y_i(1)$. In the control condition, Z is equal to 0, so the first half of the expression is zeroed out. This works fine for binary treatment variables, but the piecewise notation extends more naturally to additional potential outcomes (treatment 1, treatment 2, placebo, etc). 

```
[INSET BOX on the Fundamental Problem of Causal Inference]
The switching equation underlines the fact that at most, we can only observe the one potential outcome that happened to be revealed by the world. We either see $Y_i(1)$ or $Y_i(0)$, but we can never see both. This problem has famously been dubbed the "Fundamental Problem of Causal Inference" (Holland 1986). The problem is indeed fundamental and common to all causal inference settings. 

That said, the FUndamental Problem of Causal Inference is more *severe* in some settings than others. Consider the causal inference problem of measuring the average causal effect of a light switch on the lamp turning on. Suppose we flip the switch on at exactly 2020-07-08 12:33:38 EDT and the lamp illuminates. We literally can't know whether the lamp would be on or off if the switch were in the off position at exactly that moment -- that's the Fundamental Problem of Causal Inference at work. But we can nevertheless get a good sense of the average effect of the switch on the lamp by flicking it back and forth a few times. Conditional on everything else in the system working (the electricity to the house, the lightbulb filament, etc), the average effect of the switch on illumination is 100 percentage points. The fundamental problem just isn't a big deal in this case because we have fine control over the causal variable (the switch) and the outcome variable (is the light on?) is measured perfectly. 

Most social scientific research questions are much harder. Measuring the average effect of university education on earnings, for example, is a notoriously difficult research task. First, different kinds of people do and don't attend university; those who do attend have different experiences and develop different social networks; those who don't attend university *also* have heterogeneous experiences in their early careers. The Fundamental Problem of Causal Inference is quite severe in this case because it is very hard to know what the lives of well-educated offices worker would have been like had they not gone to college. 
```

Potential outcomes notation is especially useful for defining *estimands*. Estimands are the inferential target -- what we call the Inquiry in the MIDA framework. The most common estimand is the Average Treatment Effect (ATE), which is written like this^[The expectation operator $E[]$ is a way of describing the average of a random variable. In general, $E[X] = \sum_{x \in X} x * pr(X = x)$. Here we are slightly abusing the notation, since in a fixed population, $Y_i(1)$ and $Y_i(0)$ are not random variables. We could write the ATE as $\frac{1}{N}\sum_1^N Y_i(1) - Y_i(0)$, or we could just imagine that we drawing one unit at random from the fixed population; the expectation operator can be defined with respect to this imaginary random variable in order to save ourselves some notational headaches.]:

$$
ATE \def E[Y_i(1) - Y_i(0)]
$$

We want to emphasize that the ATE is an *average*. The difference between $Y_i(1)$ and $Y_i(0)$ for unit $i$ is an individual-level treatment effect (sometimes we'll refer to $Y_i(1) - Y_i(0)$ as $\tau_i$). The ATE averages over all of the individual-level treatment effects in the relevant population. Some units will have a $tau_i$ that is higher than the ATE, some will have a $\tau_i$ that is lower. We emphasize this because sometimes people mistakenly thing that by focusing on an ATE, researchers are "assuming" that everyone experiences the same treatment effect. This is not true. The ATE is just a single-number summary of a possibly very heterogeneous set of responses to treatment.

We can also use potential outcomes notation to define other, more complicated estimands. Throughout the book, we'll describe a series of them -- local average treatment effects, average treatment effects on the treated, average direct effects, average indirect effects, and spillover effects, to name a few -- all using potential outcomes notation. 

## The same DAG can support multiple causal theories.

In this example, we'll illustrate how using both DAGs and potential outcomes together, we can describe and learn about research designs in a more complete way. The DAG above encodes the beliefs that $Y$ is a function of $Z$, $X$, and $U$ and also that we can learn about the effects of $Z$ on $Y$ but not $X$ on $Y$. The DAG is silent, however, on the question of whether the effect of $Z$ is on average different for units with different values of $X$. In other words, the DAG doesn't reflect beliefs about treatment effect heterogeneity. 

Using potential outcomes, we can write down three new estimands: the conditional average treatment effect (CATE) of $Z$ on $Y$ for units with $X = 1$, the CATE among units with $X = 0$, and the difference-in-cates:

$$
\begin{align}
CATE_{(X = 1)} &\def E[Y_i(1) - Y_i(0) | X = 1] \\
CATE_{(X = 0)} &\def E[Y_i(1) - Y_i(0) | X = 0] \\
Diff-in-CATEs &\def E[Y_i(1) - Y_i(0) | X = 1] - E[Y_i(1) - Y_i(0) | X = 0]
\end{align}
$$

Suppose we think that the effect of treatment is larger for units with $X = 1$, which is to say we think the difference-in-cates is positive. We can supplement the DAG representation of the causal model with a design declaration that includes these beliefs.

```{r}
diff_in_cates <- 0.5
design <-
  declare_population(N = 100,
                     U = rnorm(N),
                     X = rbinom(N, 1, prob = pnorm(0.5 * U + rnorm(N)))) +
  declare_potential_outcomes(Y ~ 0.5 * X + 0.5 * Z + diff_in_cates * X * Z + 0.5 * U) +
  declare_estimands(
    ATE = mean(Y_Z_1 - Y_Z_0),
    CATE_1 = mean(Y_Z_1[X == 1] - Y_Z_0[X == 1]),
    CATE_0 = mean(Y_Z_1[X == 0] - Y_Z_0[X == 0]),
    DiC = mean(Y_Z_1[X == 1] - Y_Z_0[X == 1]) - mean(Y_Z_1[X == 0] - Y_Z_0[X == 0])
  )
```


```{r, echo = FALSE}
design %>% draw_estimands %>% kable
```

We could incorporate different beliefs about the causal model by changing the `diff_in_cates` parameter. However, notice that regardless of the value of the interaction (either zero or some other number), the DAG looks the same. So if you want to design your study for the difference-in-CATEs, then you'll need to go beyond the DAG to write down your beliefs about the extent of heterogeneity using another tool.

## Summary

DAGs and potential outcomes are two languages for expressing beliefs about causality. The DAG formulation emphasizes a mechanistic understanding of causal systems: if I change this here, that will change there. The potential outcomes formulation emphasizes counterfactuals: we can either observe what happens there if I change this here or if I don't, but not both. The two languages are useful for describing different parts of a research design. DAGs excel at describing whole causal models and indicate which variables we can learn the causal effects of. Potential outcomes are useful for precisely defining causal estimands like the ATE, the CATEs, and the difference-in-CATEs. Design declaration uses the language of potential outcomes in order to encode more specific beliefs (such as beliefs about heterogeneity) than can be represented on the DAG.

<!-- If an edge begins at node X and ends at node Y, it means that under this causal model, changing the X variable could cause changes in the Y variable for some or all units. DAGs can encode a large number of beliefs about the causal relationships between many variables are for that reason are an exceptional tool for thinking through a causal model. Writing down the DAG means conceptualizing which variables in a system are exogenous and which are endogenous. Those two "E" words are used frequently throughout social science with sometimes subtlely different meanings. When we say exogenous, we mean randomly assigned. The random assignment may be conducted by the researcher or nature. When we say exogenous, we don't mean something like "set outside the system" -- we mean randomly assigned by someone. By the same token, endogenous just means "not randomly assigned." When a variable has no edges flowing into it, it is randomly assigned. When a variable does have edges flowing into it, it is not randomly assigned, and is instead the causal consequence of earlier variables. -->




