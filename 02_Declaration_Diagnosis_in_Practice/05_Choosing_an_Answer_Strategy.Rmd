---
title: "Choosing an answer strategy"
output:
  html_document:
    toc: true
    toc_depth: 2
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# Choosing an answer strategy

<!-- make sure to rename the section title below -->

```{r choosing_an_answer_strategy, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 10
b_sims <- 10
```

```{r, echo = FALSE}
tau <- .10
N <- 8
N_sampled <- 4
population <- declare_population(N = N, e = runif(N)) 
potential_outcomes <- declare_potential_outcomes(
  Y_Z_0 = .5 < e, Y_Z_1 = .5 < e + tau)
estimand <- declare_estimand(PATE = mean(Y_Z_1 - Y_Z_0))
sampling <- declare_sampling(n = N_sampled)
assignment <- declare_assignment(prob = .5)
reveal_outcomes <- declare_reveal(Y, Z)
estimator <- declare_estimator(Y ~ Z, label = "DiM", estimand = "PATE")
simple_design <- population + potential_outcomes + estimand + 
  sampling + assignment + reveal_outcomes + estimator
simple_design_data <- draw_data(simple_design)
```


After implementing a data strategy, with data in hand you need to have a procedure to develop an answer or a decision from it. The result of an answer strategy is a quantitative or qualitative estimate that provides an answer to one of the inquiries in your design. An answer strategy is, conceptually, a function that produces that an estimate. A simple answer strategy is a difference-in-means between the outcomes in a treatment group and the outcomes in a control group, which we select as an estimator of the true average treatment effect, the mean difference in potential outcomes under treatment and potential outcomes under control. 

We also commonly produce measures of uncertainty in our estimates and also statistics related to hypothesis testing. These are also part of the answer strategy: we want to know our best guess answer to the inquiry, and also how certain we are in our answer. Regression software commonly returns an estimate, estimated standard error, test statistic, p-value, and confidence interval.

Yet there are also a set of procedures we use to return an answer to our inquiry that fall outside of this common set of estimation procedures. The answer to our inquiry is found in a set of procedures to select a final estimators, the ways we report on and interpret the estimates visually and in writing, and the ancillary analyses we conduct to build confidence in the main estimates (i.e., robustness checks). When the data strategy does not go according to plan, and for example participants do not comply with assigned treatments or respond to survey questions, the set of alternative analyses to adjust for these problems are also part of the answer strategy. If the analysis strategy depends on how the data turn out, we need to assess the if-then analysis procedure under different possible realizations of the data to know if the procedure is a good one.

### measures of uncertainty 

- in addition, we have several statistics that assess the *uncertainty of the estimate*, here the standard error and a frequentist confidence interval. the answer strategy is not just how you get to the answer, but how sure you are of it. 

- we often also have statistics related to *hypothesis testing*, here a test statistics and p-value under the null hypothesis of a zero average treatment effect. our "answer" may either be the estimate of the average treatment effect, or in some cases the decision, is there a non-zero average treatment effect. 

- your answer strategy is the full set of steps from first seeing the data until the estimate of the estimand you present in the paper, which is usually more than just the estimate, its uncertainty measure, and associated hypothesis test. 


### procedures 

- *procedures*, if any, by which you explore the data and determine a final set of estimates are part of the answer strategy. for example, we sometimes find that the model we planned to run to analyze the data cannot be estimated. in these cases, there is an iterative estimation procedure in which a first model is run, changes to the specification are made, and a second or third model is presented as the result. that full set of steps -- a decision tree, depending on what is estimable -- is the answer strategy and we can evaluate whether it is a good one not only under the realized data but under other possible realizations where the decision *tree* would be the same but the decisions different.

- *procedures where you run two procedures and pick the best fit or preferred on some dimensions* 
- show example of a procedure of this form (model selection?) where the coverage is off if you don't account for the multi step

```{r}
report_if_significant <- function(data){
  fit_nocov <- lm_robust(Y ~ Z, data)
  fit_cov <- lm_robust(Y ~ Z + X, data)
  
  # select fit with lower p.value on Z
  if(fit_cov$p.value[2] < fit_nocov$p.value[2]){
    fit_selected <- fit_cov
  } else {
    fit_selected <- fit_nocov
  }
  fit_selected %>% tidy %>% filter(term == "Z")
}

design <-
  declare_population(    
    N = 100, X = rbinom(N, 1, 0.5), u = rnorm(N)
  ) + 
  declare_potential_outcomes(Y ~ 0.25 * Z + 10 * X + u) + 
  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + 
  declare_assignment(prob = 0.5) + 
  declare_reveal(Y, Z) + 
  declare_estimator(Y ~ Z, model = lm_robust, label = "nocov", estimand = "ATE") + 
  declare_estimator(Y ~ Z, model = lm_robust, label = "cov", estimand = "ATE") + 
  declare_estimator(
    handler = label_estimator(report_if_significant),
    label = "select-lower-p-value",
    estimand = "ATE") 

dg <- diagnose_design(design, sims = sims)
```

- *procedures for testing assumptions of identification strategy before running analysis*, such as falsification or placebo tests. in these tests, you run a test and only analyze the data using the analysis strategy you proposed if it passes the test indicating a failure to reject the null of no violation of the assumptions.
- show example of RDD where the method is *biased* if you don't use the assumption test but the procedure is *unbiased*

```{r}
conditional_on_placebo_test <- function(data) {
  placebo_test <- lm_robust(Y_placebo ~ Z, data) %>% tidy %>% filter(term == "Z")
  estimate <- lm_robust(Y ~ Z, data) %>% tidy %>% filter(term == "Z")
  if(placebo_test$p.value <= 0.05) {
    tibble(estimate = NA, reject = TRUE, term = "Z")
  } else {
    estimate %>% mutate(reject = FALSE)
  }
}

library(sn)

placebo_design <- 
  declare_population(N = 100, u = rsn(n = N, xi = 0, omega = 1, alpha = 10)) +
  declare_potential_outcomes(Y ~ 0.25 * Z + u) + 
  declare_potential_outcomes(Y_placebo = 0.1 + 1.2 * u) + 
  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + 
  declare_assignment(prob = 0.5) + 
  declare_estimator(Y ~ Z, model = lm_robust, estimand = "ATE", label = "unconditional") + 
  declare_estimator(handler = label_estimator(conditional_on_placebo_test), 
                    estimand = "ATE", label = "conditional")

simulations_df <- simulate_design(placebo_design, sims = sims)

# diag <- diagnose_design(placebo_design, sims = sims)

simulations_df %>% 
  group_by(estimator_label) %>% 
  summarize(bias = mean(estimate - estimand, na.rm = TRUE))
```

- precommittment is part of the answer strategy

### robustness checks

- *robustness checks* are part of the answer strategy. often, a single estimator is presented as the main analysis but then a series of alternative specifications are displayed in an appendix (such as including or excluding covariates and their interactions, different subsets of the data, or alternative statistical models). the purpose is to provide readers with evidence about how dependent the main results are on the specification, data subset, and statistical model used. when this is the case, the decision a reader makes based on their inferences about the estimand from the estimate depend not only on the main estimate but also the robustness checks. as a result, we want to assess the properties of the two together. 

We illustrate with a simple analysis of the correlation between two variables `y1` and `y2`, who have a true positive correlation. `y2` is also a function of an observed covariate `x` and measurement error. Our main analysis is a bivariate regression predicting `y2` with `y1`. We compare this answer strategy to one in which we run that analysis, but also run a robustness check controlling for `x`. We do this because as the analyst we are unsure of the true DGP and wish to demonstrate to reviewer's that our results are not dependent on the functional form we choose. 

```{r}
bivariate_correlation_decision <- function(data) {
  fit <- lm_robust(y2 ~ y1, data) %>% tidy %>% filter(term == "y1")
  tibble(decision = fit$p.value <= 0.05)
}

interacted_correlation_decision <- function(data) {
  fit <- lm_robust(y2 ~ y1 + x, data) %>% tidy %>% filter(term == "y1")
  tibble(decision = fit$p.value <= 0.05)
}

robustness_check_decision <- function(data) {
  main_analysis <- bivariate_correlation_decision(data)
  robustness_check <- interacted_correlation_decision(data)
  tibble(decision = main_analysis$decision == TRUE & robustness_check$decision == TRUE)
}

robustness_checks_design <- 
  declare_population(
    N = 100,
    x = rnorm(N),
    y1 = rnorm(N),
    y2 = 0.15 * y1 + 0.01 * x + rnorm(N)
  ) +
  declare_estimand(y1_y2_are_related = TRUE) + 
  declare_estimator(handler = label_estimator(bivariate_correlation_decision), label = "bivariate") + 
  declare_estimator(handler = label_estimator(robustness_check_decision), label = "robustness-check")

decision_diagnosis <- declare_diagnosands(correct = mean(decision == estimand), keep_defaults = FALSE)

diag <- diagnose_design(robustness_checks_design, sims = sims, diagnosands = decision_diagnosis)
```

We evaluate the two answer strategies in terms of the rate of correctly deciding there is a correlation between `y2` and `y1`. In the main analysis, this means we judge there is a correlation when the p-value is below $0.05$. In our robustness check answer strategy, we decide there is a correlation when both the main analysis and the robustness check return p-values below $0.05$ on the coefficient on `y1`. We see that we are more likely to correctly judge there is a correlation in the simpler analysis strategy. This is because we added an additional criterion to our decision; both criteria, due to random noise, sometimes fail to reject the null of no correlation. Our second answer strategy is more robust in the sense that we have stronger evidence of a correlation when we run the two analyses together. But we are also less likely to decide (correctly) that there is a relationship. The robustness check is conservative. This exercise highlights that the properties of an answer strategy with secondary analyses will be different than the properties of the main analysis alone. If we planned (or conducted) robustness checks, we may wish to know how good the pair of strategies is together.

- distinguish this from changes to the model where we do robustnesss vis a vis a fixed answer and data strategy. i.e. two notions of "robustness". one is fix I D A and change M, is this "design" robust to changes in M. the other is, within a given run, is the estimate "robust" to changing the estimation procedure, so this is a diagnostic statistic. note I must be defined across these changes in M.

Using the MIDA way of thinking about designs, we can also think of another notion of the "robustness" of a design. The typical way we think of robustness checks is multiple secondary analyses *conditional on the observed data* to build confidence in an analysis of that fixed data. However, the motivation for these robustness checks is uncertainty about the true data generating process. By declaring a design in terms of MIDA, we can think about the robustness of a *single* estimator to multiple possible true data generating processes. An estimator that is robust in this sense is one that is unbiased with low uncertainty regardless of, say, the true functional form between `y1` and `y2`. To determine whether an estimator is robust, we can redefine a set of designs with different functional forms and assess the rate of correct decisions of our robustness checks strategy under each different model. 

```{r}
robustness_checks_design <-
  robustness_checks_design +
  declare_estimator(handler = label_estimator(interacted_correlation_decision), label = "interacted")

robustness_checks_design_dgp2 <- replace_step(
  robustness_checks_design,
  step = 1,
  new_step = 
    declare_population(
      N = 100,
      x = rnorm(N),
      y1 = rnorm(N),
      y2 = 0.15 * y1 + 0.01 * x + 0.05 * y1 * x + rnorm(N)
    )
)

robustness_checks_design_dgp3 <- replace_step(
  robustness_checks_design,
  step = 1,
  new_step = 
    declare_population(
      N = 100,
      x = rnorm(N),
      y1 = 0.15 * x + rnorm(N),
      y2 = 0.15 * x + rnorm(N)
    )
)

robustness_checks_design_dgp3 <- replace_step(
  robustness_checks_design_dgp3, 
  step = 2,
  new_step = declare_estimand(y1_y2_are_related = FALSE)
)

decision_diagnosis <- declare_diagnosands(correct = mean(decision == estimand), keep_defaults = FALSE)

diag <- diagnose_design(
  robustness_checks_design, robustness_checks_design_dgp2, robustness_checks_design_dgp3, 
  sims = sims, diagnosands = decision_diagnosis)
```


### presentation of results

- *how you present the estimates* --- graphically, in tables, and in text --- are all parts of the answer strategy. this is because the inferences readers make about the estimand from your paper do not just come from the numerical estimate. in some cases, the number may not even be presented exactly, and instead a graphic of the estimate and its confidence interval is what readers rely on.

- lots of advice to present graphically (cite), what are implications of that? the decisions made from your results by readers are not just a function of numerical estimates but how they are presented. 

We explore this by comparing two possible graphical displays of conditional avareage treatment effects in an experiment. A common presentational format is to present the average treatment effect in one group and then the other along with confidence intervals. Inferences are made --- either by the author, or by readers --- as a function of whether one is significant and not the other. If that is true, the inference is that there is a difference in CATEs. An alternative is to present the estimated difference along with the two effects. The inferences can then directly be based on whether the confidence interval of the difference crosses zero. We illustrate these two visual answer strategies below:

```{r, echo = FALSE}
ATE <- 0.0

design <- 
  declare_population(N = 1000,
                     binary_covariate = rbinom(N, 1, 0.5),
                     normal_error = rnorm(N)) +
  # crucial step in POs: effects are not heterogeneous
  declare_potential_outcomes(Y ~ ATE * Z + normal_error) +
  declare_assignment(prob = 0.5) +
  declare_estimator(Y ~ Z, subset = (binary_covariate == 0), label = "CATE(0)") + 
  declare_estimator(Y ~ Z, subset = (binary_covariate == 1), label = "CATE(1)") +
  declare_estimator(Y ~ Z * binary_covariate, 
                    model = lm_robust, term = "Z:binary_covariate", label = "Interaction")
```

```{r, echo = FALSE, purl = FALSE}
# note this was rerun a bunch of times to get the right example (one is non sig the other is sig diff and diff-in-CATE is not diff from zero)
# estimates <- draw_estimates(design)
rds_file_path <- paste0(get_dropbox_path("answer_strategy"), "/estimates_diff_in_significance_plot.RDS")
# write_rds(estimates, path = rds_file_path)
estimates <- read_rds(rds_file_path)
```

```{r, echo = FALSE, fig.height = 3}

g1 <- ggplot(data = estimates %>% filter(term == "Z"), aes(estimator_label, estimate)) + 
  geom_point() + 
  geom_errorbar(aes(x = estimator_label, ymin = conf.low, ymax = conf.high), width = 0.2) + 
  ylab("Estimate (95% confidence interval)") +
  geom_hline(yintercept = 0, lty = "dashed") +
  ggtitle("Visualization A") +
  dd_theme() + 
  theme(axis.title.x = element_blank())

g2 <- ggplot(data = estimates, aes(estimator_label, estimate)) + 
  geom_point() + 
  geom_errorbar(aes(x = estimator_label, ymin = conf.low, ymax = conf.high), width = 0.2) + 
  ylab("Estimate (95% confidence interval)") +
  geom_hline(yintercept = 0, lty = "dashed") +
  ggtitle("Visualization B") +
  dd_theme() + 
  theme(axis.title.x = element_blank())

g1 + g2
```

We now demonstrate that the answer strategy on the left is flawed. XXYY describe sims.

```{r, echo = FALSE}
# sweep across all ATEs from 0 to 0.5
designs <- redesign(design, ATE = seq(0, 0.5, 0.05))
```

```{r, echo = FALSE, eval = do_diagnosis & !exists("do_bookdown")}
simulations_one_significant_not_other <- simulate_design(designs, sims = sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("answer_strategy"), "/simulations_one_significant_not_other.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(simulations_one_significant_not_other, path = rds_file_path)
}
simulations_one_significant_not_other <- read_rds(rds_file_path)
```

```{r, echo = FALSE, fig.height = 3.5}
# Summarize simulations ---------------------------------------------------

reshaped_simulations <-
  simulations_one_significant_not_other %>%
  transmute(ATE,
            sim_ID,
            estimator_label,
            estimate,
            conf.high,
            conf.low,
            significant = p.value < 0.05) %>%
  pivot_wider(id_cols = c("ATE", "sim_ID"), names_from = "estimator_label", values_from = c("estimate", "conf.high", "conf.low", "significant"))


# Plot 1 ------------------------------------------------------------------

gg_df <- 
  reshaped_simulations %>%
  group_by(ATE) %>%
  summarize(`Significant for one group but not the other` = mean(xor(significant_CATE_0, significant_CATE_1)),
            `Difference in subgroup effects is significant` = mean(significant_interaction)) %>%
  gather(condition, power, -ATE)

ggplot(gg_df, aes(ATE, power, color = condition)) +
  geom_point() +
  geom_line() +
  geom_label(data = (. %>% filter(ATE == 0.2)),
             aes(label = condition),
             nudge_y = 0.02,
             family = "Palatino") +
  dd_theme() +
  scale_color_manual(values = c("red", "blue")) +
  theme(legend.position = "none") +
  labs(
    x = "True constant effect size",
    y = "Probability of result (akin to statistical power)"
  )
```

### multiple comparisons

- your answer strategy should take into account *how many statistical tests* you are conducting, not just focus on the estimate-estimand pair. when you present the results from many null hypothesis  tests, the rate of falsely rejecting at least one of those tests even when all are true goes up, due to the multiple comparisons problem. if you plan to adjust for this problem, those adjustments are part of your answer strategy, because they will typically adjust the p-values you report and the decisions readers make with them.

- as this seection has highlighted, the answer strategy is intimately connected with the data strategy. people often think of their entire research design as the answer strategy. but they can't be separated.

<!-- ## what are the properties of a good answer strategy -->

<!-- - typically, we want to find an estimation strategy that, given the model and data strategy, produces estimates that have as close as possible as often as possible to the estimand. there are several ways of thinking about how close they are and how often. *bias, MSE.* as we discuss in section XX (diagnosis), you should select the set of diagnosands to purpose, depending on why you are conducting the research and what decisions you expect readers to make in response to it. -->

<!-- - we often think about increasing the power of a design by changing our sample size or other aspects of the data strategy. but there is often significant room to grow power through the answer strategy alone, holding the data strategy constant. in an experiment, controlling for pretreatment covariates that are very predictive of the outcome often yields significant power gains, for example.  -->

<!-- - best in the set (among set of simple linear models, what is the best model) -->

### what you will do when data goes sideways

- to compare answer strategies, you can imagine the estimators that are possible *if things go well* as well as *if things go wrong*, when there is missing data or there are outliers in variables. a good answer strategy (which might be a single estimator, or a procedure if-this-then-that) can handle both states of the world. 

- *procedures for addressing deviations from expected analyses* are part of the answer strategy. whether a study has a PAP or not, we often have a way we expect to analyze the data if things go well. when they do not -- because data are missing, there is noncompliance to an intervention, or the study is suspended for example -- the answers will change. these procedures determine the answer the study provides (or in some cases does not), so are part of the answer strategy. *standard operating procedures* (lin and green) are documents that systematize these procedures in advange.

- properties of imputation procedure

- standard operating procedures

## Example


## In DeclareDesign

- introduce classes of estimators: qual/quant, frequentist/bayesian, design based and model based (logit probit etc.)

- talk about issue of model-based vs design based, as separated from the model you assume in M. in model based you run a procedure that assumes a dgp, which may or may not be connected to the M.

- your data strategy should shape your answer strategy (analyse as you randomize)
- assignment strategies (blocks and/or clusters, heterogeneous assignment probabilities, etc.)
- sampling strategies (strata and/or clusters, heterogeneous sampling probabilities, etc.)

- this is true not just for experiments but for surveys (how did you sample), natural experiments (how did nature assign the treatment), and other designs

- you can *select an answer strategy in advance*, by simulating data. when estimators are selected with the data in hand, choices are often made in response to the realized data through examining model fit statistics that appear ideal in the context of this data, but are not ideal from the perspective of other data that could have been collected. we want answer strategies that perform well no matter how the data turn out. 
- issues are: overfitting, selecting a suboptimal estimator from design perspective that passes a model fit statistic (show this is worse in a simple example)

## Connections to MIDA

## How you already choose answer strategies in current practice



