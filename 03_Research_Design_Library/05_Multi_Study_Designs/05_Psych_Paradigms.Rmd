---
title: "Three-study papers"
output: html_document
bibliography: ../../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

## Three-study papers

<!-- make sure to rename the section title below -->

```{r psych_paradigms, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 500
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
library(reshape2)
```


In many research projects, we seek to evaluate multiple observable implications for a single theory. In such cases, a single piece of evidence does not constitute sufficient evidence to validate the theory as a whole. Rather, we believe in the theory when multiple pieces of evidence support it.
<!-- [Examples: Psychology; APSR articles that have an experiment plus observational work; replication efforts.] -->

Conventions around what constitutes a convincing pattern of evidence vary. Some researchers will not believe a theory unless each piece of evidence in support of it is *statistically significant*. 
<!-- Reviewer / skeptic: an observable implication is X.  -->
<!-- You have failed to provide evidence of X, this is inconsistent with your theory. -->
Less stringent approaches simply seek evidence "consistent" with the theory, such as the observation that all effects are signed in the predicted direction.

Here, we declare an $N$-study design, and examine the consequences of these two different approaches to evaluating a theory in light of multiple studies. We show that, under multiple observable implications generated by the same process, conditioning on significance can lead one strongly astray. Generally speaking, looking at the sign of effects is more probative because it is much less prone to false negatives. With small numbers of studies, however, the risks of false positives are high. 

### Design Declaration

- **M**odel: We declare $N$ populations, all of which are governed by the same 
             data-generating process: `X` is exogenous and standard normally-distributed, 
             `M` is standard-normally distributed and correlated with `X` by `rho`, and `Y` is a function of the main effect of `X` as well as the interaction between `X` and `M`, with the size and sign of the direct and interactive effects determined by `tau` and `gamma`, respectively.
  
- **I**nquiry: We want to know, in a global sense, if our theory is "right." 
               Here, that means that the effect of `X` on `Y` is positive and increasing with `M`, and that `X` affects `M`. 
               When `tau` is positive, our theory is correct. When it is zero or negative, our theory is incorrect.

- **D**ata strategy: We conduct and collect independent datasets on $N$ datasets of size `n`. In the example below, we conduct three studies, assuming we only observe `X` and `Y` in the first, only `M` and `X` in the second, and `Y`, `M`, and `X`, in the third.

- **A**nswer strategy: Using linear regression, we estimate the bivariate correlation between `X` and `Y` in study 1, the bivariate correlation between `X` and `M` in study 2, and the interaction between `X` and `M` on `Y` in study 3. 

```{r}
n1 <- 100
n2 <- 100
n3 <- 100
rho <- .5
gamma <- tau <- .2

generate_study_sample <- function(n, rho, tau, gamma, data){
  fabricate(N = n, X = rnorm(N), M = rnorm(N, X * rho, sqrt(1 - rho^2)), 
            U = rnorm(N), Y = tau * X + gamma * M * X + U)
}

three_study_design <-
  # Study 1 -- Bivariate correlation between X and Y
  declare_population(
    n = n1,
    tau = tau,
    gamma = gamma,
    rho = rho,
    handler = generate_study_sample
  ) +
  declare_estimator(Y ~ X,
                    term = "X",
                    model = lm_robust,
                    label = "Study 1") +
  # Study 2 -- Bivariate correlation between M and X
  declare_population(
    n = n2,
    tau = tau,
    gamma = gamma,
    rho = rho,
    handler = generate_study_sample
  ) +
  declare_estimator(M ~ X,
                    term = "X",
                    model = lm_robust,
                    label = "Study 2") +
  # Study 3 -- Interaction in X and M
  declare_population(
    n = n3,
    tau = tau,
    gamma = gamma,
    rho = rho,
    handler = generate_study_sample
  ) +
  declare_estimator(Y ~ X + M + X:M,
                    term = "X:M",
                    model = lm_robust,
                    label = "Study 3") 
```

### DAG

```{r, echo = FALSE}
dag <- dagify(M ~ X,
              Y ~ X + M)

nodes <-
  tibble(
    name = c("X", "M", "Y"),
    label =  name,
    annotation = c(
      "**Treatment**",
      "**Mediator**",
      "**Outcome**"
    ),
    x = c(1, 3, 5),
    y = c(1.5, 3.5, 1.5), 
    nudge_direction = c("S", "N", "S"),
    data_strategy = c("assignment", "unmanipulated", "unmanipulated"),
    answer_strategy = "uncontrolled"
  )

ggdd_df <- make_dag_df(dag, nodes)

base_dag_plot %+% ggdd_df
```

### Takeaways

Let us compare the performance of the "all significant" versus "all signed" 
approaches to theory confirmation when the theory is "correct" (`tau` and `gamma` positive),
versus when it is "incorrect" (both parameters zero). 
In the first approach, a theory is deemed "supported" by the evidence when all effects
are significant. In the second, the theory is supported by the evidence when the signs of all effects are positive.

```{r, eval = do_diagnosis & !exists("do_bookdown")}
# Simulate design
simulations <- simulate_design(three_study_design)

# Simulate null design
null_three_study_design <- 
  redesign(three_study_design, tau = 0, gamma = 0, rho = 0)
null_simulations <- simulate_design(null_three_study_design)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("01_Psych_Paradigms.Rmd"), "/simulations.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(simulations, path = rds_file_path)
}
simulations <- read_rds(rds_file_path)
```
```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("01_Psych_Paradigms.Rmd"), "/null_simulations")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(null_simulations, path = rds_file_path)
}
null_simulations <- read_rds(rds_file_path)
```

In the first three rows of the table, the theory is correct in that `tau`, `gamma`, and  `rho` are positive; in the second three rows, it is incorrect because both the main effect and interaction are zero. The "power" column tells us the proportion of simulations in which the effect is significant at the $\alpha = .05$ level, the "all significant" column tells us the proportion of simulations in which all of the studies have significant effects, the "positive" column tells us how often the study found a positively signed result, while the "all positive column" tells us the proportion of simulations where all studies had a positively signed result.

```{r,echo = FALSE}
three_study_diagnosands <- 
  simulations %>% 
 group_by(sim_ID) %>% 
  mutate(all_significant = all(p.value < .05),
         any_significant = any(p.value < .05),
         all_positive = all(estimate > 0), 
         any_positive = any(estimate > 0)) %>% 
  group_by(estimator_label) %>% 
  summarize(
    all_significant = mean(all_significant),
    any_significant = mean(any_significant),
    power = mean(p.value < .05),
    all_positive = mean(all_positive),
    any_positive = mean(any_positive),
    positive = mean(estimate > 0)) %>% 
  mutate(tau = .2, gamma = .2, rho = .5)

null_three_study_diagnosands <- 
  null_simulations %>% 
  group_by(sim_ID) %>% 
  mutate(all_significant = all(p.value < .05),
         any_significant = any(p.value < .05),
         all_positive = all(estimate > 0), 
         any_positive = any(estimate > 0)) %>% 
  group_by(estimator_label) %>% 
  summarize(
    all_significant = mean(all_significant),
    any_significant = mean(any_significant),
    power = mean(p.value < .05),
    all_positive = mean(all_positive),
    any_positive = mean(any_positive),
    positive = mean(estimate > 0)) %>% 
  mutate(tau = 0, gamma = 0, rho = 0)


rbind(three_study_diagnosands,
      null_three_study_diagnosands) %>% 
  select(tau, estimator_label, gamma, power, all_significant, positive, all_positive) %>% kable(booktabs = TRUE)

```

- Notice that, because the studies are independent, the probability that all
  are significant is equal to the product of their power: 
  Pr(all studies significant) = Pr(Study 1 significant) $\times$...$\times$ Pr(Study $N$ significant). Thus, if you only believe a theory if the studies conducted to test it yield significant results, and those studies are all powered at the conventionally accepted level of 80%, you erroneously reject the theory with a probability of $.8^N$. If you do three, conventionally well-powered, randomized studies each shooting at the right quantity of interest, then in almost half of the cases where you are right, you will think 
you are wrong.
  
- Furthermore, notice how detrimental the addition of a small study can be by this metric, even if it gets at an important mechanism. As soon as you condition your inference about the theory being correct on the significance of all the observable implications, a low-powered test can sharply increase the risk of false rejection.

- What can we say about the risk of false positives? The power of the individual studies is where it should be: for a stated error rate of $\alpha = 0.05$, the studies are every so slightly anti-conservative. However, the "all significant" desideratum creates a rejection rate that is too high.

- Some of these problems, though not all, are alleviated when we disregard significance and just look at signs. When the theory is right, there is a very good chance that all of the effects we estimate are positive: we surmise the theory is correct roughly 94% of the time when it actually is. 

- When the theory is not correct in the sense that the true effects are zero, random error means that they are positive half the time and negative the other half. Consequently, the probability of erroneously accepting the theory based on the sign of effects when the true underlying effects are zero is equal to $0.5^N$. Here, that means we erroneously infer we are right at a relatively high rate of 12% of simulations. 

In the design above, the rates at which we rejected or accepted theories seemed
to depend on the number of studies we considered. In the graph below, we look
at twenty-nine different $N$-study designs, all of which seek to confirm a theory
by replicating evidence for it $N$ times. The first design is made of two studies,
each independently evaluating the hypothesis that $Y$ is positively correlated with $X$.
Again, we consider how conditioning an inference about the theory on whether 
all results are significant or all results are positive affects error rates.


```{r, echo = FALSE}
rho <- .5
tau <- .2

N_studies <- 30


replication_design <-
  declare_population(
    n = 100,
    tau = tau,
    gamma = gamma,
    rho = rho,
    handler = generate_study_sample
  ) +
  declare_estimator(Y ~ X,
                    term = "X",
                    model = lm_robust,
                    label = paste0("Study 1")) 

for(i in 2:N_studies) {
  replication_design <-
    replication_design +
    declare_population(
      n = 100,
      tau = tau,
      gamma = gamma,
      rho = rho,
      handler = generate_study_sample
    ) +
    declare_estimator(Y ~ X,
                      term = "X",
                      model = lm_robust,
                      label = paste0("Study ", i))
}


tau <- 0
null_replication_design <-
  declare_population(
    n = 100,
    tau = tau,
    gamma = gamma,
    rho = rho,
    handler = generate_study_sample
  ) +
  declare_estimator(Y ~ X,
                    term = "X",
                    model = lm_robust,
                    label = paste0("Study 1"))

for (i in 2:N_studies) {
  null_replication_design <-
    null_replication_design +
    declare_population(
      n = 100,
      tau = tau,
      gamma = gamma,
      rho = rho,
      handler = generate_study_sample
    ) +
    declare_estimator(Y ~ X,
                      term = "X",
                      model = lm_robust,
                      label = paste0("Study ", i))
}
```

```{r,echo = FALSE, eval = do_diagnosis & !exists("do_bookdown")}
# Simulate design
replication_simulations <- simulate_design(replication_design)
null_replication_simulations <- simulate_design(null_replication_design)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("01_Psych_Paradigms.Rmd"), "/replication_simulations")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(replication_simulations, path = rds_file_path)
}
replication_simulations <- read_rds(rds_file_path)

rds_file_path <- paste0(get_dropbox_path("01_Psych_Paradigms.Rmd"), "/null_replication_simulations")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(null_replication_simulations, path = rds_file_path)
}
null_replication_simulations <- read_rds(rds_file_path)
```

- Again, we see that significance is not a probative way to look for observable
  theoretical implications. As soon as there are more than four studies, 
  there is virtually no chance of confirming even a true theory by this metric.
  We see a sort of reverse multiple-comparisons problem: increasing the number
  of tests makes false rejections increasingly more likely when we are interested
  in the joint probability of all the tests saying the same thing. 
  Unless the number of studies is small, whether all produce significant results
  essentially yields no information about whether the theory is correct.

- By contrast, looking at signs only can be highly probative. In this 
  application, the optimal number of studies is about eight. At that point, 
  there is virtually no chance of erroneously inferring that the theory is 
  correct when the effects are zero, but when the theory is correct there is 
  a good chance (almost 75%) that all of the available evidence will be signed
  accordingly. As the number of studies increases, so too does the probability
  of discordant results, and using the unanimity of signs to judge whether
  the theory is correct becomes increasingly unwise.


```{r, echo = FALSE}
get_N_study_diagnosands <- function(simulations, N){
  studies <- paste0("Study ", 1:N)
  simulations %>% 
    filter(estimator_label %in% studies) %>% 
    group_by(sim_ID) %>% 
    mutate(all_significant = all(p.value < .05),
           any_significant = any(p.value < .05),
           prop_sig_95 = mean(p.value < .05) >= .95,
           all_positive = all(estimate > 0), 
           any_positive = any(estimate > 0)) %>% 
    group_by(estimator_label) %>% 
    summarize(
      all_significant = mean(all_significant),
      any_significant = mean(any_significant),
      power = mean(p.value < .05),
      prop_sig_95 = mean(prop_sig_95),
      all_positive = mean(all_positive),
      any_positive = mean(any_positive),
      positive = mean(estimate > 0)) %>% 
    select(-starts_with("estimator_label")) %>% 
    summarize_all("mean")
}

replication_diagnosands <-
  lapply(1:N_studies, get_N_study_diagnosands,
         simulations = replication_simulations) %>%
  do.call(what = "rbind")
replication_diagnosands$N_studies <- 1:nrow(replication_diagnosands)

null_replication_diagnosands <-
  lapply(1:N_studies, get_N_study_diagnosands,
         simulations = null_replication_simulations) %>%
  do.call(what = "rbind")
null_replication_diagnosands$N_studies <-
  1:nrow(null_replication_diagnosands)

plot_data <-
  rbind(
    melt(replication_diagnosands, id.vars = "N_studies") %>% mutate(tau = .2),
    melt(null_replication_diagnosands, id.vars = "N_studies") %>% mutate(tau = 0)
  )


plot_data  %>%
  filter(N_studies != 1) %>%
  filter(variable %in% c("all_significant", "all_positive")) %>%
  mutate(
    label = case_when(
      variable == "all_significant" ~ "Pr(All N studies have significant results)",
      variable == "all_positive" ~ "Pr(All N studies have positive results)",
      variable == "prop_sig_95" ~ "Pr(At least 95% of studies have significant results)"
    ),
    model = case_when(
      tau > 0 ~ "Model: all effects positive\n(Theory is correct)",
      tau == 0 ~ "Model: all effects zero\n(Theory is incorrect)"
    )
  ) %>%
  ggplot(aes(x = N_studies, y = value, color = label)) +
  geom_point() +
  facet_grid( ~ model) +
  scale_x_continuous(name = "Number of studies in design", breaks = seq(2, 30, 2)) +
  scale_y_continuous(name = "Probability observable implications support theory.") + 
  dd_theme() + 
  theme(legend.position = "bottom")

```


