---
title: "Multi-site studies"
output: html_document
bibliography: ../../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

<!-- make sure to rename the section title below -->

```{r multi_site_studies, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 1000
b_sims <- 1000
```

```{r, echo = FALSE}
library(metafor)
library(car)
```

## Multi-site trials

<!-- ## The Metaketa design -->

<!-- | Study Number | Place | What Works   | Design Implemented | Success? | -->
<!-- | ------------ | ----- | ------------ | ------------------ | -------- | -->
<!-- | 1            | A     | $Y_A ~ Z_A$  | $MIDA_A$           | Yes      | -->
<!-- | 2            | B     | $Y_B ~ Z_B$  | $MIDA_B$           | Yes      | -->
<!-- | 3            | C     | $Y_C ~ Z_C$  | $MIDA_C$           | Yes      | -->
<!-- | 4            | D     | $Y_D ~ Z_D$  | $MIDA_D$           | Yes      | -->
<!-- | 5            | E     | $Y_E ~ Z_E$  | $MIDA_E$           | Yes      | -->

<!-- The worry is that $I_A$ and $I_B$ etc are fundamentally different inquiries about different dags -->

<!-- [faceted figure of 5 different DAGs with nodes in different languages roman, greek, numeric, windings].  The purpose of *theory* is to tell us when the $I$'s across context are different, but that is HARD and that is how research has gone for YEARS. -->

<!-- The metaketa design says WAIT -- LET'S agree on a $MIDA_{MK}$ that we will implement in all 5 places, mutatis mutandis. That is, everyone "localizes" their design because things work slightly differently in different places, but if we try hard, we can implement the same "important" treatment on the same "important" outcome, even though the details (like the language of the treatment materials, the specifics of the measurement strategies.) will differ from place to place. -->

<!-- | Study Number | Place | What Works   | Design Implemented                               | Success? | -->
<!-- | ------------ | ----- | ------------ | ------------------------------------------------ | -------- | -->
<!-- | 1            | A     | $Y_A ~ Z_A$  | $\delta_A * MIDA_{MK} - (1 - \delta_A) * MIDA_A$ | ?        | -->
<!-- | 2            | B     | $Y_B ~ Z_B$  | $\delta_B * MIDA_{MK} - (1 - \delta_B) * MIDA_B$ | ?        | -->
<!-- | 3            | C     | $Y_C ~ Z_C$  | $\delta_C * MIDA_{MK} - (1 - \delta_C) * MIDA_C$ | ?        | -->
<!-- | 4            | D     | $Y_D ~ Z_D$  | $\delta_D * MIDA_{MK} - (1 - \delta_D) * MIDA_D$ | ?        | -->
<!-- | 5            | E     | $Y_E ~ Z_E$  | $\delta_E * MIDA_{MK} - (1 - \delta_E) * MIDA_E$ | ?        | -->


<!-- Coordinating trials like the metaketa design means causing teams to implement designs that they otherwise would not have. Collaboration can bring many benefits and shared struggle, but fundamentally, it pulls all the designs towards the common $MIDA_{MK}$ design. The strength of the pull is given by the $\delta$ parameters.  -->

<!-- Will the metaketa design be a success? Only if (1) the $MIDA_{MK}$ design is a strong one and it is correct about how $Y$ and $Z$ relate. If the $MIDA_{MK}$ design is poor, however, then all the studies will be a failure. Instead of learning $I_A$ in place $A$ and learning $I_B$ in place $B$, we learn nothing anywhere.  -->

<!-- The trouble with the standard approach (separate teams investigating phenomena that later theory tries to label as instances of the same thing) is that theory is hard and we can never really be sure if $I_A$ means the same thing for $M_A$ as $I_B$ means for $M_B$. The metaketa design tries to solve this theoretical problem on the front end -- let's agree on what the $M$ we all have in mind is (with local variations), then run a study. This approach works well if the $M$ we all agree on is approximately correct, and it works *worse* than the standard approach if we're wrong. -->

The starting point is a fixed budget and considering two possible designs: (1) a single large study in one context or (2) a set of five studies in five different contexts with the same intervention and outcome measures.

When there are heterogeneous effects, you can get good predictions out of sample even when average effects differ substantially (and you do better with multiple sites when sites in the population have different proportions of subject types that are correlated with heterogeneous effects). 

Two notable features of the design:
- there must be heterogeneous effects for this to work (otherwise our estimates get biased toward zero due to overfitting to the  variables).
- we have to have information about the covariate in the population and the sample (here we used the proportion of people in each heterogeneous type).

<!-- Findings: -->
<!-- - these two strategies are both unbiased -->
<!-- - the design with five sites has half the RMSE of the one-site design. this is because of the variation in the proportions of types across sites. -->
<!-- - interestingly there is poor coverage (anti-conservative) when you use the single site design -->

<!-- (when you have contextual variation as well, i.e. effect differs across sites for reasons not captured by the het fx, coverage is off for all designs. will keep this point out, seems like too much and you don't need contextual effects to get different effects across sites, those come from different proportions of types) -->

```{r, echo = FALSE}
random_effects_meta_analysis <- function(data){
  site_estimates_df <- data %>%
    group_by(site) %>%
    do(tidy(lm_robust(Y ~ Z, data = .))) %>%
    filter(term == "Z") %>%
    ungroup

  meta_fit <- rma(estimate, std.error, data = site_estimates_df, method = "REML")

  with(meta_fit, tibble(
    estimate = as.vector(beta), std.error = se, p.value = pval, conf.low = ci.lb, conf.high = ci.ub))
}

post_stratification <- function(data, pr_types_population) {
  if(length(unique(data$site)) > 1) {
    fit <- lm_robust(Y ~ Z*as.factor(subject_type) + as.factor(site), data = data)
    tidy(fit)
  } else {
    fit <- lm_robust(Y ~ Z*as.factor(subject_type), data = data)
  }
  
  alpha <- .05
  
  lh_fit <- try({ linearHypothesis(
    fit, 
    hypothesis.matrix = paste(paste(paste(pr_types_population[91:100][-1], "*", matchCoefs(fit, "Z"), sep = ""), collapse = " + "), " = 0"), 
    level = 1 - alpha) })
  
  if(!inherits(lh_fit, "try-error")) {
    tibble(estimate = drop(attr(lh_fit, "value")), 
           std.error = sqrt(diag(attr(lh_fit, "vcov"))),
           df = fit$df.residual, 
           statistic = estimate / std.error, 
           p.value = 2 * pt(abs(statistic), df, lower.tail = FALSE),
           conf.low = estimate + std.error * qt(alpha / 2, df),
           conf.high = estimate + std.error * qt(1 - alpha / 2, df))
  } else {
    tibble(error = TRUE)
  }
}
```


<!-- need to have biased sampling to get bias here -->
<!-- two kinds of populations, one in which the study type determines the subject types and you select on study type -->
<!--   a second kind where study type determines study shock  -->
<!--   in second type if you adjust for subject type then you will be able to unbiased recover global -->

```{r}
single_site_design <- 
  declare_population(
    site = add_level(
      N = 10, 
      feasible_site = sample(c(rep(1, 8), rep(0, 2)), N, replace = FALSE),
      study_effect = seq(from = -0.1, to = 0.1, length.out = N) 
    ),
    subjects = add_level(N = n_subjects_per_site, noise = rnorm(N))
  ) + 
  declare_potential_outcomes(Y ~ Z * (0.1 + study_effect + 0.025 * feasible_site) + noise) +
  declare_estimand(PATE = mean(Y_Z_1 - Y_Z_0)) + 
  declare_estimand(PATE_feasible = mean(Y_Z_1 - Y_Z_0), subset = feasible_site == TRUE) + 
  declare_sampling(clusters = site, strata = feasible_site, strata_n = c(0, 1)) + 
  declare_sampling(strata = site, n = n_subjects_per_site) + 
  declare_assignment(blocks = site, prob = 0.5) + 
  declare_estimator(Y ~ Z, model = lm_robust)

multi_site_design <- 
  declare_population(
    site = add_level(
      N = 10, 
      feasible_site = sample(c(rep(1, 8), rep(0, 2)), N, replace = FALSE),
      study_effect = seq(from = -0.1, to = 0.1, length.out = N) 
    ),
    subjects = add_level(N = n_subjects_per_site, noise = rnorm(N))
  ) + 
  declare_potential_outcomes(Y ~ Z * (0.1 + study_effect + 0.025 * feasible_site) + noise) +
  declare_estimand(PATE = mean(Y_Z_1 - Y_Z_0)) + 
  declare_estimand(PATE_feasible = mean(Y_Z_1 - Y_Z_0), subset = feasible_site == TRUE) + 
  declare_sampling(clusters = site, strata = feasible_site, strata_n = c(0, n_study_sites)) + 
  declare_sampling(strata = site, n = n_subjects_per_site) + 
  declare_assignment(blocks = site, prob = 0.5) + 
  declare_estimator(handler = label_estimator(random_effects_meta_analysis), label = "random-effects")

single_site_large_design <- redesign(single_site_design, n_subjects_per_site = 2500)

small_study_five_sites <- redesign(multi_site_design, n_study_sites = 5, n_subjects_per_site = 500)
```

```{r, warning = FALSE, eval = do_diagnosis & !exists("do_bookdown")}
simulations_small_large <- simulate_design(single_site_large_design, small_study_five_sites, sims = sims)
diagnosis_small_large <- diagnose_design(simulations_small_large, bootstrap_sims = b_sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("multi_site_studies"), "/diagnosis_small_large.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnosis_small_large, path = rds_file_path)
}
diagnosis_small_large <- read_rds(rds_file_path)
```

```{r, echo = FALSE}
kable(diagnosis_small_large %>% reshape_diagnosis %>% select(`Design Label`, `Estimand Label`, Bias, RMSE), booktabs = TRUE)
```

### Bayesian estimation can improve estimates of effects for sampled sites

To do: you can improve site-level effect estimates by analyzing with a simple Bayesian model because of its shrinkage property, even when the Bayesian model is wrong about distribution of effects in population.

<!-- this is the point from the blog post; I will modify the above design so it can also make this point, switching between the normal distribution and uniform distribution for the fx distribution -->

<!-- # ```{r, eval = FALSE} -->
<!-- # stan_model <- "  -->
<!-- # data { -->
<!-- #   int<lower=0> J;         // number of sites  -->
<!-- #   real y[J];              // estimated effects -->
<!-- #   real<lower=0> sigma[J]; // s.e. of effect estimates  -->
<!-- # } -->
<!-- # parameters { -->
<!-- #   real mu;  -->
<!-- #   real<lower=0> tau; -->
<!-- #   real eta[J]; -->
<!-- # } -->
<!-- # transformed parameters { -->
<!-- #   real theta[J]; -->
<!-- #   real tau_sq = tau^2; -->
<!-- #   for (j in 1:J) -->
<!-- #     theta[j] = mu + tau * eta[j]; -->
<!-- # } -->
<!-- # model { -->
<!-- #   target += normal_lpdf(eta | 0, 1); -->
<!-- #   target += normal_lpdf(y | theta, sigma); -->
<!-- # } -->
<!-- # " -->
<!-- #  -->
<!-- # stan_re_estimator <- function(data) { -->
<!-- #   site_estimates_df <- data %>%  -->
<!-- #     group_by(site) %>%  -->
<!-- #     do(tidy(lm_robust(Y ~ Z, data = .))) %>%  -->
<!-- #     filter(term == "Z") %>%  -->
<!-- #     ungroup  -->
<!-- #    -->
<!-- #   J      <- nrow(site_estimates_df) -->
<!-- #   df     <- list(J = J, y = site_estimates_df$estimate, sigma = site_estimates_df$std.error) -->
<!-- #   fit    <- stan(model_code = stan_model, data = site_estimates_df) -->
<!-- #   fit_sm <- summary(fit)$summary -->
<!-- #   data.frame(estimate = fit_sm[,1][c("mu", "tau", "theta[1]", "prob_pos")]) -->
<!-- # } -->
<!-- #  -->
<!-- # bayes_estimator <- declare_estimator(handler = stan_re_estimator) -->
<!-- # ``` -->

<!-- ### When things break down: confounded sampling -->

<!-- None of these designs work when you're trying to make predictions for sites that are systematically different, i.e. are not in the same population as the sampling frame. -->

<!-- The design was set up to include several sites where researchers could not feasibly set up experiments. in the original design, effects do not depend on whether sites are feasible for the experiment. When effects do vary, there are systematic differences for those target sites. Those differences might come from three sources: mean effect size differs in places that are sampled vs not sampled; individual-level heterogeneous effect sizes that systematically differ in places that are sampled to study vs others; or covariate profiles that do not exist in sites outside the sampling frame. We introduce effects in the first way and show there is substantial bias. -->

<!-- ```{r} -->
<!-- small_study_five_sites_feasible_effects <- multi_site_designer(n_study_sites = 5, n_subjects_per_site = 500, feasible_effect  = -0.25) -->
<!-- ``` -->

<!-- ```{r, warning = FALSE, eval = do_diagnosis & !exists("do_bookdown")} -->
<!-- simulations_feasible_effects <- simulate_design(small_study_five_sites_feasible_effects, sims = sims) -->
<!-- diagnosis_feasible_effects <- diagnose_design(simulations_feasible_effects %>% filter(!is.na(estimate) & !is.na(std.error) & !is.na(statistic) & !is.na(p.value) & !is.na(conf.low) & !is.na(conf.high)), bootstrap_sims = b_sims) -->
<!-- ``` -->

<!-- ```{r, echo = FALSE, purl = FALSE} -->
<!-- # figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file -->
<!-- rds_file_path <- paste0(get_dropbox_path("multi_site_studies"), "/diagnosis_feasible_effects.RDS") -->
<!-- if (do_diagnosis & !exists("do_bookdown")) { -->
<!--   write_rds(diagnosis_feasible_effects, path = rds_file_path) -->
<!-- } -->
<!-- diagnosis_feasible_effects <- read_rds(rds_file_path) -->
<!-- ``` -->

<!-- ```{r, echo = FALSE} -->
<!-- kable(get_diagnosands(diagnosis_feasible_effects) %>% reshape_diagnosis %>% select(Bias, RMSE), booktabs = TRUE, caption = "Diagnosis with confounded sampling,", digits = 3) -->
<!-- ``` -->

<!-- Other points I decided to abandon to keep this simple: -->
<!-- - tradeoff: context-specific interventions and comparability of intervention effects -->
<!-- - tradeoff: comparability and fidelity to context in outcome measurement -->

