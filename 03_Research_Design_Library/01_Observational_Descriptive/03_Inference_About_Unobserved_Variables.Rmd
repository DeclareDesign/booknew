---
title: "Inference about unobserved variables "
output: html_document
bibliography: ../../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

## Inference about unobserved variables 

Oftentimes researchers seek to measure a latent variable (`Y_star`) but have access only to proxies  (`Y_1`, `Y_2`, `Y_3`). In such cases the proxies are sometimes combined into an index which is thought to capture `Y_star` and then used for further analysis.

A difficult feature of such problems is that we do not have access to the *scale* on which `Y_star` is measured and so it may seem like a hopeless exercise to try to assess whether we have got good or bad estimates of `Y_star` when we combine the measured data. 

One way around this is to normalize the scale of both the latent variable and the measured variable so that they have a mean of 0 and unit standard deviation. But in that case we are guaranteed that our estimate of the mean of the normalized variable will be unbiased! We will certainly estimate a mean of 0! That may be but as we show in the declaration below, if you model is correct this approach may still be useful for calculating other quantities --- such as conditional means---that you don't just get right by construction.  

### Declaration

```{r}
design <-
  declare_population(N = 100, gender = rep(0:1, N/2), Y_star = gender + 2* rnorm(N)) +
  declare_estimand(Y_bar = mean(scale(Y_star)[gender == 1])) + 
  declare_measurement(Y_1 = 0.1 * Y_star + rnorm(N, sd = 0.25),
                      Y_2 = Y_star + rnorm(N, sd = 0.25),
                      Y_3 = 1 + 0.5 * Y_star + rnorm(N, sd = 0.25),
                      Y_idx = scale((scale(Y_1) + scale(Y_2) + scale(Y_3)))) + 
  declare_estimator(Y_idx ~ 1, model = lm_robust, estimand = "Y_bar", subset = gender ==1)
```

### Dag
```{r, echo = FALSE}


dag <-
  dagify(Y_1 ~ Y_star, Y_2 ~ Y_star, 
         Y_3 ~ Y_star, 
         Y_idx ~ Y_1 + Y_2 + Y_3)

nodes <-
  tibble(
    name = c("Y_star", "Y_1", "Y_2", "Y_3", "Y_idx"),
    label = c("Y^*", "Y<sup>1</sup>", "Y<sup>2</sup>", "Y<sup>3</sup>", "I"),
    answer_strategy = "uncontrolled",
    annotation = c(
      "**Latent outcome**<br>unmeasurable",
      "**Measured outcome**",
      "",
      "",
      "**Constructed Index**"),
    x = c(1, 3, 3, 3, 5),
    y = c(2.5, 3.5, 2.5, 1.5, 2.5),
    nudge_direction = c("N", "N", "S", "N", "N")
  )

ggdd_df <- make_dag_df(dag, nodes, design)
base_dag_plot %+% ggdd_df
```

### Example

`r flagit()`
Perhaps a good psychometric example would be worthwhile to explore here? Or an education example?


<!-- make sure to rename the section title below -->

```{r inference_about_unobserved_variables, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- TRUE
sims <- 1000
b_sims <- 50
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

```{r, eval = FALSE}
diagnosis <- diagnose_design(design) 
```


```{r, echo = FALSE, eval = do_diagnosis & !exists("do_bookdown")}
diagnosis <- diagnose_design(design, sims = sims, bootstrap_sims = b_sims) 
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("Random_Sampling"), "/latent_sims.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnosis, path = rds_file_path)
}
diagnosis <- read_rds(rds_file_path)
```

```{r completerandomsampling, echo = FALSE}
diagnosis %>%
  reshape_diagnosis() %>%
  select(`Mean Estimand`, Bias, RMSE) %>%
  kable(digits = 3, booktabs = TRUE, caption = "EStimation of the conditional mean of a normalized latent variable")
```

We see from the diagnosis that we do quite well here in recovering the conditional mean of the latent variable. Even here though, where we assume a lot of knowledge about the underlying model, our estimate has some bias.
### Exercises

1. Modify the design so that only two rather than three observed components are used. How is the diagnosis affected? Does it matter which indices you use?
2. Try to modify the design so that (a) indices are generated by a factor analysis procedure and (b) the different indices vary in the extent to which they are correlated with each other.
3. How is estimation affected if you have the wrong model linking measures to the latent variables? Try a design in which the measures are non linear in the latent variable. 

