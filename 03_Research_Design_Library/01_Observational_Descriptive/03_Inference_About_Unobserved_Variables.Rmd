---
title: "Inference about unobserved variables "
output: html_document
bibliography: ../../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

## Inference about unobserved variables 

Oftentimes researchers seek to measure a latent variable (`Y_star`) but have access only to proxies  (`Y_1`, `Y_2`, `Y_3`). In such cases, the proxies are sometimes combined into an index which is thought to capture `Y_star` and then used for further analysis.

A difficult feature of such problems is that we do not have access to the *scale* on which `Y_star` is measured and so it may seem like a hopeless exercise to try to assess whether we have got good or bad estimates of `Y_star` when we combine the measured data. 

One way around this is to normalize the scale of both the latent variable and the measured variable so that they have a mean of 0 and unit standard deviation. But in that case, we are guaranteed that our estimate of the mean of the normalized variable will be unbiased! We will certainly estimate a mean of 0! That may be but as we show in the declaration below, if your model is correct this approach may still be useful for calculating other quantities --- such as conditional means---that you don't just get right by construction.  

### Declaration

In the declaration below `Y_star` has a normal distribution but it is not centered on zero. The measured variables `Y_1`, `Y_2`, `Y_3` are also normally distributed but each has its own scale; they are all related to `Y_star`, though some more strongly than others. We construct two indices. One (`Y_av`) is constructed by first scaling each of these measured variables, then averaging and then scaling again. This is akin to the approach used in @kling2007experimental. The second also weights the components but this time using weights generated by 'principal components analysis' which, intuitively, seeks to find a weighting that minimizes the distance to the measured variables. 

```{r}

design <-
  declare_population(N = 250, gender = rep(0:1, N/2), Y_star = 1 + gender + 2* rnorm(N)) +
  declare_estimand(Y_bar = mean(scale(Y_star)[gender == 1])) + 
  declare_measurement(Y_1 = 3 + 0.1 * Y_star + rnorm(N, sd = 0.5),
                      Y_2 = 2 + 1.0 * Y_star + rnorm(N, sd = 1),
                      Y_3 = 1 + 0.5 * Y_star + rnorm(N, sd = 0.5),
                      Y_av = scale((scale(Y_1) + scale(Y_2) + scale(Y_3))),
                      Y_fa  = princomp(~ Y_1 + Y_2 + Y_2, cor = TRUE)$scores[,1]) + 
  declare_estimator(Y_av ~ 1, model = lm_robust, estimand = "Y_bar", subset = gender ==1, label = "Average") +
  declare_estimator(Y_fa ~ 1, model = lm_robust, estimand = "Y_bar", subset = gender ==1, label = "principal components")

```

### Dag

A DAG representing this design is given in Figure \@ref(fig:indexdag): an underlying measure gives rise to three observed measures which are then (mechanically) combined to produce a measured index.

```{r indexdag, echo = FALSE, fig.cap = "Index"}

dag <-
  dagify(Y_1 ~ Y_star, Y_2 ~ Y_star, 
         Y_3 ~ Y_star, 
         Y_fa ~ Y_1 + Y_2 + Y_3)

nodes <-
  tibble(
    name = c("Y_star", "Y_1", "Y_2", "Y_3", "Y_fa"),
    label = c("Y^*", "Y<sup>1</sup>", "Y<sup>2</sup>", "Y<sup>3</sup>", "I"),
    answer_strategy = "uncontrolled",
    annotation = c(
      "**Latent outcome**<br>unmeasurable",
      "**Measured outcome**",
      "",
      "",
      "**Constructed Index**"),
    x = c(1, 3, 3, 3, 5),
    y = c(2.5, 3.5, 2.5, 1.5, 2.5),
    nudge_direction = c("N", "N", "S", "N", "N")
  )

ggdd_df <- make_dag_df(dag, nodes, design)
base_dag_plot %+% ggdd_df
```

### Diagnosis

<!-- `r flagit()` -->
<!-- Perhaps a good psychometric example would be worthwhile to explore here? Or an education example? -->


<!-- make sure to rename the section title below -->

```{r inference_about_unobserved_variables, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 10000
b_sims <- 50
```


```{r, eval = FALSE}
diagnosis <- diagnose_design(design) 
```


```{r, echo = FALSE, eval = do_diagnosis & !exists("do_bookdown")}
diagnosis <- diagnose_design(design, sims = sims, bootstrap_sims = b_sims) 
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("Random_Sampling"), "/latent_sims.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnosis, path = rds_file_path)
}
diagnosis <- read_rds(rds_file_path)
```

The diagnosis is given in Table \@ref(tab:latentmodel). We show the RMSE and bias but also the average value of the estimand (and the estimate).

```{r latentmodel, echo = FALSE}
diagnosis %>%
  reshape_diagnosis() %>%
  select(`Estimator Label`, `Mean Estimand`, Bias, RMSE) %>%
  kable(digits = 3, booktabs = TRUE, caption = "Estimation of the conditional mean of a normalized latent variable")
```


We see from the diagnosis that we do quite well here in recovering the conditional mean of the standardized latent variable.  


### Exercises

1. Modify the design so that only two rather than three observed components are used. How is the diagnosis affected? Does it matter which indices you use?
<!-- 2. Try to modify the design so that (a) indices are generated by a factor analysis procedure and (b) the different indices vary in the extent to which they are correlated with each other. -->
2. How is estimation affected if you have the wrong model linking measures to the latent variables? Try a design in which the measures are non linear in the latent variable. 

