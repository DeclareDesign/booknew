---
title: "List experiments"
output: html_document
# bibliography: ../bib/book.bib 
bibliography: ../../bib/book.bib # use this line comment the above
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

```{r sensitive_questions, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 10
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

## List experiments

Sometimes, subjects might not tell the truth about certain attitudes or behaviors when asked directly. Responses may be affected by sensitivity bias, or the tendency of survey subjects to dissemble for fear of negative repercussions if some individual or group learns their true response [@blair2018list]. In such cases, standard survey estimates based on direct questions will be biased. One class of solutions to this problem is to obscure individual responses, providing protection from social or legal pressures. When we obscure responses systematically through an experiment, we can often still identify average quantities of interest. One such design is the list experiment (introduced in @miller1984new), which asks respondents for the count of the number of `yes' responses to a series of questions including the sensitive item, rather than for a yes or no answer on the sensitive item itself. List experiments give subjects cover by aggregating their answer to the sensitive item with responses to other questions.

For example, during the 2016 Presidential election in the U.S., some observers were concerned that pre-election estimates of support for Donald Trump might have been downwardly biased by "Shy Trump Supporters" -- survey respondents who supported Trump in their hearts, but were embarrassed to admit it to pollsters. To assess this possibility, @coppock2017did obtained estimates of Trump support that were free of social desirability bias using a list experiment. Subjects in the control and treatment groups were asked: "Here is a list of [three/four] things that some people would do and some people would not. Please tell me HOW MANY of them you would do. We do not want to know which ones of these you would do, just how many. Here are the [three/four] things:"

| Control                                                                                           | Treatment                                                                                                                                                                |
| ------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| If it were up for a vote, I would vote to raise the minimum wage to 15 dollars an hour            | If it were up for a vote, I would vote to raise the minimum wage to 15 dollars an hour                                                                                   |
| If it were up for a vote, I would vote to repeal the Affordable Care Act, also known as Obamacare | If it were up for a vote, I would vote to repeal the Affordable Care Act, also known as Obamacare                                                                        |
| If it were up for a vote, I would vote to ban assault weapons                                     | If it were up for a vote, I would vote to ban assault weapons                                                                                                            |
|                                                                                                   | If the 2016 presidential election were being held today and the candidates were Hillary Clinton (Democrat) and Donald Trump (Republican), I would vote for Donald Trump. |

The treatment group averaged 1.843 items while the control group averaged 1.548 items, for a difference-in-means estimate 0.296. Under the usual assumptions of randomized experiments, the difference-in-means is an unbiased estimator for the average treatment effect of *being asked* to respond to the treated list versus the control list. But our (descriptive) estimand is the proportion of people who support Donald Trump. 

For the difference-in-means to be an unbiased estimator for that inquiry, we invoke two additional assumptions [@Imai2011]:

- **No design effects.** The count of ``yes'' responses to control items must be the same whether a respondent is assigned to the treatment or control group. The assumption highlights that we need a good estimate of the average control item count from the control group (in the example, 1.843). We use that to net out the control item count from responses to the treated group (what is left is the sensitive item proportion). 

- **No misreporting.** The respondent must report the truthful answer to the sensitive item in the treatment group, when granted the anonymity protection of the list experiment. This assumption relies on the fact that the sensitive item is aggregated among the control items and so identifying individual responses is, in most cases, not possible, and this cover is enough to change the respondent's willingness to truthfully report. However, there are two circumstances in which the respondent is not provided any cover: if the respondent reports "zero" in the treatment group, they are exactly identified as not holding the sensitive trait; when they report the highest possible count in the treatment group, they are exactly identified as holding the trait. We describe the resulting biases below as floor and ceiling effects, respectively.

The estimate is, under these assumptions, free from sensitivity bias and an unbiased estimator for the proportion holding the sensitive item. However, list experiment estimates have much higher variance than those of direct questions. For the Trump survey example, the 95\% confidence interval for the list experiment estimate is nearly 14 percentage points wide, whereas the 95\% confidence interval for the (possibly biased!) direct question asked of the same sample is closer to 4 percentage points. The choice between the list experiments and the direct question is therefore a **bias-variance tradeoff**. List experiments may have less bias, but they are of higher variance. Direct questions may be biased, but they have less variance.

### Declaration

- **M**odel: Our model includes subjects' true support for Donald Trump ($Y_star$) and whether or not they are "shy" ($S$). These two variables combine to determine how subjects will respond when asked directly about Trump support.

    The potential outcomes model combines three types of information to determine how subjects will respond to the list experiment: their responses to the three nonsensitive control items ($X$), their true support for Trump ($Y_star$), and whether they are assigned to see the treatment or the control list ($Z$). Notice that our definition of the potential outcomes embeds the no liars and no design effects assumptions required for the list experiment design.

    We also have a global parameter that reflects our expectations about the proportion of Trump supporters who are shy. It's set at 20%.
    
```{r}
model <- 
  declare_population(
    N = 100,
    U = rnorm(N),
    X = rbinom(N, size = 3, prob = 0.5),
    Y_star = rbinom(N, size = 1, prob = 0.3),
    S = case_when(Y_star == 0 ~ 0L,
                  Y_star == 1 ~ rbinom(N, size = 1, prob = 0.2))
  ) + 
  declare_potential_outcomes(Y_list ~ Y_star * Z + X) 
```

- **I**nquiry: Our estimand is the proportion of voters who actually plan to vote for Trump. 

```{r}
inquiry <- declare_estimand(proportion = mean(Y_star))
```

- **D**ata strategy: We randomly assign 50\% of our 100 subjects to treatment and the remainder to control. In the survey, we ask subjects both the direct question ($Y_{\rm direct}$) and the list experiment question ($Y_{\rm list}$).

```{r}
data_strategy <- 
  declare_measurement(Y_direct = Y_star - S) +
  declare_assignment(prob = 0.5) + 
  declare_reveal(Y_list, Z)
```

- **A**nswer strategy: We estimate the proportion of truthful Trump voters in two ways. First, we take the mean of answers to the direct question. Second, we take the difference in means in the responses to the list experiment question.

```{r}
answer_strategy <-
  declare_estimator(Y_direct ~ 1,
                    model = lm_robust,
                    estimand = "proportion",
                    label = "direct") +
  declare_estimator(Y_list ~ Z, estimand = "proportion", label = "list")
```

```{r}
design <- model + inquiry + data_strategy + answer_strategy
```

### DAG

The no liars assumption of list experiments is evident in the DAG: Sensitivity bias $S$ is not a parent of the list experiment outcome $Y^L$ by assumption (no liars). The no design effects assumption is not directly visible. 

```{r, echo = FALSE}
dag <- dagify(Y_direct ~ Y_star + S,
              Y_list ~ Y_star + X + Z,
              S ~ U,
              X ~ U,
              Y_star ~ U)

nodes <-
  tibble(
    name = c("U", "S", "Y_star", "X", "Y_direct", "Y_list", "Z"),
    label = c(
      "U",
      "S",
      "Y<sup>*</sup>",
      "X",
      "Y<sup>D</sup>",
      "Y<sup>L</sup>",
      "Z"
    ),
    annotation = c(
      "Unknown<br>heterogeneity",
      "**Sensitivity bias**",
      "**Latent**<br> Sensitive trait",
      "Control item count",
      "**Outcome 1**<br> Direct question",
      "**Outcome 2**<br> List question",
      "**Random assignment**<br>List experiment condition"
    ),
    x = c(1, 7/3, 7/3, 7/3, 11/3, 11/3, 5),
    y = c(2.5, 4, 2.5, 1, 4, 2.5, 2.5),
    nudge_direction = c("N", "N", "S", "S", "N", "N", "S"),
    answer_strategy = "uncontrolled"
  )

ggdd_df <- make_dag_df(dag, nodes, design)

base_dag_plot %+% ggdd_df
```

### Diagnosis

```{r, eval = do_diagnosis & !exists("do_bookdown")}
simulations_list <- simulate_design(design, sims = sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("sensitive_questions"), "/simulations_list.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(simulations_list, path = rds_file_path)
}
simulations_list <- read_rds(rds_file_path)
```

The plot shows the sampling distribution of the direct and list experiment estimators. The sampling distribution of the direct question is tight but biased; the list experiment (if the requisite assumptions hold) is unbiased, but higher variance. The choice between these two estimators of the prevalence rate depends on which -- bias or variance -- is more important in a particular setting. See @blair2018list for an extended discussion of how the choice of research design depends deeply on the purpose of the project.

```{r, echo=FALSE}
summary_df <- 
  simulations_list %>%
  filter(estimand_label == "proportion_truthful_trump_vote") %>% 
  gather(key, value, estimand, estimate) %>%
  group_by(estimator_label, key) %>%
  summarize(average_value = mean(value))

simulations_list %>%
  ggplot(aes(estimate)) +
  geom_histogram(bins = 30) +
  geom_vline(data = summary_df, aes(xintercept = average_value, color = key, linetype = key)) +
  facet_wrap(~estimator_label)
```

### Assumption violations

Recent work on list experiments emphasizes the possibility of violations of both the no liars and the no design effects assumptions. We can diagnose the properties of our design under plausible violations of each. 

First, we consider violations of the no design effects assumption, which means that the control item count differs depending on whether a subject is assigned to treatment or control. Typically, this means that the inclusion of the sensitive item changes responses to the control items, because they are judged in relative terms or because the respondent became suspicious of the researcher's intentions due to the taboo of asking a sensitive question.

We declare a modified design below that defines two different potential control item counts depending on whether the respondent is in the treatment group ($X_{\rm treat}$) or control group ($X_{\rm control}$). The potential outcomes for the list outcome also change: $X_{\rm treat}$ is revealed in treatment and $X_{\rm control}$ in control.

```{r}
model_design_effects <- 
  declare_population(
    N = 100,
    U = rnorm(N),
    X_control = rbinom(N, size = 3, prob = 0.5),
    X_treat = rbinom(N, size = 3, prob = 0.25),
    Y_star = rbinom(N, size = 1, prob = 0.3),
    S = case_when(Y_star == 0 ~ 0L,
                  Y_star == 1 ~ rbinom(N, size = 1, prob = 0.2))
  ) + 
  declare_potential_outcomes(
    Y_list ~ (Y_star + X_treat) * Z + X_control * (1 - Z)
  )

design_design_effects <- 
  model_design_effects + inquiry + data_strategy + answer_strategy
```

```{r, eval = do_diagnosis}
diagnose_design_effects <- diagnose_design(design_design_effects, sims = sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("sensitive_questions"), "/list_design_fx_diagnosis.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnose_design_effects, path = rds_file_path)
}
diagnose_design_effects <- read_rds(rds_file_path)
```

```{r, echo = FALSE}
diagnose_design_effects %>%
  get_diagnosands %>%
  select(estimator_label, estimand_label, bias, rmse) %>%
  kable(digits = 3, booktabs = TRUE)
```

Second, a violation of no liars implies that respondents do not respond truthfully even when provided with the privacy protection of the list experiment. Two common circumstances researchers worry about are ceiling effects and floor effects. In ceiling effects, respondents respond with the maximum number of control items rather than with their truthful response of that plus one, to avoid being identified as holding the sensitive trait. The floor effects problem is the reverse, when respondents hide not holding the sensitive trait by responding one *more* than their truthful count in the treatment group.

```{r}
model_liars <- 
  declare_population(
    N = 100,
    U = rnorm(N),
    X = rbinom(N, size = 3, prob = 0.5),
    Y_star = rbinom(N, size = 1, prob = 0.3),
    S = case_when(Y_star == 0 ~ 0L,
                  Y_star == 1 ~ rbinom(N, size = 1, prob = 0.2))
  ) + 
  declare_potential_outcomes(
    Y_list ~ if_else(X == 3 & Y_star == 1 & Z == 1, 3, Y_star * Z + X)
  )

design_liars <- model_liars + inquiry + data_strategy + answer_strategy
```

```{r, eval = do_diagnosis}
diagnose_liars <- diagnose_design(design_liars, sims = sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("sensitive_questions"), "/list_liars_diagnosis.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnose_liars, path = rds_file_path)
}
diagnose_liars <- read_rds(rds_file_path)
```

```{r, echo = FALSE}
diagnose_liars %>%
  get_diagnosands %>%
  select(estimator_label, estimand_label, bias, rmse) %>%
  kable(digits = 3, booktabs = TRUE)
```

Ceiling and floor effects are not the only ways in which the no liars assumption might be violated. Respondents, when noting the sensitive item among the list, might always respond zero (or the highest number) regardless of their control item count to hide their response. A declaration could be made for these other kinds of violations, also by changing the potential outcomes for $Y_{\rm list}$.

### Redesign

Researchers have control of three important design parameters that affect the inferential power of list experiments: sample size, the number of control items, the selection of control items, and the proportion of the sample assigned to treatment. 

<!-- sample size -->

**Sample size.** The bias-variance tradeoff in the choice between list and direct questioning can be diagnosed by examining the root mean-squared error (a measure of the efficiency of the design) across two varying parameters: sample size and the amount of sensitivity. We declare a new design with varying $N$ and varying $\mathrm{proportion_shy}$, the proportion of Trump voters who withhold their truthful response when asked directly:

```{r}
model_sample_size <- 
  declare_population(
    N = N,
    U = rnorm(N),
    X = rbinom(N, size = 3, prob = 0.5),
    Y_star = rbinom(N, size = 1, prob = 0.3),
    S = case_when(Y_star == 0 ~ 0L,
                  Y_star == 1 ~ rbinom(N, size = 1, prob = proportion_shy))
  ) +
  declare_potential_outcomes(Y_list ~ Y_star * Z + X) 

design <- model_sample_size + inquiry + data_strategy + answer_strategy

designs <- redesign(design, proportion_shy = seq(from = 0, to = 0.5, by = 0.05), N = seq(from = 500, to = 5000, by = 500))
```

```{r, eval = do_diagnosis}
diagnosis_tradeoff <- diagnose_design(designs, sims = sims, bootstrap_sims = b_sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("sensitive_questions"), "/sample_size_tradeoff.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnosis_tradeoff, path = rds_file_path)
}
diagnosis_tradeoff <- read_rds(rds_file_path)
```

Diagnosing this design, we see that at low levels of sensitivity and low sample sizes, the direct question is preferred on RMSE grounds. This is because though the direct question is biased for the proportion of Trump voters in the presence of any sensitivity bias (positive ${\rm proportion_shy}$), it is much more efficient than the list experiment. When we have a large sample size, then we begin to prefer the list experiment for its low bias. At high levels of sensitivity, we prefer the list on RMSE grounds despite its inefficiency, because bias will be so large. Beyond the list experiment, this diagnosis illustrates that when comparing two possible designs we need to understand both the bias and the variance of the designs in order to select the best one in our setting. In other designs, it will not be the proportion who are shy but some other feature of the model or data and answer strategy that affect bias. 

```{r}
# make a plot
diagnosis_tradeoff %>%
  get_diagnosands %>%
  filter(proportion_shy < 0.35) %>% 
  mutate(estimator_lbl = factor(estimator_label, levels = c("direct", "list"), labels = c("Direct question", "List experiment"))) %>% 
  ggplot(aes(N, rmse, group = estimator_lbl, color = estimator_lbl)) +
  # bias line
  geom_line() + 
  scale_color_discrete("Question type") + 
  labs(y = "RMSE") + 
  facet_wrap(~ proportion_shy, ncol = 4) + 
  dd_theme() + 
  theme(legend.position = "bottom")
```

In the upper left, we see that when there is no sensitivity bias we always prefer the direct question due to the inefficiency of the list experiment. The red line is always below the blue. However, when we get to 0.1, there are sample sizes at which we prefer the direct question to the list: below 3000 subjects. However, above 3000 subjects the RMSE of the list experiment is better than the direct question. When we get to 25 percent of Trump supporters misreporting, we always prefer the list experiment in terms of RMSE. In other words, at such high levels of sensitivity bias we are always willing to tolerate the efficiency loss to get an unbiased estimate in this region.

<!-- N control items -->

**How many control items**. After sample size, an early choice list researchers must make is how many control items to select. Here we also face a tradeoff: the more control items, the more privacy protection for the respondent; but the more items the more variance and the less efficient our estimator of the proportion holding the sensitive item. We can quantify the amount of privacy protection provided as the average width of the confidence interval on the posterior prediction of the sensitive item given the observed count. The efficiency can be quantified as the RMSE.

```{r}
model_control_item_count <-
  declare_population(
    N = 100,
    U = rnorm(N),
    X = rbinom(N, size = J, prob = 0.5),
    Y_star = rbinom(N, size = 1, prob = 0.3),
    S = case_when(Y_star == 0 ~ 0L,
                  Y_star == 1 ~ rbinom(N, size = 1, prob = 0.2))
  ) + 
  declare_potential_outcomes(Y_list ~ Y_star * Z + X) 

design <- model_control_item_count + inquiry + data_strategy + answer_strategy

designs <- redesign(design, J = 2:5)
```

```{r, eval = do_diagnosis}
diagnosis_control_item_count <- diagnose_design(designs, sims = sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("sensitive_questions"), "/J_diagnosis.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnosis_control_item_count, path = rds_file_path)
}
diagnosis_control_item_count <- read_rds(rds_file_path)
```

```{r, echo = FALSE}
diagnosis_control_item_count %>%
  get_diagnosands %>%
  filter(estimator_label == "list") %>% 
  select(J, bias, rmse) %>%
  kable(digits = 3, booktabs = TRUE)
```

<!-- selection of control items -->

**Which control items**. The choice of which set of control items to ask can be as or more important than the number. There are three aims with their selection: reduce bias from ceiling and floor effects, provide sufficient cover to respondents so the no liars assumption is met, and increase efficiency of the estimates. The first goal can be met by reducing the number of people whose latent control count is between one and $J-1$, one above and one below the lowest and highest numbers possible in the treated group. Respondents in this band will not feel pressured to subtract (add) from their responses to hide that they (do not) hold the sensitive item. One solution to this is to add an item with high prevalance and an item with low prevalence. Though this would address problem one, it would violate problem two: items that are obviously high and low prevalence do nothing to add to privacy protection. The ideal control item count is one with low variance around the middle of the range of the count. To achieve this while providing sufficient cover, items that are inversely correlated can be added. 

```{r}
model_control_item_correlation <-
  declare_population(
    N = 100,
    U = rnorm(N),
    X_1 = draw_binary(0.5, N), 
    X_2 = correlate(given = X_1, rho = rho, draw_binary, prob = 0.5),
    X_3 = draw_binary(0.5, N),
    X = X_1 + X_2 + X_3,
    Y_star = rbinom(N, size = 1, prob = 0.3),
    S = case_when(Y_star == 0 ~ 0L,
                  Y_star == 1 ~ rbinom(N, size = 1, prob = 0.2))
  ) + 
  declare_potential_outcomes(Y_list ~ Y_star * Z + X) 

design <- model_control_item_correlation + inquiry + data_strategy + answer_strategy

designs <- redesign(design, rho = seq(from = 0, to = 1, by = 0.25))
```

```{r, eval = do_diagnosis}
diagnose_control_item_correlation <- diagnose_design(designs, sims = sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("sensitive_questions"), "/diagnose_control_item_correlation.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnose_control_item_correlation, path = rds_file_path)
}
diagnose_control_item_correlation <- read_rds(rds_file_path)
```

```{r, echo = FALSE}
diagnose_control_item_correlation %>%
  get_diagnosands %>%
  filter(estimator_label == "list") %>% 
  select(rho, estimator_label, estimand_label, bias, rmse) %>%
  kable(digits = 3, booktabs = TRUE)
```

<!-- proportion of sample treated -->

**Proportion treated.** A design parameter not often carefully considered in experiments in general and list experiments in particular is the proportion who are treated in the random assignment procedure. Often, researchers retain the default of 50-50 allocation in two-arm trials. However, as shown in Section XX, when the variance in the outcome differs across treatment and control, this is not the optimal allocation rule. Instead, more units should be assigned to the group in which the variance is higher. In the list experiment, the variance of the outcome is typically higher in the treatment group for the simple reason that the number of control items is higher. There is more variability from more items. To address this, researchers can set the proportion treated to be higher than 50%. We explore this tradeoff by redesigning the study with varying probabilities of assignment from 20% to 80% and examining the RMSE.

```{r}
data_strategy_prop_treated <- 
  declare_measurement(Y_direct = Y_star - S) +
  declare_assignment(prob = prop_treated) + 
  declare_reveal(Y_list, Z)

design <- model + inquiry + data_strategy_prop_treated + answer_strategy

designs <- redesign(design, prop_treated = seq(from = 0.2, to = 0.8, by = 0.1))
```

```{r, eval = do_diagnosis}
diagnose_prop_treated <- diagnose_design(designs, sims = sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("sensitive_questions"), "/prop_treated_diagnosis.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnose_prop_treated, path = rds_file_path)
}
diagnose_prop_treated <- read_rds(rds_file_path)
```

```{r, echo = FALSE}
diagnose_prop_treated %>%
  get_diagnosands %>%
  filter(estimator_label == "list") %>% 
  select(prop_treated, estimator_label, estimand_label, rmse) %>%
  kable(digits = 3, booktabs = TRUE)
```


<!-- See Blair and Imai (2012) and Li (2019) for methods for addressing violations no liars assumption through modeling and bounds. -->

<!-- Li, Y. (n.d.). Relaxing the No Liars Assumption in List Experiment Analyses. Political Analysis, 1-16. doi:10.1017/pan.2019.7 -->

<!-- Changes the answer strategy have been proposed to address both the no design effects and no liars assumption. @blair2012statistical propose a statistical test for the design effects assumption; if it does not pass, they suggest not analyzing the list experiment data (i.e., this is a procedure that makes up an answer strategy). Scholars have also identified improvements to the answer strategy to address violations of no liars: @blair2012statistical provides a model that adjusts for ceiling and floor effects and @li2019relaxing provides a bounds approach that relaxes the assumption. -->

### Related designs

```{r, echo = FALSE}
rr_forced_known <- function(formula, data) {
  fit  <- try(rrreg(formula, data = data, p = 2/3, p0 = 1/6, p1 = 1/6, design = "forced-known"))
  pred <- try(as.data.frame(predict(fit, avg = TRUE, quasi.bayes = TRUE)))
  if(class(fit) != "try-error" & class(pred) != "try-error") {
    names(pred) <- c("estimate", "std.error", "conf.low", "conf.high")
    pred$p.value <- with(pred, 2 * pnorm(-abs(estimate / std.error)))
  } else {
    pred <- data.frame(estimate = NA, std.error = NA, conf.low = NA, conf.high = NA, p.value = NA, error = TRUE)
  }
  pred
}

rr_forced_known <- label_estimator(rr_forced_known)
```

The list experiment is one of several experimental designs for answering descriptive inquiries about sensitive topics (for a review see XX). Most can target the same inquiry: the proportion of subjects who hold a sensitive trait. The randomized response technique is another such design, itself with many variants. In a "forced response" randomized response design (see @blairimaizhou), respondents are asked to roll a dice, and depending on the dice result they either answer honestly or are "forced" to answer either "yes" or "no." With a six-sided dice, respondents might be asked to answer "yes" if they roll a 6, "no" if they roll a "1", and to answer the question truthfully if they roll any other number, two through five. Because the probability of rolling a 1 and 6 are known, we can back out the probability of answering the sensitive item from the observed data. Declaring this design necessitates changes in M (potential outcomes are a function of the dice roll); D (random assignment is the dice roll itself); and A (an estimator that is a function of the observed outcomes and the known probability of being forced into each response). We declare one below:

```{r}
library(rr)

model_rr <- 
  declare_population(
    N = 100,
    U = rnorm(N),
    X = rbinom(N, size = 3, prob = 0.5),
    Y_star = rbinom(N, size = 1, prob = 0.3),
    S = case_when(Y_star == 0 ~ 0L,
                  Y_star == 1 ~ rbinom(N, size = 1, prob = 0.2))
  ) + 
  declare_potential_outcomes(
    Y_rr ~ 
      case_when(
        dice == 1 ~ 0L,
        dice %in% 2:5 ~ Y_star,
        dice == 6 ~ 1L
      ),
    conditions = 1:6, assignment_variable = "dice")

data_strategy_rr <- 
  declare_measurement(Y_direct = Y_star - S) +
  declare_assignment(prob_each = rep(1/6, 6), conditions = 1:6,
                     assignment_variable = "dice") + 
  declare_reveal(Y_rr, dice)

answer_strategy_rr <-
  declare_estimator(Y_direct ~ 1,
                    model = lm_robust,
                    estimand = "proportion",
                    label = "direct") +
  declare_estimator(Y_rr ~ 1, handler = rr_forced_known, 
                    label = "forced_known", estimand = "proportion")


design <- model_rr + inquiry + data_strategy_rr + answer_strategy_rr
```

### Further readings

- @miller1984new
- @Imai2011
- @blair2018list

