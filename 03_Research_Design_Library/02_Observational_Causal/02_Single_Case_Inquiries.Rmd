---
title: "Causal inquiries about a single unit"
output:
  pdf_document: default
  html_document: default
bibliography: ../../bib/book.bib
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

<!-- make sure to rename the section title below -->

```{r observational-single-unit, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 5000
b_sims <- 500
```

```{r, echo = FALSE}
library(Synth)
library(CausalQueries)
```

## Causal inquiries about a single unit

We sometimes start a causal investigation wanting to know the causal effect for a single unit. For example, for a unit that experienced an event or process, we want to know the causal effect of that event. To do so, we need to know what would have happened if the event did not happen, the *counter*factual outcome as opposed to the factual outcome that actually occurred. Due to the fundamental problem of causal inference, we cannot *observe* what would have happened if that counterfactual case had happened. We have to guess---or infer---what would have happened. Social scientists have developed a large array of tools for guessing missing counterfactual outcomes --- what would have happened in the counterfactual case, if the event had not happened.^[We are also interested in the opposite case sometimes: we have a unit that did not experience an event, and we want to know the causal effect of *not* having it. In this case, we need to guess what would have happened if the event did happen. The same tools apply in reverse.]

A common estimand in this setting is whether an outcome was *due* to a cause. This "attribution" estimand or "Cause of Effect" estimand can be written $CoE:=1-Y(0)| X=1 \text{ \& } Y=1$. For a unit with  $X=1$ and $Y=1$,  $CoE=1$ if $Y(0)=0$. One might also be interested in the treatment effect of the treated case (TET). The TET is the difference between the treated potential outcome and the control potential outcome in the posttreatment period for the treated unit of interest---but not conditioning on knowledge of the outcome. 

```{r, echo = FALSE, include = FALSE}
dag <- dagify(Y ~ X + D + U,
              D ~ X)
nodes <-
  tibble(
    name = c("X", "U", "D", "Y"),
    label = c("X", "U", "D", "Y"),
    annotation = c(
      "**Control variable**",
      "**Unknown heterogeneity**",
      "**Treatment**",
      "**Outcome variable**"
    ),
    x = c(1, 5, 3, 5),
    y = c(4, 4, 2.5, 2.5),
    nudge_direction = c("N", "N", "W", "S"),
    data_strategy = "unmanipulated",
    answer_strategy = c("controlled",  "uncontrolled", "uncontrolled", "uncontrolled")
  )


ggdd_df <- make_dag_df(dag, nodes)

base_dag_plot %+% ggdd_df
```

### Process tracing

<!-- fearon1991, biqq -->

"Process tracing" is a prominent strategy for assessing causes of effects [@BennettCheckel2015PT; @fairfield2017explicit]. Here, following for example @humphreys2017qualitative, we think of process tracing as a procedure in which researchers provide a theory, in the form of a causal model, that is rich enough to characterize the probability of observing ancillary data ("Causal process observations" [@brady2004data]) given underlying causal relations. If equipped with priors, such a model in turn lets one use Bayes' rule to form posteriors over causal relations when these observations are observed. 

For intuition, say we are interested in whether a policy caused a change in economic outcomes. We expect that for the policy to matter it at least had to be implemented and so if we find out that it was not implemented we infer that it did not matter. Thus we make theory dependent inferences that are reasonable *insofar as* the theory is reasonable. In this example, if there are plausible channels through which a policy might have mattered even if not implmenented, then our conclusion would not be warranted.

To illustrate design choices for a process tracing study we consider a setting in which we have observed $X=1$ and $Y=1$ and we are interested in figuring out whether $Y=1$ *because* $X=1$. More specifically we imagine a model with two ancillary variables, *M* and  $W$. We posit that $X$ causes $Y$ via *M*---with negative effects of $X$ on *M* and of *M* and $Y$ ruled out. And we posit that $W$ is a cause of both *M* and $Y$, specifically we posit that if $W=1$ then $X$ causes  *M* and *M* causes $Y$ for sure. Under this model *M* and $W$ each serve as "clues" for the causal effect of $X$ on $Y$. Using the language popularized by @VanEvera1997, *M* provides a "hoop" test---if you look for data on *M* and find that $M=0$ then you infer that $X$ did not cause $Y$; if on the other hand you examine $W$ and find that $W=1$ then you have a "smoking gun" test and you infer that $X$ did indeed cause $Y$. If you find both $M=0$ and $W=1$ then you know your model is wrong. 

The model can be described using the `CausalQueries` package thus:

```{r}
model <- make_model("X -> M -> Y <- W -> M") %>%
  set_restrictions("(M[X=1] < M[X=0]) | (M[X=1, W=1] == M[X=0, W=1])") %>%
  set_restrictions("(Y[M=1] < Y[M=0]) | (Y[M=1, W=1] == Y[M=0, W=1])")

plot(model)
```

This model definition describes the DAG but also specifies a set of restrictions on causal relations. By default flat priors are then placed over all other possible causal relations, though  other prior beliefs could also be  specified. 

We now have all we need to assess what inferences we might make given different sorts of observations using `CausalQueries::query_model`.  This function lets you ask any causal question of of a model conditional on observed (or counterfactual) conditions. We can use it to calculate beliefs, likely data, and conditional inferences thus: 

```{r}  
queries <- 
  CausalQueries::query_model(
    model,
    query = list('Prob(CoE=1)' = "Y[X=1] > Y[X=0]",
                 'Prob(M=1)' = "M==1",
                 'Prob(CoE=1 | M=0)' = "Y[X=1] > Y[X=0]",
                 'Prob(CoE=1 | M=1)' = "Y[X=1] > Y[X=0]"),
    given = list("Y==1 & X==1",
                 "Y==1 & X==1",
                 "Y==1 & X==1 & M==0",
                 "Y==1 & X==1 & M==1"),
    using = "parameters") 
```

```{r ptprobative, echo = FALSE}
queries %>% select(-Using) %>% #, - Case.estimand) %>%
  kable(caption = "Beliefs for a case with $X=1, Y=1$. The first row gives the prior belief that $X=1$ caused $Y=1$.  The second row gives the expecation that M=1. The last rows gives posterior beliefs that X=1 caused Y=1 after M is observed, depending on what is found.", digits = 2)
```

From the model specification, we can calculate directly the probability of observing $M=0$ or $M=1$ (or $W=0$ or $W=1$) and what we would infer in each case. Table \@ref(tab:ptprobative) shows an example of these values. From a table like this we can calculate directly the expected posterior variance associated with a process tracing data strategy and a Bayesian answer strategy. For instance, here our prior on CoE is `r round(queries$mean[1],2)` which implies a variance of `r round((1-queries$mean[1])*queries$mean[1],2)`. If we gather data on *M* however our posterior variance will be either `r round((1-queries$mean[4])*queries$mean[4],2)` (with probability `r round(queries$mean[2],2)`) or 0.

Thus we see that we can already imagine how our a design in which we seek data on *M* only will perform in expectation in terms of reducing our uncertainty.  No simulation is required. Even still, we think it useful to fold these quantities into a design declaration so that users can access the data strategies and answer strategies in the same way as they would for any other problem.

The design declaration below uses a custom data function that draws a value of the estimand (EoC) using the prior in the first row of Table \@ref(tab:ptprobative)  and then draws values of *M*,  using values from the second row of Table \@ref(tab:ptprobative) (and similarly for $W$). The estimation step also uses a custom function, which simply returns the posterior estimates---like those in the last rows of Table \@ref(tab:ptprobative), but for both *M* and $W$. No data strategy is provided because we imagine estimation given all possible data strategies (observation of *M*, of $W$, and of both).


```{r, eval = FALSE}
design <- 
  declare_population(data = data_function()) + 
  declare_inquiry(CoE = CoE) +
  declare_estimator(handler = my_estimator_function)
```

Given such a model, a case in which $X=Y=1$, and limited resources, we now want to know whether we would be better gathering data on *M* or on $W$ or both?  The answers are given in Table \@ref(tab:ptdiagnosis). We see only modest declines in expected posterior variance from observation of the mediator *M*, consistent with the manual calculation above; but large declines from observing $W$.



```{r ptdiagnosis, echo = FALSE, purl = FALSE, fig.cap = "Diagnosis of process tracing strategies"}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("single_case_inquiries"), "/process_tracing.RDS")
if (do_diagnosis & !exists("do_bookdown")) {

data_possibilities <- 
  list("M==0", "M==1", "W==0", "W==1", 
       "M==0 & W==0", "M==0 & W==1", "M==1 & W==0", "M==1 & W==1")

probative_values <- query_model(
  model, 
  query = "Y[X=1] > Y[X=0]", 
  given = c("X==1 & Y==1", paste0(data_possibilities, " & X==1 & Y==1")), 
  using = "parameters", expand_grid = TRUE) 

data_probabilities <- 
  lapply(0:1, function(M) lapply(0:1, function(W)
    query_model(
      model,
      query = paste("M==", M, "& W==", W),
      given = c("X==1 & Y==1 & Y[X=0]==0", "X==1 & Y==1 & Y[X=0]==1"),
      using = "parameters", expand_grid = TRUE) %>%
      mutate(M = M, W = W))) %>% 
  bind_rows %>%
  mutate(CoE = Given == "X==1 & Y==1 & Y[X=0]==0") %>% 
  arrange(Given) %>%
  select(M, W, CoE, mean) 

my_estimator_function <- function(data){
  
  givens <- with(data, c("X==1 & Y==1",
    paste0("M==", M, " & X==1 & Y==1"),
    paste0("W==", W, " & X==1 & Y==1"),
    paste0("M==", M, " & W==", W, " & X==1 & Y==1")))

  probative_values %>% 
    filter(Given %in% givens) %>%
    mutate(
      estimate = mean,
      estimator_label = c("Prior", "M only", "W only", "Both"),
      inquiry_label = "CoE") %>% 
    data.frame(stringsAsFactors = FALSE)
}

data_function <- function() {
    type <- runif(1) < probative_values %>% 
      filter(Given == "X==1 & Y==1") %>% pull(mean)
    data_probabilities %>% filter(CoE==type) %>% 
      mutate(sample = rmultinom(1,1, mean)) %>% filter(sample ==1)
}

design <- 
  declare_population(data =  data_function()) + 
  declare_estimand(CoE = CoE) +
  declare_estimator(handler = my_estimator_function)

diagnosands <- declare_diagnosands(
  bias = mean(estimate - estimand),
  mse = mean((estimate - estimand) ^ 2),
  mean_posterior_var = mean(estimate*(1-estimate)),
  mean_estimate = mean(estimate),
  mean_estimand = mean(estimand)
)

diagnosis <- diagnose_design(design, sims = 1000, diagnosands = diagnosands)

  
  write_rds(diagnosis, rds_file_path)
}
diagnosis <- read_rds(rds_file_path)

diagnosis$diagnosands_df  <- diagnosis$diagnosands_df %>% mutate(estimator_label = factor(estimator_label, c("Prior", "M only", "W only", "Both")))

diagnosis %>% reshape_diagnosis() %>% select(-`Design Label`) %>%
  kable(caption = "Inference upon observation of *M* or $W$ for cases in which $X = 1$ and $Y = 1$. We see that expected posterior variance (equivalently, expected mean squared error) falls modestly when *M* is observed but substantially when $W$ is observed. The gains from observing $W$ are modest if *M* is already observed.", digits = 3)

```

Note here that the causal model used for *M* and *I* is the same model used for *A*. This does not *have* to be the case however. If instead we used different models for these two parts we could assess how well or poorly our strategy performs when our model is wrong. In this case, and unlike the results in table   \@ref(tab:ptdiagnosis) we would find that the expected posterior variance (where the expectation is taken with respect to the model in *M* but posterior taken with respect to the model in *A*) will not be the same as the expected mean squared error. Can you see why not?

 

### Before-after comparison

A natural idea for guessing what would have happened if an event did not occur for a single unit is to look within that unit at what happened *before* the event did occur. Within-unit over-time comparisons use outcomes in pretreatment periods to impute the posttreatment counterfactual outcome. 

In order for the pretreatment outcome to be a good stand-in for the posttreatment control potential outcome, we must invoke two assumptions: excludability of time; and no interference between time periods. The excludability assumption says that the only change between pre and post is the fact that the treatment is administered. This rules out time trends and any other form of time-varying heterogeneity aside from the treatment status. The second assumption is that the unit is assigned to control in the pretreatment period cannot affect outcomes in the posttreatment period, and the fact that the unit is treated in the second period cannot affect outcomes in the first period. This is often called a no carryover assumption, and most importantly in this case rules out the possibility of effects of anticipating treatment in the next period. If the unit expects to be treated in period 1, this may affect the outcome in period 0 even though treatment has not yet been administered. Often anticipation effects wash out treatment effects; it is as if the treatment has already taken place. 

To declare this design, we consider violations of the excludability assumption. The inquiry is the TET, as in all designs in this section. In the model, we consider two sources of heterogeneity in the outcome: time trends (an effect of time) and time-varying heterogeneity (an effect of $U_{\rm time}$). Potential outcomes are a function of these two variables and the treatment variable. Our data strategy is to measure $Y$ in both periods, and this will reveal the control potential outcome in the first period, before treatment, and the control potential outcome in the second period, after treatment. Our answer strategy is to take the difference in the outcome between the post- and pretreatment periods. 

```{r}
design <- 
  declare_model(
    N = 2, 
    time = 0:1,
    U_time = rnorm(N),
    potential_outcomes(
      Y ~ time_trend * time + time_specific_effect * U_time + 0.5 * Z
    ),
    Z = 1
  ) + 
  declare_inquiry(TET = Y_Z_1 - Y_Z_0, subset = time == 1) + 
  declare_measurement(Y = ifelse(time == 0, Y_Z_0, Y_Z_1)) + 
  declare_estimator(
    estimate = Y[time == 1] - Y[time == 0], 
    inquiry_label = "TET", handler = summarize)

designs <- redesign(
  design, time_trend = c(0, 0.5), time_specific_effect = c(0, 1))
```


```{r, eval = do_diagnosis & !exists("do_bookdown")}
diagnosis <- diagnose_design(designs, sims = sims, bootstrap_sims = b_sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("single_case_inquiries"), "/diagnosis_over_time.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnosis, file = rds_file_path)
}
diagnosis <- read_rds(rds_file_path)
```

We diagnose the design in four settings, the two-by-two of: with and without time trends, and with and without time-specific effects. We see that the over-time within-unit design is only unbiased when there are neither. If there are time trends, there is bias, due to a violation of the excludability of time assumption. If there is time-varying heterogeneity aside from the treatment, the excludability assumption is also violated. There is no test for these two conditions that lead to bias, and there is no diagnostic like the pretrends comparison for the difference-in-difference design. 

```{r, echo = FALSE}
kable(get_diagnosands(diagnosis) %>% select(time_trend, time_specific_effect, bias, `se(bias)`), booktabs = TRUE, digits = 3)
```

### Posttreatment comparison to an untreated case

A second natural design is to compare the outcomes of the treated case after to treatment to the outcomes in another unit in the same time period that did not receive the treatment. Instead of filling in the control potential outcome of the treated unit with its own outcome in the pretreatment period, we fill it in with the outcomes of this second "control" unit.

We can invoke an exactly parallel set of assumptions to the over time within unit design: excludability of the unit difference (rather than time), i.e., the only difference between the two units is the treatment status and not other unit-specific heterogeneity. This strong assumption is often relaxed in favor of a "selection-on-observables" assumption by accounting for observable differences between units and selecting a comparison unit that is similar in all observable ways. 

In declaring the design, we target the same TET inquiry, and swap our model of over time heterogeneity with with between-unit heterogeneity ($U_{\rm unit}$) (there is not a parallel of time trends). Our data strategy is to measure Y for both the treated unit and the comparison unit in the posttreatment time period; the treated unit reveals its treated potential outcome, while the comparison unit reveals its control potential outcome. The answer strategy is the posttreatment comparison between the treated and control unit.

```{r}
design <- 
  declare_model(
    N = 2, 
    time = 1,
    U_unit = rnorm(N),
    potential_outcomes(Y ~ 0.5 * Z + unit_specific_effect * U_unit),
    Z = if_else(U_unit == max(U_unit), 1, 0)
  ) + 
  declare_inquiry(TET = Y_Z_1 - Y_Z_0, subset = time == 1) + 
  declare_measurement(Y = ifelse(Z == 0, Y_Z_0, Y_Z_1)) + 
  declare_estimator(
    estimate = Y[Z == 1] - Y[Z == 0], 
    inquiry_label = "TET", handler = summarize)

designs <- redesign(design, unit_specific_effect = c(0, 1))
```

```{r, eval = do_diagnosis & !exists("do_bookdown")}
diagnosis <- diagnose_design(designs, sims = sims, bootstrap_sims = b_sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("single_case_inquiries"), "/diagnosis_across_unit.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnosis, file = rds_file_path)
}
diagnosis <- read_rds(rds_file_path)
```

We diagnose the two-unit posttreatment comparison design under two settings: with and without unit-specific differences not accounted for by observables, aside from the treatment. With these differences, the excludability assumption is violated. We see that the design is only unbiased for the TET if there are no such unit-specific differences besides treatment. 

```{r, echo = FALSE}
kable(get_diagnosands(diagnosis) %>% select(unit_specific_effect, bias, `se(bias)`), booktabs = TRUE, digits = 3)
```

### Difference-in-differences 

When the over-time excludability of treatment and the selection-on-observables assumptions are unreasonable, an alternative is the difference-in-differences design. We difference out unit characteristics, whether observable or not, that do not vary over time by comparing the outcomes before and after treatment. What is left is time trends and other unit-invariant time-varying factors, and we difference these out by comparing the change over time in the treated unit to the change over time in the comparison unit. Each difference takes out one class of factors that would violate the excludability assumptions of over-time within unit designs alone or posttreatment comparison across-unit designs alone.

The difference-in-difference design does not rely on the assumptions the earlier two designs do. But it adds a new assumption: the parallel trends assumption. In order for the difference-in-difference design to work, the change between before and after treatment in the control potential outcomes must be equal (i.e., parallel). Because this assumption depends on the change in values of the unrealized (and thus unobservable) control potential outcome in the treated unit, it cannot be tested. There is a widely-used diagnostic of the difference in observed trends before treatment [...]

Declaring this design combines the elements of the within-unit over time and between-unit posttreatment designs. We have two units (the treated unit and its comparison unit), and two time periods, before (0) and after treatment (1). There is unit-specific, time invariant heterogeneity ($U_{\rm unit}$), and unit-invariant over time heterogeneity ($U_{\rm time}$). The potential outcomes are a function of treatment, these heterogeneity variables, and time trends. We target the same TET inquiry as the other designs. We measure the outcome in a way that combines the two previous designs: the control potential outcome is revealed in both periods for the comparison untreated unit *and* in the pretreatmetn period for the treated unit, and the treated potential outocme is revealed in the posttreatment period of the treated unit. The answer strategy is the difference-in-differences, first differencing off within-unit changes from first to second period then across-unit changes in the comparison unit.

```{r}
design <- 
  declare_model(
    unit = add_level(N = 2, U_unit = rnorm(N, sd = 0.5), Z = if_else(U_unit == max(U_unit), 1, 0)),
    period = add_level(N = 2, time = 0:1, U_time = rnorm(N), nest = FALSE),
    unit_period = cross_levels(
      by = join(unit, period), 
      U = rnorm(N, sd = 0.01),
      potential_outcomes(
        Y ~ time_trend * 0.5 * time + 
          time_specific_effect * U_time + 
          unit_specific_effect * U_unit + 
          1.25 * Z + U)
    )
  ) + 
  declare_inquiry(TET = Y_Z_1 - Y_Z_0, subset = time == 1) + 
  declare_measurement(Y = if_else(Z == 0 | period == 1, Y_Z_0, Y_Z_1)) + 
  declare_estimator(
    estimate = 
      (mean(Y[Z == 1 & time == 2]) - mean(Y[Z == 1 & time == 1])) - 
      (mean(Y[Z == 0 & time == 2]) - mean(Y[Z == 0 & time == 1])), 
    inquiry_label = "ATT", handler = summarize)

designs <- redesign(design, 
                    time_trend = c(0, 0.5), 
                    time_specific_effect = c(0, 1), 
                    unit_specific_effect = c(0, 1))
```

```{r, eval = do_diagnosis & !exists("do_bookdown")}
diagnosis <- diagnose_design(designs, sims = sims, bootstrap_sims = b_sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("single_case_inquiries"), "/diagnosis_diff_in_diff.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnosis, file = rds_file_path)
}
diagnosis <- read_rds(rds_file_path)
```

We diagnose the difference-in-difference design under eight cases, the combinations of: with and without time trends in outcomes; with and without other, time-varying unit-invariant heterogeneity; and with and without unit-specific time-invariant heterogeneity. We see the power of the difference-in-difference design: under any of these alternatives, the design is unbiased for the TET inquiry. In the over-time within-unit design and the posttreatment comparison across units design, the design is unbiased only if the relevant one of these conditions holds. Unfortunately, there are no tests of the validity of those assumptions.

```{r, echo = FALSE}
kable(get_diagnosands(diagnosis) %>% select(time_trend, time_specific_effect, unit_specific_effect, bias, `se(bias)`), booktabs = TRUE, digits = 3)
```


### Synthetic controls

```{r, echo=FALSE}
synth_weights_tidy <- function(data, predictors, time.predictors.prior, dependent, unit.variable, time.variable, treatment.identifier, controls.identifier) {
  dataprep.out <- dataprep(
    foo = data,
    predictors = predictors,
    predictors.op = "mean",
    time.predictors.prior = time.predictors.prior,
    dependent = dependent,
    unit.variable = unit.variable,
    time.variable = time.variable,
    treatment.identifier = treatment.identifier,
    controls.identifier = controls.identifier, 
    time.optimize.ssr = time.predictors.prior,
    time.plot = time.predictors.prior)
  capture.output(fit <- synth(data.prep.obj = dataprep.out))
  tab <- synth.tab(dataprep.res = dataprep.out, synth.res = fit) 
  
  weights_df <- tab$tab.w %>% mutate(synth_weights = w.weights) %>% 
    dplyr::select(synth_weights, !!unit.variable := unit.numbers)
  
  data %>%
    left_join(weights_df) %>%
    mutate(synth_weights = replace_na(synth_weights, 1))
}

```


```{r}
design <- 
  declare_model(
    units = add_level(N = 10, unit_ID = 1:10, U_unit = rnorm(N), X = rnorm(N), Z = if_else(unit_ID == 1, 1, 0)), # if_else(U_unit == max(U_unit), 1, 0)),
    periods = add_level(N = 3, time = -1:1, U_time = rnorm(N), nest = FALSE),
    unit_periods = cross_levels(
      by = join(units, periods), 
      U = rnorm(N),
      potential_outcomes(Y ~ time_trend * 0.5 * time + time_specific_effect * U_time + 
          unit_specific_effect * U_unit + 
            1.25 * Z + X + U),
      Y = if_else(Z == 0 | time <= 0, Y_Z_0, Y_Z_1)
    )
  ) + 
  declare_inquiry(ATT = mean(Y_Z_1 - Y_Z_0), subset = time == 1) + 
  declare_measurement(predictors = "X",
                    time.predictors.prior = -1:0,
                    dependent = "Y",
                    unit.variable = "unit_ID",
                    time.variable = "time",
                    treatment.identifier = 1,
                    controls.identifier = 2:10, 
                    handler = synth_weights_tidy) +
  declare_estimator(Y ~ Z, subset = time == 1, weights = synth_weights, 
                    model = lm_robust, label = "synth")
```

```{r, eval = FALSE}
designs <- redesign(design, 
                    time_trend = c(0, 0.5), 
                    time_specific_effect = c(0, 1), 
                    unit_specific_effect = c(0, 1))

diagnosis <- diagnose_design(designs, sims = 5000, diagnosands = declare_diagnosands(bias = mean(estimate - estimand, na.rm = TRUE)))

get_diagnosands(diagnosis) %>% select(time_trend, time_specific_effect, unit_specific_effect, bias, `se(bias)`) %>% round(2)
```


