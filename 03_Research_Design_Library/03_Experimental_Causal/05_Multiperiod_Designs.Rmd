---
title: "Multiperiod Designs"
output: html_document
bibliography: ../../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

## Multiperiod Designs


- Baseline-endline (connection to covariate control, e.g., controlling for Y_pre)
- multiple rounds of post-treatment measurement. 
    - One justification is we're interested in persistence.
    - multiple measurements decrease variance.
    - Inquiries: "persistence ratio" ATE_t2 / ATE_t1 versus "average persistence" (Y_{t2}(1) - Y_{t2}(0))/(Y_{t1}(1) - Y_{t1}(0))
- Crossover
- Stepped Wedge
- Downstream experimentation. add ONE MORE PERIOD! 

- Literature: "more T", Durably reducing Transphobia. Nollywood is stepped wedge. Progressa for baseline endline? 
- massive point is that we DONT use time to infer causal effects, we still use the random assignment.



In a stepped wedge design, individuals are randomly assigned to enter into treatment in different stages and at each stage, their outcomes are remeasured. Figure \@ref(fig:steppedwedge) illustrates where the study gets its name: as two units are randomly added to the treatment group in each of the three waves, the treatment group increases in "steps" while the control group diminishes.

Often, stepped wedge designs are vaunted for their policy appeal because they allow for everyone to be (eventually) treated in the context of an experiment. However, you could achieve the same goal in a regular two-arm trial by treating everyone after the final wave of measurement, and not all stepped wedge designs involve treating everyone. Beyond its ethical and logistical appeal, the design can squeeze more power out of a small sample by treating each wave as though it were its own study.

### Declaration

- **M**odel: Eight units are randomized and remeasured at three time points. Importantly, any unit at any given point in time reveals one of only two potential outcomes --- treated or untreated. Their untreated potential outcomes consist of a unit- and a unit-period-specific shock. Treated potential outcomes increase relative to the control potential outcome by a rate of 1 each period --- in other words, there are bigger treatment effects in later periods.

- **I**nquiry: Our estimand is the average treatment effect over all units and periods in the sample. Averaged over the three periods, the ATE is (1 + 2 + 3) / 3 = 2.

- **D**ata strategy: Our assignment strategy adds two units (one-quarter of the total sample) to the treatment at random in each wave, leaving two units who are not treated in any wave at all---see the remaining two orange squares at the top-right corner of  Figure \@ref(fig:steppedwedge). Notice that, while every *unit* is assigned to a treatment wave with equal probability, each wave reveals treated and untreated potential outcomes of *unit-periods* with different probabilities. In the first wave, one-quarter of the units reveal their treated potential outcomes; in the second wave, one-half of the units reveal their treated potential outcomes, and in the final wave, three-quarters of the units reveal treated potential outcomes. In essence, our experiment is like a block-randomized trial, with unit-periods grouped into period-specific blocks that have differential probabilities of assignment.

- **A**nswer strategy: Just as in a block-randomized trial with differential probabilities, we need to take account of the fact that our assignment strategy "under-represents" treated potential outcomes in the first wave and "over-represents" them in the last wave. Inverse-propensity weights (IPWs) are one way to correct for such disparities. Each unit's IPW is represented in Figure \@ref(fig:steppedwedge). Using these weights, we get a "representative" estimate of the average control and treatment potential outcomes in each wave. If we didn't apply the weights, we wouldn't get a representative view of the unobserved potential outcomes we're sampling, and our estimates would be biased. So what does all this buy us? We also declare a "two-arm" estimator that focuses on wave 2 only, where half of the units are in control and half in treatment. Diagnosis shows that all the remeasuring and randomizing gets us (.52 - .41) / .41 = 27% higher power.

```{r}
design <-
  declare_model(
    unit = add_level(N = 8,
                     X = rnorm(N)),
    period = add_level(
      N = 3,
      time = as.numeric(period),
      p = c(1 / 4, 1 / 4 + 1 / 4, 1 / 4 + 1 / 4 + 1 / 4),
      nest = FALSE
    ),
    obs = cross_levels(by = join(unit, period),
                       U = rnorm(N))
  ) +
  declare_potential_outcomes(Y ~ X + U + Z * time) +
  declare_assignment(
    clusters = unit,
    conditions = 1:4,
    assignment_variable = "wave"
  ) +
  declare_assignment(Z = as.numeric(time >= wave),
                     ipw = 1 / (Z * p + (1 - Z) * (1 - p)),
                     handler = fabricate) +
  declare_reveal(Y, Z) +
  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +
  declare_estimator(
    Y ~ Z,
    model = lm_robust,
    estimand = "ATE",
    label = "1: Stepped Wedge",
    weights = ipw,
    clusters = unit
  ) +
  declare_estimator(
    Y ~ Z,
    model = lm_robust,
    estimand = "ATE",
    label = "2: Wave 2 Only",
    subset = period == 2
  )
```

```{r steppedwedge, fig.cap = "Illustration of random assignment in a stepped-wedge design.", echo = FALSE}
dat <- draw_data(design)

treatment_color <- "royalblue2"
control_color <- "darkorange3"

dat %>% 
  arrange(wave,unit) %>% 
  group_by(period) %>%
  mutate(unit = 1:n(), Assignment = ifelse(Z == 1, "Treatment", "Control")) %>% 
  ggplot(aes(x = period, y = unit, fill = Assignment)) +
  geom_tile(color = "white") + 
  scale_fill_manual(values = c(control_color, treatment_color)) +
  # scale_fill_grey(start = .9,end = .5) +
  geom_text(aes(label = round(ipw,2))) +
  dd_theme() +
  theme(legend.position = "right")
```

### DAG

```{r, echo=FALSE}
dag <- dagify(Y ~ Z + X + U + time,
              Z ~ time)

nodes <-
  tibble(
    name = c("X", "U", "time", "Z", "Y"),
    label = c("X", "U", "T", "Z", "Y"),
    annotation = c(
      "**Unknown heterogeneity**<br>Unit effects",
      "**Unknown heterogeneity**<br>",
      "**Time period**<br>",
      "**Treatment assignment**<br>",
      "**Outcome variable**<br>"
    ),
    x = c(1, 5, 1, 3, 5),
    y = c(4, 4, 1, 2.5, 2.5), 
    nudge_direction = c("N", "N", "S", "N", "S"),
    data_strategy = c("unmanipulated", "unmanipulated", "unmanipulated", "assignment", "unmanipulated"),
    answer_strategy = "uncontrolled"
  )

ggdd_df <- make_dag_df(dag, nodes)

base_dag_plot %+% ggdd_df
```

### Example

### Exercises

1. Add an answer strategy to the design that doesn't include IPWs and diagnose the design. 
  
  i. How does this affect the bias in the estimates? Explain your answer.
  ii. Now change the potential outcomes function so that the treatment effect is constant across periods. How does this affect the bias? Explain your answer.
  iii. How does the relative power of the stepped-wedge over the single-period, two-arm trial change when the treatment effect is restricted to be constant across periods? Explain your answer.

2. Our design makes a subtle but crucial assumption about the potential outcomes: namely, that the treated potential outcome revealed in one period is the same irrespective of whether the unit was treated in the previous (or subsequent) periods. What would a violation of this assumption look like? 




<!-- - Comparing 1 and 2, stepped wedge gives a big improvement in power, is -->
<!-- unbiased, and gets the coverage correct. Clearly better than just doing a -->
<!-- cross-section at wave 2.  -->

<!-- - Estimator 3 shows two pitfalls, however, that can lead us to overestimate -->
<!-- benefits of SW. -->

<!-- 1. First, it is biased: [explain how treated POs are more commonly observed in -->
<!-- periods when they're higher, we need to weight for this. How weights come about: -->
<!-- There is one way that a unit can be observed in a treated state in wave 1: they -->
<!-- are assigned to W1 with probability `p_W1`. There are two ways in which a unit -->
<!-- can be treated in wave 2: they are assigned in W1 with `p_W1` or in W2 with -->
<!-- `p_W2`. Because being assigned in W1 or in W2 are exclusive and independent -->
<!-- events, we can get the prob of being treated in any period by wave 2 by summing -->
<!-- the probs.] -->

<!-- 2. Second, standard errors are wrong. Units should be treated as clusters (draw -->
<!-- analogue to two-stage random assignment in saturation design?) -->

<!-- A natural question is how power changes as more or fewer units are assigned.  -->
<!-- Here, we consider variations on the stepped wedge design declared above, in  -->
<!-- which we hold constant at 3 the number of units assigned to the pure control,  -->
<!-- and shift an increasing proportion of the assignment to later waves. -->

<!-- ```{r} -->
<!-- designs <- list( -->
<!--   a = redesign(design, p_00 = 3/8, p_W1 =  3/8, p_W2 = 1/8, p_W3 = 1/8), -->
<!--   b = redesign(design, p_00 = 3/8, p_W1 =  2/8, p_W2 = 2/8, p_W3 = 1/8), -->
<!--   c = redesign(design, p_00 = 3/8, p_W1 =  2/8, p_W2 = 1/8, p_W3 = 2/8), -->
<!--   d = redesign(design, p_00 = 3/8, p_W1 =  1/8, p_W2 = 3/8, p_W3 = 1/8), -->
<!--   e = redesign(design, p_00 = 3/8, p_W1 =  1/8, p_W2 = 2/8, p_W3 = 2/8), -->
<!--   f = redesign(design, p_00 = 3/8, p_W1 =  1/8, p_W2 = 1/8, p_W3 = 3/8) -->
<!--   ) -->
<!-- ``` -->

<!-- ```{r, eval = do_diagnosis & !exists("do_bookdown")} -->
<!-- # Diagnose design -->
<!-- diagnoses <- diagnose_designs(designs) -->
<!-- ``` -->

<!-- ```{r, echo = FALSE, purl = FALSE} -->
<!-- # figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file -->
<!-- rds_file_path <- paste0(get_dropbox_path("05_Stepped_Wedge_Designs.Rmd"), "/diagnoses.RDS") -->
<!-- if (do_diagnosis & !exists("do_bookdown")) { -->
<!--   write_rds(diagnoses, path = rds_file_path) -->
<!-- } -->
<!-- diagnoses <- read_rds(rds_file_path) -->
<!-- ``` -->


<!-- ```{r, echo = FALSE} -->
<!-- designs_data <- names(designs) %>%  -->
<!--   lapply(function(name) designs[[name]] %>%  -->
<!--            draw_data %>%  -->
<!--            mutate(design_id = name)) %>%  -->
<!--   do.call(what = rbind,args = .) %>%  -->
<!--   arrange(design_id,wave,i) %>%  -->
<!--   group_by(t,design_id) %>% -->
<!--   mutate(i = 1:n(), Assignment = ifelse(Z == 1, "Treatment", "Control"))  -->


<!-- diags <- diagnoses %>%  -->
<!--   get_diagnosands() %>%  -->
<!--   filter(estimator_label == "2: Weighted, clustered SW") %>%  -->
<!--   mutate( -->
<!--     design_name = paste0("Design ", design_label, ": W1 = ",8*p_W1, -->
<!--                          "; W2 = ", 8*p_W2,  -->
<!--                          "; W3 = ", 8*p_W3,",\nPower = ",round(power,2) -->

<!--                          ) -->
<!--   ) -->

<!-- designs_data <- left_join(designs_data, diags %>% select(design_name, design_label), by = c("design_id" = "design_label")) -->

<!-- designs_data %>%  -->
<!--   ggplot(aes(x = t, y = i, fill = Assignment)) + -->
<!--   geom_tile(color = "white") +  -->
<!--   scale_fill_manual(values = c(control_color, treatment_color)) + -->
<!--   dd_theme() + facet_wrap(~ design_name, nrow = 1) +  -->
<!--   scale_y_continuous("",labels = NULL) -->


<!-- ``` -->

<!-- - We see a monotonic decrease in the power as more of the sample is treated  -->
<!-- later. This occurs because treating units later means observing less of the -->
<!-- potential outcomes. The potential outcomes are more variant, so we're better -->
<!-- off when we observe more of them [I think!] -->

<!-- ### Spillovers -->

<!-- An important assumption is that the potential outcomes revealed are not a -->
<!-- function of what wave the unit entered into treatment. In other words, there are -->
<!-- no Show how, in case of such spillovers, you can just treat the effects as -->
<!-- different, but then you lose power gains (and possibly change estimand) -->





