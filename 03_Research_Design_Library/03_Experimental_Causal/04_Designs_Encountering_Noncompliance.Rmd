---
title: "Designs encountering noncompliance"
output: html_document
bibliography: ../../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

<!-- make sure to rename the section title below -->

```{r encouragement, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```


## Designs encountering noncompliance

- designs with noncompliance
- encouragement designs
- placebo-controlled design (note one part is measurement one half is causal)


Experiments encounter noncompliance when units' treatment *status* differs from their treatment *assignment*. Noncompliance occurs when units assigned to treatment fail to take treatment or when units assigned to control find a way to take treatment. This section will describe a range of strategies that experimenters use to address noncompliance, and along the way, we'll point out some common pitfalls.

Any time a data strategy entails contacting subjects in order to deliver a treatment like a bundle of information or some good, noncompliance is a potential problem. Emails go undelivered, unopened, and unread. Letters get lost in the mail. Phone calls are screened, text messages get blocked, DMs are ignored. People don't come to the door when you knock, either because they aren't home or they don't trust strangers. Noncompliance can affect noninformational treatments as well: goods may be difficult to deliver to remote locations, subjects may refuse to participate in assigned experimental activities, or research staff might simply fail to respect the realized treatment schedule out of laziness or incompetence.

<!-- A common form of noncompliance is one-sided noncompliance, which occurs when some units in the assigned treatment group fail to take treatment and all units in the control group remain untreated. One-sided noncompliance is common when experimenters "do nothing" to the control group but try (and sometimes fail) to deliver treatment to the treatment group.   -->

What can be done? Experimenters who anticipate noncompliance should make compensating adjustments to their research designs (relative to the canonical two arm design). These adjustments ripple through M, I, D, and A. 

The biggest change to M is developing beliefs about compliance types, also called "principal strata" [@frangakis2002principal]. In a two-arm trial, subjects can be one of four compliance types, depending on how their treatment status responds to their treatment assignment. The four types are described in Table \@ref(tab:compliancetypes2). $D_i(Z = 0)$ is a potential outcome -- it is the treatment status that unit $i$ would express if assigned to control. Likewise, $D_i(Z = 1)$ is the treatment status that unit $i$ would express if assigned to treatment. These potential outcomes can take each take on a value of 0 or 1, so their intersection allows for four types.  For Always-takers, $D_i$ is equal to 1 regardless of the value of $Z$ -- they always take treatment. Never-takers are the opposite -- $D_i$ is equal to 0 regardless of the value of $Z$. For Always-takers and Never-takers, assignment to treatment *does not change* whether they take treatment. 

Compliers are units that take treatment if assigned to treatment and do not take treatment if assigned to control. Their name "compliers" connotes that something about their disposition as subjects makes them "compliant" or otherwise docile, but this connotation is misleading. Compliance types are generated by the confluence of subject behavior and data strategy choices. Whether or not a subject answers the door when the canvasser comes calling is at least in part a function of whether the subject is at home. Data strategies that attempt to deliver treatment in the evenings and on weekends might generate more (or different) compliers than those that attempt treatment during working hours.

| Compliance Type | $D_i(Z_i = 0)$ | $D_i(Z_i = 1)$ |
|-----------------|----------------|----------------|
| Never-taker     | 0              | 0              |
| Complier        | 0              | 1              |
| Defier          | 1              | 0              |
| Always-taker    | 1              | 1              |

Table: (\#tab:compliancetypes2) Compliance types

The last compliance type to describe are defiers. These strange birds refuse treatment when assigned to treatment, but find a way to obtain treatment when assigned to control. Whether or not "defiers" exist turns out to be a consequential assumption that must be made in the model. We have good reason to believe that defiers are rare -- assignment to treatment almost always has a positive average effect on treatment take-up, and we are aware of no cases in which assignment caused a decrease in take-up, even among a subgroup.

Without further assumptions, we can never be sure of any unit's compliance type. Subjects assigned to the control group who take take treatment ($D_i(0) = 1$) could be defiers or always-takers. Subjects assigned to the treatment group who do not take treatment ($D_i(1) = 0$) could be defiers or never-takers. Our inability to be sure of compliance types is another facet of the fundamental problem of causal inference. Even though a subject's compliance type (with respect to a given design) is a stable trait, it is defined by how the subject would act in multiple counterfactual worlds. We can't tell what type a unit is because we would need to see whether they take treatment when assigned to treatment and also when assigned to control. 

The inclusion of noncompliance and compliance types to the model also necessitate changes to the inquiry. Always-takers and Never-takers present a real problem for causal inference. Even with the power to randomly assign, we can't change what treatments these units take. As a result, *we don't get to learn* about the effects of treatment among these groups. Even if our inquiry were the average effect of treatment among the never-takers, the experiment (as designed) would not be able to generate empirical estimates of it.^[We write "as designed" because compliance types are defined with respect to a particular design. If it were possible to induce the never-takers to take treatment (i.e., under a different data strategy, these units might be compliers), this inquiry would not *necessarily* be out of reach.] Our inquiry has to fall back to the average effects among those units that whose treatment status we can successfully manipulate -- the compliers.

We call this inquiry the complier average causal effect (the CACE). It is defined as $\E[Y_i(1) - Y_i(0) | d_i(1) > d_i(0)]$. Just like the average treatment effect, it refers to an average over individual causal effects, but this average is taken over a specific subset of units, the compliers. Compliers are the only units for whom $d_i(1) > d_i(0)$, because for compliers, $d_i(1)  = 1$ and $d_i(0) = 0$. When assignments and treatments are binary, the CACE is mathematically identical to the local average treatment effect (LATE) described in chapter XXX. Whether we write CACE or LATE sometimes depends on academic discipline, with LATE being more common among economists. An advantage of "CACE" over "LATE" is that we are specific about which units the effect is "local" to -- it is local to the compliers. 

When experiments encounter noncompliance, the CACE is *usually* the most important inquiry for theory, since it refers to the average effect of the causal variable under study, at least for a subset of the units in the study. However, two other common inquiries are important to address here as well. 

The first is the intention-to-treat (ITT), which is defined as $\E[Y_i(D_i(Z = 1), Z = 1) - Y_i(D_i(Z = 0), Z = 0)]$. Assignment to treatment $Z$ has a total effect on $Y$ that is mediated in whole or in part by the treatment status. Sometimes the ITT is the policy-relevant estimand, since it describes what would happen if a policy maker implemented the policy in the same way as the experiment, *inclusive* of noncompliance. Consider a randomized controlled trial on the effectiveness of a tax webinar on tax compliance. Even if the webinar is very effective among people willing to watch it (the CACE is large), the main trouble faced by the policy maker will be getting people to sit through the webinar. The ITT describes the average effect of *inviting* people to the webinar, which could be quite small if very few people are willing to join.

The second additional inquiry is the compliance rate, sometimes referred to as the $ITT_D$. It describes the average effect of assignment on treatment, and is written $\E[(D_i(Z = 1) - D_i(Z = 0)]$. A small bit of algebra shows that the $ITT_D$ is equal to the fraction of the sample that are compliers minus the fraction that are defiers. 

These three inquiries are tightly related. Under five very important assumptions (described below), we can write: 

$$
\begin{align*}
CACE = \frac{ITT}{ITT_D}
\end{align*}
$$

A derivation of this relationship is given in our section on instrumental variables (section XXX). The five assumptions described in that section are identical to the assumptions required here. In an experimental setting, "exogeneity of the instrument" is guaranteed by features of the data strategy. Since we use random assignment, we know for sure that the "instrument" (the assignment) is exogenous. Excludability of the instrument refers to the idea that the effect of the assignment variable on the outcome is fully mediated by the treatment. This assumption could be violated if the mere act of assignment changes outcomes. Stated differently, if never-takers or always-takers reveal *different* potential outcomes in treatment and control ($Y_i(D_i(Z = 1), Z = 1) \neq Y_i(D_i(Z = 0), Z = 0)$), it must be because assignment itself changes outcomes. Non-interference in this setting means that units' treatment status and outcomes do not depend on the assignment or treatment status of other units. In an experimental context, the assumption of monotonicity rules out the existence of defiers. This assumption is often made plausible by features of the data strategy (perhaps it is impossible for those who are not assigned to treatment to obtain treatment) or features of the model ("defiant" responses to assignment are behaviorially unlikely). The final assumption -- nonzero effect of the instrument on the treatment -- can also be assured by features of the data strategy. In order to learn about the effects of treatment, data strategies must successfully cause at least some units assigned to treatment to take treatment.

Experimental designs rely very heavily on their data strategies. First, we must choose a random assignment procedure that accommodates important social structures (by clustering at the right level) and that generates tight sampling distributions (by blocking on covariates that are correlated with the outcome). Second, we have to ensure that units comply with their assignments as determined by that carefully calibrated procedure. When experimenters expect that noncompliance will be a problem, they should take steps to mitigate that problem in the data strategy. Sometimes doing so means quite simply trying harder: investigating the patterns of noncompliance, attempting to deliver treatment on multiple occasions, or offering subjects incentives for participation. "Trying harder" is about turning more subjects into compliers by choosing a data strategy that encounters less noncompliance.

A second important change to the data strategy is the explicit measurement of treatment status as distinct from treatment assignment. For some designs, measuring treatment status is easy: we simply record which units in the treatment group we were able to treat and which we were unable to treat. For other, measuring compliance is trickier. For example, if treatments are emailed, we might never know if subjects read the email. Perhaps our email service will track read receipts, in which case one facet of this measurement problem is solved. We won't know, however, how many subjects read the subject line -- and if the subject line contains any treatment information, then even subjects who don't click on the email may be "partially" treated. Our main advice is to measure compliance in the most conservative way: if treatment emails bounce altogether, then subjects are not treated. [REWRITE]

Estimation of the CACE is not as straightforward subseting the analysis to compliers. A plug-in estimator of the CACE with good properties takes the ratio of the $ITT$ estimate to the $ITT_d$ estimate. Since the $ITT_d$ must be a number between zero and one, this estimator "inflates" the $ITT$ by the compliance rate. Another way of thinking about this is that the $ITT$ is deflated by all the never-takers and always-takers, among whom the $ITT$ is by construction 0, so instead of "inflating" we are "re-inflating" the ITT to the level of the CACE. Two-stage least squares in which we instrument the treatment with the random assignment is a numerically equivalent procedure when treatment and assignments are binary. Two-stage least squares has the further advantage of being able to seamlessly incorporate covariate information to increase precision.

### Declaration

This design declaration will show how two-stage least squares provides good estimates of the CACE not the ATE, which we can't learn about, since always-takers and never-takers don't respond to treatment assignment. Alternative estimators (per-protocol and as treated) are biased for both the CACE and the ATE.


```{r}
design <-
  declare_model(
    N = 100,
    type = 
      rep(c("Always-Taker", "Never-Taker", "Complier", "Defier"),
          c(0.2, 0.2, 0.6, 0.0)*N),
    U = rnorm(N),
    potential_outcomes(
      Y ~ case_when(
        type == "Always-Taker" ~ -0.25 - 0.50 * D + U,
        type == "Never-Taker" ~ 0.75 - 0.25 * D + U,
        type == "Complier" ~ 0.25 + 0.50 * D + U,
        type == "Defier" ~ -0.25 - 0.50 * D + U
      ),
      conditions = list(D = c(0, 1))
    ),
    potential_outcomes(
      D ~ case_when(
        Z == 1 & type %in% c("Always-Taker", "Complier") ~ 1,
        Z == 1 & type %in% c("Never-Taker", "Defier") ~ 0,
        Z == 0 & type %in% c("Never-Taker", "Complier") ~ 0,
        Z == 0 & type %in% c("Always-Taker", "Defier") ~ 1
      ),
      conditions = list(Z = c(0, 1))
    )
  ) +
  declare_inquiry(
    ATE = mean(Y_D_1 - Y_D_0),
    CACE = mean(Y_D_1[type == "Complier"] - Y_D_0[type == "Complier"])) +
  declare_assignment(Z = conduct_ra(N = N, prob = 0.5), handler = fabricate) +
  declare_measurement(D = fabricatr::reveal_outcomes(D ~ Z),
                      Y = fabricatr::reveal_outcomes(Y ~ D)) +
  declare_estimator(
    Y ~ D | Z,
    model = iv_robust,
    estimand = c("ATE", "CACE"),
    label = "2SLS"
  ) +
  declare_estimator(
    Y ~ D,
    model = lm_robust,
    estimand = c("ATE", "CACE"),
    label = "As treated"
  ) +
  declare_estimator(
    Y ~ D,
    model = lm_robust,
    estimand = c("ATE", "CACE"),
    subset = D == Z,
    label = "Per protocol"
  )

```



This chunk is set to `echo = TRUE` and `eval = do_diagnosis`
```{r, eval = do_diagnosis & !exists("do_bookdown")}
dx <- diagnose_design(design)
```

Right after you do simulations, you want to save the simulations rds. 

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("encouragement"), "/dx.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(dx, file = rds_file_path)
}
dx <- read_rds(rds_file_path)
```

```{r cacesim, echo=FALSE, fig.width = 6.5, fig.height=3, fig.cap="sampling distribution of three estimators"}
inquiries_df <- draw_estimands(design) 

label_df <- inquiries_df %>% mutate(estimator_label = "2SLS", 
                                    x = c(-0.1, 0.8),
                                    y = 75)

dx$simulations_df %>% filter(estimand_label == "ATE") %>%
  ggplot(aes(estimate)) +
  geom_histogram(bins = 30) +
  facet_wrap(~estimator_label, ncol = 1) +
  geom_vline(data = inquiries_df, aes(xintercept = estimand, color = estimand_label), linetype = "dashed") +
  geom_text(data = label_df, aes(x = x, y = y, label = estimand_label)) +
  dd_theme() +
  theme(legend.position = "none")
  
```




### DAG

```{r, echo = FALSE}
dag <- dagify(Y ~ D + type + U,
              D ~ Z + type + U,
              type ~ U)

nodes <-
  tibble(
    name = c("Z", "D", "U", "Y", "type"),
    label = c("Z", "D", "U", "Y", "C"),
    annotation = c(
      "**Random assignment**",
      "**Treatment received**",
      "**Unknown heterogeneity**",
      "**Outcome**",
      "**Principal stratum**<br>Compliance type"
    ),
    x = c(1, 3, 4, 5, 2),
    y = c(1, 1, 4, 1, 4),
    nudge_direction = c("S", "S", "N", "S", "N"),
    data_strategy = c("assignment", "unmanipulated", "unmanipulated", "unmanipulated", "unmanipulated"),
    answer_strategy = "uncontrolled", 
  )

ggdd_df <- make_dag_df(dag, nodes)

base_dag_plot %+% ggdd_df
```

### Example

To do: demonstrate how violations of no defiers and excludability leads to bias.

```{r}
types <- c("Always-Taker", "Never-Taker", "Complier", "Defier")
direct_effect_of_encouragement <- 0.0
proportion_defiers <- 0.0

design <-
  declare_model(
    N = 500,
    type = sample(
      types,
      N,
      replace = TRUE,
      prob = c(0.1, 0.1, 0.8 - proportion_defiers, proportion_defiers)
    ),
    noise = rnorm(N)
  ) +
  declare_potential_outcomes(
    D ~ case_when(
      Z == 0 & type %in% c("Never-Taker", "Complier") ~ 0,
      Z == 1 & type %in% c("Never-Taker", "Defier") ~ 0,
      Z == 0 & type %in% c("Always-Taker", "Defier") ~ 1,
      Z == 1 & type %in% c("Always-Taker", "Complier") ~ 1
    )
  ) +
  declare_potential_outcomes(
    Y ~ 0.5 * (type == "Complier") * D +
      0.25 * (type == "Always-Taker") * D +
      0.75 * (type == "Defier") * D +
      direct_effect_of_encouragement * Z + noise,
    assignment_variables = c("D", "Z")
  ) +
  declare_inquiry(CACE = mean((Y_D_1_Z_1 + Y_D_1_Z_0) / 2 -
                                 (Y_D_0_Z_1 + Y_D_0_Z_0) / 2),
                   subset = type == "Complier") +
  declare_assignment(prob = 0.5) +
  declare_reveal(D, assignment_variable = Z) +
  declare_reveal(Y, assignment_variables = c(D, Z)) +
  declare_estimator(Y ~ D | Z, model = iv_robust, inquiry = "CACE")

```

```{r, eval = do_diagnosis & !exists("do_bookdown")}
designs <- redesign(
  design,
  proportion_defiers = seq(0, 0.3, length.out = 5),
  direct_effect_of_encouragement = seq(0, 0.3, length.out = 5)
)

simulations_df <- simulate_design(designs, sims = sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("04_Encouragement_Designs.Rmd"), "/simulations_df.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(simulations_df, path = rds_file_path)
}
simulations_df <- read_rds(rds_file_path)
```


```{r, echo = FALSE}
gg_df <-
  simulations_df %>%
  group_by(proportion_defiers,
           direct_effect_of_encouragement) %>%
  summarize(bias = mean(estimate - estimand))

ggplot(gg_df,
       aes(
         proportion_defiers,
         bias,
         group = direct_effect_of_encouragement,
         color = direct_effect_of_encouragement
       )) +
  geom_point() +
  geom_line() + 
  dd_theme() + 
  theme(legend.position = "bottom")
```





