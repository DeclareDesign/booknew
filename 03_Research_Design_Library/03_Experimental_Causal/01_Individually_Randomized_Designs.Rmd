---
title: "Individually-randomized designs"
output: html_document
bibliography: ../../bib/book.bib
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

<!-- make sure to rename the section title below -->

```{r two_arm_trials, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

## Individually-randomized designs

`r flagit()`

All two-arm randomized trials have in common that subjects can be randomly assigned to one of two conditions, typically, one treatment condition and one control condition. Some two-arm trials eschew the pure control condition in favor of a placebo control condition, or even a second treatment condition. The uniting feature of all these designs is that the model includes two and only two potential outcomes for each unit and that the data strategy randomly assigns which of these potential outcomes will be revealed.

A key choice in the design of two arm trials is the random assignment procedure. Will we use simple (coin flip, or Bernoulli) random assignment or will we use complete random assignment? Will the randomization be blocked or clustered? Will we "restrict" the randomization so that only randomizations that generate acceptable levels of balance on pre-treatment characteristic are permitted? We will explore the implications of some of these choices in the coming sections, but for the moment, the main point is that saying "treatments were assigned at random" is insufficient -- we need to describe the randomization procedure in detail in order to know how to analyze the resulting experiment. See Section \@ref(p2assignment) for a description of many different random assignment procedures.

For the remainder of this section, we'll consider the canonical two arm-trial design described in @Gerber2012. In short, the canonical design conducts complete random assignment in a fixed population, then uses difference-in-means to estimate the average treatment effect. We'll now unpack this shorthand into the components of M, I, D, and A.

The model specified a fixed sample of $N$ subjects. Here we aren't imagining that we are sampling from a larger population first -- we have in mind a fixed set of units among whom we will conduct our experiment. Under our model, each unit is endowed with two latent potential outcomes: a treated potential outcome and an untreated potential outcome. The potential outcomes themselves have a correlation of $\rho$. If units with higher untreated potential outcomes also have higher treated potential outcomes, $\rho$ will be positive. Reflecting on how treatment effects might vary from unit to unit gives another way to think about plausible values of $rho$. If treatment effects very similar from unit to unit, $rho$ will be close to 1. In the limiting case of exactly constant effects (the difference between the treated and untreated potential outcome is exactly the same for every unit), $rho$ is equal to 1. It is difficult (but not impossible) to imagine settings in which $rho$ is negative. If the potential outcomes are negatively correlated, then units with higher treated potential outcomes have lower untreated potential outcomes (large, positive effects) and units with lower treated potential outcomes have higher untreated potential outcomes (large, negative effects). 

Developing intuitions about $rho$ is frustrated by the fundamental problem of causal inference: since we can only ever observe a unit in its treated or untreated state (but not both), we can't directly observe the correlation in potential outcomes. In order to make a guess about $rho$, we need to reason about treatment effect heterogeneity. When effects are close to homogeneous, $rho$ will be positive. Some patterns of treatment effect heterogeneity will cause $rho$ to be negative, but not all. An example of this might be a "surprising" partisan cue. Imagine that in the control condition, Democratic subjects tend to support a policy ($Y_i(0)$ is high) and Republicans tend to oppose it ($Y_i(0)$ is low). The treatment is a "surprise" endorsement of the by a Republican elite: treatment group Republicans will find themselves supporting the policy ($Y_i(1)$ is high) whereas treatment group Democrats will infer from the Republican endorsement that the policy must not be a good one ($Y_i(1)$ is low.) Treatments with extreme heterogeneity like this example could in principle cause negatively correlated potential outcomes.

Because the model specifies a fixed sample, the inquiries are also be defined at the sample level. The most common inquiry for a two-arm trial is the sample average treatment effect, or SATE. It is equal to the average difference between the treated and untreated potential outcomes for the units in the sample: $\E_{i\in N}[Y_i(1) - Y_i(0)]$. Two-arm trials can also support other inquiries like the SATE among a subgroup (called a conditional average treatment effect, or CATE), but we'll leave those inquiries to the side for the moment. 

The data strategy uses complete random assignment in which exactly $m$ of $N$ units are assigned to treatment ($Z = 1$) and the remainder are assigned to control ($Z = 0$). We measure observed outcomes in such a way that we measure the treated potential outcome in the treatment group and untreated potential outcomes in the control group: $Y = Y_i(1) * Z + Y_i(0)*(1 - Z)$. This expression is sometimes called the "switching equation" because of the way it "switches" which potential outcome is revealed by the treatment assignment. It also embeds a crucial assumption -- that indeed units reveal the potential outcomes they are assigned to. If the experiment encounters noncompliance, this assumption is violated. It's also violated if we violate "excludability," i.e., if something other than treatment moves with assignment to treatment. For example, if the treatment group is measured differently from the control group, excludability would be violated.

The answer strategy is the difference-in-means estimator with Neyman standard errors:

\begin{align}
\widehat{DIM} &= \frac{\sum_1^mY_i}{m} - \frac{\sum_{m + 1}^NY_i}{N-m} \\
\widehat{se(DIM)} &= \sqrt{\frac{\widehat{Var}(Y_i|Z = 1)}{m} - \frac{\widehat{Var}(Y_i|Z = 0)}{N-m}}\\
\end{align}

The estimated standard error can be used as an input for two other statistical procedures: null hypothesis significance testing via a $t$-test and the construction of a 95% confidence interval.

The DAG corresponding to a two-arm randomized trial is very simple. An outcome $Y$ is affected by unknown factors $U$ and a treatment $Z$. The measurement procedure $Q$ affects $Y$ in the sense that it measures a latent $Y$ and records it in a dataset. No arrows lead into $Z$ because it is randomly assigned. No arrow leads from $Z$ to $Q$, because we assume no excludability violations wherein the treatment changes how units are measured. This simple DAG confirms that the average causal effect of Z on Y is nonparametrically identified because no back-door paths lead from Z to Y.

```{r, echo=FALSE, fig.height = 3.5, fig.width = 7}
dag <- dagify(Y ~ Z + Q + U)

nodes <-
  tibble(
    name = c("Y", "Z", "U", "Q"),
    label = c("Y", "Z", "U", "Q"),
    annotation = c(
      "**Outcome**",
      "**Random assignment**",
      "**Unknown heterogeneity**",
      "**Measurement procedure**"),
    x = c(5, 1, 5, 1),
    y = c(2.5, 2.5, 4, 4),
    nudge_direction = c("S", "S", "N", "N"),
    data_strategy = c("unmanipulated", "assignment","unmanipulated", "measurement"),
    answer_strategy = "uncontrolled"
  )

ggdd_df <- make_dag_df(dag, nodes)

base_dag_plot %+% ggdd_df + coord_fixed(ylim = c(2, 4.5), xlim = c(0.5, 5.5))
```


### Analytic design diagnosis

The statistical theory for the canonical two-arm design is very well explored, so analytic expressions for many diagnosands are available. 

1. Bias of the difference-in-means estimator. Equation 2.14 in @Gerber2012 demonstrates that regardless of the values (except in degenerate cases) of $m$, $N$, or $rho$, the bias diagnosand is equal to zero. This is the "unbiasedness" property of many randomized experimental designs. On average, the difference-in-means estimates from the canonical design will equal the average treatment effect. As we'll explore later in this chapter, not every experimental design yields unbiased estimates. Some (like blocked experiments with differential probabilities of assignments) require fix-ups in the answer strategy and others (like clustered experiments with unequal cluster sizes) require fix-ups in the data strategy.

2. The true standard error of the difference-in-means estimator. Equation 3.4 in @Gerber2012 provides an exact expression for the true standard error of the canonical two-arm trial.

$$
SE(DIM)= \sqrt{\frac{1}{n-1}\left\{\frac{m\V(Y_i(0))}{n-m} + \frac{(N-m)\V(Y_i(1))}{m} + 2Cov(Y_i(0), Y_i(1))\right\}}
$$

This equation contains many design lessons. It shows how the standard error decreases as sample size ($N$) increases and as the variances of the potential outcomes decrease. It provides a justification for "balanced" designs that assign the same proportion of subjects to treatment and control: if the variances of $Y_i(0)$ and $Y_i(1)$ are equal, then a balanced split of subjects across conditions will yield the lowest standard error. If the variances of the potential outcomes are not equal, the expression suggests allocating more units to the condition with the higher variance. 

3. Bias of the standard error estimator. Equation 3.4 is the *true* standard error. We also learn from analytic design diagnosis that the standard error *estimator* is upwardly biased, which is to say that it is conservative. The intuition for this bias is that we can't directly estimate the covariance term in Equation 3.4, so we bound the variance under a worst-case assumption.^[See @aronow_green_lee_2014 for an alternative bounding procedure.] The amount of bias in the standard error estimator depends on how wrong this worst case assumption is. When $rho$ is equal to 1, the bias goes to zero.

4. Coverage. Since the standard errors are upwardly biased -- they are "too big" -- the statistics that are built on them will inherit this bias as well. The 95\% confidence intervals will also be "too big," so the coverage diagnosand will be above nominal, that is, 95% confidence intervals will cover the true parameter more frequently than 95% of the time.

5. Power. Our answer strategy involves conducting a statistical significance test against the null hypothesis that the average outcome in the control group is equal to the average outcome in the treatment group. This test is also built on the estimated standard error, so the upward bias in the standard error estimator will put downward pressure on statistical power. In section \@ref(p2analyticdiagnosis), we reproduced the formula given in @Gerber2012 for statistical power that makes two further restrictions on the canonical design: equally-sized treatment groups and equal variances in the potential outcomes.

Analytic design diagnosis is tremendously useful, for two reasons. First, we obtain guarantees for a large class of designs. *Any* experiment that fits into the canonical design will have these properties. Second, we learn from the analytic design diagnosis what the important design parameters are. In our model, we need to think about treatment effect heterogeneity in order to develop expectations about the variances and covariances of the potential outcomes. In our inquiry, we need to be thinking about specific average causal effects -- the SATE, not the PATE or the CATE or the LATE. The data strategy in the canonical design is complete random assignment, so we need to think about how many units to assign to treatment ($m$) relative to control ($N-m$). The answer strategy is difference-in-means with neyman standard errors -- difference in means is unbiased for the ATE, but the Neyman standard error estimator is upwardly biased. This means our coverage will be conservative and we'll take a small hit to statistical power. We can learn all this from theory, not from simulation.

### Design diagnosis through simulation

But of course we can declare this design and conduct design diagnosis using simulation. This process will confirm the analytic results, as well as provide estimates of diagnosands for which statisticians have not yet derived analytic expressions. This code produces a "designer" that allows us to easily vary the important components of the design (which of course we only learned about by studying the statistical theory!).

```{r}
eq_3.4_designer <-
  function(N, m, var_Y0, var_Y1, cov_Y0_Y1, mean_Y0, mean_Y1) {
    
    fixed_sample <-
      MASS::mvrnorm(
        n = N,
        mu = c(mean_Y0, mean_Y1),
        Sigma = matrix(c(var_Y0, cov_Y0_Y1, cov_Y0_Y1, var_Y1), nrow = 2),
        empirical = TRUE # this line makes the means and variances "exact" in the sample data
      ) %>%
      magrittr::set_colnames(c("Y_Z_0", "Y_Z_1"))
    
    declare_model(data = fixed_sample) +
      declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +
      declare_assignment(m = m) +
      declare_reveal(Y, Z) +
      declare_estimator(Y ~ Z, inquiry = "ATE")
    
  }
```

This simulation investigates how much of the sample we should allocate to the treatment group if the treatment group variance is twice as large as the control group variance. The diagnosis confirms the bias is zero, the true standard errors are what Equation 3.4 predicts, coverage is above nominal, and that we are above the 80% power target for a middle range of $m$. We learn that power is maximized (and the true standard error is minimized) when we allocate 60 or 70 units (of 100 total) to treatment. We also learn from this that the gains from choosing the unbalanced design (relative to a 50/50 allocation) are very small. Even when the variance in the treatment group is twice as large as the variance in the control group, we don't lose much when sticking with the balanced design. Since we can never be sure of the relative variances of the treatment and control groups ex ante, this exercise provides further support for choosing balanced designs in many design settings.

```{r, eval=FALSE}
designs <- 
  expand_design(designer = eq_3.4_designer,
                N = 100,
                m = seq(10, 90, 10),
                var_Y0 = 1,
                var_Y1 = 2,
                cov_Y0_Y1 = 0.5,
                mean_Y0 = 1.0,
                mean_Y1 = 1.75)

dx <- diagnose_designs(designs, sims = 100, bootstrap_sims = FALSE)

```

```{r, eval = do_diagnosis & !exists("do_bookdown")}
designs <- 
  expand_design(designer = eq_3.4_designer,
                N = 100,
                m = seq(10, 90, 10),
                var_Y0 = 1,
                var_Y1 = 2,
                cov_Y0_Y1 = 1,
                mean_Y0 = 1.0,
                mean_Y1 = 1.75)

dx <- diagnose_designs(designs, sims = 200, bootstrap_sims = FALSE)
```


```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("two_arm_trials"), "/dx.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(dx, file = rds_file_path)
}
dx <- read_rds(rds_file_path)
```


```{r, echo=FALSE}
gg_df <-
  dx %>%
  get_diagnosands() %>%
  pivot_longer(c(power, sd_estimate, coverage, bias))


dx %>%
  get_diagnosands() %>%
  filter(sd_estimate == min(sd_estimate))

eq_3.4 <- function(N, m, var_Y0, var_Y1, cov_Y0_Y1) {
  n <- N - m
  sqrt(1 / (N - 1) * (m / n  * var_Y0 + n / m * var_Y1 + 2 * cov_Y0_Y1))
}

theory_df <- 
  tibble(
    m = seq(10, 90, 1),
    sd_estimate = eq_3.4(N = 100, m = m, var_Y0 = 1, var_Y1 = 2, cov_Y0_Y1 = 0.5),
    bias = 0,
    power = 0.80,
    coverage = 0.95
  ) %>%
  pivot_longer(c(power, sd_estimate, coverage, bias))

g <- 
  ggplot(gg_df, aes(m, value)) +
  geom_point() +
  geom_line() +
  geom_line(data = theory_df, color = "purple") +
  theme_minimal() +
  facet_wrap(~name) +
  labs(y = "Diagnosand value", x = "Number of treated units (m)", title = "Simulating a two arm trial", subtitle = c("N = 100, var_Y0 = 1, var_Y1 = 2, cov_Y0_Y1 = 0.5, mean_Y0 = 1.0, mean_Y1 = 1.75"), caption = "Theoretical values and design targets in purple.")
g
```


### Increasing precision through blocking

Block random assignment almost always represents a precision improvement over complete random assignment. With block random assignment, we conduct "mini-experiments" within separate subsets of the sample (called blocks), defined by pre-treatment covariates. Within each block, we conduct complete random assignment.^[One mistake sometimes made by new experimenters is to conduct *simple* random assignment within each block -- none of the gains from blocking described here apply if simple random assignment is conducted in each block, because that procedure produces the identical randomization distribution as a simple random assignment procedure without any blocking, provided that the probability of assignment is the same in each block.] 

One way of thinking about why blocking reduces sampling variability is that it rules out (by design) random assignments in which too many or too few units is a particular subgroup are assigned to treatment: exactly $m_B$ units will be treated in each block $B$. When potential outcomes are correlated the blocking variable, those "extreme" assignments produce estimates that are in the tails of the sampling distribution associated with complete random assignment.


```{r, eval = do_diagnosis & !exists("do_bookdown")}

fixed_pop <-
  fabricate(
    N = 12,
    X = complete_ra(N),
    U = rnorm(N, sd = 0.25)
  )

design <-
  declare_model(data = fixed_pop) +
  declare_potential_outcomes(Y ~ 0.2*Z + X + U) +
  declare_assignment(Z = complete_ra(N = N), handler = fabricate) + 
  declare_reveal() +
  declare_estimator(Y ~ Z, label = "DIM") +
  declare_estimator(X ~ Z, label = "balance") +
  declare_estimator(Y ~ Z + X, model = lm_robust, label = "OLS")

simulations <- simulate_design(design)
```


```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("two_arm_trials"), "/blocking_simulations.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(simulations, file = rds_file_path)
}
simulations <- read_rds(rds_file_path)
```

This inuition behind blocking is illustrated in Figure \@ref(fig:completecovariate), which shows the sampling distribution of the difference-in-means estimator under *complete* random assignment. The histogram is shaded according to whether the random assignment happened to perfectly balance a pre-treatment covariate $X$. The sampling distribution of the estimator among the set of assignments that happen to be balanced is more tightly distributed around the true average treatment effect than the estimates associated with assignments that are not perfectly balanced. Here we can see the value of a blocking procedure -- it *it rules out by design* those assignments that are not perfectly balanced. 

```{r completecovariate, echo=FALSE, fig.cap='Sampling distribution under complete random assignment, by covariate balance', fig.height = 3, fig.width = 7, echo = FALSE}

gg_df <- 
simulations %>%
  select(sim_ID, estimator_label, estimate) %>%
  pivot_wider(names_from = estimator_label, values_from = estimate) %>%
  mutate(balanced = balance == 0 )

ggplot(gg_df, aes(DIM, fill = balanced)) +geom_histogram(position = "identity", bins = 30, alpha = 0.8) +
  scale_fill_manual(values = c(dd_light_gray, dd_dark_blue)) +
  dd_theme() +
  geom_vline(xintercept = 0.2, color = dd_dark_blue, linetype = "dashed") +
  theme(legend.position = "none",
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank()) +
  annotate("text", x = 0.30, y = 50, label = "Assignments that\nexactly balance X", color = dd_dark_blue,hjust = 0) +
  annotate("text", x = -0.40, y = 20, label = "Assignments that do not\nexactly balance X", color = dd_light_gray,hjust = 1) +
  xlim(-1.5, 1.5) +
  xlab("Difference-in-means estimate")
```
 
### Increasing precision through covariate adjustment

Choosing block random assignment over complete random assignment is a method for incorporating covariate information into the data strategy for the purpose of decreasing sampling variability. We can also incorporate covariate information into the answer strategy for the same purpose, by controlling for covariates or otherwise conditioning on them when estimating the average treatment effect. In observational settings like the one we explore in [selection on observeables], conditioning on covariates is used to block back-door paths to address confounding. Here, confounding is no problem -- the treatment is assigned at random by design, so we do not need to control for covariates in order to decrease bias. Instead, we control for covariates in order to reduce sampling variability.

Figure \@ref(fig:completecovariateadjustment) illustrates this point. The sampling distribution under difference in means is shown on the top line and the sampling distribution under ordinary least squares (Y ~ Z + X) is shown on the bottom line.  Estimates that are on the extreme ends of the distribution under difference-in-means are pulled in more tightly to center on the ATE. One interesting wrinkle that this graph reveals is that covariate adjustment does not tighen up the estimates for assignments that exactly balance X -- they only help the assignments that are slightly imbalanced.

```{r completecovariateadjustment, echo=FALSE, fig.cap='Impact of covariate adjustment on the sampling distribution', fig.height = 3, fig.width = 7, echo = FALSE}
gg_df2 <-
  gg_df %>%
  pivot_longer(cols = c(DIM, OLS),
               names_to = "estimator",
               values_to = "estimate") %>%
  mutate(estimator = factor(estimator, levels = c("OLS", "DIM")))



ggplot(gg_df2, aes(estimate, estimator, color = balanced, group = sim_ID)) +
  geom_line(data = (. %>% filter(balanced)), alpha = 0.9) +
  geom_line(data = (. %>% filter(!balanced)), alpha = 0.3) +
  scale_color_manual(values = c(dd_light_gray, dd_dark_blue)) +
  geom_vline(xintercept = 0.2, color = dd_dark_blue, linetype = "dashed") +
  annotate(
    "text",
    x = 0.20,
    y = 2.25,
    label = "Assignments that\nexactly balance X",
    color = dd_dark_blue
  ) +
  annotate(
    "text",
    x = -0.40,
    y = 1.25,
    label = "Assignments that do not\nexactly balance X",
    color = dd_light_gray,
    hjust = 1
  ) +
  xlim(-1.5, 1.5) +
  xlab("Average treatment effect estimate") +
  theme(legend.position = "none")

```



### Simulation comparing blocking to covariate adjustment

Adjusting for pre-treatment covariates that are predictive of the outcome almost always increases precision; blocking on covariates that are predictive of the outcome almost always increases precision too. Another way of putting this is that covariate information can be incorporated in answer strategy (covariate adjustment) or in the data strategy (blocking) and that in this way, the two procedures are approximately equivalent.

We'll now declare and diagnose four closely-related experimental designs. To begin, we describe a fixed population of 100 units with a binary covariate $X$ and unobserved heterogeneity $U$. Potential outcomes are a function of the treatment $Z$ and are correlated with $X$. Throughout this exercise, our inquiry is the ATE.

```{r}
fixed_pop <-
  fabricate(
    N = 100,
    X = rbinom(N, 1, 0.5),
    U = rnorm(N)
  )

design <-
  declare_model(data = fixed_pop) +
  declare_potential_outcomes(Y ~ 0.2*Z + X + U) +
  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0))
```

We have two answer strategies and two data strategies that we'll mix-and-match.

```{r}
# Data strategies
complete_assignment <- 
  declare_assignment(Z = complete_ra(N = N), handler = fabricate) + 
  declare_reveal()
blocked_assignment <- 
  declare_assignment(Z = block_ra(blocks = X), handler = fabricate) + 
  declare_reveal()

# Answer strategies
unadjusted_estimator <- declare_estimator(Y ~ Z, inquiry = "ATE")
adjusted_estimator <- declare_estimator(Y ~ Z + X, model = lm_robust, inquiry = "ATE")
```

These combine to create four designs, which we then diagnose.

```{r}
design_1 <- design + complete_assignment + unadjusted_estimator
design_2 <- design + blocked_assignment + unadjusted_estimator
design_3 <- design + complete_assignment + adjusted_estimator
design_4 <- design + blocked_assignment + adjusted_estimator
```

```{r, eval=FALSE}
diagnose_designs(list(design_1, design_2, design_3, design_4))
```


```{r, eval = do_diagnosis & !exists("do_bookdown")}
simulations <- simulate_designs(list(design_1, design_2, design_3, design_4))
```


```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("two_arm_trials"), "/covariate_simulations.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(simulations, file = rds_file_path)
}
simulations <- read_rds(rds_file_path)
```

```{r, echo=FALSE}
gg_df <-
  simulations %>%
  group_by(design_label) %>%
  summarise(sd_estimate = sd(estimate), avg_std_error = mean(std.error), .groups = "drop") %>%
  transmute(`Data Strategy` = if_else(design_label %in% c("design_1", "design_3"), 
                                   "Complete Random Assignment",
                                   "Block Random Assignment"),
         `Answer Strategy` = if_else(design_label %in% c("design_1", "design_2"), 
                                   "Difference-in-means",
                                   "Covariate Adjustment"),
         `True Standard Error` = sd_estimate,
         `Average Estimated Standard Error` = avg_std_error)
kable(gg_df, digits = 3)
```

The diagnosis shows that incorporating covariate information either in the data strategy or in the answer strategy yields similar gains to the true standard error. Relative to the canonical design (complete random assignment with difference-in-means), any of the alternatives represents an improvement. Blocking on $X$ in the data strategy decreases sampling variability. Controlling for $X$ in the answer strategy decreases sampling variability. Doing both -- blocking on $X$ and controlling for $X$ -- does not yield *additional* gains, but controlling for $X$ is nevertheless appropriate when using a blocked design. The reason for this can be seen in the "average estimated standard error" diagnosand. If we block, but still use the difference-in-means estimator, the *estimated* standard errors do not decrease relative to complete random assignment. The usual Neyman variance estimator doesn't "know" about the blocking. A number of fixes to this problem are available. You can, as we do in the simulation, control for the blocking variable in an OLS regression. Alternatively, you can use the "stratified" estimator that obtains block-level ATE estimates, then averages them together, weighting by block size. The stratified estimator has an associated standard error estimator -- see gerber and green page 73-74. The stratified estimator is an instance of the "analyze as you randomize" principle. Respecting the data strategy in the answer strategy (by adjusting for the blocking) brings down the estimated standard error as well.



### Randomization checks

- Confirm the number of units in each condition
- Check balance. Covariate by covariate. Omnibus test.
- sub sub sub bullet point: if you're really worried about it, you can do rerandomization, but it's really a waste of time, just block.

### What can go wrong

Even for the simplest two-arm trial design, many things can go wrong. Naturally, the sorts of problems can be described in terms of M, I, D, and A. 

The most crucial assumption we made in the model is that that are exactly two potential outcomes for each unit. This is violated if there are "spillovers" between units, say between housemates. If a unit's outcome depends on the treatment status of their housemate, we could imagine four potential outcomes for each unit: only unit $i$ is treated, only unit $i$'s housemate is treated, both are treated, or neither is treated. If there are indeed spillovers, but they are ignored, the very definition of the inquiry is malformed. If units don't even have clearly defined "treated" and "untreated" potential outcomes because of spillovers from others, then we can't define the ATE in the usual way. The solution to this problem is to *elaborate* the model to account for all the potential outcomes, then to redefine the inquiry with respect to *those* potential outcomes.

Other ways for a two arm-trial to go wrong concern the data strategy. If you *think* you are using complete random assignment but you in fact are not, bias may creep in. A nonrandom assignment procedure might be something like "first-come, first-served." If you assign the "first" $m$ units to treatment and the remainder to control, the assignment procedure is not randomized. Bias will occur if the potential outcomes of the first $m$ are unlike the potential outcomes of the remaining $N-m$ units. 

Sometimes researchers do successfully conduct random assignment, but the random assignment happened to produce treatment and control groups that are unlike each other in observable ways. The unbiasedness property applies to the whole procedure -- over many hypothetical iterations of the experiment, the average estimate will be equal to the value of the inquiry. But any *particular* estimate can be close or far from the true value. A solution to this problem is to change the answer strategy to adjust estimates for covariates, though we would recommend adjusting for covariates regardless of whether the treatment and control groups appear imbalanced (see the following chapters).

Other data strategy problems include noncompliance and attrition. Noncompliance occurs when units' treatment status differs from their treatment assignment. This is a big enough problem that we devote an entire library entry to it (see "encouragement designs".). Attrition occurs when we fail to measure outcomes for some units.



### Example

ON THE HUNT FOR

- a two arm trial with complete random assignment. No blocking, no clustering, no multiple treatment arms.

[Lauren's study?]
