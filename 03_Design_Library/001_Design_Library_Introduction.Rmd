# Design Library

This section of the book enumerates a series of common social science research designs. Each entry will include description of the design in terms of MIDA and also a declaration of the design in code. We'll often diagnose designs over the range of values of some design parameters in order to point out especially interesting or unusual features of the design.

Our goal in this section is not to provide a comprehensive accounting of all empirical research designs. It's also not to describe any of the particular designs in exhaustive detail, because we are quite sure that in order for these designs to be useful for any practical purpose, they will need to be modified. The entries in the design library are not recipes that, if you follow the instructions, out will come high-quality research. 

Instead, we hope that the entries provide inspiration for how to tailor a particular class of designs -- the blocked-and-clustered randomized trial or the catch-and-release design -- to your own research setting. The basic structure of the design library entry will be useful, but the specifics about plausible ranges of outcomes, sample size constraints, etc, will be different in each particular setting.

We've split up designs by Inquiry and by Data strategy. Inquires can be descriptive or causal and Data strategies can be observational or experimental. This leads to four categories of research: Observational descriptive, Experimental descriptive, Observational Causal, and Experimental causal. A third dimension along which studies can vary is whether the Answer strategy is qualitative or quantitative. If we include this dimension in our typology, we'd end up with eight broad categories of research design. We don't see the qualitative-quantitative difference in answer strategy to be as fundamental as the differences in inquiry and data strategy, so we'll just include both qualitative and quantitative designs in each of our four categories. Besides, social scientists always appreciate a good two-by-two:

\begin{table}
    \begin{tabular}{lll}
    ~                    & Data Strategy: Observational & Data Strategy: Experimental \\
    Inquiry: Descriptive & Sample survey or Case study  & List experiment or Participant Observation  \\
    Inquiry: Causal      & Regression Discontinuity Design or Process Tracing    & RCT \\
    \end{tabular}
\end{table}

In the broadest terms, descriptive inquiries can be described as $f(\mathbf{Y(Z = Realized)})$, where $f()$ is some function and $\mathbf{Y(Z = Realized)}$ is a vector of realized outcomes. That is, descriptive designs seek to summarize (using $f()$) the world as it is (as represented by $\mathbf{Y(Z = Realized)}$). Descriptive designs can be better or worse at answering that inquiry. The quality of descriptive research designs depends on the extent of measurement, sampling, and estimation error.

Causal inquiries can be described as $f(\mathbf{Y(Z)})$, where $Z$ is not a realized vector of treatments, but is instead is a vector that could take on counterfactual values. A standard causal inquiry is the Average Treatment Effect, in which $f()$ is the function that takes the average of the difference between two potential outcome vectors, $Y(Z = 1)$ and $Y(Z = 0)$.  But there are many causal inquiries beyond the ATE -- the thing they all have in common is that they are functions not of realized outcomes, but of potential outcomes.  The quality of causal research designs depends on everything that a descriptive design depends on, but also on the understanding and quality of the mechanism that assigns units to treatment conditions. 

All research designs suffer from some kind of missing data problem. Rubin pointed out missing data in surveys come from people you didn't survey or people who refused to answer.  In causal inference problems, the data that are missing are the potential outcomes that were not revealed by the world.  In Descriptive studies, the data that are missing are the true values of the things to be measured.  Measurement error is a missing data problem too!

Observational research designs are typified by researchers having no impact on the units under study. They simply record the outcomes that happened in the world and would have happened even if the study did not occur. Experimental research designs are more active -- they cause some potential outcomes to be revealed but not others.  In this way, researchers have an impact on the units they study. For this reason, experimental studies tend to raise more ethical questions than do observational studies. Experimenters literally change what potential outcomes become realized outcomes.

Sometimes the lines between types of research become blurry. The Hawthorne effect is the name given to the idea that measuring a thing changes it. If there are Hawthorne effects, than observational research designs also change which potential outcomes are revealed. That is, if there is a difference between Y(Z = measured) and Y(Z = unmeasured), then the act of observation changes that which is observed. Passive data collection methods are sometimes preferred on these grounds.





