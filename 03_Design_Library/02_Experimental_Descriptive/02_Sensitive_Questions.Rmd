---
title: "Experiments for sensitive questions"
output: html_document
# bibliography: ../bib/book.bib 
bibliography: ../../bib/book.bib # use this line comment the above
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

## Experiments for sensitive questions

- setup: a descriptive estimand, the proportion holding sensitive characteristic; two experimental designs to recover it, list experiments and randomized response
- if identification assumptions are violated (focus on ceiling/floor), estimates of ATE still unbiased but not for the descriptive estimand
- compare design where the ceiling/floor categories are minimized through Glynn (2013) design advice to use negatively-correlated items and a high prevalence and a low prevalence item
- both designs exhibit bias-variance tradeoff (more control of variance with RR)

### List experiments

```{r sensitive_questions, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 500
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

Sometimes, subjects might not tell the truth when \textit{directly} asked about certain attitudes or behaviors. Responses may be affected by sensitivity bias, or the tendancy of survey subjects to dissemble for fear of negative repercussions if some reference group learns their true response [@blair2018list]. In such cases, standard survey estimates based on direct questions will be biased. One class of solutions to this problem is to obscure individual responses, providing protection from social or legal pressures. When we obscure responses systematically through an experiment, we can often still identify average quantities of interest. One such design is the list experiment (introduced by @miller1984new), which asks respondents for the count of the number of `yes' responses to a series of questions including the sensitive item, rather than for a yes or no answer on the sensitive item itself. List experiments give subjects cover by aggregating their answer to the sensitive item with responses to other questions.

During the 2016 Presidential Election in the U.S., some observers were concerned that pre-election estimates of support for Donald Trump might have been downwardly biased by "Shy Trump Supporters" -- survey respondents who supported Trump in their hearts, but were embarrased to admit it to pollsters. To assess this possibility, @coppock2017did obtained estimates of Trump support that were free of social desirability bias using a list experiment. Subjects in the control and treatment groups were asked: "Here is a list of [three/four] things that some people would do and some people would not. Please tell me HOW MANY of them you would do. We do not want to know which ones of these you would do, just how many. Here are the [three/four] things:"

| Control                                                                                           | Treatment                                                                                                                                                                |
| ------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| If it were up for a vote, I would vote to raise the minimum wage to 15 dollars an hour            | If it were up for a vote, I would vote to raise the minimum wage to 15 dollars an hour                                                                                   |
| If it were up for a vote, I would vote to repeal the Affordable Care Act, also known as Obamacare | If it were up for a vote, I would vote to repeal the Affordable Care Act, also known as Obamacare                                                                        |
| If it were up for a vote, I would vote to ban assault weapons                                     | If it were up for a vote, I would vote to ban assault weapons                                                                                                            |
|                                                                                                   | If the 2016 presidential election were being held today and the candidates were Hillary Clinton (Democrat) and Donald Trump (Republican), I would vote for Donald Trump. |

The treatment group averaged 1.843 items while the control group averaged 1.548 items, for a difference-in-means estimate of support for Donald Trump of 29.6\% (note that this estimate is representative of US adults and not of US adults who would actually vote). The trouble with this estimate is that, while it's plausibly free from social desirability bias, it's also much higher variance. The 95\% confidence interval for the list experiment estimate is nearly 14 percentage points wide, whereas the the 95\% confidence interval for the (possibly biased!) direct question asked of the same sample is closer to 4 percentage points.

The choice between list experiments and direct question is therefore a **bias-variance tradeoff**. List experiments may have less bias, but they are higher variance. Direct questions may be biased, but they have less variance.

#### Declaration

- **M**odel: Our model includes subjects' true support for Donald Trump and whether or not they are "shy".  These two variables combine to determine how subjects will respond when asked directly about Trump support.

    The potential outcomes model combines three types of information to determine how subjects will respond to the list experiment: their responses to the three nonsensitive control items, their true support for Trump, and whether they are assigned to see the treatment or the control list. Notice that our definition of the potential outcomes embeds the "No Liars" and "No Design Effects" assumptions required for the list experiment design (see Blair and Imai 2012 for more on these assumptions).

    We also have a global parameter that reflects our expectations about the proportion of Trump supporters who are shy. It's set at 6%, which is large enough to make a difference for polling, but not so large as to be implausible.

- **I**nquiry: Our estimand is the proportion of voters who actually plan to vote for Trump. 

- **D**ata strategy: First we sample 500 respondents from the U.S. population at random, then we randomly assign 250 of the 500 to treatment and the remainder to control. In the survey, we ask subjects both the direct question and the list experiment question.

- **A**nswer strategy: We estimate the proportion of truthful Trump voters in two ways. First, we take the mean of answers to the direct question. Second, we take the difference in means in the responses to the list experiment question.


```{r}
# Model -------------------------------------------------------------------
proportion_shy <- .06

list_design <-
  
  # Model
  declare_population(
    N = 5000,
    # true trump vote (unobservable)
    truthful_trump_vote = draw_binary(.45, N),
    
    # shy voter (unobservable)
    shy = draw_binary(proportion_shy, N),
    
    # direct question response (1 if Trump supporter and not shy, 0 otherwise)
    Y_direct = if_else(truthful_trump_vote == 1 & shy == 0, 1, 0),
    
    # nonsensitive list experiment items
    raise_minimum_wage = draw_binary(.8, N),
    repeal_obamacare = draw_binary(.6, N),
    ban_assault_weapons = draw_binary(.5, N)
  ) +
  
  declare_potential_outcomes(
    Y_list_Z_0 = raise_minimum_wage + repeal_obamacare + ban_assault_weapons,
    Y_list_Z_1 = Y_list_Z_0 + truthful_trump_vote
  ) +
  
  # Inquiry
  declare_estimand(proportion_truthful_trump_vote = mean(truthful_trump_vote),
                   ATE = mean(Y_list_Z_1 - Y_list_Z_0)) +
  
  # Data Strategy
  declare_sampling(n = 500) +
  declare_assignment(prob = .5) +
  declare_reveal(Y_list) +
  
  # Answer Strategy
  declare_estimator(
    Y_direct ~ 1, model = lm_robust, term = "(Intercept)", estimand = "proportion_truthful_trump_vote", label = "direct") +
  declare_estimator(
    Y_list ~ Z, model = difference_in_means, estimand = c("proportion_truthful_trump_vote", "ATE"), label = "list")

```


```{r, eval = do_diagnosis & !exists("do_bookdown")}
simulations_list <- simulate_design(list_design, sims = sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("sensitive_questions"), "/simulations_list.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(simulations_list, path = rds_file_path)
}
simulations_list <- read_rds(rds_file_path)
```

The plot shows the sampling distribution of the direct and list experimetn estimators. The sampling distribution of the direct question is tight but biased; the list experiment (if the requisite assumptions hold) is unbiased, but higher variance. The choice between these two estimators of the prevalence rate depends on which -- bias or variance -- is more important in a particular setting. See @blair2018list for an extended discussion of how the choice of research design depends deeply on the purpose of the project. 

```{r, echo=FALSE}
summary_df <- 
  simulations_list %>%
  filter(estimand_label == "proportion_truthful_trump_vote") %>% 
  gather(key, value, estimand, estimate) %>%
  group_by(estimator_label, key) %>%
  summarize(average_value = mean(value))

simulations_list %>%
  ggplot(aes(estimate)) +
  geom_histogram(bins = 30) +
  geom_vline(data = summary_df, aes(xintercept = average_value, color = key, linetype = key)) +
  facet_wrap(~estimator_label)
```

#### Violations of identifying assumptions

```{r}
list_design_ceiling <- replace_step(
  list_design, step = 2, 
  new_step = declare_potential_outcomes(
    Y_list_Z_0 = raise_minimum_wage + repeal_obamacare + ban_assault_weapons,
    Y_list_Z_1_no_liars = Y_list_Z_0 + truthful_trump_vote,
    Y_list_Z_1 = ifelse(Y_list_Z_1_no_liars == 4, 3, Y_list_Z_1_no_liars)
  )
)
```

```{r, eval = do_diagnosis}
diagnosis_list_ceiling <- diagnose_design(list_design_ceiling, sims = sims, bootstrap_sims = b_sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("sensitive_questions"), "/list_ceiling_diagnosis.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnosis_list_ceiling, path = rds_file_path)
}
diagnosis_list_ceiling <- read_rds(rds_file_path)
```

```{r}
kable(diagnosis_list_ceiling %>% get_diagnosands %>% select(estimator_label, estimand_label, bias, rmse))
```

See Blair and Imai (2012) and Li (2019) for methods for addressing violations no liars assumption through modeling and bounds.

Li, Y. (n.d.). Relaxing the No Liars Assumption in List Experiment Analyses. Political Analysis, 1-16. doi:10.1017/pan.2019.7

#### Mitigating the risk of ceiling effects

Glynn (2013)

```{r}
list_design_glynn <- replace_step(
  list_design, step = 1, 
  new_step = declare_population(
    N = 5000,
    # true trump vote (unobservable)
    truthful_trump_vote = draw_binary(0.45, N),
    
    # shy voter (unobservable)
    shy = draw_binary(proportion_shy, N),
    
    # direct question response (1 if Trump supporter and not shy, 0 otherwise)
    Y_direct = if_else(truthful_trump_vote == 1 & shy == 0, 1, 0),
    
    list2_item1 = draw_binary(0.5, N),
    list2_item2 = correlate(given = list2_item1, rho = -.5, draw_binary, prob = 0.5), 
    list2_item3 = draw_binary(0.1, N)  # low prevalence
  )
)

list_design_glynn <- replace_step(
  list_design_glynn, step = 2, 
  declare_potential_outcomes(
    Y_list_Z_0 = list2_item1 + list2_item2 + list2_item3,
    Y_list_Z_1_no_liars = Y_list_Z_0 + truthful_trump_vote,
    Y_list_Z_1 = ifelse(Y_list_Z_1_no_liars == 4, 3, Y_list_Z_1_no_liars)
  )
)
```

```{r, eval = do_diagnosis}
diagnosis_list_glynn <- diagnose_design(list_design_glynn, sims = sims, bootstrap_sims = b_sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("sensitive_questions"), "/list_glynn_diagnosis.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnosis_list_glynn, path = rds_file_path)
}
diagnosis_list_glynn <- read_rds(rds_file_path)
```

```{r}
kable(diagnosis_list_glynn %>% get_diagnosands %>% filter(estimator_label == "list") %>% select(estimator_label, bias, rmse))
```

### Randomized response technique

```{r}
library(rr)

rr_forced_known <- function(data) {
  fit  <- try(rrreg(Y_forced_known ~ 1, data = data, p = 2/3, p0 = 1/6, p1 = 1/6, design = "forced-known"))
  pred <- try(as.data.frame(predict(fit, avg = TRUE, quasi.bayes = TRUE)))
  if(class(fit) != "try-error" & class(pred) != "try-error") {
    names(pred) <- c("estimate", "std.error", "conf.low", "conf.high")
    pred$p.value <- with(pred, 2 * pnorm(-abs(estimate / std.error)))
  } else {
    pred <- data.frame(estimate = NA, std.error = NA, conf.low = NA, conf.high = NA, p.value = NA, error = TRUE)
  }
  pred
}

rr_mirrored <- function(data) {
  fit  <- try(rrreg(Y_mirrored ~ 1, data = data, p = 2/3, design = "mirrored"))
  pred <- try(as.data.frame(predict(fit, avg = TRUE, quasi.bayes = TRUE)))
  if(class(fit) != "try-error" & class(pred) != "try-error") {
    names(pred) <- c("estimate", "std.error", "conf.low", "conf.high")
    pred$p.value <- with(pred, 2 * pnorm(-abs(estimate / std.error)))
  } else {
    pred <- data.frame(estimate = NA, std.error = NA, conf.low = NA, conf.high = NA, p.value = NA, error = TRUE)
  }
  pred
}

proportion_shy <- .06

rr_design <-
  declare_population(
    N = 100, 
    
    # true trump vote (unobservable)
    truthful_trump_vote = draw_binary(.45, N),
    
    # shy voter (unobservable)
    shy = draw_binary(proportion_shy, N),
    
    # Direct question response (1 if Trump supporter and not shy, 0 otherwise)
    Y_direct = as.numeric(truthful_trump_vote == 1 & shy == 0)) +
  
  declare_estimand(sensitive_item_proportion = mean(truthful_trump_vote)) +
  
  declare_potential_outcomes(Y_forced_known ~ (dice == 1) * 0 + (dice %in% 2:5) * truthful_trump_vote + (dice == 6) * 1, conditions = 1:6, assignment_variable = "dice") +
  declare_potential_outcomes(Y_mirrored ~ (coin == "heads") * truthful_trump_vote + (coin == "tails") * (1 - truthful_trump_vote), conditions = c("heads", "tails"), assignment_variable = "coin") +
  
  declare_assignment(prob_each = rep(1/6, 6), conditions = 1:6, assignment_variable = "dice") +
  declare_assignment(prob_each = c(2/3, 1/3), conditions = c("heads", "tails"), assignment_variable = "coin") +
  
  declare_reveal(Y_forced_known, dice) +
  declare_reveal(Y_mirrored, coin) +
  
  declare_estimator(handler = tidy_estimator(rr_forced_known), label = "forced_known", estimand = "sensitive_item_proportion") +
  declare_estimator(handler = tidy_estimator(rr_mirrored), label = "mirrored", estimand = "sensitive_item_proportion") +
  declare_estimator(Y_direct ~ 1, model = lm_robust, term = "(Intercept)", label = "direct", estimand = "sensitive_item_proportion")

rr_design <- set_diagnosands(rr_design, diagnosands = declare_diagnosands(select = c(mean_estimate, bias, rmse, power)))
```

```{r, eval = do_diagnosis}
rr_diagnosis <- diagnose_design(rr_design, sims = sims, bootstrap_sims = b_sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("sensitive_questions"), "/rr_diagnosis.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(rr_diagnosis, path = rds_file_path)
}
rr_diagnosis <- read_rds(rds_file_path)
```

```{r}
kable(reshape_diagnosis(rr_diagnosis))
```

#### Bias-variance tradeoff

```{r, eval = do_diagnosis}
rr_designs <- redesign(rr_design, proportion_shy = c(0, 0.1, 0.25, 0.5), N = c(500, 1000, 2000, 5000))
rr_tradeoff_diagnosis <- diagnose_design(rr_designs, sims = sims, bootstrap_sims = b_sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("sensitive_questions"), "/rr_tradeoff_diagnosis.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(rr_tradeoff_diagnosis, path = rds_file_path)
}
rr_tradeoff_diagnosis <- read_rds(rds_file_path)
```

```{r}
# make a plot
kable(reshape_diagnosis(rr_tradeoff_diagnosis))
```


### References