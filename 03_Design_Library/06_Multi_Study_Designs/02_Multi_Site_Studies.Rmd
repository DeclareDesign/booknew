---
title: "Multi-site studies"
output: html_document
bibliography: ../../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

## Multi-site studies

<!-- make sure to rename the section title below -->

```{r multi_site_studies, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
library(metafor)
library(car)
```

### why do this in multiple settings: what can you generalize to?

what is the population you are interested in?
can you sample from it? if not, what is the population from which you are effectively sampling from?

starting point: fixed budget and you're thinking about a single large study in one context or a set of five studies in five different contexts with the same intervention and outcome measures

```{r}
meta_re_estimator <- function(data){
  site_estimates_df <- data %>% 
    group_by(site) %>% 
    do(tidy(lm_robust(Y ~ Z, data = .))) %>% 
    filter(term == "Z") %>% 
    ungroup 
  
  meta_fit <- rma(estimate, std.error, data = site_estimates_df, method = "REML")
  
  with(meta_fit, tibble(
    estimate = as.vector(beta), std.error = se, p.value = pval, conf.low = ci.lb, conf.high = ci.ub))
}

post_strat_estimator <- function(data, pr_types_population) {
  if(length(unique(data$site)) > 1) {
    fit <- lm_robust(Y ~ Z*as.factor(subject_type) + as.factor(site), data = data)
    tidy(fit)
  } else {
    fit <- lm_robust(Y ~ Z*as.factor(subject_type), data = data)
  }
  
  alpha <- .05
  
  lh_fit <- try({ linearHypothesis(
    fit, 
    hypothesis.matrix = paste(paste(paste(pr_types_population[91:100][-1], "*", matchCoefs(fit, "Z"), sep = ""), collapse = " + "), " = 0"), 
    level = 1 - alpha) })
  
  if(!inherits(lh_fit, "try-error")) {
    tibble(estimate = drop(attr(lh_fit, "value")), 
           std.error = sqrt(diag(attr(lh_fit, "vcov"))),
           df = fit$df.residual, 
           statistic = estimate / std.error, 
           p.value = 2 * pt(abs(statistic), df, lower.tail = FALSE),
           conf.low = estimate + std.error * qt(alpha / 2, df),
           conf.high = estimate + std.error * qt(1 - alpha / 2, df))
  } else {
    tibble(error = TRUE)
  }
}
```

```{r}
# need to have biased sampling to get bias here
# two kinds of populations, one in which the study type determines the subject types and you select on study type
#   a second kind where study type determines study shock 
#   in second type if you adjust for subject type then you will be able to unbiased recover global

multi_site_designer <- function(
  N_sites = 10,
  n_study_sites = 5,
  n_subjects_per_site = 1000,
  feasible_effect = 0,
  subject_type_effects = seq(from = -0.1, to = 0.1, length.out = 10),
  pr_types = c( # rows are sites, columns are types
    0.005, 0.005, 0.09, 0.15, 0.25, 0.1, 0, 0.1, 0.15, 0.15,
    0.1, 0.15, 0.15, 0.15, 0.25, 0.005, 0, 0.1, 0.09, 0.005,
    0.15, 0.15, 0.15, 0.005, 0.005, 0, 0.25, 0.09, 0.1, 0.1,
    0, 0.15, 0.005, 0.09, 0.005, 0.15, 0.25, 0.1, 0.1, 0.15,
    0.005, 0.1, 0.09, 0.25, 0.15, 0.15, 0.005, 0, 0.1, 0.15,
    0.005, 0.15, 0.25, 0.1, 0, 0.1, 0.005, 0.15, 0.09, 0.15,
    0.15, 0.15, 0.005, 0.25, 0.1, 0.15, 0.09, 0.005, 0.1, 0,
    0.25, 0.1, 0.15, 0, 0.005, 0.15, 0.15, 0.1, 0.005, 0.09,
    0.005, 0.1, 0.1, 0.15, 0, 0.25, 0.15, 0.09, 0.005, 0.15,
    0.005, 0.09, 0.15, 0.1, 0, 0.1, 0.15, 0.005, 0.25, 0.15)
) {
  declare_population(
    site = add_level(N = N_sites, feasible_site = c(rep(1, 9), 0)),
    subject_types = add_level(
      N = 10,
      subject_type = 1:10,
      subject_type_effect = subject_type_effects,
      type_proportion = pr_types,
      N_subjects = ceiling(2500 * type_proportion)
    ),
    subjects = add_level(N = N_subjects, noise = rnorm(N))
  ) + 
    declare_potential_outcomes(Y ~ Z * (0.1 + subject_type_effect) + feasible_effect * feasible_site + noise) +
    declare_estimand(ATE_OOS = mean(Y_Z_1 - Y_Z_0), subset = site == "10") + # true effect for out-of-sample site
    declare_sampling(clusters = site, strata = feasible_site, n = c(0, n_study_sites)) + 
    declare_sampling(strata = site, n = n_subjects_per_site) + 
    declare_assignment(blocks = site, prob = 0.5) + 
    declare_estimand(study_site_ATE = mean(Y_Z_1 - Y_Z_0)) +
    declare_estimator(handler = tidy_estimator(post_strat_estimator), pr_types_population = pr_types, label = "post-strat")
}

single_site_large_design <- multi_site_designer(n_study_sites = 1, n_subjects_per_site = 2500)

small_study_five_sites <- multi_site_designer(n_study_sites = 5, n_subjects_per_site = 500)

# sims <- simulate_design(single_site_large_design, small_study_five_sites,
#                         sims = 1000)
# diag <- diagnose_design(sims %>% filter(!is.na(estimate) & !is.na(std.error) & !is.na(statistic) & !is.na(p.value) & !is.na(conf.low) & !is.na(conf.high)), bootstrap_sims = 1000)
```

## When there are heterogeneous effects, you can get good predictions out of sample even when average effects differ substantially (and you do better with multiple sites when sites in the the population have different proportions of subject types that are correlated with het fx)

Two notable features of the design:
- there must be het fx for this to work (otherwise our estimates get biased toward zero because of overfitting to the het variables)
- we have to have information about the covariate in the population and the sample (here we used the proportion of people in each het type)

Findings:
- these two strategies are both unbiased
- the design with five sites has half the RMSE of the one-site design. this is because of the variation in the proportions of types across sites. 
- interestingly there is poor coverage (anti-conservative) when you use the single site design

(when you have contextual variation as well, i.e. effect differs across sites for reasons not captured by the het fx, coverage is off for all designs. will keep this point out, seems like too much)

## you can improve site-level effect estimates by analyzing with simple Bayesian model because of its shrinkage property, even when the Bayesian model is wrong about distribution of effects in population

- This is the point from the blog post; I will modify the above design so it can also make this point, switching between the normal distribution and uniform distribution for the fx distribution

### when things break down: confounded sampling

<!-- - het fx that systematically differ in places that are sampled to study vs others -->
- mean effect differs in places that are sampled vs not sampled
<!-- - covariate profiles that do not exist in other contexts -->


Other points I decided to abandon to keep this simple:
- tradeoff: context-specific intervention and comparability
- tradeoff: comparability and fidelity to context in outcome measurement

