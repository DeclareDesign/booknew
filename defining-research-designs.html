<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 2 Defining research designs | Research Design: Declare, Diagnose, Redesign</title>
<meta name="author" content="Graeme Blair, Alexander Coppock, and Macartan Humphreys">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.4.9002/tabs.js"></script><script src="libs/bs3compat-0.2.4.9002/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<link href="libs/bs4_book-1.0.0/dd_imgpopup.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/bs4_book-1.0.0/dd_imgpopup.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://hypothes.is/embed.js" async></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS --><link rel="stylesheet" href="headers/style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Research Design: Declare, Diagnose, Redesign</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li class="book-part">Introduction</li>
<li><a class="" href="preamble.html"><span class="header-section-number">1</span> Preamble</a></li>
<li><a class="active" href="defining-research-designs.html"><span class="header-section-number">2</span> Defining research designs</a></li>
<li><a class="" href="research-design-principles.html"><span class="header-section-number">3</span> Research design principles</a></li>
<li><a class="" href="primer.html"><span class="header-section-number">4</span> Software primer</a></li>
<li><a class="" href="part-i-exercises.html"><span class="header-section-number">5</span> Part I Exercises</a></li>
<li class="book-part">Declaration, Diagnosis, Redesign</li>
<li><a class="" href="declaration-1.html"><span class="header-section-number">6</span> Declaration</a></li>
<li><a class="" href="specifying-the-model.html"><span class="header-section-number">7</span> Specifying the model</a></li>
<li><a class="" href="defining-the-inquiry.html"><span class="header-section-number">8</span> Defining the inquiry</a></li>
<li><a class="" href="crafting-a-data-strategy.html"><span class="header-section-number">9</span> Crafting a data strategy</a></li>
<li><a class="" href="choosing-an-answer-strategy.html"><span class="header-section-number">10</span> Choosing an answer strategy</a></li>
<li><a class="" href="p2diagnosis.html"><span class="header-section-number">11</span> Diagnosis</a></li>
<li><a class="" href="redesign-2.html"><span class="header-section-number">12</span> Redesign</a></li>
<li><a class="" href="part-ii-exercises.html"><span class="header-section-number">13</span> Part II Exercises</a></li>
<li class="book-part">Research Design Library</li>
<li><a class="" href="research-design-library.html"><span class="header-section-number">14</span> Research Design Library</a></li>
<li><a class="" href="observational-descriptive.html"><span class="header-section-number">15</span> Observational | descriptive</a></li>
<li><a class="" href="observational-causal.html"><span class="header-section-number">16</span> Observational | causal</a></li>
<li><a class="" href="experimental-causal.html"><span class="header-section-number">17</span> Experimental | causal</a></li>
<li><a class="" href="experimental-descriptive.html"><span class="header-section-number">18</span> Experimental | descriptive</a></li>
<li><a class="" href="complex-designs-1.html"><span class="header-section-number">19</span> Complex designs</a></li>
<li><a class="" href="part-iii-exercises.html"><span class="header-section-number">20</span> Part III Exercises</a></li>
<li class="book-part">Research Design Lifecycle</li>
<li><a class="" href="research-design-lifecycle.html"><span class="header-section-number">21</span> Research Design Lifecycle</a></li>
<li><a class="" href="brainstorming.html"><span class="header-section-number">22</span> Brainstorming</a></li>
<li><a class="" href="planning.html"><span class="header-section-number">23</span> Planning</a></li>
<li><a class="" href="realization.html"><span class="header-section-number">24</span> Realization</a></li>
<li><a class="" href="integration.html"><span class="header-section-number">25</span> Integration</a></li>
<li><a class="" href="part-iv-exercises.html"><span class="header-section-number">26</span> Part IV Exercises</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="defining-research-designs" class="section level1">
<h1>
<span class="header-section-number">2</span> Defining research designs<a class="anchor" aria-label="anchor" href="#defining-research-designs"><i class="fas fa-link"></i></a>
</h1>
<p>At its heart, a research design is a procedure for generating empirical answers to theoretical questions. Research designs can be strong or weak – assessing whether a design is strong requires having a clear sense of what the question is and knowing whether the answers a study is likely to deliver are reliable. This book offers a language for describing research designs and an algorithm for selecting among them. In other words, it is a set of tools for weighing and describing the dozens of choices we make in our research activities that together determine whether we can provide useful answers to our questions.</p>
<p>Remarkably, we can use the same basic language to describe research designs whether they target causal or descriptive questions, whether they are focused on theory testing or inductive learning, and whether they use quantitative, qualitative, or mixed methods. The algorithm for selecting a strong design is declare-diagnose-redesign. Once a design is declared in simple enough language that a computer can understand it, its properties can be diagnosed at the click of a button. We can then engage in redesign, or the process of comparing among alternative designs. The same language we use to talk to the computer can be used to talk to others. Reviewers, advisors, students, funders, journalists, and the public – and yes the computer too – need to know four basic things to understand your design.</p>
<div id="the-four-components-of-research-design" class="section level2">
<h2>
<span class="header-section-number">2.1</span> The four components of research design<a class="anchor" aria-label="anchor" href="#the-four-components-of-research-design"><i class="fas fa-link"></i></a>
</h2>
<p>Empirical research designs share in common that they all have a model, an inquiry, a data strategy, and an answer strategy. The four together, which we refer to as MIDA, represent both your suppositions about how the world works and the choices you make as the researcher to intervene in and learn about the world.</p>
<p><strong>Model.</strong> The model comprises speculations about what causes what and how. It includes (possibly quite well-informed) guesses about how important variables are distributed, how things are correlated, and the sequences of events. Of course, we don’t know the true model – if we did we wouldn’t have to do any empirical research. A fundamental challenge is that we have to design studies under a set of uncertain beliefs about the true model.</p>
<p>What’s in a model? The model defines a set of units that we wish to study. Often, this set of units is larger than the set of units that we will actually study empirically, but we can nevertheless define this larger set about which seek to make inferences. The units might be all of the citizens in Lagos, Nigeria or every police beat in New Delhi. The set may be restricted to the mayors of cities in California or the catchment areas of schools in rural Uganda.</p>
<p>The model includes beliefs about units’ baseline characteristics – how many of each kind of unit there are and how features of the units may be correlated. For descriptive and casual questions alike, models are <em>causal</em> models. They include a set of outcome variables that may be functions of baseline characteristics and the effects of treatments. These treatments might be delivered naturally by the world or may be set as the result of researcher intervention. The values that an outcome variable takes on depending on the level of a treatment are called <em>potential</em> outcomes. In the simplest case of a binary treatment, a unit’s treated potential outcome arises when that unit receives the treatment; the untreated potential outcome arises when it does not. The model therefore includes beliefs about causal effects, which are defined with respect to these potential outcomes. The casual effect of a particular treatment is usually defined as the difference between the treated and untreated potential outcomes.</p>
<p>Because we are uncertain about the model, we have to entertain a multiplicity of possibilities. The correlation between two variables might be large and positive, but it could just as well be zero. We might believe that, conditional on some background variables, a treatment has been as-if randomly assigned by the world – but we might wrong about that too. We hope that the true, unknown causal model is in the set of models we consider so that we can correctly imagine what will happen when our design is applied in the real world.</p>
<p>Defining the model can feel like an odd exercise. Since researchers presumably want to learn about the model, declaring it in advance may seem to beg the question. In practice, declaring a model about which we are uncertain is already familiar to any researcher who has calculated the power of a design, which requires the specification of effect sizes. The seeming arbitrariness of the declared model can be mitigated by assessing the sensitivity of the diagnosis to the set of multiple possibilities in the model. For example, a diagnosis can describe the properties of a design if effect sizes are high and if they are low.</p>
<p>Researchers can refine their models in order to reduce uncertainty. They can draw on existing data, such as baseline surveys, or on new information gathered from pilot studies. Reducing uncertainty over the possible set of models is a core purpose of theoretical reflection, literature review, and formative research.</p>
<p><strong>Inquiry</strong>. The research question – what we call the inquiry – is a summary of the model we want to learn. Our theories are rich, so in a single model, there may be many possible inquiries that we may seek to learn about. A model may be a complex dance of ten or more interrelated variables, but an inquiry is something like the average causal effect of a single variable on another, the descriptive distribution of a third variable, or a prediction about the value a variable in the future.</p>
<p>Inquiries are defined with respect to units, conditions, and variables; they are summaries of characteristics of units in or across conditions. Inquiries may be causal, as in the average treatment effect (ATE). The ATE is the average difference in the outcome variable across two conditions: the treatment condition and the control condition. Inquiries may be descriptive, as in a population average of a characteristic. While it may seem that descriptive inquiries do not involve conditions, they do. The population average of a variable is defined with respect to the conditions under which it is measured. Sometimes the mere act of observation changes that which is observed, underlining the general point that researchers inevitably learn about outcomes under specific conditions. As a terminological note, we use the word “estimand” to refer to the true value of an inquiry.</p>
<!-- Please don't delete this sentence! -->
<p>Together, the model and data form the theoretical half of a research design. The second half is empirical, and its two components mirror those in the theoretical half.</p>
<p><strong>Data strategy.</strong> The data strategy is the full set of procedures we use to gather information from the world. The three basic groups of data strategy procedures parallel the three features of inquiries: units are sampled, conditions are assigned, and variables are measured. All data strategies involve sampling in the sense that no empirical strategy is comprehensive – some units are sampled into the study and some units aren’t. Even comprehensive research designs like a population census have a sampling strategy in that they don’t sample respondents in different years or different countries. Assignment procedures describe how researchers generate variation in the world. If you ask some subjects one question, but other subjects a different question, you’ve generated variation on the basis of an assignment procedure. We think of assignment procedures most often when they are randomized, as in a randomized experiment, but many kinds of research designs engage in assignment procedures that are not randomized. Measurement procedures are the ways in which researchers reduce the complex and multidimensional social world into a relatively parsimonious set of data. These data need not be quantitative data in the sense of being numbers or values on a pre-defined scale – qualitative data are data too. Measurement is the vexing but necessary reduction of reality to a few choice representations. Measurement always carries the possibility of measurement error, because this reduction is hard.</p>
<p><strong>Answer strategy.</strong> The answer strategy is how we summarize the data produced by the data strategy. Just like the inquiry summarizes a part of the model, the answer strategy summarizes a part of the data. Complex, multidimensional datasets don’t just speak for themselves – they need to be summarized and explained. Answer strategies are functions that take in data and return answers. For some research designs, this is a literal function like <code>lm_robust</code> that estimates an ordinary least squares (OLS) regression with robust standard errors. For some research designs, the function is embodied by the researchers themselves when they read documents and summarize their meanings in a case study.</p>
<p>Importantly, the answer strategy is more than the choice of an estimator. It includes the full set of procedures that begin with cleaning the dataset and end with answers in words, tables, and graphs. These activities include data cleaning, data transformation, estimation, plotting, and interpretation. Not only do we define our choice of OLS as the estimator, we also specify that we will focus attention on a particular coefficient estimate, assess uncertainty using a 95% confidence interval, and construct a coefficient plot to visualize the inference. The answer strategy also includes all of the if-then procedures that researchers implicitly or explicitly take depending on initial results and features of the data. In a stepwise regression procedure, the answer strategy is not the final regression that results from iterative model selection, but that whole procedure itself.</p>
</div>
<div id="declare-diagnose-redesign" class="section level2">
<h2>
<span class="header-section-number">2.2</span> Declare-Diagnose-Redesign<a class="anchor" aria-label="anchor" href="#declare-diagnose-redesign"><i class="fas fa-link"></i></a>
</h2>
<div id="declaration" class="section level3">
<h3>
<span class="header-section-number">2.2.1</span> Declaration<a class="anchor" aria-label="anchor" href="#declaration"><i class="fas fa-link"></i></a>
</h3>
<p>Declaring a design entails separating out which parts of your design belong in <em>M</em>, <em>I</em>, <em>D</em>, and <em>A</em>. The declaration process can be a challenge because mapping your ideas and excitement about your project into <em>MIDA</em> is not always straightforward. We promise it is a rewarding task. When you can express your research design in terms of these four components you are newly able to think about its properties.</p>
<p>Designs can be declared in words, but declarations often become much more specific when carried out in code. You can declare a design in any statistical programming language: Stata, R, Python, Julia, SPSS, SAS, Mathematica, or many others. Design declaration is even possible – though somewhat awkward – in Excel. We wrote the companion software, <code>DeclareDesign</code>, in R because of the availability of other useful tools in R and because it is free, open-source, and high-quality. We have designed the book so that you can read it even if you do not use R, but you will have to translate the code into your own language of choice. On our Web site, we have <a href="https://declaredesign.org/pap">pointers</a> for how you might declare designs in Stata, Python, and Excel. In addition, we link to a <a href="https://eos.wzb.eu/ipi/DDWizard/">“Design wizard”</a> that lets you declare and diagnose variations of standard designs via a point-and-click web interface. Chapter 4 provides an introduction to <code>DeclareDesign</code> in R.</p>
</div>
<div id="diagnosis" class="section level3">
<h3>
<span class="header-section-number">2.2.2</span> Diagnosis<a class="anchor" aria-label="anchor" href="#diagnosis"><i class="fas fa-link"></i></a>
</h3>
<p>Once you’ve declared your design, you can diagnose it. Design diagnosis is the process of simulating your research design in order to understand the range of ways the study could turn out. Each run of the design comes out differently because different units are sampled, or the randomization allocated different units to treatment, or outcomes were measured with different error. We let computers do the simulations for us because imagining the full set of possibilities is – to put it mildly – cognitively demanding.</p>
<p>Diagnosis is the process of assessing the properties of designs, and represents an opportunity to write down what would make the study a success. For a long time, researchers have classified studies as successful or not based on statistical significance. Accordingly, statistical power (the probability of a statistically significant result) has been the most front-of-mind property when researchers plan studies. As we learn more about the pathologies of relying on statistical significance, we learn that features beyond power are just as, if not more, important. For example, the “credibility revolution” throughout social science has trained a laser-like focus on the bias that may result from omitted or “lurking” variables.</p>
<p>Design diagnosis relies on two new concepts: diagnostic statistics and diagnosands.</p>
<p>A diagnostic statistic is a summary statistic generated from a single simulation of a design. For example, the statistic <span class="math inline">\(e\)</span> refers to the difference between the estimate and the estimand. The statistic <span class="math inline">\(s\)</span> refers to whether the estimate was deemed statistically significant at the 0.05 level.</p>
<p>A diagnosand is a summary of the distribution of a diagnostic statistic across many simulations of the the design. The bias diagnosand is defined as the average value of the <span class="math inline">\(e\)</span> statistic and the power diagnosand is defined as the average value of the <span class="math inline">\(s\)</span> statistic. Other diagnosands include things like root-mean-squared-error (RMSE), Type I and Type II error rates, whether any subjects were harmed, and average cost. We describe these diagnosands in much more detail in section @ref(#p2diagnosis).</p>
<p>One especially important diagnosand is the “success rate,” which is the average value of the “success” diagnostic statistic. As the researcher, <em>you</em> get to decide what would make your study a success. What matters most in your research scenario? Is it statistical significance? If so, optimize your design with respect to power. Is what matters most in your research setting whether the answer has the correct sign or not? Then diagnose how frequently your answer strategy yields an answer with the same sign as your inquiry. Diagnosis involves articulating what would make your study a success and then figuring out, through simulation, how often you obtain that success. Success is often a multidimensional aggregation of multiple diagnosands, such as obtaining high statistical power while keeping costs manageable.</p>
<p>We diagnose studies over the set of causal possibilities encoded in the model, since we want to learn the value of diagnosands under many possible scenarios. A clear example of this is the power diagnosand over many possible values of the true effect size. This idea extends well beyond statistical power. Whatever the set of important diagnosands, we want to ensure that our design performs well across all model possibilities.</p>
<p>Computer simulation is not the only way to do design diagnosis. Designs can be declared in writing or mathematical notation and then diagnosed using analytic formulae. Enormous research design progress has been made with this approach. Methodologists across the social sciences have described diagnosands such as bias, power, and root-mean-squared-error for large classes of designs. Not only can this work provide closed-form solutions for many diagnosands, it can also yield insights about the pitfalls to watch out for when constructing similar designs. That said, pen-and-paper diagnosis is challenging for the majority of social science research designs, first because many designs have idiosyncratic features that are hard to incorporate and second because the analytic formulae for many diagnosands have not yet been worked out by statisticians, even for very common designs (see Section <a href="p2diagnosis.html#p2diagnosis">11</a>).</p>
<p>We are enthusiastic about the ability of diagnosis to evaluate designs based on their ex ante properties, rather than on the ex post results that a design produces. That said, design diagnosis doesn’t solve every problem and like any tool, can be misused. We outline two main concerns.</p>
<p>The first is the worry that the diagnoses are plain wrong. Given that design declaration includes conjectures about the world it is possible to choose inputs such that a design passes any diagnostic test set for it. For instance, a simulation-based claim to unbiasedness that incorporates all features of a design is still only good with respect to the precise conditions of the simulation. In contrast, analytic results, when available, may extend over general classes of designs. Still worse, simulation parameters might be chosen opportunistically. A power analysis, for instance, may be useless if implausible parameters are chosen to raise power artificially. While our framework may encourage more honest declarations, nothing about it enforces honesty. As ever, garbage-in, garbage-out.</p>
<p>Second, we see a risk that research may be evaluated on the basis of a narrow, but perhaps inappropriate, set of diagnosands. Statistical power is often invoked as a key design feature, but well-powered studies that are biased are of little theoretical use. The importance of particular diagnosands can depend on the values of other diagnosands in complex ways, so researchers should take care to evaluate their studies along many dimensions.</p>
</div>
<div id="redesign" class="section level3">
<h3>
<span class="header-section-number">2.2.3</span> Redesign<a class="anchor" aria-label="anchor" href="#redesign"><i class="fas fa-link"></i></a>
</h3>
<p>Once your design has been declared, and you have learned to diagnose it with respect to the most important diagnosands, the last step is redesign.</p>
<p>Redesign entails tweaking parameters of the data and answer strategies to understand how they matter for your diagnosands. Many diagnosands such as power, RMSE depend on the size of the study. We can redesign the study, varying the sample size feature of the data strategy to determine how big it needs to be to achieve a target diagnosand: 90% power, say, or an RMSE of 0.02. We could also vary an aspect of the answer strategy, say, the covariates used to adjust a regression model. Sometimes the changes to the data and answer strategies interact: if we use better covariates to increase the precision of the estimates in the answer strategy, we have to collect that information as a part of the data strategy. The redesign question now becomes, is it better to collect pre-treatment information from all subjects or is the money better spent on increasing the total number of subjects?</p>
<p>The redesign process is mainly about optimizing your research design given ethical, logistical, and financial constraints. If related diagnosands such as total harm to subjects, total researcher hours, or total project cost exceed acceptable levels, the design is not feasible. We want to choose the best design we can among the feasible set. If the designs remaining in the feasible set are underpowered, biased, or are otherwise scientifically inadequate, the project may need to be abandoned – something that’s best learned as soon as possible.</p>
<p>In our experience, it’s during the redesign process that designs become <strong>simpler</strong>. We learn that our experiment has too many arms or that the expected level of heterogeneity is too small to be detected by our design. We learn that in our theoretical excitement, we’ve built a design with too many bells and too many whistles. Some of the complexity needs to be cut, or the whole design will be a muddle. The upshot of many redesign sessions is that our designs pose fewer inquiries, but obtain better answers.</p>
</div>
</div>
<div id="example-encouraging-political-candidacy-in-pakistan" class="section level2">
<h2>
<span class="header-section-number">2.3</span> Example: Encouraging political candidacy in Pakistan<a class="anchor" aria-label="anchor" href="#example-encouraging-political-candidacy-in-pakistan"><i class="fas fa-link"></i></a>
</h2>
<div id="declaration-in-words" class="section level3">
<h3>
<span class="header-section-number">2.3.1</span> Declaration in words<a class="anchor" aria-label="anchor" href="#declaration-in-words"><i class="fas fa-link"></i></a>
</h3>
<p>We illustrate the <em>MIDA</em> framework with a study of the political motivations among office-seekers in Pakistan. <span class="citation">Gulzar and Khan (<a href="references.html#ref-Gulzar2020" role="doc-biblioref">2020</a>)</span> conducted an experiment that estimated the effects of two alternative motivations for becoming a politician: helping the community or generating personal benefits. The researchers randomly assigned villages to receive different encouragements to run and measured the rates of running for office, the types of people who chose to run, and the congruence of elected politicians’ policy positions with those of the general population.</p>
<p>The model for this study applies to citizens who are eligible to run for office in study villages. The model describes subjects’ individual characteristics and their potential outcomes depending on which motivation treatment they receive. The alternative theories that the model encompasses are that politicians run for office only to help themselves, only to help others, neither, or both. Among theories that include room for both motivations, some claim that intrinsic motivations are more powerful than extrinsic, while other claim the reverse. We define these potential outcomes in terms of subjects’ latent probability of running for office which is tightly related to the binary choice to run or not to run.</p>
<p>The study has two inquiries that are the average treatment effects of each encouragement, defined as the average difference in potential outcomes between receiving and not receiving each encouragement to run for office. We could imagine a third inquiry that is the difference between these two average treatment effects, but we’ll leave that complication to the side for the moment.</p>
<p>The <em>data strategy</em> includes three elements. First, we randomly sample 50 citizens who are eligible to stand for election from each village. Next, we assign subjects to a personal benefits encouragement, a prosocial encouragement, or no encouragement (control). All subjects in a village are assigned to the same treatment condition, which is to say that this experiment used cluster random assignment. Lastly, the data strategy measures the decision to run for office by checking whether a subject’s name appears on the official candidate lists of released by the Election Commission of Pakistan. In contrast to the continuous latent outcome variable in the model, the outcome variable as measured in the data strategy is binary.</p>
<p>The <em>answer strategy</em> is the difference-in-means estimator with standard errors clustered at the village level. The clustering of the errors at the village level is an example of how choices in the answer strategy must respect choices made in the data strategy.</p>
</div>
<div id="declaration-in-code" class="section level3">
<h3>
<span class="header-section-number">2.3.2</span> Declaration in code<a class="anchor" aria-label="anchor" href="#declaration-in-code"><i class="fas fa-link"></i></a>
</h3>
<p>We are now ready to declare the <span class="citation">Gulzar and Khan (<a href="references.html#ref-Gulzar2020" role="doc-biblioref">2020</a>)</span> study in code. In the model, we describe a hierarchical structure with 500 villages each of which is home to 100 citizens who are eligible to run for elected office. Each citizen harbors three potential outcomes, <code>Y_Z_neutral</code>, <code>Y_Z_personal</code>, and <code>Y_Z_social</code>. <code>Y_Z_neutral</code> is the citizen’s latent probability of standing for election if treated with a neutral appeal; <code>Y_Z_personal</code> is the probability if treated with an appeal that emphasizes the personal returns to office, and <code>Y_Z_social</code> is the probability if treated with an appeal that underlines the benefits to the community. Our simplified model includes a constant treatment effect of 2 percentage points for the personal appeal and 3 percentage points for the social appeal.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;We use the &lt;code&gt;pnorm&lt;/code&gt; function to define a latent variable representing the potential to run for office. We then in the measurement section measure our binary outcome, which springs from this latent outcome &lt;span class="math inline"&gt;\(Y\)&lt;/span&gt;.&lt;/p&gt;'><sup>2</sup></a></p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">model</span> <span class="op">&lt;-</span>
  <span class="fu">declare_model</span><span class="op">(</span>
    villages <span class="op">=</span> <span class="fu">add_level</span><span class="op">(</span>N <span class="op">=</span> <span class="fl">500</span>, U_village <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span>, sd <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span><span class="op">)</span>,
    citizens <span class="op">=</span> <span class="fu">add_level</span><span class="op">(</span>
      N <span class="op">=</span> <span class="fl">100</span>, 
      U_citizen <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span>,
      <span class="fu">potential_outcomes</span><span class="op">(</span>
        <span class="va">Y</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span>
          <span class="va">U_citizen</span> <span class="op">+</span> <span class="va">U_village</span> <span class="op">+</span> 
            <span class="fl">0.1</span> <span class="op">*</span> <span class="op">(</span><span class="va">Z</span> <span class="op">==</span> <span class="st">"personal"</span><span class="op">)</span> <span class="op">+</span> <span class="fl">0.15</span> <span class="op">*</span> <span class="op">(</span><span class="va">Z</span> <span class="op">==</span> <span class="st">"social"</span><span class="op">)</span><span class="op">)</span>,
          conditions <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>Z <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"neutral"</span>, <span class="st">"personal"</span>, <span class="st">"social"</span><span class="op">)</span><span class="op">)</span>
        <span class="op">)</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>We have two inquiries, representing the average treatment effects in the population for the personal and social appeals compared to the neutral appeal, defined as the average differences in potential outcomes:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">inquiry</span> <span class="op">&lt;-</span> <span class="fu">declare_inquiry</span><span class="op">(</span>
  ATE_personal <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">Y_Z_personal</span> <span class="op">-</span> <span class="va">Y_Z_neutral</span><span class="op">)</span>,
  ATE_social <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">Y_Z_social</span> <span class="op">-</span> <span class="va">Y_Z_neutral</span><span class="op">)</span>
<span class="op">)</span></code></pre></div>
<p>The data strategy consists of three steps: sampling, assignment, and measurement. In sampling, we sample 48 of the eligible citizens from each village into the study. In assignment, we cluster assign one third of the villages to the neutral condition, one third to the personal appeal, and the remaining third to the social appeal. The measurement step maps the realized (but still latent!) potential outcome to the observed choice to run or not.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">n_villages</span> <span class="op">&lt;-</span> <span class="fl">192</span>
<span class="va">citizens_per_village</span> <span class="op">&lt;-</span> <span class="fl">48</span>

<span class="va">data_strategy</span> <span class="op">&lt;-</span>
  <span class="fu">declare_sampling</span><span class="op">(</span>
    S_village <span class="op">=</span> <span class="fu">cluster_rs</span><span class="op">(</span>clusters <span class="op">=</span> <span class="va">villages</span>, n <span class="op">=</span> <span class="va">n_villages</span><span class="op">)</span>,
    filter <span class="op">=</span> <span class="va">S_village</span> <span class="op">==</span> <span class="fl">1</span>,
    legacy <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span>
  
  <span class="fu">declare_sampling</span><span class="op">(</span>
    S_citizen <span class="op">=</span> <span class="fu">strata_rs</span><span class="op">(</span>strata <span class="op">=</span> <span class="va">villages</span>, n <span class="op">=</span> <span class="va">citizens_per_village</span><span class="op">)</span>,
    filter <span class="op">=</span> <span class="va">S_citizen</span> <span class="op">==</span> <span class="fl">1</span>,
    legacy <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span>
  
  <span class="fu">declare_assignment</span><span class="op">(</span>
    Z <span class="op">=</span> <span class="fu">cluster_ra</span><span class="op">(</span>
      clusters <span class="op">=</span> <span class="va">villages</span>, 
      conditions <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"neutral"</span>, <span class="st">"personal"</span>, <span class="st">"social"</span><span class="op">)</span><span class="op">)</span>,
    legacy <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span> 
  
  <span class="fu">declare_measurement</span><span class="op">(</span>
    Y_latent <span class="op">=</span> <span class="fu">reveal_outcomes</span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">Z</span><span class="op">)</span>,
    Y_observed <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">N</span>, <span class="fl">1</span>, prob <span class="op">=</span> <span class="va">Y_latent</span><span class="op">)</span>
  <span class="op">)</span></code></pre></div>
<p>The answer strategy consists of an ordinary least squares regression (as implemented by <code>lm_robust</code>) of the outcome on the treatments. The standard errors clustered at the village level in order to account for the clustering in the assignment procedure. The regression will return three coefficients: an intercept and two treatment effect estimates. We ensure that the estimates are mapped to the relevant inquiry by explicitly linking them.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">answer_strategy</span> <span class="op">&lt;-</span> 
  <span class="fu">declare_estimator</span><span class="op">(</span><span class="va">Y_observed</span> <span class="op">~</span> <span class="va">Z</span>, term <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Zpersonal"</span>, <span class="st">"Zsocial"</span><span class="op">)</span>, 
                    clusters <span class="op">=</span> <span class="va">villages</span>, 
                    model <span class="op">=</span> <span class="va">lm_robust</span>,
                    inquiry <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"ATE_personal"</span>, <span class="st">"ATE_social"</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>When we concatenate all four elements with the <code><a href="https://rdrr.io/r/base/Arithmetic.html">+</a></code> we get a design:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">design</span> <span class="op">&lt;-</span> <span class="va">model</span> <span class="op">+</span> <span class="va">inquiry</span> <span class="op">+</span> <span class="va">data_strategy</span> <span class="op">+</span> <span class="va">answer_strategy</span></code></pre></div>
<p>For most designs that we declare in the book, we will also present a graphical representation of the design. In Figure <a href="defining-research-designs.html#fig:gulzarkhandag">2.1</a>, we visualize the Gulzar-Khan design.</p>
<div class="figure">
<span id="fig:gulzarkhandag"></span>
<img src="book_files/figure-html/gulzarkhandag-1.svg" alt="Simplified DAG for Gulzar Khan study" width="100%"><p class="caption">
Figure 2.1: Simplified DAG for Gulzar Khan study
</p>
</div>
<p>To diagnose the design, we first define a set of diagnosands (see Section <a href="p2diagnosis.html#p2diagnosis">11</a>), which are statistical properties of the design. In this case, we select the bias (difference between the estimate and the estimand, which is the PATE); the statistical power of the design; the root mean-squared error, which is a weighted average of the bias and efficiency of the design; and its total cost.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">diagnosands</span> <span class="op">&lt;-</span> <span class="fu">declare_diagnosands</span><span class="op">(</span>
  bias <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">estimate</span> <span class="op">-</span> <span class="va">estimand</span><span class="op">)</span>,
  rmse <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">estimate</span> <span class="op">-</span> <span class="va">estimand</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span>,
  power <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">p.value</span> <span class="op">&lt;=</span> <span class="fl">0.05</span><span class="op">)</span>,
  cost <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="fl">10</span> <span class="op">*</span> <span class="va">n_villages</span> <span class="op">+</span> <span class="fl">1</span> <span class="op">*</span> <span class="va">n_villages</span> <span class="op">*</span> <span class="va">citizens_per_village</span><span class="op">)</span>
<span class="op">)</span></code></pre></div>
<p>We then diagnose the design, which involves simulating the design and again and again, and then calculate the diagnosands based on the simulations data.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">diagnosis</span> <span class="op">&lt;-</span> <span class="fu">diagnose_design</span><span class="op">(</span><span class="va">design</span>, diagnosands <span class="op">=</span> <span class="va">diagnosands</span>, sims <span class="op">=</span> <span class="va">sims</span>, bootstrap_sims <span class="op">=</span> <span class="va">b_sims</span><span class="op">)</span></code></pre></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:simpledesigndiagnosis">Table 2.1: </span>Diagnosis of the simplified Gulzar-Khan design.
</caption>
<thead><tr>
<th style="text-align:left;">
inquiry_label
</th>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
bias
</th>
<th style="text-align:right;">
rmse
</th>
<th style="text-align:right;">
power
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
ATE_personal
</td>
<td style="text-align:left;">
Zpersonal
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.013
</td>
<td style="text-align:right;">
0.514
</td>
</tr>
<tr>
<td style="text-align:left;">
ATE_social
</td>
<td style="text-align:left;">
Zsocial
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.013
</td>
<td style="text-align:right;">
0.871
</td>
</tr>
</tbody>
</table></div>
<p>The diagnosis reveals that the design is unbiased for both parameters, and that for the social treatment inquiry powered above the standard 80% threshold. However, it is not powered for the personal inquiry. This gives us a sense of what effect sizes the design is powered for, since the only difference in the design between these two inquiries is the assumed true effect size in the model. We could redesign this design to increase the sample size for the personal treatment, or use this diagnosis to better interpret the results of the study.</p>
<p>We redesign across possible combinations of numbers of villages and citizens per village, important design parameters in the control of the researchers:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">designs</span> <span class="op">&lt;-</span> <span class="fu">redesign</span><span class="op">(</span><span class="va">design</span>, n_villages <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">192</span>, <span class="fl">295</span>, <span class="fl">397</span>, <span class="fl">500</span><span class="op">)</span>, citizens_per_village <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">25</span>, <span class="fl">50</span>, <span class="fl">75</span>, <span class="fl">100</span><span class="op">)</span><span class="op">)</span>
<span class="va">diagnosis</span> <span class="op">&lt;-</span> <span class="fu">diagnose_design</span><span class="op">(</span><span class="va">designs</span>, diagnosands <span class="op">=</span> <span class="va">diagnosands</span><span class="op">)</span></code></pre></div>
<p>In Figure <a href="#fig:gulzarkhanredesign"><strong>??</strong></a>, we illustrate the results of our redesign exercise across all four diagnosands. On the x-axis of each plot is the number of citizens per village and the y-axis the value of the diagnosand. The plot is faceted by diagnosand, and each line represents a different possible number of villages. The diagnoses are for the social treatment (we could create the same plot, one for each treatment).</p>
<p>What we see is that bias is invariant to these choices; our study will be unbiased, regardless of the number of villages and number of citizens we interview per village. However, our other three diagnosands do change. Power is increasing in the number of citizens per village, and always higher with more villages. We might reject designs with 192 villages with only 25 citizens per village, because they fall below the 80% power threshold (in fact, the number chosen by the researchers, 48, is just over the threshold, suggesting they chose the most cost-effective design in terms of power). Root mean-squared error, a measure capturing both bias and efficiency of the design, is improving (decreasing) in the number of citizens per village and number of villages. Cost is, of course, increasing in both sample size parameters. We can use the cost parameters to make decisions about what sample sizes to choose accounting both for scientific diagnosands of the design (i.e., power) and cost at the same time.</p>
<div class="inline-figure"><img src="book_files/figure-html/gulzarkhanredesign-1.svg" width="100%"></div>
</div>
</div>
<div id="putting-designs-to-use" class="section level2">
<h2>
<span class="header-section-number">2.4</span> Putting designs to use<a class="anchor" aria-label="anchor" href="#putting-designs-to-use"><i class="fas fa-link"></i></a>
</h2>
<p>The two pillars of our approach are the language for describing research designs (<em>MIDA</em>) and the algorithm for selecting high-quality designs (Declare-Diagnose-Redesign). Together, these two ideas can shape research design decisions throughout the lifecycle of a project. The full set of implications is drawn out in Part IV but we emphasize the most important ones here.</p>
<p>Broadly speaking, the lifecycle of an empirical research project has four phases: Brainstorming, Planning, Realization, and Integration. Planning, Realization, and Integration describe what happens before, during, and after the implementation of a research design. We pre-pend a fourth phase – Brainstorming – to reflect the idea that research doesn’t just begin with “planning” – research ideas have to being with some spark of inspiration.</p>
<p>Brainstorming is the process of growing a research idea from a kernel into a skeleton specification of the model, inquiry, data strategy, and answer strategy. The inspiration for a good research project can come from many sources – frustration with an article you’re reading, a golden opportunity with a potential research partner, a conversation with a colleague (or adversary!). The spark of an idea might be some bit of a model, perhaps an inquiry in particular, maybe a portion of a data strategy, or just an itch to apply a new answer strategy you learned about. Wherever that kernel of an idea starts, the point of brainstorming is to develop all parts of <em>M</em>, <em>I</em>, <em>D</em>, and <em>A</em> enough that the design becomes a coherent whole.</p>
<p>After an idea has been fleshed out sufficiently, its time to start planning. Planning entails some or all of the following steps, depending on the design: conducting an ethical review, seeking IRB approval, gathering criticism from colleagues and mentors, running pilot studies, and preparing preanalysis documents. The design as encapsulated by <em>M</em>, <em>I</em>, <em>D</em>, and <em>A</em> will go through many iterations and refinements during this period. Planning is the time when frequent re-application of the Declare-Diagnose-Redesign algorithm will pay the highest dividends. How should you investigate the ethics of a study? By casting the ethical costs and benefits as diagnosands. How should you respond to (good and bad) criticism? By re-interpreting the feedback in terms of <em>M</em>, <em>I</em>, <em>D</em>, and <em>A</em>. How can you convince funders and partners that your research project is worth investing in? By credibly communicating your study’s diagnosands – its statistical power, its unbiasedness, its high chance of success, however the partner or funder defines it. What belongs in a pre-analysis plan? You guessed it – a specification of the model, inquiry, data strategy and answer strategy. Iterating and improving design details is the essence of good planning.</p>
<p>Realization is the phase of research in which all those plans are executed. You implement your data strategy in order to gather information from the world. Once that’s done, you follow your answer strategy in order to finally generate answers to the inquiry. Of course, that’s only if things go exactly according to plan – which has never happened once in our own careers. Survey questions don’t work as we imagined, partner organizations over-promise and under-deliver, subjects move or become otherwise unreachable. A critic or a reviewer may insist you change your answer strategy – or may think a different inquiry altogether is the theoretically appropriate one. You may yourself change how you think of the design as you embark on writing up the research project. It is inevitable that some features of <em>M</em>, <em>I</em>, <em>D</em>, and <em>A</em> will change during the realization phase. Some design changes have very bad properties, like sifting through the data ex-post, finding a statistically significant result, then back-fitting an new <em>M</em> and a new <em>I</em> to match the new <em>A</em>. This bad practice goes by the name HARKing – Hypothesizing After Results are Known (<span class="citation">Kerr (<a href="references.html#ref-Kerr1998" role="doc-biblioref">1998</a>)</span>). Indeed, if we declare and diagnose this actual answer strategy (sifting through data ex-post), we can show through design diagnosis that it is badly biased (away from zero) for any of the inquiries it could end up choosing. Other changes made along the way may help the design quite a bit – if the planned design did not include covariate adjustment but a friendly critic suggests adjusting for the pre-treatment measure of the outcome, the RMSE diagnosand might drop nicely. The point here is that design changes during the implementation process – whether necessitated by unforeseen logistical constraints or required by the review process – can be understood using in terms of <em>M</em>, <em>I</em>, <em>D</em>, and <em>A</em> by reconciling the planned design with the design as implemented.</p>
<p>A happy realization ends with the publication of results. When that acceptance email finally hits your inbox, you can celebrate a job well done and a design well realized. But the research design lifecycle is not finished – the design must be integrated into the scientific community. Studies must be archived, along with design information, to prepare for reanalysis. Future scholars may well want to reanalyze your design in order to learn more than is represented in the published article or book. Good reanalysis requires a full understanding of the design as implemented, so archiving design information along with code and data is critical. Not only may your design be reanalyzed, it may also be replicated with fresh data. Ensuring that replication studies answer the same theoretical questions as original studies requires explicit design information without which replicators and original study authors may simply talk past one another.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;For a discussion of the distinctions between different modes of replication, see &lt;span class="citation"&gt;Clemens (&lt;a href="references.html#ref-Clemens2017" role="doc-biblioref"&gt;2017&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;'><sup>3</sup></a> Indeed, as your study is integrated into the scientific literature and beyond, you should anticipate disagreement over your claims. Resolving disputes is very difficult if parties do not share a common understanding of the research design.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;For example, much of the debate over instrumental variables designs in economics centers on which of two inquiries is the “correct” theoretical target, the average treatment effect or the local average treatment effect (&lt;span class="citation"&gt;Deaton (&lt;a href="references.html#ref-deaton2009instruments" role="doc-biblioref"&gt;2010&lt;/a&gt;)&lt;/span&gt;, &lt;span class="citation"&gt;Imbens (&lt;a href="references.html#ref-imbens2010better" role="doc-biblioref"&gt;2010&lt;/a&gt;)&lt;/span&gt;).&lt;/p&gt;'><sup>4</sup></a> Finally, you should anticipate that your results will be formally synthesized with others’ work via meta-analysis. Meta-analysts need design information in order to be sure they aren’t inappropriately mixing together studies that ask importantly different questions or answer them too poorly to be of use.</p>
</div>
<div id="further-reading" class="section level2">
<h2>
<span class="header-section-number">2.5</span> Further Reading<a class="anchor" aria-label="anchor" href="#further-reading"><i class="fas fa-link"></i></a>
</h2>
<p>TBD</p>
<!-- - @brady2010rethinking -->
<!-- We build on two influential research design frameworks. [@kkv1994, p. 13] enumerate four components of a research design: a theory, a research question, data, and an approach to using the data. @geddes2003paradigms articulates the links between theory formation, research question formulation, case selection and coding strategies, and strategies for case comparison and inference. In both cases, the set of components are closely aligned to those in the framework we propose. -->
<!-- Here follows the introduction from the paper: -->
<!-- As empirical social scientists, we routinely face two research design problems. First, we need to select high-quality designs, given financial and practical constraints. Second, we need to communicate those designs to readers and reviewers. To select strong designs, we often rely on rules of thumb, simple power calculators, or principles from the methodological literature that typically address one component of a design while assuming optimal conditions for others. These relatively informal practices can result in the selection of suboptimal designs, or worse, designs that are simply too weak to deliver useful answers. To convince others of the quality of our designs, we often defend them with references to previous studies that used similar approaches, with power analyses that may rely on assumptions unknown even to ourselves, or with ad hoc simulation code. In cases of dispute over the merits of different approaches, disagreements sometimes fall back on first principles or epistemological debates rather than on demonstrations of the conditions under which one approach does better than another. -->
<!-- In this paper we describe an approach to address these problems. We introduce a framework --- MIDA --- that asks researchers to specify information about their background model (M), their inquiry (I), their data strategy (D), and their answer strategy (A). We then introduce the notion of "diagnosands," or quantitative summaries of design properties. Familiar diagnosands include statistical power, the bias of an estimator with respect to an estimand, or the coverage probability of a procedure for generating confidence intervals.  -->
<!-- Many aspects of design quality can be assessed through design diagnosis, but many cannot. For instance the contribution to an academic literature, relevance to a policy decision, and impact on public debate are unlikely to be quantifiable ex ante. -->
<!-- Using this framework, researchers can declare a research design as a computer code object and then diagnose its statistical properties on the basis of this declaration. A researcher may declare the features of designs in our framework for their own understanding and declaring designs may be useful before or after the research is implemented. Researchers can declare and diagnose their designs with the companion software for this paper, DeclareDesign, but the principles of design declaration and diagnosis do not depend on any particular software implementation. -->
<!-- The formal characterization and diagnosis of designs before implementation can serve many purposes. First, researchers can learn about and improve their inferential strategies. Done at this stage, diagnosis of a design and alternatives can help a researcher select from a range of designs, a process we call "redesign." Later, a researcher may include design declaration and diagnosis as part of a preanalysis plan or in a funding request. At this stage, the full specification of a design serves a communication function and enables third parties to understand a design and an author’s intentions. Even if declared ex post, formal declaration still has benefits. The characterization can help readers understand the properties of a research project, facilitate transparent replication, and can help guide future (re-)analysis of the study data. -->
<!-- Together, the language and the algorithm help researchers address two main problems.  -->
<!-- First, they have to select high-quality research designs that can be relied upon to generate credible answers to their research questions. Without a way to measure the quality of design, it's very difficult to choose strong ones over weak ones. Second, researchers need to communicate their designs to others. Without a way to describe a design in detail, it's very difficult to explain to other scholars why they are of high quality and why they are the right design for the question.  -->
<!-- Our language for research designs helps with both problems. -->
<!-- -->
<!-- ^[MIDA is of course itself a model, we present it here as a simple four step sequence though as will become clear in applications in some cases some of these steps are barely visible, some repeat, and, within a given project, the order of steps can be complex.]  -->
<!-- An inquiry is a function of the exogenous characteristics of units, of endogenous outcome variables, or both. It may be defined over all units in the population defined by the model, as in the average treatment effect for all units, or it may be defined over a subset of units, as in the conditional average treatment effect for women. Because we defined the distribution of the variables in the model, we can define the probability distribution of inquiries, which are a function of those variables.  -->
<!-- How can we rule out theoretical models? First, we need to enumerate the many models that are possible, at least in theory. The exhaustive enumeration of possible theoretical models is a daunting challenge. We describe below an approach that begins with a "kernel" -- a small portion of a theoretical model from which we grow theoretical possibilities. Suppose we have a list of possibilities in hand. A research question -- what we call an "inquiry" -- refers to a fact that, if known, would rule out some models in favor of others. If we can show through a credible research design that $Z$ causes $Y$, we can rule out all models in which $Z$ does not cause $Y$. The *reason* that we seek to learn about inquiries is to distinguish among theoretical possibilities. -->
<!-- Typically, endogenous outcome variables are random variables, either because they are a function of an exogenous baseline variable for which we defined a probability distributions or because assignment to treatment or control is random as part of the data strategy. -->
<!-- # we should turn this into a picture labeling MIDA -->

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
<!-- start post here, do not edit above -->
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="preamble.html"><span class="header-section-number">1</span> Preamble</a></div>
<div class="next"><a href="research-design-principles.html"><span class="header-section-number">3</span> Research design principles</a></div>
</div></main><div id="on-this-page-nav" class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#defining-research-designs"><span class="header-section-number">2</span> Defining research designs</a></li>
<li><a class="nav-link" href="#the-four-components-of-research-design"><span class="header-section-number">2.1</span> The four components of research design</a></li>
<li>
<a class="nav-link" href="#declare-diagnose-redesign"><span class="header-section-number">2.2</span> Declare-Diagnose-Redesign</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#declaration"><span class="header-section-number">2.2.1</span> Declaration</a></li>
<li><a class="nav-link" href="#diagnosis"><span class="header-section-number">2.2.2</span> Diagnosis</a></li>
<li><a class="nav-link" href="#redesign"><span class="header-section-number">2.2.3</span> Redesign</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#example-encouraging-political-candidacy-in-pakistan"><span class="header-section-number">2.3</span> Example: Encouraging political candidacy in Pakistan</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#declaration-in-words"><span class="header-section-number">2.3.1</span> Declaration in words</a></li>
<li><a class="nav-link" href="#declaration-in-code"><span class="header-section-number">2.3.2</span> Declaration in code</a></li>
</ul>
</li>
<li><a class="nav-link" href="#putting-designs-to-use"><span class="header-section-number">2.4</span> Putting designs to use</a></li>
<li><a class="nav-link" href="#further-reading"><span class="header-section-number">2.5</span> Further Reading</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="download-code" href="./defining-research-designs.R"><i class="far fa-file-code"></i> Download R code</a></li>
          
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Research Design: Declare, Diagnose, Redesign</strong>" was written by Graeme Blair, Alexander Coppock, and Macartan Humphreys. </p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
