[{"path":"index.html","id":"research-design-declaration-diagnose-redesign","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"Research Design: Declaration, Diagnose, Redesign","text":"","code":""},{"path":"index.html","id":"graeme-blair-alexander-coppock-and-macartan-humphreys","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"Graeme Blair, Alexander Coppock, and Macartan Humphreys","text":"","code":""},{"path":"index.html","id":"draft-manuscript-under-advance-contract-princeton-university-press","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"Draft manuscript under advance contract, Princeton University Press","text":"","code":""},{"path":"index.html","id":"selection-on-observables","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"0.1 Selection on observables","text":"Main points hit :can satisfy back-door criterion, can can assert selection--observablesThe next problem adjust (matching procedure, functional form)adjust post-treatment variables, bias may resultSometimes simply can’t satisfy selection observables.","code":""},{"path":"index.html","id":"declaration","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"0.1.1 Declaration","text":"","code":"\ndesign <-\n  declare_population(N = 100, \n                     X_1 = rnorm(N),\n                     X_2 = rnorm(N),\n                     D = if_else(X_1 + X_2 > 0, 1, 0),\n                     U = rnorm(N)) +\n  declare_potential_outcomes(Y ~ D + X_1 + X_2 + U,\n                             assignment_variable = D) +\n  declare_estimand(ATE = mean(Y_D_1 - Y_D_0)) +\n  reveal_outcomes(Y, D) +\n  declare_estimator(Y ~ D + X_1 + X_2, model = lm_robust, \n                    estimand = \"ATE\") "},{"path":"index.html","id":"dag","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"0.1.2 DAG","text":"","code":""},{"path":"index.html","id":"example","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"0.1.3 Example","text":"(matching regression etc.)","code":""},{"path":"index.html","id":"what-if-you-have-unobserved-confounding","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"0.1.4 What if you have unobserved confounding?","text":"","code":"\ndesign <-\n  declare_population(N = 100, \n                     X_1 = rnorm(N),\n                     X_2 = rnorm(N),\n                     U = rnorm(N),\n                     D = if_else(X_1 + X_2 + U > 0, 1, 0)) +\n  declare_potential_outcomes(Y ~ D + X_1 + X_2 + U,\n                             assignment_variable = D) +\n  declare_estimand(ATE = mean(Y_D_1 - Y_D_0)) +\n  reveal_outcomes(Y, D) +\n  declare_estimator(Y ~ D + X_1 + X_2, model = lm_robust, \n                    estimand = \"ATE\") "},{"path":"index.html","id":"what-if-an-observed-covariate-is-post-treatment","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"0.1.5 What if an observed covariate is post-treatment?","text":"","code":"\ndesign <-\n  declare_population(N = 100, \n                     X_2 = rnorm(N),\n                     D = if_else( X_2 > 0, 1, 0),\n                     U = rnorm(N)) +\n  declare_potential_outcomes(Y ~ D + X_2 + U,\n                             assignment_variable = D) +\n  declare_estimand(ATE = mean(Y_D_1 - Y_D_0)) +\n  reveal_outcomes(Y, D) +\n  declare_measurement(X_1 =  D + rnorm(N)) +\n  declare_estimator(Y ~ D + X_1 + X_2, model = lm_robust, \n                    estimand = \"ATE\") "},{"path":"index.html","id":"dag-1","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"0.1.6 DAG","text":"","code":""},{"path":"index.html","id":"further-reading","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"0.1.7 Further reading","text":"Rosenbaum (2002) matching","code":""},{"path":"index.html","id":"difference-in-differences","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"0.2 Difference-in-differences","text":"","code":""},{"path":"index.html","id":"declaration-1","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"0.2.1 Declaration","text":"","code":"\ndesign <- \n  declare_population(\n    unit = add_level(N = 2, Uu = rnorm(N, sd = 0.5)),\n    period = add_level(N = 2, nest = FALSE),\n    unit_period = cross_levels(by = join(unit, period), \n                               Ui = rnorm(N, sd = 0.01))\n  ) + \n  declare_potential_outcomes(Y ~ Uu + 0.5 * as.numeric(period) + D + Ui,\n                             assignment_variable = D) + \n  declare_estimand(ATT = mean(Y_D_1 - Y_D_0), subset = period == 2) + \n  declare_step(D = Uu == max(Uu), handler = mutate) + \n  reveal_outcomes(Y = if_else(D == 0 | period == 1, Y_D_0, Y_D_1), \n                  handler = mutate) +\n  declare_estimator(Y ~ period + unit + D, model = lm_robust, estimand = \"ATT\", se_type = \"none\")"},{"path":"index.html","id":"dag-2","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"0.2.2 DAG","text":"","code":""},{"path":"index.html","id":"example-1","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"0.2.3 Example","text":"Montalvo (2011) clear example. two groups -person voters absentee voters. average, absentee voters prefer liberal candidates -person voters . Madrid bombing 3 days 2004 election “treats” -person voters absentee voters. parallel trends assumption bolstered inspection pre-treatment changes L-R voting.","code":""},{"path":"index.html","id":"two-period-two-group-setting","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"0.2.4 Two-period two-group setting","text":"Show comparison T C period 2 biased comparison T period 1 2 biased, unbiased presence confounding treatment assignment (unit higher unit shock always treated) time trends","code":"\nN_units <- 2\nN_time_periods <- 2\n\ntwo_period_two_group_design <- \n  \n  declare_population(\n    units = add_level(N = N_units, unit_shock = rnorm(N, sd = 0.5)),\n    periods = add_level(N = N_time_periods, nest = FALSE,\n                        time = (1:N_time_periods) - N_time_periods + 1),\n    unit_period = \n      cross_levels(by = join(units, periods), \n                   unit_time_shock = rnorm(N, sd = 0.01))\n  ) + \n  \n  declare_potential_outcomes(\n    Y_D_0 = unit_shock + 0.5 * time + unit_time_shock, \n    Y_D_1 = Y_D_0 + 1) +\n  \n  declare_estimand(ATE = mean(Y_D_1 - Y_D_0), subset = time == 1) + \n  \n  declare_assignment(\n    D = unit_shock == max(unit_shock), \n    handler = mutate\n  ) + \n  \n  reveal_outcomes(\n    Y = case_when(D == 0 | time < 1 ~ Y_D_0, TRUE ~ Y_D_1), \n    handler = mutate) +\n  \n  declare_estimator(\n    estimate = (mean(Y[D == 1 & time == 1]) - \n                  mean(Y[D == 0 & time == 1])) -\n      (mean(Y[D == 1 & time == 0]) - mean(Y[D == 0 & time == 0])),\n    estimator_label = \"DiD\", \n    handler = summarize, \n    label = \"DiD\") +\n  \n  declare_estimator(\n    estimate = mean(Y[D == 1 & time == 1]) - \n      mean(Y[D == 1 & time == 0]),\n    estimator_label = \"Diff\", \n    handler = summarize, \n    label = \"Over-Time\") +\n  \n  declare_estimator(\n    estimate = mean(Y[D == 1 & time == 1]) - \n      mean(Y[D == 0 & time == 1]),\n    estimator_label = \"DiM\", \n    handler = summarize, \n    label = \"DiM\")\ndiagnosis_two_period_two_group <- diagnose_design(\n  two_period_two_group_design, \n  diagnosands = declare_diagnosands(bias = mean(estimate - estimand)),\n  sims = sims, bootstrap_sims = FALSE)"},{"path":"index.html","id":"parallel-trends-assumption","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"0.2.5 Parallel trends assumption","text":"Introduce assumption visual testFormal test (T = -1 T = 0 periods, .e. year backward )result shows two-step procedure parallel trends assumption test passes shows poor coverage SEs final (https://arxiv.org/abs/1804.01208).","code":"\n# add an additional pretreatment time period in order to visually test for parallel pre-trends\nthree_period_two_group_design <- \n  redesign(two_period_two_group_design, N_time_periods = 3)"},{"path":"index.html","id":"multi-period-design","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"0.2.6 Multi-period design","text":"Switch regression context 20 periods, 100 units show results hold two-way FE (controlling one period T insufficient remove bias)Show case units switch back forth T C panel bias (point Imai Kim weighted FE estimator fix )","code":"\nN_units <- 20\nN_time_periods <- 20\n\nmulti_period_design <- \n  \n  declare_population(\n    units = add_level(\n      N = N_units, \n      unit_shock = rnorm(N), \n      unit_treated = 1*(unit_shock > median(unit_shock)), \n      unit_treatment_start = \n        sample(2:(N_time_periods - 1) - N_time_periods + 1, N, \n               replace = TRUE)),\n    periods = add_level(\n      N = N_time_periods, nest = FALSE, \n      time = (1:N_time_periods) - N_time_periods + 1),\n    unit_period = \n      cross_levels(by = join(units, periods),\n                   noise = rnorm(N), \n                   pretreatment = 1*(time < unit_treatment_start))\n  ) + \n  \n  declare_potential_outcomes(\n    Y_D_0 = unit_shock + 0.5 * time + noise, \n    Y_D_1 = Y_D_0 + 0.2) +\n  \n  declare_estimand(ATE = mean(Y_D_1 - Y_D_0), subset = time == 1) + \n  \n  declare_assignment(D = 1*(unit_treated & pretreatment == FALSE), \n                     handler = fabricate) + \n  reveal_outcomes(Y, D) + \n  \n  declare_estimator(Y ~ D + time, fixed_effects = ~ units + periods, \n                    model = lm_robust, \n                    label = \"twoway-fe\", \n                    estimand = \"ATE\") "},{"path":"index.html","id":"bayesianprocesstracing","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"0.3 Bayesian process-tracing","text":"Process-tracing qualitative method uses evidence -depth interviews written records test causal theories. Process-tracing designs often focus “causes--effects” inquiries (e.g., presence strong middle class cause revolution?), rather “effects--causes” inquiries (e.g., average effect strong middle class probability revolution happening?) Goertz Mahoney (2012).Causes--effects inquiries imply hypothesis – “strong middle class caused revolution,” say. One widely-promoted answer strategy suggests evaluating whether hypotheses correct given presence absence different “clues” found archives interviews [Collier, Brady, Seawright (2004); Mahoney (2012); Bennett Checkel (2015); fairfield2013going]. Van Evera (1997) categorizes different kinds clues according whether one believe hypothesis one observed clue (necessity) whether observing clue suffice infer hypothesis correct (sufficiency).1Of course, rare certainty—typically attach varying degrees belief statements truth. Bayesian process-tracing uses probability theory form posterior belief hypothesis, given beliefs whether observe different pieces evidence right wrong~. Extending Van Evera (1997), “hoop tests” clues nearly certain seen hypothesis true, likely either way, “smoking-guns” unlikely seen general extremely unlikely hypothesis false, “straws---wind” likely hypothesis true still somewhat likely , ``doubly-decisive’’ clues likely seen hypothesis true unlikely false.","code":""},{"path":"index.html","id":"declaration-2","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"0.3.1 Declaration","text":"\\(M\\) Model: posit population 195 cases, exhibit presence outcome, \\(Y \\\\{0,1\\}\\). sake illustration, suppose \\(Y\\) represents presence absence civil war. case also exhibits presence absence potential cause, \\(Z \\\\{0,1\\}\\). example, might suppose \\(Z\\) represents presence absence natural resources. random, 30% cases get \\(Z=1\\). also assume researchers observe clue, \\(C\\), informative whether \\(Z\\) causal relationship \\(Y\\).\npotential outcomes \\(Y\\) depend whether cause, \\(Z\\), present case “type” causal relation case exhibits. Conceptually, exactly four distinct types causal relations. First, presence \\(Z\\) might cause \\(Y\\): \\(Z = 0\\), \\(Y = 0\\) \\(Z = 1\\) \\(Y = 1\\). words, civil wars happen cases country natural resources. Second, absence \\(Z\\) might cause \\(Y\\): \\(Z = 0\\) \\(Y = 1\\) \\(Z = 1\\) \\(Y = 0\\). cases, civil war breaks country natural resources, break country natural resources. Finally, \\(Y\\) might present irrespective \\(Z\\) \\(Y\\) might absent irrespective \\(Z\\). Continuing analogy, countries civil war peace, irrespective whether also natural resources (.e., war related causal process). specify model civil war governed causal pathway 1 (\\(Z\\) causes \\(Y\\)) roughly 20% cases, pathway 2 (\\(\\neg Z\\) causes \\(Y\\)) 10% cases, pathway 3 (\\(Y\\) irrespective \\(z\\)) 20% countries, pathway 4 (\\(\\neg Y\\) irrespective \\(Z\\)) half countries.\nalso potential outcomes clue, \\(C\\). depend case’s causal type. Specifically, clue appears .25 probability case one \\(Z\\) causes \\(Y\\), probability .005 one \\(Y\\) occurs regardless. clue appear types cases. Crucially, outcome, cause, clue observable researcher, type .\\(M\\) Model: posit population 195 cases, exhibit presence outcome, \\(Y \\\\{0,1\\}\\). sake illustration, suppose \\(Y\\) represents presence absence civil war. case also exhibits presence absence potential cause, \\(Z \\\\{0,1\\}\\). example, might suppose \\(Z\\) represents presence absence natural resources. random, 30% cases get \\(Z=1\\). also assume researchers observe clue, \\(C\\), informative whether \\(Z\\) causal relationship \\(Y\\).potential outcomes \\(Y\\) depend whether cause, \\(Z\\), present case “type” causal relation case exhibits. Conceptually, exactly four distinct types causal relations. First, presence \\(Z\\) might cause \\(Y\\): \\(Z = 0\\), \\(Y = 0\\) \\(Z = 1\\) \\(Y = 1\\). words, civil wars happen cases country natural resources. Second, absence \\(Z\\) might cause \\(Y\\): \\(Z = 0\\) \\(Y = 1\\) \\(Z = 1\\) \\(Y = 0\\). cases, civil war breaks country natural resources, break country natural resources. Finally, \\(Y\\) might present irrespective \\(Z\\) \\(Y\\) might absent irrespective \\(Z\\). Continuing analogy, countries civil war peace, irrespective whether also natural resources (.e., war related causal process). specify model civil war governed causal pathway 1 (\\(Z\\) causes \\(Y\\)) roughly 20% cases, pathway 2 (\\(\\neg Z\\) causes \\(Y\\)) 10% cases, pathway 3 (\\(Y\\) irrespective \\(z\\)) 20% countries, pathway 4 (\\(\\neg Y\\) irrespective \\(Z\\)) half countries.also potential outcomes clue, \\(C\\). depend case’s causal type. Specifically, clue appears .25 probability case one \\(Z\\) causes \\(Y\\), probability .005 one \\(Y\\) occurs regardless. clue appear types cases. Crucially, outcome, cause, clue observable researcher, type .\\(\\) Inquiry: wish know answer sample-specific “cause effects” question: given specific case sampled, probability \\(Z\\) caused \\(Y\\)? formally, want know \\(\\Pr(Y_i(Z_i=0)=0| Z_i=1, Y_i(Z_i=1)=1)\\)—, chances \\(Y\\) 0 \\(Z\\) 0 unit \\(\\) \\(Z\\) 1 \\(Y\\) 1. equivalent asking probability case type 1. inquiry thus takes value 0 1 depending type case.\\(\\) Inquiry: wish know answer sample-specific “cause effects” question: given specific case sampled, probability \\(Z\\) caused \\(Y\\)? formally, want know \\(\\Pr(Y_i(Z_i=0)=0| Z_i=1, Y_i(Z_i=1)=1)\\)—, chances \\(Y\\) 0 \\(Z\\) 0 unit \\(\\) \\(Z\\) 1 \\(Y\\) 1. equivalent asking probability case type 1. inquiry thus takes value 0 1 depending type case.\\(D\\) Data Strategy: fundamental problem researcher faces observational equivalence: different causal types can cause data patterns. issue mitigated following controversial sampling strategy: selecting \\(Y\\). selecting random one case \\(Z\\) \\(Y\\) present, researcher can narrow uncertainty two candidate types: second fourth causal types incapable producing data \\(Z = 1, Y = 1\\). natural resources cause peace (type 2), peace happened irrespective natural resources (type 4), country civil war natural resources.\\(D\\) Data Strategy: fundamental problem researcher faces observational equivalence: different causal types can cause data patterns. issue mitigated following controversial sampling strategy: selecting \\(Y\\). selecting random one case \\(Z\\) \\(Y\\) present, researcher can narrow uncertainty two candidate types: second fourth causal types incapable producing data \\(Z = 1, Y = 1\\). natural resources cause peace (type 2), peace happened irrespective natural resources (type 4), country civil war natural resources.\\(\\) Answer Strategy: researcher uses Bayes’ rule update probability \\(Z\\) caused \\(Y\\) given \\(C\\).\\(\\) Answer Strategy: researcher uses Bayes’ rule update probability \\(Z\\) caused \\(Y\\) given \\(C\\).","code":"\ntypes <- c('Z_caused_Y', 'Z_caused_not_Y', 'always_Y', 'always_not_Y')\n\ndesign <-\n  declare_population(\n    N = 195,\n    Z = draw_binary(prob = .3, N = N),\n    type = sample(x = types, size = N, replace = TRUE, \n                  prob = c(.2, .1, .2, .5)))  +\n  declare_potential_outcomes(\n    Y ~ Z * (type == \"Z_caused_Y\") + \n      (1 - Z) * (type == \"Z_caused_not_Y\") + \n      (type == \"always_Y\"),\n    conditions = list(Z = c(0, 1), type = types)) +\n  declare_potential_outcomes(\n    pr_C_1 ~ Z * (.25 * (type == \"Z_caused_Y\") + \n                    .005 * (type == \"always_Y\")),\n    conditions = list(Z = c(0, 1), type = types)) +\n  reveal_outcomes(c(Y, pr_C_1), c(Z, type)) +\n  declare_measurement(C = draw_binary(prob = pr_C_1)) +\n  declare_sampling(handler = function(data){\n    data %>%\n      filter(Z == 1 & Y == 1) %>%\n      sample_n(size = 1)\n    }) +\n  declare_estimand(did_Z_cause_Y = type == 'Z_caused_Y') +\n  declare_estimator(\n    pr_type_Z_caused_Y = 0.500,\n    pr_C_1_type_Z_caused_Y = 0.250,\n    pr_C_1_type_always_Y = 0.005,\n    pr_C_type_Z_caused_Y = \n      C * pr_C_1_type_Z_caused_Y + \n      (1 - C) * (1 - pr_C_1_type_Z_caused_Y),\n    pr_C_type_always_Y = \n      C * pr_C_1_type_always_Y + \n      (1 - C) * (1 - pr_C_1_type_always_Y),\n    posterior =\n      pr_type_Z_caused_Y * pr_C_type_Z_caused_Y / \n      (pr_type_Z_caused_Y * pr_C_type_Z_caused_Y + \n         pr_C_type_always_Y * (1 - pr_type_Z_caused_Y)),\n    estimator_label = \"Smoking Gun\",\n    estimand_label = \"did_Z_cause_Y\",\n    handler = summarize) "},{"path":"index.html","id":"dag-3","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"0.3.2 DAG","text":"","code":""},{"path":"index.html","id":"exercises","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"0.3.3 Exercises","text":"Inspect DAG. removing arrow pointing type C affect inferences researcher draw?Inspect DAG. removing arrow pointing type C affect inferences researcher draw?Run draw_data(design).\nInterpret value type variable.\nLook values Z Y. Explain types produce values .\nvariable pr_C_1_Z_1_type_Z_caused_Y indicates “potential outcome clue probability, given case type Z causes Y,” pr_C_1_Z_1_type_always_Y gives corresponding potential outcome cases whose causal type one Y always happens regardless Z. potential outcomes make clue smoking gun?\nExplain potential outcome Y_Z_0_type_always_Y takes value 1 whereas potential outcome Y_Z_0_type_Z_caused_Y takes value 0.\nRun draw_data(design).Interpret value type variable.Interpret value type variable.Look values Z Y. Explain types produce values .Look values Z Y. Explain types produce values .variable pr_C_1_Z_1_type_Z_caused_Y indicates “potential outcome clue probability, given case type Z causes Y,” pr_C_1_Z_1_type_always_Y gives corresponding potential outcome cases whose causal type one Y always happens regardless Z. potential outcomes make clue smoking gun?variable pr_C_1_Z_1_type_Z_caused_Y indicates “potential outcome clue probability, given case type Z causes Y,” pr_C_1_Z_1_type_always_Y gives corresponding potential outcome cases whose causal type one Y always happens regardless Z. potential outcomes make clue smoking gun?Explain potential outcome Y_Z_0_type_always_Y takes value 1 whereas potential outcome Y_Z_0_type_Z_caused_Y takes value 0.Explain potential outcome Y_Z_0_type_always_Y takes value 1 whereas potential outcome Y_Z_0_type_Z_caused_Y takes value 0.Using code , diagnose design interpret diagnosands.Using code , diagnose design interpret diagnosands.Look estimator declaration.\nprior beliefs pr_C_1_type_Z_caused_Y pr_C_1_type_always_Y represent?\nsetting beliefs value affect design, ?\nimplication clue selection Bayesian Process Tracing designs?\nLook estimator declaration.prior beliefs pr_C_1_type_Z_caused_Y pr_C_1_type_always_Y represent?prior beliefs pr_C_1_type_Z_caused_Y pr_C_1_type_always_Y represent?setting beliefs value affect design, ?setting beliefs value affect design, ?implication clue selection Bayesian Process Tracing designs?implication clue selection Bayesian Process Tracing designs?Declare new_design modify prior belief case belongs first causal type changing pr_type_Z_caused_Y different value (say, .7).\nDiagnose new_design. bias parameter change, ?\nKeeping modification just made, modify code new_design clue potential outcomes function pr_C_1 ~ Z * (.9999 * (type == \"Z_caused_Y\") + .0001 * (type == \"always_Y\" clue priors pr_C_1_type_Z_caused_Y = .9999 pr_C_1_type_always_Y = .0001. kind clue researcher now?\nDiagnose new_design. changes clue reduce bias?\nDeclare new_design modify prior belief case belongs first causal type changing pr_type_Z_caused_Y different value (say, .7).Diagnose new_design. bias parameter change, ?Diagnose new_design. bias parameter change, ?Keeping modification just made, modify code new_design clue potential outcomes function pr_C_1 ~ Z * (.9999 * (type == \"Z_caused_Y\") + .0001 * (type == \"always_Y\" clue priors pr_C_1_type_Z_caused_Y = .9999 pr_C_1_type_always_Y = .0001. kind clue researcher now?Keeping modification just made, modify code new_design clue potential outcomes function pr_C_1 ~ Z * (.9999 * (type == \"Z_caused_Y\") + .0001 * (type == \"always_Y\" clue priors pr_C_1_type_Z_caused_Y = .9999 pr_C_1_type_always_Y = .0001. kind clue researcher now?Diagnose new_design. changes clue reduce bias?Diagnose new_design. changes clue reduce bias?","code":"\ndiagnose_design(design,\n                diagnosands = declare_diagnosands(\n                  bias = mean(posterior - estimand),\n                  rmse = sqrt(mean((posterior - estimand) ^ 2)),\n                  mean_estimand = mean(estimand),\n                  mean_posterior = mean(posterior)), \n                sims = 1000)"},{"path":"index.html","id":"synthetic-controls","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"0.4 Synthetic controls","text":"","code":""},{"path":"index.html","id":"declaration-3","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"0.4.1 Declaration","text":"","code":"\ndesign <- \n  declare_population(\n    unit = add_level(N = 10, units = 1:N, X = rnorm(N, sd = 0.5)),\n    period = add_level(N = 3, time = 1:N, nest = FALSE),\n    unit_period = cross_levels(by = join(unit, period), U = rnorm(N))\n  ) + \n  declare_potential_outcomes(Y ~ X + 0.5 * as.numeric(period) + Z + U) + \n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0), subset = period == 3) + \n  declare_step(handler = mutate, Z = unit == \"01\") + \n  reveal_outcomes(Y = if_else(Z == 0 | period < 3, Y_Z_0, Y_Z_1), \n                  handler = mutate) +\n  declare_step(predictors = \"X\",\n               time.predictors.prior = 1:2,\n               dependent = \"Y\",\n               unit.variable = \"units\",\n               time.variable = \"time\",\n               treatment.identifier = 1,\n               controls.identifier = 2:10, \n               handler = synth_weights_tidy) +\n  declare_estimator(Y ~ Z, subset = time >= 3, weights = synth_weights, \n                    model = lm_robust, label = \"synth\")"},{"path":"index.html","id":"dag-4","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"0.4.2 DAG","text":"","code":""},{"path":"index.html","id":"example-2","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"0.4.3 Example","text":"","code":""},{"path":"index.html","id":"further-reading-1","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"0.4.4 Further reading","text":"","code":""},{"path":"preamble.html","id":"preamble","chapter":"1 Preamble","heading":"1 Preamble","text":"book, introduce way thinking research designs social sciences can make designs transparent robust. hope approach make designing research studies easier: making easier produce good designs, also easier share designs build designs others developed.get DAGs.\n– “identification strategy” way thinking focuses mix D , doesn’t separate , often clear inquiry (estimand whatever identification strategy shoots , e.g., LATE).\nget design theoryThe core idea book start think design object can interrogated. object four main characteristics: model, inquiry, data strategy, answer strategy. understand four also interrelate. design encodes beliefs world, describes questions, lays go answering questions, terms data use use . key idea features can provided code —done right—information provided enough estimate quality design simulation.think way thinking research design pays dividends multiple points research lifecycle: Choosing question, synthesizing previous answers, conducting ethics review, piloting parts empirical strategy, crafting preanalysis plan, implementing study, summarizing results, writing paper, publishing result, archiving materials, engaging critical scholarship.","code":""},{"path":"preamble.html","id":"how-to-read-this-book","chapter":"1 Preamble","heading":"1.1 How to read this book","text":"multiple audiences mind writing book. First, ’re thinking college seniors produce course research paper. students need framework thinking ways part research process fit together. ’re also thinking graduate students seminar courses main purpose course read papers discuss well theory matches empirics. MIDA framework introduced Part way structure tasks accommodates many different empirical approaches: qualitative quantitative, descriptive causal, observational experimental. 30 minutes person try get understand book , give Part .Part II involved. provide mathematical foundations MIDA framework. walk component research design detail: Models, Inquiries, Data strategies, Answer Strategies. describe finer points research design diagnosis carry “redesign.” imagine Part II assigned early graduate course research design social sciences. [non student audiences: Experimenters, applied researchers prospective empirical work. Theories interested using empirical information distinguish theoretical models. Many ideas resonate study “empirical implications theoretical models.”]Part III, apply general framework specific research designs. result library common research designs. Many empirical research designs included library, . set entries covers large portion see current empirical practice, meant exhaustive. thinking two kinds audiences design library. first researcher wants know specific research design operates watch . turn ``regression discontinuity’’ entry learn ten important things know design. 10 important things everything know design. , refer end entry set --date methodological treatments topic. entry help simulate design’s properties explore design tradeoffs implementing . second reader mind person studying entry learn specifics regression discontinuity design necessarily, instead learn become research designer. regression discontinuity entry instance research design general lessons research design can drawn. Readers want research designers read design library entirety.last section book describes detail framework can help step research process. sections book readable anyone read Part . entry preanalysis plans can assigned experiments course guidance students filing first PAP. entry research ethics shared among coauthors start project. entry writing research paper assigned college seniors trying finish essays time.","code":""},{"path":"preamble.html","id":"how-to-work-this-book","chapter":"1 Preamble","heading":"1.2 How to work this book","text":"many times throughout book, describe research designs just words, computer code.\n\nwant work code exercises, fantastic. path requires investment R, tidyverse DeclareDesign sets software packages. Chapter 3 helps get started. think way rewarding, understand learning curve. , course, tackle declaration, diagnosis, redesign processes using bespoke simulations computer language like,2 easier DeclareDesign.want nothing code, can skip code exercises just focus text. written book understanding code required order understand research design concepts.\nfree, online version book many extra ways engage material. can download section code can play interactive simulations. bells whistles necessary understanding, learn different ways.","code":""},{"path":"preamble.html","id":"what-this-book-will-not-do","chapter":"1 Preamble","heading":"1.3 What this book will not do","text":"book research design, statistics textbook. derive estimators, provide guarantees general optimality designs, present mathematical proofs. provide answers practical questions might design.offer, hope, language express research designs. can help learn language can describe design . can declare design language, can diagnose , improve redesign.– EITM","code":""},{"path":"improving-research-designs.html","id":"improving-research-designs","chapter":"2 Improving research designs","heading":"2 Improving research designs","text":"purpose conducting empirical research study learn answers questions , known, rule incorrect theories. theory incorrectly predicts average causal effect D Y positive, though good research design learn negative, ’ve made scientific progress. Ruling incorrect theories – even just incorrect portions otherwise correct theories – main goal empirical research.can rule theoretical models? First, need enumerate many models possible, least theory. exhaustive enumeration possible theoretical models daunting challenge. describe approach begins “kernel” – small portion theoretical model grow theoretical possibilities. Suppose list possibilities hand. research question – call “inquiry” – refers fact , known, rule models favor others. can show credible research design \\(Z\\) causes \\(Y\\), can rule models \\(Z\\) cause \\(Y\\). reason seek learn inquiries distinguish among theoretical possibilities.Research designs procedures learning inquiries. book offers language describing research designs algorithm selecting among . Remarkably, basic structure can used whether researchers interested causal questions descriptive questions, whether focused theory testing inductive learning, whether use quantitative, qualitative, mixed methods. language use talk computer can used talk others. Reviewers, advisors, students, funders, journalists, public – yes computer – need know four basic things understand design.","code":""},{"path":"improving-research-designs.html","id":"the-four-components-of-research-design","chapter":"2 Improving research designs","heading":"2.1 The four components of research design","text":"Empirical research designs share common model, inquiry, data strategy, answer strategy. four together, refer MIDA, represent suppositions world works choices make researcher intervene learn world.model background theory system study. includes beliefs causes . includes beliefs important variables distributed, things correlated, sequences events. course, know true model—research. stipulating sets alternative possible models can assess research design perform model fact correct. hope true, unknown causal model set models entertain can correctly imagine happen design applied real world.model defines set units, people neighborhoods social groups, wish study. Often, set units large population units afford take measurements , can nevertheless define make inferences sampling studying subset units. model includes set baseline characteristics describe unit probability distributions characteristic (.e., heights normally distributed, skew comes stunting infants). Finally, model includes set endogenous outcome variables may functions exogenous (pretreatment) characteristics effects interventions. endogenous outcome variable function defines variables affect values takes . outcome depends intervention, potential outcome, can define value outcome take unit received treatment outcome unit take receive treatment. Typically, endogenous outcome variables random variables, either function exogenous baseline variable defined probability distributions assignment treatment control random part data strategy.Defining model can feel like odd exercise. Since researchers presumably want learn model, declaring advance may seem beg question. confusion can allayed thinking model kind “reference model”, can assess performance. Declaring reference model often unavoidable declaring research designs. practice, already familiar researcher calculated power design, requires specification effect sizes. seeming arbitrariness declared model can mitigated assessing sensitivity diagnosis alternative models strategies (see Section 10). , researchers can inform reference models existing data, baseline surveys. Just power calculators focus attention minimum detectable effects, design declaration offers tool demonstrate design properties change depending researcher assumptions.model research question. research question – call inquiry – feature model want measure. theories rich, single model, may many possible inquiries researcher seek learn . model may complex dance ten interrelated variables, inquiry something like average causal effect single variable another, descriptive distribution third variable, prediction value single variable take time future. Importnat inquiries distinguish alternative models consideration.Inquiries defined respect units, conditions, features; summaries features units across conditions. Inquiries may causal, average treatment effect (ATE). ATE average difference outcome variable across two conditions: treatment condition control condition. Inquiries can descriptive, population average. may seem descritipive inquiries involve conditions, – population average defined respect conditions measure characteristics study units. -called Hawthorne effects occur act observation changes observed, underlining general point researchers inevitably learn specific potential outcomes congizant .Together, model data form theoretical half research design. second half empirical, two components mirror theoretical half.data strategy full set procedures use gather information world. includes three basic sets procedures: sampling, assignment, measurement. Sampling refers fact empirical strategy comprehensive – units sampled study units aren’t. Even research designs (like census, example) sampling strategy don’t sample respondents different years different countries. Assignment procedures describe researchers generate variation world. ask subjects one question, subjects different question, ’ve generated variation basis assignment procedure. think assignment procedures often randomized, randomized experiment, many kinds research designs engage assignment procedures randomized. Measurement procedures ways researchers reduce complex multidimensional social world relatively parsimonious set data. data need “quantitative data” sense numbers values pre-defined scale – qualitative data data . Measurement vexing necessary reduction reality choice representations. Measurement always carries possibility measurement error, reduction hard. Sampling, assignment, measurement deeply parallel three features inquiries: units, conditions, features.answer strategy summarize data data strategy produces. Just like inquiry summarizes part model, answer strategy summarizes data. Complex, multidimensional datasets don’t just speak – need summarized explained. Answer strategies functions take data return answers. research designs, literal function like lm_robust estimates ordinary least squares regression robust standard errors. research designs, function embodied researchers read documents summarize meanings case study.answer strategy choice estimator. includes full set procedures receiving dataset providing answer words, tables, graphs. includes data cleaning, data transformation, estimation, plotting, interpretation. choice OLS must defined, focus attention coefficient estimate Z variable assess uncertainty using confidence interval construct coefficient plot certain way visualize inference. answer strategy also includes -procedures researchers implicitly explicitly take depending initial results features data. stepwise regression procedure, answer strategy final regression results iterative model selection, procedure – answer reflect features change depending sampling variability. Just like values inquiry, values estimates result answer strategy probability distribution, result variables defined model (probability distributions) data strategy (sampling treatment assignment defined probability distributions).theoretical empirical halves research design go hand--hand complete description research design generally needs halves. ask, “’s research design?” respond “’s regression discontinuity design,” ’ve maybe learned answer strategy might , don’t yet enough information decide whether ’s strong design learn model, inquiry, whether data answer strategies indeed well suited .Declaring design entails separating parts design belong \\(M\\), \\(\\), \\(D\\), \\(\\). declaration process can challenge mapping ideas excitement project MIDA always straightforward. promise rewarding task. can express research design terms four components newly able think properties.","code":""},{"path":"improving-research-designs.html","id":"how-to-choose-mida","chapter":"2 Improving research designs","heading":"2.2 How to choose MIDA","text":"grow dag M-vectorpick inquiry distinguishes among many modelsensure parallelism across theoretical empirical halves","code":""},{"path":"improving-research-designs.html","id":"declare-diagnose-redesign","chapter":"2 Improving research designs","heading":"2.3 Declare-Diagnose-Redesign","text":"","code":""},{"path":"improving-research-designs.html","id":"declaring-a-design-in-code","chapter":"2 Improving research designs","heading":"2.3.1 Declaring a design in code","text":"can implement MIDA framework software package. Indeed, design declared writing mathematical notation diagnosed using analytical formula. discuss Section 10 analytical diagnoses challenging majority designs social sciences: account specific features research designs varying numbers units per cluster interaction choices made model, inquiry, data strategy, answer strategy.Social scientists use number tools conducting statistical analysis: Stata, R, Python, Julia, SPSS, SAS, Mathematica, . time writing, Stata R commonly used platforms academic work social scientists. wrote companion software book, DeclareDesign, R statistical environment availability useful tools R free, open-source, high-quality.illustrate application MIDA framework study motivations political office-seekers Pakistan. Gulzar Khan (2020) conducted experiment test whether prosocial personal benefits important motivations running office. randomly-assigned villages receive different encouragements run measured affected rate running office, types people chose run, congruence policy positions general population.model describes citizens villages conduct study, village live . gives description individual characteristics potential outcomes response possible treatment assignments. inquiry average treatment effect: difference potential outcomes receiving receiving encouragement run office. data strategy (1) random sampling 50 citizens per village; (2) random assignment villages either encouragement run office (treatment) encouragement (control); (3) measurement via posttreatment survey. answer strategy calculating difference--means, standard errors accounting clustering treatment assignment village.illustrate declaring design code, declare simplified version Gulzar Khan (2020) study–declare complete design Section 5.1.model, describe hierarchical structure 192 villages home 48 randomly sampled citizens eligible run elected office. citizen harbors three potential outcomes, Y_Z_neutral, Y_Z_personal, Y_Z_social. Y_Z_neutral citizen’s latent probability standing election treated neutral appeal; Y_Z_personal probability treated appeal emphasizes personal returns office, Y_Z_social probability treated appeal underlines benefits community . simplified model includes constant treatment effect 2 percentage points personal appeal 3 percentage points social appeal.inquiry average treatment effect population, defined average difference potential outcomes:data strategy consists four steps:assignment: randomly assign one third neutral condition, one third personal appeal, remaining third social appeal.reveal outcomes: Subjects reveal outcomes (.e., probability running office) according randomly-assigned condition.measurement: measure binary outcome Yobs unobserved, latent variable Y: units latent outcome 0.97 Yobs 1 0 otherwise.answer strategy consists difference--means \\(Y\\) depending \\(Z\\), standard errors clustered village level order respect clustering assignment data strategy. link estimator ATE estimand explicitly link answer strategy inquiry. principle one always try maintain—estimator estimand link .concatenate four aspects get design:designs declare book, also present graphical representation design. Figure 2.1, visualize simplified form Gulzar-Khan design.\nFigure 2.1: Simplified DAG Gulzar Khan study\ndesign declared, can learn number ways:can “diagnose” simulate properties via diagnose_design(design)can simulate data explore possible estimation strategies analyzing real data draw_data(design)can use implement study, instance implementing assignment real units,can obtain simulated estimates via draw_estimates(design), data comes ,can apply planned design data via get_estimates(design, data = study_data).introduce software detail next section.designed rest book can read even use R, translate code language choice. Web site, pointers might declare steps Stata, Python, Excel. addition, link “Design wizard” lets declare diagnose variations standard designs via web interface.","code":"\nmodel <-\n  declare_population(\n    villages = add_level(N = 192),\n    citizens = add_level(N = 48, U = runif(N))\n  ) +\n  declare_potential_outcomes(Y_Z_neutral = U,\n                             Y_Z_personal = Y_Z_neutral + 0.02,\n                             Y_Z_social = Y_Z_neutral + 0.03)\ninquiry <- declare_estimand(\n  ATE_personal = mean(Y_Z_personal - Y_Z_neutral),\n  ATE_social = mean(Y_Z_social - Y_Z_neutral)\n)\ndata_strategy <-\n  declare_assignment(\n    clusters = villages, \n    conditions = c(\"neutral\", \"personal\", \"social\")\n  ) + \n  reveal_outcomes(outcome_variables = Y, assignment_variables = Z) +\n  declare_measurement(Yobs = if_else(Y > 0.97, 1, 0))\nanswer_strategy <- \n  declare_estimator(Y ~ Z, term = c(\"Zpersonal\", \"Zsocial\"), \n                    clusters = villages, \n                    model = lm_robust,\n                    estimand = c(\"ATE_personal\", \"ATE_social\"))\ndesign <- model + inquiry + data_strategy + answer_strategy"},{"path":"improving-research-designs.html","id":"assessing-research-design-quality-design-diagnosis","chapter":"2 Improving research designs","heading":"2.3.2 Assessing research design quality: design diagnosis","text":"’ve declared design, can diagnose . Design diagnosis process simulating research design order understand range possible ways study turn . diagnosis stage define design properties desirable research setting. let computers simulations us imagining design choices influence sampling distributions —put mildly—cognitively demanding.Diagnosis opportunity write make study success. long time, researchers classified studies successful based statistical significance. Accordingly, statistical power (probability statistically significant result) front--mind statistic researchers set designing studies. learn pathologies relying statistical significance, learn features beyond power just , important. example, “credibility revolution” throughout social science trained laser-like focus bias. Studies coming new criticism lacking “strong identification,” usually implies data answer strategies lead biased answers depending incorrect model . Randomized experimentation promises unbiased answers inquiries, least data answer strategies implemented well.Design diagnosis relies two concepts, functions research designs. quantities researcher third party calculate respect design.first diagnostic statistic, summary statistic generated “run” design—, results given possible realization variables, given model data strategy. example, statistic \\(e\\) refers “difference estimated actual average treatment effect.” \\(e\\) statistics depends model (since ATE depends model’s assumptions potential outcomes). statistic \\(s = \\mathbb{1}(p \\leq 0.05)\\), interpreted “result considered statistically significant 5% level,” presupposes answer strategy reports \\(p\\)-value. Diagnostic statistics governed probability distributions arise model data generation, given model, may stochastic.Second, diagnosand summary distribution diagnostic statistic. example, bias average value \\(e\\) statistic power average value \\(s\\) statistic. diagnosands include things like root-mean-squared-error (RMSE), Type , Type II, Type M, Type S error rates.One especially important diagnosand “success rate,” average value “success” diagnostic statistic. researcher, get decide make study success. matters research scenario? statistical significance? , optimize design respect power. matters research setting answer correct sign ? diagnose frequently answer strategy yields answer sign inquiry. Diagnosis opportunity articulate make study success figure , simulation, often obtain success. Often, success multidimensional aggregation multiple diagnosands.diagnose design, first define set diagnosands (see Section 10), statistical properties design. case, select bias (difference estimate estimand, PATE); root mean-squared error; statistical power design.diagnose design, involves simulating design , calculate diagnosands based simulations data.\nTable 2.1: Diagnosis simplified Gulzar-Khan design.\n","code":"\ndiagnosands <- declare_diagnosands(\n  bias = mean(estimate - estimand),\n  rmse = sqrt(mean((estimate - estimand)^2)),\n  power = mean(p.value <= 0.05)\n)\ndiagnosis <- diagnose_design(design, diagnosands = diagnosands)"},{"path":"improving-research-designs.html","id":"redesign","chapter":"2 Improving research designs","heading":"2.3.3 Redesign","text":"subtitle book “Declaration, Diagnosis, Redesign” emphasize three important steps conceptualization research design. far, ’ve outlined first two points: declaration diagnosis. design declared, learned diagnose respect important diagnosands, last step redesign.Redesign entails playing design parameters understand implications important diagnosands. can mean variety things. Many diagnosands (power, RMSE) depend size study. can redesign study, varying “sample size” feature data strategy determine big needs achieve target diagnosand: 90% power, say, RMSE 0.02. also vary aspect answer strategy, say, covariates used adjust regression model. Sometimes changes data answer strategies interact: use better covariates increase precision estimates answer strategy, collect information part data strategy. redesign question now becomes, better collect pre-treatment information subjects money better spent increasing total number subjects? Finally, redesign sometimes means changing model. , sometimes want understand whether design yields right inferences even underlying data generating processes shift beneath feet. summary, redesign entails enumerating set possible designs given resource theoretical constraints picking best one.DeclareDesign, redesign() function replaces key inputs design form new design.","code":""},{"path":"improving-research-designs.html","id":"avoiding-declaration-and-diagnosis-pitfalls","chapter":"2 Improving research designs","heading":"2.3.4 Avoiding declaration and diagnosis pitfalls","text":"Designing high-quality research difficult comes many pitfalls, subset addressed MIDA framework. Others fail help entirely , cases, may even exacerbate . outline four concerns.first worry evaluative weight get placed essentially meaningless diagnoses. Given design declaration includes declarations conjectures world possible choose inputs design passes diagnostic test set . instance, simulation-based claim unbiasedness incorporates features design still good respect precise conditions simulation (contrast, analytic results, available, may extend general classes designs). Still worse, simulation parameters might selected properties. power analysis, instance, may useless implausible parameters chosen raise power artificially. framework may encourage honest declarations, nothing enforces . ever, garbage-, garbage-.Second, see risk research may get evaluated basis narrow, perhaps inappropriate set diagnosands. Statistical power often invoked key design feature – even well-powered studies biased away targets interest little theoretical use. appropriateness diagnosand depends purposes study. framework guide researchers critics appropriate set diagnosands evaluate design. advantage approach however choice diagnosands gets highlighted new diagnosands can generated response substantive concerns.Third, emphasis statistical properties design can obscure substantive importance question answered qualitative features design. similar concern raised, example Huber (2013), regarding ``identification revolution’’ focus identification risks crowding attention importance questions addressed. framework can help researchers determine whether particular design answers question well (), also nudges make sure questions defined clearly independently answer strategies. , however, help researchers choose good questions.Finally, see risk variation suitability design declaration different research strategies may taken evidence relative superiority different types research strategies. believe range strategies can declared diagnosed wider one might first think possible, strong designs might declarable, either ex ante ex post. advantage framework, hope, can help clarify strategy can completely declared. design declared, nondeclarability framework provides, cases urge caution drawing conclusions design quality.specification model, inquiry, data strategy, answer strategy captures analysis-relevant features design, describe substantive elements, theories derived interventions implemented. Yet many aspects design explicitly labeled features enter framework analytically relevant. example, logistical details data collection duration time treatment administered endline data collection enter model longer time data collection affects subjects’ recall treatment. However, information design declaration typically insufficient assess substantive elements, important separate part assessing quality research study.","code":""},{"path":"improving-research-designs.html","id":"putting-designs-to-use","chapter":"2 Improving research designs","heading":"2.4 Putting designs to use","text":"REWRITE preview research design lifecycleAdding design declaration stage workflow busy times may seem onerous. think though may easier seems first delivers multiple payoffs. think design declaration hard hard good reason, instance making clear features research question answer strategy clear thought .outline three phases scientific process MIDA declaration-diagnosis-redesign framework can assist study authors, readers, research funders.Making design choices. move towards increasing credibility research social sciences places premium considering alternative data strategies analysis strategies early stages research projects, reduces researcher discretion, importantly can improve quality final research design. nothing new idea determining features sampling estimation strategies ex ante, practice many designs finalized late research process, data collected. Frontloading design decisions difficult existing tools rudimentary often misleading, clear current practice features design must considered ex ante.provide framework identifying features affect assessment design’s properties, declaring designs diagnosing inferential quality, frontloading design decisions. Declaring design’s features code enables direct exploration alternative data analysis strategies using simulated data; evaluating alternative strategies diagnosis; exploring robustness chosen strategy alternative models. Researchers can undertake step study implementation data collection.Communicating design choices. Bias published results can arise many reasons. example, researchers may deliberately inadvertently select analysis strategies produce statistically significant results. Proposed solutions reduce kind bias focus various types preregistration analysis strategies researchers (Rennie 2004; Zarin Tse 2008; Casey, Glennerster, Miguel 2012; Nosek et al. 2015; Green Lin 2016). Study registries now operating numerous areas social science, including hosted American Economic Association, Evidence Governance Politics, Center Open Science. Bias may also arise reviewers basing publication recommendations statistical significance. Results-blind review processes introduced journals address form bias (e.g., Findley et al. 2016).However, effectiveness design registries results-blind review reducing scope either form publication bias depends clarity elements must included describe design. practice, registries rely checklists preanalysis plans exhibit great variation, ranging lists written hypotheses --results journal articles. view, solution problem lie ever--specific questionnaires, rather new way characterizing designs whose analytic features can diagnosed simulation.actions taken researchers described data strategy answer strategy; two features design clearly relevant elements preregistration document. order know design choices made ex ante arrived ex post, researchers need communicate data answer strategies unambiguously. However, assessing whether data answer strategies good requires specifying model inquiry. Design declaration can clarify researchers third parties aspects study need specified order meet standards effective preregistration.Declaration design code also enables final infrequently practiced step registration process, researcher ``reports reconciles’’ final planned analysis. Identifying whether features design diverge ex ante ex post declarations highlights deviations preanalysis plan. magnitude deviations determines whether results considered exploratory confirmatory. present, exercise requires review dozens pages text, differences (similarities) immediately clear even close readers. Reconciliation designs declared code can conducted automatically, comparing changes code (e.g., move use stratified sampling function simple random sampling) comparing key variables design sample sizes.Challenging Design Choices. independent replication results studies publication essential component shift toward credible science. Replication — whether verification, reanalysis original data, reproduction using fresh studies — provides incentives researchers clear transparent analysis strategies, can build confidence findings.3In addition rendering design transparent, design declaration can allow different approach re-analysis critique published research. standard practice replicators engaging reanalysis propose range alternative strategies assess robustness data-dependent estimates different analyses. problem approach , divergent results found, third parties clear grounds decide results believe. issue compounded fact , changing analysis strategy, replicators risk departing estimand original study, possibly providing different answers different questions. worst-case scenario, can difficult determine learned original study replication.coherent strategy facilitated design simulations use design declaration conduct “design replication.” design replication, scholar restates essential design characteristics learn study revealed, just original author reports revealed. procedure helps us understand conditions results study can believed. emphasizing abstract properties design, design replication provides grounds support alternative analyses basis original authors’ intentions basis degree divergence results. Conversely, provides authors grounds question claims made critics.","code":""},{"path":"improving-research-designs.html","id":"principles-of-good-research-design","chapter":"2 Improving research designs","heading":"2.5 Principles of good research design","text":"book develops set principles choosing good research designs.properties design measured diagnosands; important diagnosand ability distinguish among theories.two halves research design (theoretical empirical) aligned. , estimates obtained analysis empirical data targeted well-posed theoretical questions.Data strategies approximate random sampling units, potential outcomes, measurements.Deviations random sampling data strategy addressed answer strategy.","code":""},{"path":"improving-research-designs.html","id":"further-reading-2","chapter":"2 Improving research designs","heading":"2.6 Further Reading","text":"Brady Collier (2010)","code":""},{"path":"primer.html","id":"primer","chapter":"3 Software primer","heading":"3 Software primer","text":"chapter serves brief introduction DeclareDesign package R. DeclareDesign software implementation every step design-diagnose-redesign process. can course declare, diagnose, redesign design using nearly programming language, DeclareDesign structured make easy mix--match design elements handling tedious simulation bookkeeping behind scenes.","code":""},{"path":"primer.html","id":"installing-r","chapter":"3 Software primer","heading":"3.1 Installing R","text":"can download statistical computing environment R free CRAN. also recommend free program RStudio, provides friendly interface R. R RStudio available Windows, Mac, Linux.R RStudio installed, open install DeclareDesign related packages. include three packages enable specific steps research process (fabricatr simulating social science data; randomizr random sampling random assignment; estimatr design-based estimators). can also install DesignLibrary, gets standard designs --running one line. install , copy following code R console:also recommend install get know tidyverse suite packages data analysis, use throughout book:introductions R tidyverse especially recommend free resource R Data Science.","code":"\ninstall.packages(c(\n  \"DeclareDesign\",\n  \"fabricatr\",\n  \"randomizr\",\n  \"estimatr\",\n  \"DesignLibrary\"\n))\ninstall.packages(\"tidyverse\")"},{"path":"primer.html","id":"building-a-step-of-a-research-design","chapter":"3 Software primer","heading":"3.2 Building a step of a research design","text":"research design concatenation design steps. best way learn build design learn make step. start making—declaring—step implements random assignment.Almost steps take dataset input return dataset output. imagine input data describes set voters Los Angeles. research project planning involves randomly assigning voters receive (receive) knock door canvasser. data look like :\nTable 3.1: Example data\n100 voters dataset.want function takes dataset, implements random assignment, adds dataset, returns new dataset containing random assignment.write function can also use one declare_* functions DeclareDesign designed write functions. one functions kind function factory: takes set parameters research design like number units random assignment probability inputs, returns function output.\nexample declare_assignment step.big idea object created, simple_random_assignment_step, particular assignment, function conducts assignment called. can run function data:\nTable 3.2: Data output following implementation assignment step.\noutput simple_random_assignment_step(voter_file) call original dataset new column indicating treatment assignment (Z) appended. bonus, data also includes probability unit assigned condition (Z_cond), extremely useful number know many analysis settings. important thing understand steps “dataset-, dataset-” functions. simple_random_assignment_step took voter_file dataset returned dataset assignment information appended.Every step research design declaration can written using one declare_* functions. Table 3.3 collects according four elements research design. , walk common uses declaration functions.Table 3.3:  Declaration functions DeclareDesign","code":"\nsimple_random_assignment_step <- declare_assignment(prob = 0.6)\nsimple_random_assignment_step(voter_file) "},{"path":"primer.html","id":"options-and-defaults","chapter":"3 Software primer","heading":"3.2.1 Options and defaults","text":"declare_* functions many options. general, specify default values usually provided. instance, might noticed ran assignment step , new variable created called Z. declare_assignment argument assignment_variable defaults Z. can change course whatever want.subtly, declare_* functions also default “handlers” default arguments. handlers generally well-developed sets functions implement tasks needed declare_ function. instance, assignment_handler defaults conduct_ra function randomizr package. declaration passes additional arguments give conduct_ra, , token, assumes default values handler. example , prob = 0.6 argument. look documentation, prob argument declare_assignment argument conduct_ra, default value 0.5. left bit gotten function assigned treatment probability 0.5. software, learning defaults take time can looked help files, e.g. ?declare_assignment.","code":""},{"path":"primer.html","id":"your-own-handlers","chapter":"3 Software primer","heading":"3.2.2 Your own handlers","text":"built-functions provide DeclareDesign package quite flexible handle many major designs, . framework built never constrained provide. point, rather using default handlers (conduct_ra), can write function implements procedures. discipline framework imposes write procedure function takes data sends data back.example turn functions design steps.\nTable 3.4: Data generated using custom function\n","code":"\ncustom_assignment <- function(data) {\n  mutate(data, Z = rbinom(n = nrow(data), 1, prob = 0.5))\n}\n\nmy_assignment_step <- declare_assignment(handler = custom_assignment)\n\nmy_assignment_step(voter_file)  "},{"path":"primer.html","id":"research-design-steps","chapter":"3 Software primer","heading":"3.3 Research design steps","text":"section, walk declare step research design using DeclareDesign. next section, build steps research design, describe interrogate design.","code":""},{"path":"primer.html","id":"model","chapter":"3 Software primer","heading":"3.3.1 Model","text":"model defines structure world, size background characteristics well interventions world determine outcomes. DeclareDesign, split model two functions: declare_population declare_potential_outcomes.","code":""},{"path":"primer.html","id":"population","chapter":"3 Software primer","heading":"3.3.1.1 Population","text":"population defines number units population, multilevel structure data, background characteristics. can define population several ways. cases, may start design data population. happens, need simulate . can simply declare data population:\nTable 3.5: Draw fixed population\ncomplete data population, simulate . Relying data simulation functions fabricatr package, declare_population asks size variables population. instance, want function generates dataset 100 units random variable U write:run population function, get different 100-unit dataset time, shown Table 3.6.\nTable 3.6: Five draws population.\nfabricatr package can simulate many different types data, including various types categorical variables different types data structures, panel multilevel structures. can read fabricatr website vignette get started simulating data.example two-level hierarchical data structure, declaration 100 households random number individuals within household. two-level structure declared :always, can exit built-way things bring code. useful complex designs, already written code design want use directly. example custom population declaration:","code":"\ndeclare_population(data = voter_file)\ndeclare_population(N = 100, U = rnorm(N))\ndeclare_population(\n  households = add_level(\n    N = 100,\n    individuals_per_hh = sample(1:6, N, replace = TRUE)\n  ),\n  individuals = add_level(\n    N = individuals_per_hh, \n    age = sample(1:100, N, replace = TRUE)\n  )\n)\ncomplex_population_function <- function(data, N_units) {\n  data.frame(U = rnorm(N_units))\n}\n\ndeclare_population(\n  handler = complex_population_function, N_units = 100\n)"},{"path":"primer.html","id":"potential-outcomes","chapter":"3 Software primer","heading":"3.3.1.2 Potential outcomes","text":"Defining potential outcomes easy single expression per potential outcome. Potential outcomes may depend background characteristics, potential outcomes, R functions.\nTable 3.7: Adding potential outcomes population.\ndeclare_potential_outcomes function also includes alternative interface defining potential outcomes uses R’s formula syntax. formula syntax lets specify “regression-like” outcome equations. One downside mildly obscures names eventual potential outcomes columns named. build names potential outcomes columns outcome name (Y left-hand side formula) assignment_variables argument (Z).Either way creating potential outcomes works; one may easier harder code given research design setting.","code":"\ndeclare_potential_outcomes(\n  Y_Z_0 = U, \n  Y_Z_1 = Y_Z_0 + 0.25)\ndesign <- \n  declare_population(N = 100, U = rnorm(N)) +\n  declare_potential_outcomes(Y ~ 0.25 * Z + U)\n\ndraw_data(design)\ndeclare_potential_outcomes(Y ~ 0.25 * Z + U, assignment_variables = Z)"},{"path":"primer.html","id":"inquiry","chapter":"3 Software primer","heading":"3.3.2 Inquiry","text":"define inquiry, declare estimand. Estimands typically summaries data produced declare_population declare_potential_outcomes. define average treatment effect follows:Notice defined PATE (population average treatment effect), said nothing special related population – looks like just defined average treatment effect. order matters. want define SATE (sample average treatment effect), sampling occurred. see moment.","code":"\ndeclare_estimand(PATE = mean(Y_Z_1 - Y_Z_0))"},{"path":"primer.html","id":"data-strategy","chapter":"3 Software primer","heading":"3.3.3 Data strategy","text":"data strategy constitutes one steps representing interventions researcher makes world sampling assignment measurement.","code":""},{"path":"primer.html","id":"sampling","chapter":"3 Software primer","heading":"3.3.3.1 Sampling","text":"sampling step relies randomizr package conduct random sampling. See Section 8.1 overview many kinds sampling possible. define procedure drawing 50-unit sample population:draw data simple design point, fewer rows: shrunk 100 units population data frame 50 units representing sample. new data frame also includes variable indicating probability included sample. case, every unit population equal inclusion probability 0.5.\nTable 3.8: Sampled data.\nSampling also non-random, accomplished using custom handler.","code":"\ndeclare_sampling(n = 50)"},{"path":"primer.html","id":"assignment","chapter":"3 Software primer","heading":"3.3.3.2 Assignment","text":"default handler declare_assignment also relies randomizr package random assignment. , define assignment procedure allocates subjects treatment probability 0.5. One subtlety default, declare_assignment conducts complete random assignment (exactly \\(m\\) \\(N\\) units assigned treatment, \\(m\\) = prob * \\(N\\)).treatments assigned, potential outcomes revealed. Treated units reveal treated potential outcomes untreated units reveal untreated potential outcomes. reveal_outcomes function performs switching operation.Adding two declarations design results data frame additional indicator Z assignment well corresponding probability assignment. , assignment probabilities constant, designs described Section 8.2 crucial information analysis stage. outcome variable Y composed unit’s potential outcomes depending treatment status.\nTable 3.9: Sampled data assignment indicator.\n","code":"\ndeclare_assignment(prob = 0.5)\nreveal_outcomes(Y, Z)"},{"path":"primer.html","id":"measurement","chapter":"3 Software primer","heading":"3.3.3.3 Measurement","text":"Measurement critical part every research design; sometimes beneficial explicitly declare measurement procedures design, rather allowing implicit ways variables created declare_population declare_potential_outcomes. example, might imagine normally distributed outcome variable Y latent outcome translated binary outcome measured researcher:\nTable 3.10: Sampled data explicitly measured outcome.\n","code":"\ndeclare_measurement(Y_binary = rbinom(N, 1, prob = pnorm(Y)))"},{"path":"primer.html","id":"answer-strategy","chapter":"3 Software primer","heading":"3.3.4 Answer strategy","text":"model data strategy steps, simulated dataset two key inputs answer strategy: assignment variable outcome. answer strategies, pretreatment characteristics model might also relevant. data look like :\nTable 3.11: Data revealed outcomes.\nestimator difference--means estimator, compares outcomes group assigned treatment assigned control. difference_in_means() function estimatr package calculates estimate, standard error, \\(p\\)-value confidence interval :\nTable 3.12: Difference--means estimate simulated data.\nNow, order declare estimator, can send name modeling function declare_estimator. R many modeling functions work declare_estimator, including lm, glm, ictreg function list package, among hundreds others. Throughout book, using many estimators estimatr fast calculate robust standard errors easily. Estimators (almost always) associated estimands.4 , targeting population average treatment effect difference--means estimator.","code":"\ndifference_in_means(Y ~ Z, data = simple_design_data)\ndeclare_estimator(\n  Y ~ Z, model = difference_in_means, estimand = \"PATE\"\n)"},{"path":"primer.html","id":"two-finer-points-model_summary-and-label_estimator","chapter":"3 Software primer","heading":"3.3.4.1 Two finer points: model_summary and label_estimator","text":"Many answer strategies use modeling functions like lm, lm_robust, glm. output modeling functions typically complicated list objects contain large amounts information modeling process. typically want summary pieces information model objects, like coefficient estimates, standard errors, confidence intervals. use model summary functions passed model_summary argument declare_estimator . Model summary functions take models inputs return data frames outputs.default model summary function tidy:also use glance get model fit statistics like \\(R^2\\).Occasionally, ’ll need write model summary function takes model fit object returns data.frame information need. example, order calculate average marginal effects estimates logistic regression, run glm model margins function margins package; need “tidy” output margins using tidy function. ’re also asking 95% confidence interval.answer strategy use model function, ’ll need provide function takes data input returns data.frame estimate. Set handler label_estimator(your_function_name) take advantage DeclareDesign’s mechanism matching estimands estimators. use label_estimator, can provide estimand, DeclareDesign keep track estimates match estimand. example, calculate mean outcome, write estimator way:","code":"\ndeclare_estimator(\n  Y ~ Z, model = lm_robust, model_summary = tidy\n)\ndeclare_estimator(\n  Y ~ Z, model = lm_robust, model_summary = glance\n)\ntidy_margins <- function(x) {\n  tidy(margins(x, data = x$data), conf.int = TRUE)\n}\n\ndeclare_estimator(\n  Y ~ Z + X,\n  model = glm,\n  family = binomial(\"logit\"),\n  model_summary = tidy_margins,\n  term = \"Z\"\n) \nmy_estimator <- function(data){\n  data.frame(estimate = mean(data$Y))\n}\ndeclare_estimator(handler = label_estimator(my_estimator), label = \"mean\", estimand = \"Y_bar\")## declare_estimator(estimand = \"Y_bar\", handler = label_estimator(my_estimator), \n##     label = \"mean\")"},{"path":"primer.html","id":"other-design-steps","chapter":"3 Software primer","heading":"3.3.5 Other design steps","text":"main declare_* functions cover many elements research designs, . can include operations haven’t explicitly included steps design , using declare_step. , must define specific handler. handlers may useful dplyr verbs mutate summarize, fabricate function fabricatr package.add variable using fabricate:district-month data may want analyze district level, collapsing across months:","code":"\ndeclare_step(handler = fabricate, added_variable = rnorm(N))\ncollapse_data <- function(data, collapse_by) {\n  data %>% \n    group_by({{ collapse_by }}) %>% \n    summarize_all(mean, na.rm = TRUE)\n}\n\ndeclare_step(handler = collapse_data, collapse_by = district)\n\n# Note: The `{{ }}` syntax is handy for writing functions in `dplyr` \n# where you want to be able to reuse the function with different variable \n# names. Here, the `collapse_data` function will `group_by` the \n# variable you send to the argument `collapse_by`, which in our \n# declaration we set to `district`. The pipeline within the function \n# then calculates the mean in each district."},{"path":"primer.html","id":"building-a-design-from-design-steps","chapter":"3 Software primer","heading":"3.4 Building a design from design steps","text":"last section, defined set individual research steps. draw one version together :construct research design object can operate — diagnose , redesign , draw data , etc. — add together + operator, just %>% makes dplyr pipelines + creates ggplot objects.usually declare designs compactly, concatenating steps directly +:","code":"\npopulation <- \n  declare_population(N = 100, U = rnorm(N)) \n\npotential_outcomes <- \n  declare_potential_outcomes(Y ~ 0.25 * Z + U) \n\nestimand <- \n  declare_estimand(PATE = mean(Y_Z_1 - Y_Z_0)) \n\nsampling <- \n  declare_sampling(n = 50) \n\nassignment <- \n  declare_assignment(prob = 0.5) \n\nreveal <- \n  reveal_outcomes(outcome_variables = Y, assignment_variables = Z) \n\nestimator <- \n  declare_estimator(\n    Y ~ Z, model = difference_in_means, estimand = \"PATE\"\n  )\ndesign <- \n  population + potential_outcomes + estimand + \n  sampling + assignment + reveal + estimator\ndesign <- \n  declare_population(N = 100, U = rnorm(N)) +\n  declare_potential_outcomes(Y ~ 0.25 * Z + U) +\n  declare_estimand(PATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_sampling(n = 50) +\n  declare_assignment(prob = 0.5) +\n  reveal_outcomes(outcome_variables = Y, assignment_variables = Z) +\n  declare_estimator(\n    Y ~ Z, model = difference_in_means, estimand = \"PATE\"\n  )"},{"path":"primer.html","id":"order-matters","chapter":"3 Software primer","heading":"3.4.1 Order matters","text":"defining design, order steps included design via + operator matters. Think order design temporal order steps take place. , since estimand comes sampling assignment, population estimand, population average treatment effect.define estimand sample average treatment effect putting estimand sampling:","code":"\npopulation + potential_outcomes + estimand + \n  sampling + assignment + reveal + estimator\npopulation + potential_outcomes + sampling +\n  estimand + assignment + reveal + estimator"},{"path":"primer.html","id":"simulating-a-research-design","chapter":"3 Software primer","heading":"3.5 Simulating a research design","text":"Diagnosing research design — learning properties — requires first simulating running design . need simulate data generating process, calculate estimands, calculate resulting estimates.design defined object, can learn kind data generates, values estimand estimates, features. example, draw simulated data based design, use draw_data:\nTable 3.13: Simulated data draw.\ndraw_data runs “data steps” design, model (population potential outcomes) data strategy (sampling, assignment, measurement).simulate estimands single run design, use draw_estimands. runs two operations : draws data, calculates estimands point defined design. example, design, estimand comes just potential outcomes. design, draw_estimands run first two steps calculate estimands estimand function declared:\nTable 3.14: Estimands calculated simulated data.\nSimilarly, can draw estimates single run draw_estimates simulates data , appropriate moment, calculates estimates.\nTable 3.15: Estimates calculated simulated data.\nsimulate designs, use simulate_design function draw data, calculate estimands estimates, repeat process .\nTable 3.16: Simulations data frame.\n","code":"\ndraw_data(design)\ndraw_estimands(design)\ndraw_estimates(design)\nsimulate_design(design)"},{"path":"primer.html","id":"diagnosing-a-research-design","chapter":"3 Software primer","heading":"3.6 Diagnosing a research design","text":"Using simulations data frame, can calculate diagnosands like bias, root mean-squared-error, power estimator-estimand pair. DeclareDesign, two steps. First, declare diagnosands, functions summarize simulations data. software includes many pre-coded diagnosands (see Section 10), though can write like :Second, apply diagnosand declaration simulations data frame diagnose_design function:\nTable 3.17: Design diagnosis.\ncan also single step sending diagnose_design design object. function first run simulations , calculate diagnosands simulation data frame results.","code":"\nstudy_diagnosands <- declare_diagnosands(\n  bias = mean(estimate - estimand),\n  rmse = sqrt(mean((estimate - estimand)^2)),\n  power = mean(p.value <= 0.05)\n)\ndiagnose_design(simulation_df, diagnosands = study_diagnosands)\ndiagnose_design(design, diagnosands = study_diagnosands)"},{"path":"primer.html","id":"redesign-1","chapter":"3 Software primer","heading":"3.6.1 Redesign","text":"declaration phase, often want learn diagnosands change design features change. can using redesign:alternative way write “designer.” designer function makes designs based design parameters. Designer help researchers flexibly explore design variations. ’s simple designer based running example:create single design, based original parameters 100-unit sample size treatment effect 0.25, can run:Now simulate multiple designs, can use DeclareDesign function expand_design. examine simple design several possible sample sizes, might want conduct minimum power analysis. hold effect size constant.simulation diagnosis tools can take list designs simulate , creating column called design_label keep track. example:","code":"\nredesign(design, N = c(100, 200, 300, 400, 500))\nsimple_designer <- function(sample_size, effect_size) {\n  declare_population(N = sample_size, U = rnorm(N)) +\n    declare_potential_outcomes(Y ~ effect_size * Z + U) +\n    declare_estimand(PATE = mean(Y_Z_1 - Y_Z_0)) +\n    declare_sampling(n = 50) +\n    declare_assignment(prob = 0.5) +\n    reveal_outcomes(outcome_variables = Y, assignment_variables = Z) +\n    declare_estimator(\n      Y ~ Z, model = difference_in_means, estimand = \"PATE\"\n    )\n}\ndesign <- simple_designer(sample_size = 100, effect_size = 0.25)\ndesigns <- expand_design(\n  simple_designer, \n  sample_size = c(100, 500, 1000), \n  effect_size = 0.25\n)\ndiagnose_design(designs)"},{"path":"primer.html","id":"comparing-designs","chapter":"3 Software primer","heading":"3.6.2 Comparing designs","text":"Alternatively, can compare pair designs directly compare_designs function. function useful comparing differences planned design implemented design (see Section 26.5).Similarly, can compare two designs basis diagnoses:","code":"\ncompare_designs(planned_design, implemented_design)\ncompare_diagnoses(planned_design, implemented_design)"},{"path":"primer.html","id":"library-of-designs","chapter":"3 Software primer","heading":"3.6.3 Library of designs","text":"DesignLibrary package, created set common designs designers (functions create designs just parameters), can get started quickly.","code":"\nlibrary(DesignLibrary)\n\nb_c_design <- block_cluster_two_arm_designer(N = 1000, N_blocks = 10)"},{"path":"primer.html","id":"further-reading-3","chapter":"3 Software primer","heading":"3.7 Further Reading","text":"primer includes everything need know read code book. much detail help, recommend following resources.DeclareDesign.orgrandomizr cheatsheetestimatr cheatsheetDeclareDesign cheatsheetR Data ScienceRStudio R primersComputational social science bootcamp","code":""},{"path":"part-i-exercises.html","id":"part-i-exercises","chapter":"4 Part I Exercises","heading":"4 Part I Exercises","text":"completed.","code":""},{"path":"part-i-exercises.html","id":"question-1","chapter":"4 Part I Exercises","heading":"4.1 Question 1","text":"Read abstract paper:Becker, Sascha O., Irena Grosfeld, Pauline Grosjean, Nico Voigtländer, Ekaterina Zhuravskaya. 2020. “Forced Migration Human Capital: Evidence Post-WWII Population Transfers.” American Economic Review, 110 (5): 1430-63.study long-run effects forced migration investment education. World War II, millions Poles forcibly uprooted Kresy territories eastern Poland resettled ( primarily) newly acquired Western Territories, Germans expelled. combine historical censuses newly collected survey data show , pre-WWII differences educational attainment, Poles family history forced migration significantly educated today Poles. results driven shift preferences away material possessions toward investment human capital.can tell basis abstract alone, paper’s model, inquiry, data strategy, answer strategy words? information need know complete declaration design?can tell basis abstract alone, paper’s model, inquiry, data strategy, answer strategy words? information need know complete declaration design?Now download paper highlight four colors parts paper contain model, inquiry, data strategy, answer strategy. example look like, see Chapter ??.Now download paper highlight four colors parts paper contain model, inquiry, data strategy, answer strategy. example look like, see Chapter ??.Using highlighted portions, fill missing details part () complete declaration paper’s design words. still remaining details need know declare design?Using highlighted portions, fill missing details part () complete declaration paper’s design words. still remaining details need know declare design?","code":""},{"path":"part-i-exercises.html","id":"question-2","chapter":"4 Part I Exercises","heading":"4.2 Question 2","text":"reading Chapter 3, initialize following design:value estimand, ATE, one run design? Use draw_estimands(design) calculate.value estimand, ATE, one run design? Use draw_estimands(design) calculate.Draw data design using draw_data(design) run regression Y outcome Z X predictors. estimated average treatment effect Z Y based regression?Draw data design using draw_data(design) run regression Y outcome Z X predictors. estimated average treatment effect Z Y based regression?Add estimator using regression design, link ATE estimand using code:Add estimator using regression design, link ATE estimand using code:Diagnose design running diagnose_design(design, sims = 500). bias design? statistical power?","code":"\ndesign <- \n  declare_population(N = 100, X = rnorm(N), U = rnorm(N)) +\n  declare_potential_outcomes(Y ~ 0.25 * Z + X + U) + \n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + \n  declare_assignment(prob = 0.5) + \n  reveal_outcomes(outcome_variables = Y, assignment_variables = Z) \ndeclare_estimator(Y ~ Z + X, model = lm_robust, estimand = \"ATE\")## declare_estimator(Y ~ Z + X, model = lm_robust, estimand = \"ATE\")"},{"path":"declaration-4.html","id":"declaration-4","chapter":"5 Declaration","heading":"5 Declaration","text":"Chapter 2, gave high-level overview framework describing research designs terms models, inquiries, data strategies, answer strategies, process diagnosing properties, general purpose approach improving better fit research tasks. Now chapter, place approach firmer formal footing. , employ elements Pearl’s (2009) approach causal modeling (directed acyclic graphs, DAGs short), provides syntax mapping design inputs design outputs. also use potential outcomes framework presented, example, Imbens Rubin (2015), many social scientists use clarify inferential targets.goal formalization formalization’s sake. Describing research design DAG helps us see fundamental symmetries across theoretical (M ) empirical (D ) halves research design. recurring theme book research designs tend stronger relationship M mirrored relationship D ; aim chapter make somewhat mystical claim concrete.define research design \\(\\Delta\\), four elements \\(<M,,D,>\\). Describing research design entails “declaring” four elements.\\(M\\) reference model, set reference models, world works. Following Pearl’s definition probabilistic causal model, model \\(M\\) contains three core elements. first specification variables \\(X\\) research conducted. includes endogenous exogenous variables (\\(V\\) \\(U\\) respectively) ranges variables. formal literature, sometimes called signature model (Halpern 2000). second element (\\(F\\)) specification endogenous variable depends variables. can considered functional relations , Imbens Rubin (2015), potential outcomes. third final element probability distribution exogenous variables, written \\(P(U)\\). Sometimes useful think draws \\(U\\) implying distinct models , case might think \\(M\\) family models particular model \\(m\\) element \\(M\\).inquiry \\(\\) summary variables \\(X\\), perhaps given interventions variables. inquiry might average value outcome \\(Y\\): \\(\\mathbb{E}[Y] = \\sum\\left({y\\times Pr(Y=y)}\\right)\\), average value outcome conditional value treatment \\(Z\\): \\(\\mathbb{E}[Y|Z=1] = \\sum\\left({y\\times Pr(Y=y|Z=1)}\\right)\\). Using Pearl’s notation can distinguish descriptive inquiries causal inquiries. Causal inquiries summarize distributions arise interventions, indicated \\(()\\) operator, e.g., \\(\\Pr(Y | (Z = 1))\\). Descriptive inquiries summarize distributions arise without intervention, \\(\\Pr(Y | Z =1)\\).let \\(^m\\) denote answer \\(\\) model. Conditional model, \\(^m\\) value estimand, quantity researcher wants learn . connection \\(^m\\) model given : \\(^m = (m)\\).saying goes, models wrong may perhaps useful. denote true causal model world \\(W\\) realized world \\(w\\) draw true causal model—reference model particular case. true answer, , \\(^w = (w)\\). answer reference model \\(^m\\) may close far true value \\(^w\\), say wrong. model \\(m\\) far \\(w\\), course \\(^m\\) need correct. note moreover \\(^w\\) might even undefined, since inquiries can stated terms theoretical models. theoretical model wrong enough—instance conditioning events fact arise—inquiry might nonsensical applied real world.data strategy, \\(D\\), generates data \\(d\\). Data \\(d\\) arises, model \\(M\\) probability \\(P_M(d|D)\\). data strategy includes sampling strategies assignment strategies, denote \\(P_S\\) \\(P_Z\\) respectively. Measurement techniques also part data strategies can thought selection observable variables carry information unobservable variables. data strategy operates \\(w\\) produce observed data: \\(D(w) = d\\).\\(M\\) reflects beliefs true underlying causal processes work \\(D\\) procedure results collection creation data. light, phrase “data generating process” imprecise. Scholars usually use phrase “true data generating process” refer \\(M\\), even though \\(M\\) doesn’t create data \\(D\\) applied real world.answer strategy, \\(\\), generates answer \\(^d\\) using data \\(d\\). encode relationship \\((d) = ^d\\).full set causal relationships \\(M\\), \\(\\), \\(D\\), \\(\\) respect \\(m\\), \\(^m\\), \\(d\\), \\(^d\\), \\(w\\), \\(^w\\) can seen DAG’s schematic representation research design.\nFigure 5.1: MIDA DAG\nFigure 5.1 illustrates research design correspondence \\((m) = ^m\\) \\((d) = ^d\\). theoretical half research design produces answer inquiry theory. empirical half research design produces empirical estimate answer inquiry.Neither answer necessarily close truth \\(^w\\), course. , shown figure, truth directly accessible either us theory empirics. gamble empirical research, however, theoretical models close enough truth: truth like set models imagine.Table 5.1:  Elements research design.discussion, see striking analogy \\(M\\), \\(\\) relationship \\(D\\), \\(\\) relationship. answer aim gotten applying \\(\\) draw \\(M\\). answer access gotten applying \\(\\) draw \\(D\\). hope, usually, two answers . cases, suggests \\(\\) “like” \\(\\): instance, interested mean population access random sample, data available us \\(D\\) like ideal data observe nodes edges \\(M\\) directly. indeed taking mean data likely good strategy. principle hold general matter. instance, data sampled non-uniform probabilities, simple mean observed data likely good strategy calculate average population. Rather, focus extent two sets answers line .","code":""},{"path":"declaration-4.html","id":"p2gulzarkhanrevisited","chapter":"5 Declaration","heading":"5.1 Example","text":"section, declare Gulzar Khan (2020) example finer detail. aim capture analytically-relevant features design. Chapter 2, declared words code simplified version design, order communicate key features. make move throughout book: present simplified versions canonical designs text, also provide worked examples details specific studies. Moving back forth two , hope, enable learn general principles declare designs always involve fine details research implementation.display DAG design Figure 5.2.\nFigure 5.2: DAG Gulzar-Khan design.\n","code":""},{"path":"declaration-4.html","id":"model-1","chapter":"5 Declaration","heading":"5.1.1 Model","text":"model study describes set units: citizens living two Pakistani districts study took place , Haripur Abbottabad, comprises 300 villages. Approximately 6500 citizens eligible run elected office live village. main outcome interest whether citizen filed papers run office. outcome depends level three treatments. neutral appeal simply informs potential candidates run; two emphasize either social personal benefits holding public office. expect neutral appeal effects social personal appeals additional effect neutral appeal.","code":"\nmodel <-\n  declare_population(\n    districts = add_level(\n      N = 2,\n      district_name = c(\"Haripur\", \"Abbottabad\"),\n      N_villages = c(311, 359)\n    ),\n    villages = add_level(N = N_villages),\n    citizens = add_level(N = 6500,\n                         U = runif(N))\n  ) +\n  declare_potential_outcomes(\n    Y_Z_control = U,\n    Y_Z_neutral = U + 0.02,\n    Y_Z_personal = U + 0.04,\n    Y_Z_social = U + 0.05\n  )"},{"path":"declaration-4.html","id":"inquiry-1","chapter":"5 Declaration","heading":"5.1.2 Inquiry","text":"model supports many inquiries, ’ll focus two: average treatment effect appeal versus pure control condition. inquiry theoretically important refers mere act inviting citizens stand office. second inquiry subtle: average difference effectiveness appeals run focus pro-social reasons stand office versus appeals emphasize personal returns.","code":"\ninquiry <- \n  declare_estimand(\n    ATE = mean((Y_Z_neutral + Y_Z_personal + Y_Z_social) / 3 -\n                 Y_Z_control),\n    ATE_social_vs_personal = mean(Y_Z_social - Y_Z_personal)\n  ) "},{"path":"declaration-4.html","id":"data-strategy-1","chapter":"5 Declaration","heading":"5.1.3 Data strategy","text":"data strategy entailed three steps. First, 192 670 villages selected, via cluster sampling, serve treatment villages. 48 assigned neutral appeal, 72 social appeal, 72 personal appeal. 478 nonsampled villages receive treatment, serve pure control. Second, 48 citizens treated village sampled via random walk, receive candidacy appeals. Third, measurement procedure study obtain, via administrative record, names candidates stood office. way, outcome observed subjects experiment, sampled .","code":"\ndata_strategy <- \n  declare_assignment(\n    m_each = c(478, 48, 72, 72),\n    clusters = villages,\n    conditions = c(\"control\", \"neutral\", \"social\", \"personal\")\n  ) + \n  reveal_outcomes(Y, Z) +\n  declare_step(any_treatment = if_else(Z == \"control\", 0, 1),\n               handler = fabricate) +\n  declare_sampling(strata = villages, \n                   n_unit = if_else(Z == \"control\", 0, 48), \n                   drop_nonsampled = FALSE) "},{"path":"declaration-4.html","id":"answer-strategy-1","chapter":"5 Declaration","heading":"5.1.4 Answer strategy","text":"Gulzar Khan (2020) design supports many alternative answer strategies. , focus two answer strategies particular. First, compare rate running untreated villages rate among sampled units treatment villages, using OLS regression standard errors clustered village level.5 Second, compare rates running personal social conditions among sampled units , using OLS regression standard errors clustered village level.elements hand, can declare full design display one draw design:print first six rows simulated data Table 5.2:\nTable 5.2: Five rows simulated data Gulzar-Khan design.\ndata, present simulated estimates two inquiries well simulated estimand values Table 5.3:\nTable 5.3: Simulated estimates estimands one run Gulzar-Khan design.\nTable 5.4, display number people villages treatment assignment sampling condition illustrate data strategy:\nTable 5.4: Count units treatment assignment sampling status.\n","code":"\nanswer_strategy <- \n  declare_estimator(\n    Y ~ any_treatment,\n    clusters = villages,\n    model = lm_robust,\n    se_type = \"CR0\",\n    subset = (Z == \"control\" | S == 1),\n    estimand = \"ATE\",\n    label = \"ATE\"\n  ) +\n  declare_estimator(\n    Y ~ Z,\n    clusters = villages,\n    model = lm_robust,\n    se_type = \"CR0\",\n    subset = (S == 1 & Z != \"neutral\"),\n    estimand = \"ATE_social_vs_personal\",\n    label = \"ATE_social_vs_personal\"\n  )\ndesign <- \n  model + inquiry + data_strategy + answer_strategy\ndat <- draw_data(design)\nestimates <- run_design(design)"},{"path":"declaration-4.html","id":"further-reading-4","chapter":"5 Declaration","heading":"5.2 Further reading","text":"Imbens Rubin (2015) potential outcomesHalpern (2000) causal models","code":""},{"path":"specifying-the-model.html","id":"specifying-the-model","chapter":"6 Specifying the model","heading":"6 Specifying the model","text":"research design – whether research question fundamentally causal descriptive – implicitly relies models. Models can enter multiple stages, example guiding research question analysis stage. focus role models background understanding world works, performance design can assessed. call “reference models” (distinguished analytic models later). role provide stipulation world works—instance, variables important interrelate—allows us ask questions : reference model true, answer question able figure answer?\\(M\\) MIDA refers reference models. describe often think \\(M\\) set models, typical element \\(m\\), though cases \\(M\\) may just single model.Critically, whether design good bad depends reference model. data analysis strategy might fare well one model world poorly another. Thus get point can assess design need make reference model—many cases, family reference models—explicit. chapter go difficult task.’ll make use two different formal languages describing causal models: DAGs potential outcomes.potential outcomes formalization emphasizes counterfactual notion causality. \\(Y_i(Z = 0)\\) outcome unit \\(\\) occur causal variable \\(Z\\) equal zero \\(Y_i(Z = 1)\\) outcome occur \\(Z\\) set one. difference defines effect treatment outcome unit \\(\\). Since one potential outcome can ever revealed, least one two potential outcomes necessarily counterfactual. Usually, potential outcomes notation \\(Y_i(Z)\\) reports outcomes depend one feature, \\(Z\\), ignoring determinants outcomes. say don’t matter——just focus. sense, contained subscript \\(\\) since units carry relevant features \\(Z\\). can generalize settings want focus one cause, case use expressions form \\(Y(0,0)\\) \\(Y(0,1)\\).graphical model formalization makes use DAGs “directed acyclic graphs” characterize causal relations. node graph variable edges connect represent possible causal effects. arrow “parent” node “child” node indicates value parent sometimes determines outcome child; formally: parent’s value argument functional equation determining child’s outcome. Though consistent counterfactual notion causality, DAGs emphasize mechanistic accounts: exposure variable changes, outcome variable changes result—though possibly different ways different units. DAGs nonparametric. means encode beliefs full causal model. don’t show variables related, just related.Defining reference model requires defining set variables (meanings ranges) stipulating relate . potential outcomes framework, captured differences potential outcomes different conditions. graphical modeling literature, define set nonparametric structural causal relationships, indicating variables depend variables. variables hand, can visualize model using directed acyclic graph (DAG).Despite may inferred sometimes heated disagreements scholars prefer one formalization , DAGs potential outcomes compatible systems thinking causality. use language causal graphs use language potential outcomes. choose use languages useful expressing different facets research design. use DAGs describe web causal interrelations concise way (writing potential outcomes every relationship model tedious). use potential outcomes want zoom particular causal relationships make fine distinctions inquiries apply different sets units (’s difficult describe effect heterogeneity graphical models).illustrate ideas using simple DAG describe model abstract research design collect information \\(N\\) units. assign treatment \\(Z\\) random, collect posttreatment outcome \\(Y\\). know determinants outcome beyond \\(Z\\), don’t need write beliefs main inquiry average treatment effect \\(Z\\) \\(Y\\). ’ll say determinants causally related \\(Y\\), \\(Z\\), since \\(Z\\) randomly assigned us.nonparametric structural equation determining \\(Y\\) can written like :\\[\\begin{align*}\nY &= f_Y(Z, U)\n\\end{align*}\\]parametric structural equation can written like :\\[\\begin{align*}\nY &= Z + U\n\\end{align*}\\]Equivalently, potential outcomes (\\(Y\\)) might written, \\(\\\\{1,2,\\dots, n\\}\\) :\\[\\begin{align*}\nY_i(0) &= u_i  \\\\\nY_i(1) &= 1 + u_i \n\\end{align*}\\]DAG encodes model graphical form:\nFigure 6.1: Simple DAG. U unobserved.\n","code":""},{"path":"specifying-the-model.html","id":"visualizing-research-design-elements","chapter":"6 Specifying the model","heading":"6.1 Visualizing research design elements","text":"visual display DAGs typically includes nodes connected edgesedges point nodes, edges. frameworks visualizing parametric causal models encode presence interaction edge pointing another edge, use approach .edges point nodes, edges. frameworks visualizing parametric causal models encode presence interaction edge pointing another edge, use approach .Pearl (pg. XXX) denotes conditioning variable answer stragegy box around variable; follow convention.Pearl (pg. XXX) denotes conditioning variable answer stragegy box around variable; follow convention.CITE introduces intervention nodes – framework, variables manipulated researcher (random assignement) marked []. spirit, introduce shapes three kinds operations data strategy: circles random assignments, triangles sampling, squares measurement procedures.CITE introduces intervention nodes – framework, variables manipulated researcher (random assignement) marked []. spirit, introduce shapes three kinds operations data strategy: circles random assignments, triangles sampling, squares measurement procedures.","code":""},{"path":"specifying-the-model.html","id":"what-is-the-population","chapter":"6 Specifying the model","heading":"6.2 What is the population?","text":"first choice make declaring \\(M\\) set units wish make inferences. largely determined inquiry (\\(\\)). might usefully distinguish three types sets units—refer “population.”Finite population. inquiry population Americans Brazilians, reference model describe characteristics Americans Brazilians. might analyze data sample , seek nevertheless make inferences (finite) population.Finite population. inquiry population Americans Brazilians, reference model describe characteristics Americans Brazilians. might analyze data sample , seek nevertheless make inferences (finite) population.Finite sample. Moving , inquiry might sample finite population. interested sample average treatment effect, need describe units sample. practical purposes sample population. Subtly however, interested sample know sampling procedure units population yet, inquiry might concern typical sample population. estimand change depending realized sample.Finite sample. Moving , inquiry might sample finite population. interested sample average treatment effect, need describe units sample. practical purposes sample population. Subtly however, interested sample know sampling procedure units population yet, inquiry might concern typical sample population. estimand change depending realized sample.Superpopulation. Moving , might imagine units draws infinite population. interested “happens shake fizzy drink?”: set instances infinite characterize superpopulation, want describe distribution possible cases, just finite set cases.Superpopulation. Moving , might imagine units draws infinite population. interested “happens shake fizzy drink?”: set instances infinite characterize superpopulation, want describe distribution possible cases, just finite set cases.population mind affects reference populations modeled also draws population interpreted.illustrate consider simple model believe \\(Y\\) depends upon \\(Z\\) \\(Z\\) randomly assigned, probability .5. Say believe potential outcomes, treatment effects, different person. model, like many book, might declared like :model can used generate data, like :\nTable 6.1: Data simple model\nNote first now declaration stochastic component. X randomized individual level features U treatment effects tau. Note also “data” contains potential outcomes—, draw includes description world work different conditions simply description possibly observable data. words, reports data generating process, just data.three distinct ways interpret model specification.Large Population Interpretation M: large population—perhaps superpopulation. run model get see new sample use sample learn large population. interpretation \\(M\\) singleton—specified one model.Large Population Interpretation M: large population—perhaps superpopulation. run model get see new sample use sample learn large population. interpretation \\(M\\) singleton—specified one model.Priors Interpretation M: second interpretation stochastic component represents uncertainty set possible worlds. might , instance, design performs well possible worlds poorly others, case nature distribution important assessing expectation design perform well. interpretation, \\(M\\) singleton.Priors Interpretation M: second interpretation stochastic component represents uncertainty set possible worlds. might , instance, design performs well possible worlds poorly others, case nature distribution important assessing expectation design perform well. interpretation, \\(M\\) singleton.Multiple Worlds Interpretation M: third interpretation data draw (non-stochastic) model distribution functions provide handy way generating multiple plausible models. want learn design performance possible worlds. interpretation, M collection models may interested performance \\(m\\) \\(M\\).Multiple Worlds Interpretation M: third interpretation data draw (non-stochastic) model distribution functions provide handy way generating multiple plausible models. want learn design performance possible worlds. interpretation, M collection models may interested performance \\(m\\) \\(M\\).interpretation matters diagnosis (see Section 10).","code":"\nM <-\n  declare_population(N = 100, \n                     U = rnorm(N), \n                     tau = rnorm(N, mean = 1, sd = 0.1), \n                     Z = rbinom(N, 1, prob = 0.5)) +\n  declare_potential_outcomes(Y ~ tau * Z + U)"},{"path":"specifying-the-model.html","id":"what-variables-to-include","chapter":"6 Specifying the model","heading":"6.3 What variables to include?","text":"","code":""},{"path":"specifying-the-model.html","id":"types-of-variables","chapter":"6 Specifying the model","heading":"6.3.1 Types of variables","text":"key components model reference models examine “variables” (“nodes”) ways stand relation . Informally, variable quantity can take different values: day week, winner election, height mountain. variables formally similar can play distinguished roles model.Often variables labeled according role play causal model , later discussions, often refer variables way using following terms:Outcome variables: variable whose level responses want understand, generally referred \\(Y\\), Figure 6.2. Variously described “dependent variables,” “endogenous variables,” “left-hand side variables,” “response variables.”Explanatory variables: variables affect outcome variables, often referred \\(X\\)s sometimes \\(Z\\), \\(D\\), \\(W\\). use \\(D\\) often refer main causal variable interest particular study.Conditioning moderating variables. Variables might alter effect \\(D\\) \\(Y\\). See example \\(X2\\) Figure 6.2. Note figure simply indicates \\(X2\\) cause \\(Y\\) graph indicate interaction \\(X1\\) \\(X2\\). One account general matter two variables cause outcome surprising interact way.Mediators. Variables “along path” explanatory variables outcomes. \\(M\\) example mediator figure. Mediators often studied assess “” \\(D\\) causes \\(Y\\).Confounders. Variables introduce noncausal correlation \\(D\\) \\(Y\\). figure, \\(X1\\) unobserved confounder causes \\(X\\) \\(Y\\) introduce correlation even \\(D\\) cause \\(Y\\).Instruments. instrumental variable variable can induce exogenous change explanatory variable help us figure relationship \\(D\\) \\(Y\\), used instrumentally—sake. give much detailed treatment variables Section 15.3. \\(Z\\) often reserved instruments; see Figure 6.2.Colliders. Colliders variables caused two variables. Colliders can important conditioning collider introduces statistical noncausal relationship causes collider. figure 6.2, \\(K\\) collider can create correlation \\(D\\) \\(Y\\) (via \\(U\\)) conditioned upon.Note labels reflect researcher’s interest much position model. Another researcher examining graph might, instance, label \\(M\\) explanatory variable \\(K\\) outcome interest.\nFigure 6.2: DAG explanatory variable interest (D), outcome interest (Y), mediator (M), confounder (X1), moderator (X2), instrument (Z), collider (K).\n","code":""},{"path":"specifying-the-model.html","id":"what-variables-are-needed-for-declaration-and-diagnosis","chapter":"6 Specifying the model","heading":"6.3.2 What variables are needed for declaration and diagnosis?","text":"considering variables include model, researchers often torn two conflicting goals. First, want learn world works words, fill large causal model. infinite number nodes edges model — people vote save spend money find romantic partners, interrelated. Second, generally seek simple explanations possible. Indeed accounts, point research generate simplified representations world. analogy map often used: useful map usually less detail object mapping.used considerations vying choosing variables “interest” include model. instance, causes examine explaining outcome. highlight MIDA framework, first consideration – realism – likely carries weight settings. reason model, understood , analysis benchmark analysis assessed. excellent “analytic model” might ignore unimportant details problem, reference model might specify details order establish indeed can ignored without harming inferences.design declaration framework, variables need specify \\(M\\) general comprised () need latter three elements research design, inquiry \\(\\), data strategy \\(D\\), answer strategy \\(\\), (b) need diagnosis.Consider first set first:\\(\\): order reason whether data collect able provide answer inquiry, need define variables used construct inquiry. descriptive research, mean variables summarize. causal research, mean potential outcomes different states world, treatment control. example, studying effects voter mobilization campaign vote choice three candidates running primary election, define vote choice variable values takes two circumstances: presence voter mobilization campaign (treatment) without (control).\\(D\\): data strategy — sampling, treatment assignment, measurement — defines many variables need specify model. Sampling procedures often involve stratification (e.g., sampling equal proportions men women), clustering (e.g., sampling individuals household participate research), . model, need define variables used stratify cluster. Similarly, treatment assignment can involve assigning treatments within blocks cluster assignment units assigned status. variables used construct blocks form clusters defined model. Finally, variables measured also defined model. measure latent variables imperfectly, example sensitive questions true characteristic exists respondents always admit , define latent trait measured responses.\\(\\): Finally, answer strategies rely collected data provide answer inquiry, variable collected data defined model. (variables also defined measurement strategy.) Beyond outcomes treatment variables, may need variables define clusters used clustered standard errors, construct weights poststratification estimates match population characteristics, visualize data., enough diagnosis? might . engage diagnosis may want ask design fares world worked particular way. instance, gender subjects might primary interest us enter inquiry, data strategy, analysis, might worry gender confounds inferences—example affects treatment uptake outcomes. case, may include gender reference model order assess extent analysis sensitive .","code":""},{"path":"specifying-the-model.html","id":"specifying-structural-relations","chapter":"6 Specifying the model","heading":"6.4 Specifying structural relations","text":"DAGs convey beliefs whether two variables causally related, encode beliefs related. criticism DAGs — just don’t encode causal beliefs system. assess many properties research design need go . need specify probability distribution exogenous variables functional forms endogenous variables (relate parent variables). include incorporating beliefs effect sizes, also correlations variables, intra-class correlations (ICCs), interactions. say: order declare diagnose designs, need make leap nonparametric models parametric structural causal models. move without costs – specific choice make nonparametric model opportunity wrong world!slightly complex example helps demonstrate can use model declaration facilitate representation class possibly stochastic models. Suppose according model, effect \\(Z\\) larger units \\(X = 1\\). can encode belief design declaration . declare_population function, write \\(U\\) normally distributed mean 0 standard deviation 1; \\(X\\) follows Bernoulli distribution. declare_potential_outcomes function, describe average effect treatment depends \\(X\\).design describes complex world treatment effects \\(Z\\) depend particular way background variable \\(X\\). incorporate different beliefs causal model changing tau_X0 tau_X1 parameters. However, regardless value interaction (either zero number), DAG looks ability design answer questions interest vary. highlights point DAGs enough characterize model—design declaration requires functional equations also: either specific equations families equations. Thus, although might feel uncomfortable specifying particular values tau_X0 tau_X1, beholden particular values. can easily vary see design performs range models, even expand declaration specify distribution values tau_X0 tau_X1.","code":"\ntau_X0 <- 0.5\ntau_X1 <- 1\n\nM <-\n  declare_population(\n    N = 100, \n    U = rnorm(N), \n    X = rbinom(N, 1, .5)\n  ) +\n  declare_potential_outcomes(\n    Y ~ (X == 0) * Z * tau_X0 + (X == 1) * Z * tau_X1 + U\n  )"},{"path":"specifying-the-model.html","id":"substantive-justifications-for-choices-of-nodes-and-equations","chapter":"6 Specifying the model","heading":"6.5 Substantive justifications for choices of nodes and equations","text":"far described formal considerations described substantive considerations including particular variables stipulating particular relations .justification choice reference model depend purpose design. Broadly distinguish reality tracking models, discursive models, sufficient models.","code":""},{"path":"specifying-the-model.html","id":"reality-tracking-models","chapter":"6 Specifying the model","heading":"6.5.1 Reality tracking models","text":"Reality tracking reference models seek approximate truth well possible.content models typically comes two places: reading past literature qualitative research. Past theoretical work can guide set nodes relevant connected edges. Past empirical work can provide insight set edges exist (). However, past research thin topic, substitute insights gained qualitative data collection: focus groups interviews key informants know aspects model hidden researcher; archival investigations understand causal process actors longer alive, gain insights contained administrative records; immersive participant observation see eyes social actors behave. Fenno (1978) calls “soaking poking.” mode inquiry, discovery, separate qualitative research designs provide answer inquiry deductively. examine throughout book. Instead, qualitative insights , Lieberman (2005) labels “model-building” case studies, aim answer question rather yield new theoretical model. Quantitative research often seen distinct qualitative research, model building phase qualitative.next step — selecting statistical distributions parameters describe exogenous variables functional forms endogenous variables — often uncomfortable. know magnitude effect intervention research correlation two outcomes goal research. ’s conducting study! However, fully dark cases can make educated guesses parameters like effect sizes, intraclass correlations, correlations variables.can conduct meta-analyses past relevant studies topic identify range plausible effect sizes, intraclass correlations, correlations variables, model parameters. Conduct meta-analysis might simple collecting three papers measured similar outcomes past calculating average intraclass correlation range across three. sophisticated still straightforward analysis calculate precision-weighted average effect sizes use baseline effect size model, also calculate predictive interval random effects meta-analysis characterize expected range effect sizes across differing contexts.key question conducting meta-analysis select studies “relevant.” four dimensions might want compare past studies current setting: similarity type units, treatments, outcomes, contexts (Cronbach Shapiro 1982). Except case pure replication studies, typically studying (possibly new) treatment new setting, new participants, new outcomes, perfect overlap. However, variation effects across contexts dimensions help structure range guesses specified model.past studies especially close , may want define probability distributions approximate causal model world, use data past study directly stand-. , instead declaring variables distributions potential outcomes, can resample past data obtain simulated alternative possible worlds.past studies sufficiently similar dimensions, can collect new data pilot studies. discuss risks relying data small pilot studies planning new research Chapter 26.1. short, rely effect size estimates small pilot studies, use parameters range outcome data standard errors estimates less noisy declaring new design.cases, can define many variables early data collection study, e.g., baseline survey. conduct baseline survey, can use set individuals selected study baseline characteristics define many variables model. can define expectations effect sizes features endogenous variables, rely correlations exogenous variables baseline data. danger fix characteristics model using baseline data, consider data revealed sampling procedure yielded different set study participants conducted data collection month later.","code":""},{"path":"specifying-the-model.html","id":"discursive-models","chapter":"6 Specifying the model","heading":"6.5.2 Discursive models","text":"purposes, reference model might developed track reality, reflect assumptions scholarly debate. instance, purpose might question whether given conclusion valid assumptions maintained scholarly community. Indeed possible reference model used specifically researcher thinks inaccurate, allowing show even wrong assumptions world \\(M\\), analysis produce useful answers.particularly important class discursive models might called “agnostic models”: models make weaker assumptions world researcher good faith holds. directed acyclic graph, every arrow indicates possible relation cause outcome. big assumptions models, however, seen arrows absence arrows: every missing arrow represents claim outcome affected possible cause. Analysis strategies often depend upon assumptions. Even arrows included, functional relations might presuppose particular features important inference. instance, researcher using “instrumental variables” analysis generally assume \\(Z\\) causes \\(Y\\) \\(D\\) paths. assumption absent arrows. analysis might also assume \\(Z\\) never affects \\(D\\) negatively. assumption functional forms. agnostic reference model might loosen assumptions, allow possibility violations exclusion restrictions violations monotonicity assumptions use see analysis fares conditions researcher believes true, skeptic might willing entertain.","code":""},{"path":"specifying-the-model.html","id":"sufficient-models","chapter":"6 Specifying the model","heading":"6.5.3 Sufficient models","text":"purposes, validity inferences lessons learned diagnosis depend realized world \\(w\\) among set possible draws \\(M\\) , relevant question whether kinds inferences one might draw given stipulated reference models also hold reasonable well true data generating process. instance, aim assess whether analysis strategy generates unbiased estimate treatment effect may go pains make sure model treatment assignment carefully modeling size treatment effect correctly may important. idea learn model study sufficient inferences broader class models within true data generating process might lie.","code":""},{"path":"specifying-the-model.html","id":"getting-it-right","chapter":"6 Specifying the model","heading":"6.6 Getting it right?","text":"","code":""},{"path":"specifying-the-model.html","id":"robustness-to-multiple-models","chapter":"6 Specifying the model","heading":"6.6.1 Robustness to multiple models","text":"noted uncomfortable part declaring \\(M\\) choosing point estimates parameters effect size, mean variance normal distribution describes unknown heterogeneity. also emphasized general need specify single point! Indeed, uncertainty model usually lead us range even empirical distribution past studies parameter.suggest three strategies choosing ranges: logical bounds parameter, choosing range possible effect sizes based largest change bottom scale top scale; empirical distribution past studies, either full range parameter predictive interval random effects meta-analysis; best case-worst case bounds, based substantive interpretation results light past results, example ranging effect size zero largest plausible effect size. design performs well terms power bias one three ranges parameter might labeled “robust multiple models.”separate goal assessing performance research design different models implied alternative theories. good design provide probative evidence model correct regardless model aligns true causal model world.important example assessing performance research design “null model” true effect size zero. good research design report high probability insufficient evidence reject null effect. research design, alternative model large effect size, high probability return evidence rejecting null hypothesis zero effect. example makes clear order understand whether research design strong, need understand performs just multiple models, models implied alternative theoretical understandings world.Two alternative theories \\(X\\) causes \\(Y\\), mediator \\(M_1\\) mediator \\(M_2\\) (), imply two different structural causal models. inquiry variable mediates relationship, need understand research design performs providing evidence correct possibilities.","code":""},{"path":"specifying-the-model.html","id":"fundamental-uncertainty","chapter":"6 Specifying the model","heading":"6.6.2 Fundamental uncertainty","text":"Even best reference models sure simplifications true data generating process. true causal structure world \\(W\\) generates draws world \\(w\\). inquiry \\(\\) might even defined \\(W\\). , \\((w)\\) might \\(NA\\). Applying data strategy \\(D\\) \\(w\\) might produce unexpected results. , \\(D(w)\\) need anything like \\(D(m)\\). disjuncture large part point research first place. know \\(W\\) – require omniscience. learned parts \\(W\\) put \\(M\\) – ’s science. research produces unexpected results, ’s indication something MIDA whack opportunity learning. next research project amend MIDA order bring \\(M\\) closer \\(W\\).","code":""},{"path":"specifying-the-model.html","id":"sets-of-models","chapter":"6 Specifying the model","heading":"6.7 Sets of models","text":"Every research design starts small part \\(W\\) study, often hypothesized relationship two variables \\(D\\) \\(Y\\). return end section even simpler starting point: single variable \\(Y\\) wish describe. initial model describes just “starting point”:\nFigure 6.3: Starting model.\norder consider design fares alternative possible models, causal relationship \\(D\\) \\(Y\\), first expand model included unmeasured characteristics \\(U\\) measured characteristics \\(X\\) units. variables may serve confounders, colliders, mediators, moderators, instruments. fact, consider possibilities variables expanding possible relationships \\(D\\), \\(Y\\), \\(U\\), \\(X\\). six edges variables, three possible relations: variable may cause (e.g., \\(D \\rightarrow Y\\)), caused (e.g., \\(D \\leftarrow Y\\)), causally related every variable. three possible relationships six edges, \\(3^6 = 729\\) conceptually possible DAGs. rule DAGs variables cause \\(U\\), defined \\(U\\) unobserved confounder. leaves 216 possible combinations. know one! goal researchers narrow possible DAGs.\nFigure 6.4: 216 possible DAGs acyclicity.\nillustrate possible beliefs edges visualizing 216 DAGs \\(U\\) unobserved confounder (see Figure 9.2). large groups three squares nine group DAGs based \\(U\\) affects variables model: top left grouping 27 DAGs represents \\(U\\) affects \\(D\\), \\(X\\) \\(Y\\). Within grouping, three squares nine squares. group nine squares represents set DAGs single relationship \\(D\\) \\(Y\\). top left group nine DAGs \\(D\\rightarrow Y\\).enumerating possibility, draw logic, theory, past evidence rule implausible impossible models. judiciously: model rule represents addition possibly false assumption. Past evidence theoretical model may provide conclusive reasons include exclude certain edges reference models. One common exclusion assumption acyclicity: commonly operate view cycling possible world, imply variables simultaneously cause . Figure 9.2, DAGs gray ruled acyclicity.final step setting model defining mapping theory possible DAGs. aim provide evidence generalizable theories, must know theories ruled empirical evidence. first step define DAGs consistent inconsistent theory. (Later, define evidence result rejection retention DAG; two steps together allow us define theories rejected retained evidence.)example, code model \\(D\\) causes \\(Y\\) consistent “Theory ” \\(D\\) cause \\(Y\\) consistent “Theory B” Table 6.2.\nTable 6.2: DAGs coded consistency two theories.\nDAGs mind, can declare design turn causal relationship:","code":"\npossible_models %>% \n  ungroup %>% \n  sample_n(size = n(), replace = FALSE) %>% \n  mutate(theory_A = DY == \"D->Y\",\n         theory_B = DY == \"D Y\") %>% \n  select(var, theory_A, theory_B) %>% \n  head() %>% \n  kable(caption = \"DAGs coded by consistency with two theories.\")\nM <-\n  declare_population(N = 100, \n                     U = rnorm(N), \n                     D = rbinom(N, 1, prob = 0.5),\n                     X = runif(N)) +\n  declare_potential_outcomes(Y ~ DY * Z + YU * U + YX * X)\n\ndesigns <- redesign(M, DY = c(0, 1), YU = c(0, 1), YX = c(0, 1))\n\ndiagnose_design(designs, sims = 5)"},{"path":"defining-the-inquiry.html","id":"defining-the-inquiry","chapter":"7 Defining the inquiry","heading":"7 Defining the inquiry","text":"inquiry summary reference model. Suppose reference model \\(D\\) possibly affects \\(Y\\). Using framework provided Pearl Mackenzie (2018), one inquiry might descriptive, associational: average level \\(Y\\) \\(D=1\\)? second might effects interventions: average treatment effect \\(D\\) \\(Y\\)? third counterfactuals: share units \\(Y\\) different \\(D\\) different? theory involves variables, many questions open , instance regarding effect one variable passes , modified , another.inquiry research question. Simple complex, causal descriptive, inquiry can thought summary data generating process. Like models, inquiries—questions ask—theoretical objects. easy confuse inquiries output answer strategies. theory posits existence Average Treatment Effect, might use answer strategy like difference--means estimate , estimate fundamentally distinct inquiry. Estimates empirical, inquiries theoretical.general, inquiry summary function \\(\\) operates instance model \\(m \\M\\).6 summarize model inquiry, obtain “answer model.” formalized \\((m) = ^m\\). can think difference \\(\\) \\(^m\\) difference question answer. \\(\\) question ask model \\(^m\\) answer. Alternatively, can think \\(\\) “estimand” (estimated) \\(^m\\) value estimand.inquiry defined four items: summary function, three arguments, units, treatment conditions, outcomes.7 units set units within model inquiry refers , either subset. treatment conditions represents set chosen study. descriptive inquiry summary single condition (reality), whereas causal inquiry summary multiple conditions. last element outcomes, set nodes model inquiry concerns. inquiry might focus single outcome summary one index.\nbook talk inquiries, usually referring single-number summaries models. common estimands descriptive, means, conditional means, correlations, partial correlations, quantiles, truth statements variables model. Others causal, average difference one variable second variable set two different values. can think single-number inquiry atom research question.inquiries “atomic” way, inquiries complex single-number summary. example, best linear predictor \\(Y\\) given \\(X\\) two-number summary: pair numbers (slope intercept) minimizes total squared distance line value \\(Y\\). Note causal estimand still well defined. need stop two-number summaries though. imagine best quadratic predictor \\(Y\\) given \\(X\\) (three-number summary), . See Figure 7.1. inquiry full conditional expectation function \\(Y\\) given \\(X\\), matter wiggly, nonlinear, nuanced shape function – principle 1,000 number summary model, much .\nFigure 7.1: Inquiries based different numbers parameters\ncourse, need number : answer question might “blue” “normal distribution.” might set. instance, qualitative comparative analysis (QCA) Boolean variables (“crisp-set QCA”), common inquiry : minimal set conditions sufficient produce \\(Y\\) (Ragin 1987). instance, \\(Y\\) happens time causal factor \\(\\) present \\(\\) absent \\(B\\) present, \\(^M\\) set: \\(\\{, \\neg \\& B\\}\\).set questions explanatory model. instance, researcher might articulate handful important questions model come certain way model rejected. complex inquires made series atomic inquiries – ’re interested sub-inquiries insofar help us understand real inquiry – model world good one .","code":""},{"path":"defining-the-inquiry.html","id":"elements-of-inquiries","chapter":"7 Defining the inquiry","heading":"7.1 Elements of inquiries","text":"","code":""},{"path":"defining-the-inquiry.html","id":"units","chapter":"7 Defining the inquiry","heading":"7.1.1 Units","text":"ATE vs ATT vs ATC; PATE vs SATE; ATE vs LATE; etc.","code":""},{"path":"defining-the-inquiry.html","id":"treatment-conditions","chapter":"7 Defining the inquiry","heading":"7.1.2 Treatment conditions","text":"ATE vs treatment - placebo comparison","code":""},{"path":"defining-the-inquiry.html","id":"outcomes","chapter":"7 Defining the inquiry","heading":"7.1.3 Outcomes","text":"single outcome, indices, latent outcome vs observable outcome","code":""},{"path":"defining-the-inquiry.html","id":"three-families-of-inquiries","chapter":"7 Defining the inquiry","heading":"7.2 Three families of inquiries","text":"Pearl Mackenzie (2018) usefully describes “ladder causation” can used categorize classes inquiries. bottom rung ladder focuses descriptive inquiries world , , . second rung focuses questions effects interventions: happens \\(Y\\) something \\(X\\)? third rung counterfactuals: \\(Y\\) different \\(X\\) different. last two rungs involve causal inquiries, questions world , , future variables set different levels.Descriptive causal inference common difficulty inferring unseen things observed data. fundamental problem causal inference well known. unit simultaneously treated untreated, can observe one potential outcome particular unit can never measure causal effects – infer . Similar problems confront descriptive inference. concepts want measure latent constructs. perfect measurement generally possible, measurements latent constructs usually include measurement error.\nspecifically, descriptive inference conclusion features latent variable \\(Y^*\\) sets units basis observations measured variable \\(Y\\), measurements possibly taken different set units. feature seek describe—inquiry—summaries \\(Y^*\\) mean perhaps covariance second latent variable \\(X^*\\). descriptive research, draw inferences features nodes latent causal model \\(M\\). measurement can imperfect two reasons: get observe quantity interest directly units study, units study subset units interest. challenges give rise focus descriptive inference rather measurement alone.causal inference conclusion responses variables changes variables. Even exceptional job measuring \\(X^*\\) \\(Y^*\\) \\(X\\) \\(Y\\), still trouble learning causal effects causal effects quite literally unobservable, inferred.","code":""},{"path":"defining-the-inquiry.html","id":"descriptive-inquiries","chapter":"7 Defining the inquiry","heading":"7.2.1 Descriptive inquiries","text":"Descriptive inquiries usually latent variables since mostly care true values variables models. Measured variables often distinct latent variables, gives rise problem descriptive inference. Normally, define inquiries terms true latent variables rather measured counterparts; making distinction explicit design object especially important risks measurement error loom large.Table 7.1 enumerates common descriptive estimands. estimands common need counterfactual quantities order define. Note especially covariance (similarly, correlation) \\(X\\) \\(Y\\) enters descriptive estimand, line best fit \\(Y\\) given \\(X\\).Table 7.1:  Descriptive inquiries","code":""},{"path":"defining-the-inquiry.html","id":"inquiries-about-causal-effects","chapter":"7 Defining the inquiry","heading":"7.2.2 Inquiries about causal effects","text":"Inquiries causal effects involve comparison least two possible worlds. example, inquiry might causal effect \\(X\\) \\(Y\\) single unit. order infer causal effect, need know value \\(Y\\) two worlds: one world \\(X\\) set 1 one \\(X\\) set 0.Table 7.2 enumerates common causal estimands. estimands vary population refer —instance statements samples (SATEs) populations (PATEs)? can also depend upon possibly unobservable characteristics populations—value covariate (CATEs)–way respond causes (CACE targets effect treatment specifically compliers —take treatment encouraged ). Finally, may summaries one potential outcome: instance, interaction effect defined individual level effect one treatment effect another treatment.Table 7.2:  Causal inquiriesGenerations students told excise words connote causality empirical writing. “Affects” becomes “associated ” “impacts” becomes “moves .” careful causal language course important (’s really true correlation imply causation!). change language usually accompanied change inquiry. Many times faced drawing causal inferences less ideal data – deficiencies data strategy lead us far away inferential targets. inquiry causal inquiry, move “causes” “correlated ” might good description actual data analysis, doesn’t move us closer providing answer inquiry.","code":""},{"path":"defining-the-inquiry.html","id":"causalattribution","chapter":"7 Defining the inquiry","heading":"7.2.3 Causal attribution inquiries","text":"Another kind data-dependent inquiry distinct notion causal effect causal attribution. causal effect inquiry focuses change outcome induced change causal variable (unit-level across units) irrespective values outcome takes. contrast, causal attribution inquiries focus probabilities condition realized outcomes, , “probability absence outcome hypothetical absence treatment (\\(Y_i(0) = 0\\)) given actual presence (\\(D_i = Y_i = 1\\))” (Yamamoto 2012, 240–41). Goertz Mahoney (2012) refers causal attribution inquiries cause--effects questions start outcome (effect) seek validate hypothesis cause.dependence inquiries actual outcomes makes harder (though impossible!) answer tools quantitative science, though often central interest scientific policy agendas occupied large number qualitative studies. Questions like “‘economic crisis necessary democratization Southern Cone Latin America?’ ‘high levels foreign investment combination soft authoritarianism export-oriented policies sufficient economic miracles South Korea Taiwan?’” examples inquiries (Goertz Mahoney 2012). Though bear resemblance related causal effects inquiries focus observed subsets (average treatment effect treated, ATT)8 important confuse two kinds inquiries, often happens.increasingly common explicitly formalize causal effect inquiries, less common formalize causal attribution inquiries. , however, can important provide specificity required diagnose design computer. Pearl (1999) provides formal definitions inquiries using language causal necessity sufficiency, depicted table . put inquiries context democratic peace hypothesis, example, given country dyad-year, \\(Y_i = 1\\) \\(D_i = 1\\) represent “Peace” “democracies” \\(Y_i = 0\\) \\(D_i = 0\\) represent “War” “democracies.” \\(\\Pr(Y_i(D_i = 0) = 0 \\mid D_i = Y_i = 1)\\) asks, among peaceful, fully democratic dyads, proportion wars democracies—, proportion dyad-years democracy necessary cause peace? Similarly, \\(\\Pr(Y_i(D_i = 1)=1 \\mid D_i = Y_i = 0)\\) asks, among dyads war least one non-democracy given year, proportion experienced peace countries democracies—words, proportion cases democracy sufficient cause peace? Yamamoto (2012) extends account focus causal attribution inquiries focus important subsets, compilers.Table 7.3:  Causal attribution inquiriesLike designs, causal attribution inquiries can declared, simulated, diagnosed computer. Something consider, however, model may produce datasets effect occur, questions defined units occurred undefined. One way avoid construct model event occurs least one unit probability one.","code":""},{"path":"defining-the-inquiry.html","id":"estimands-declared","chapter":"7 Defining the inquiry","heading":"7.3 Estimands declared","text":"Table 7.4 shows “draws” three estimands. declare simple reference model inquiry—rather set inquiries. estimand draws implemented drawing particular model \\(m\\) \\(M\\) applying \\(\\) draw.\nTable 7.4: One model three estimands.\nfirst estimand descriptive estimand: average outcome \\(Y\\) \\(D = 1\\). Note declaration estimand make use counterfactual quantities.second estimand causal effect. inquiry asked model time inquiry takes average difference, across individuals, two potential outcomes. Unlike descriptive estimand, estimand uses information \\(Y\\)’s potential outcomes use information distribution \\(D\\).third inquiry asks attribution question, case continuous data. case, ask share units positive values \\(Y\\) \\(D=1\\) negative values \\(Y\\) \\(D=0\\). estimand requires information potential outcomes actual outcomes (\\(D\\)).illustration, “run” model + inquiry provides different value estimand (, precisely, true design first third estimand). variation understood? noted last chapter, might think superpopulation distribution estimands interest understanding superpopulation distribution, might think model, \\(M\\), representing set possible worlds interested performance . approach take matter turn diagnosis.\n","code":"\nmodel <-\n  declare_population(N = 100,\n                     U = rnorm(N),\n                     D = rbinom(N, 1, .5)) +\n  declare_potential_outcomes(Y ~ 0.5 * D + U, \n                             assignment_variable = \"D\") +\n  reveal_outcomes(Y, D)\n\ninquiry <-\n  declare_estimand(\n    treatment_group_mean = mean(Y[D == 1]),\n    ATE = mean(Y_D_1 - Y_D_0),\n    probability_of_causation = mean((Y_D_0 < 0)[D == 1 & Y_D_1 > 0])\n  )\n\ndraw_estimands(model + inquiry)"},{"path":"defining-the-inquiry.html","id":"common-complexities-in-defining-estimands","chapter":"7 Defining the inquiry","heading":"7.4 Common complexities in defining estimands","text":"","code":""},{"path":"defining-the-inquiry.html","id":"data-dependent-inquiries","chapter":"7 Defining the inquiry","heading":"7.4.1 Data-dependent inquiries","text":"inquiries introduced thus far depend variables model, features data answer strategies (attribution estimands exception). However, common inquiries depend realizations research design.first type depends realizations data \\(d\\): inquiries units within sample depend units enter sample; inquiries treated units depend treated. sample average treatment effect common inquiry used experimental researchers wish worry effects generalize population identifying causal effect within units front . true sample average treatment effect \\(^w\\) every possible sample draw. fixed values selected inquiry define \\(\\) sample average effect depends sample actually selected sampling procedure. true condition inquiries set treated units—case, estimand defined conditional received treatment.curiosity one might wonder assess whether given sampling scheme assignment procedure good, ex ante, cases question posed conditional sampling assignment. Odder still one might wonder conduct statistics effect estimates sample treatment assignment fixed, since, conditional upon features, hard see variation sampling distribution estimates might come . reasonable response concerns many cases one can pose ex ante question following form: say committed family designs allocate 50 units treatment, unit equally likely assigned, effect receive treatment, whoever end ? case, “run” study produces different treatment group quality design assessed respect distribution possible treatment groups; similarly, errors calculated distribution. Moreover, given constraints admissible assignment schemes, one can assess whether one scheme fares better another different criteria.Table 7.5:  Data-dependent inquiries.second design-dependent inquiry class depends \\(d\\) \\((d) = ^d\\), answer given data. \\(^d\\)-dependent inquiry, know inquiry seeing results study. collect data ten variables calculate correlation ten separate outcome variable, report magnitude correlation seventh variable - one statistically distinguishable zero. inquiry “seven variables significant” problem, long include multiple comparisons correction adjust probability finding one significant null effects due random chance. started question correlation variable seven outcome, also problem. However, procedure think inquiry correlation variable seven outcome, problem: accounting multi-step procedure answer strategy able provide good answers inquiry. short, although can dangers guided realization answer selecting inquiry, problem procedure-based answer strategies—just must honest original inquiry. problem comes descriptive inference looking multiple correlations example, also many places searching heterogeneity treatment effects experiments nested research designs iterate levels data (Lieberman 2005).","code":""},{"path":"defining-the-inquiry.html","id":"non-decomposable-inquiries","chapter":"7 Defining the inquiry","heading":"7.4.2 Non-decomposable inquiries","text":"inquiries looked group level summaries individual level inquiries. decomposable sense can think average effect large group average set average effects smaller groups. However, inquiries form. —best linear fit—complex summaries possible data. Others still notional. Consider, example, estimand Regression Discontinuity Design (RDD) shoots . RDD model (see Section 15.4), imagine units \\(Y_i(1)\\), \\(Y_i(0)\\). \\(\\) also value “running variable”, \\(x_i\\), units receive treatment \\(x_i>0\\). case “effect point discontinuity” might written:\\[E_{|x_i = 0}(Y_i(Z=1) - Y_i(Z=0))\\]Curiously, however, may units \\(x_i = 0\\), easily think estimand summary individual potential outcomes.One way resolve think \\(X\\) randomized, least locally, case can define estimand :\\[\\mathbb{E}_{}(Y_i(Z=1, X=0) - Y_i(Z=0, X=0))\\]conceptualization estimand average individual effects, challenge need make inference condition data.Another approach imagine complex summary construct best fit continuous potential outcomes function cases \\(Z=1\\) \\(Z=0\\) evaluate difference \\(X=0\\). Though average individual effects, difference nevertheless summary potential outcomes. Note though nature estimand (recall discussing estimand, estimate) depends potential outcomes function constructed—, instance, linear function higher-order polynomial?","code":""},{"path":"defining-the-inquiry.html","id":"model-dependent-inquiries","chapter":"7 Defining the inquiry","heading":"7.4.3 Model-dependent inquiries","text":"similar conceptualization can used settings model parameters estimands. instance, might access data decisions subjects lab seek estimate “coefficient risk aversion.” reference model might include coefficient—indeed might willing believe number really exists subjects. Yet might still ask: access possible choices, coefficient best summarize choices agent. answer provides estimand seek estimate finite data.\n","code":""},{"path":"defining-the-inquiry.html","id":"complex-counterfactual-inquiries","chapter":"7 Defining the inquiry","heading":"7.4.4 Complex counterfactual inquiries","text":"counterfactual queries described far involve imagining change one variable assessing effects another, yet much complex causal queries imaginable including quantities events possible even reference model. example “controlled direct effect.” imagine instance \\(X\\) causes \\(Y\\) directly indirectly. Say particular \\(X\\) causes \\(M\\) conditional \\(M=0\\) \\(X\\) causes \\(Y\\) directly. controlled direct effect \\(X\\) \\(Y\\) :\\[\\mathrm{CDE} = Y(X=1, M=1) - Y(X=0, M=1)\\]well-defined quantity potential outcomes DAG formulation. can require mental gymnastics situations , say \\(M=1\\) never arise indeed \\(X\\) 0. instance: Democrat win presidential elections affect success Democrats mid-term elections, conditional presidency remaining Republican hands.","code":""},{"path":"defining-the-inquiry.html","id":"inquiries-with-continuous-causal-variables","chapter":"7 Defining the inquiry","heading":"7.4.5 Inquiries with continuous causal variables","text":"Inquiries focus small number potential outcomes usually quite easy write : binary treatment model spillovers, two potential outcomes, average treatment effect defined average difference across units. treatment like income, might millions potential outcomes, one corresponding different dollar amount? truly continuous treatments, might even infinite number potential outcomes.One approach think estimand data-dependent marginal effect. example, \\(E[Y_i(X_i) - Y_i(X_i-1)]\\) captures expected average difference observed data, people income \\(X_i\\), unobservable outcome one dollar less. Another approach think continuous-treatment estimands parameter regression omniscient run. example, might define \\(\\alpha\\) \\(\\beta\\) solutions :\n\\[\\min_{(\\alpha,\\beta)}\\sum_i\\int \\left(Y_i(x) - \\alpha - \\beta x\\right)^2f(x)dx\\]\n, \\(Y_i(x)\\) (unknown) potential outcome unit \\(\\) condition \\(x\\). Estimand \\(\\beta\\) can thought coefficient one get \\(x\\) one able regress possible potential outcomes possible conditions units (given density interest \\(f(x)\\)). Often, straightforward approach define inquiry finite number potential outcomes drawn range treatment variable.\n","code":""},{"path":"defining-the-inquiry.html","id":"undefined-and-unanswerable-inquiries","chapter":"7 Defining the inquiry","heading":"7.4.6 Undefined and unanswerable inquiries","text":"Declaring design terms \\(MIDA\\) may lead two awkward conclusions: inquiry \\(\\) returns \\((m) = ^m = \\mathrm{NA}\\), .e., answer inquiry undefined; answer may (currently) unanswerable, , feasible \\(D\\) \\(\\) yield answer \\(\\). cases, one option change inquiry. often selected \\(\\) importance, may want try find answer. case undefined inquiries, option select new one. case unanswerable inquiries, can work identify novel \\(D\\) \\(\\) change set feasible designs provide answer. cases, however, fact inquiry unanswerable may due unchangeable limitations research fundamental problems causal inference descriptive inference.","code":""},{"path":"defining-the-inquiry.html","id":"example-3","chapter":"7 Defining the inquiry","heading":"7.4.7 Example","text":"Björkman Svensson (2009) reports results cluster-randomized trial effects community-based monitoring health clinics Uganda improve children’s health. main inquiry average treatment effect child mortality rate. study showed program success: control group, child mortality rate 144 per 1000 live births, compared 97 treatment group, 33% reduction child mortality.study also considered second inquiry: average treatment effect weight--age children 18 months. inquiry harder think , precisely know community-based monitoring saved lives many children. see problem, consider Table 7.6, shows four types infants distinguished basis potential outcomes. Type (“Adverse”) alive control, dies treatment. Type B (“Beneficial”) just reverse: child dies untreated, survives treated. Type C (“Chronic”) die either condition Type D (“Destined”) live either condition. first three types, child dies one condition, , . Since weight--age exists child survives, treatment effect weight , B, C types undefined.main trouble average treatment effect averages four types ATE also undefined. Since inquiry undefined, ’ll need select different one.might want switch inquiry average effect among D types . However, inquiry different problem – ’s unanswerable. Even though inquiry undefined, ’s unanswerable won’t able learn D types . treatment group, can’t tell Bs Ds control group, can’t tell Ds. reason can’t tell types apart using realized data fundamental problem causal inference.Table 7.6:  Latent types.can done situation? might try find covariate \\(X\\) correlated D type condition analysis effect weight covariate. Note among subgroup, effect mortality equal zero. Finding covariate hard, though perhaps impossible.Alternatively, might define inquiry average treatment average weight living children within cluster, inquiry requires careful interpretation. treatment saves lives children whose health marginal particular, effect average weight easily negative, even though treatment improves health. specifically interpret answer inquiry refer effect treatment health. said, might nevertheless useful number know position administering healthcare resources – ’d want send additional help treatment areas lives unhealthy children (otherwise perished) saved.","code":""},{"path":"defining-the-inquiry.html","id":"how-should-you-select-inquiries","chapter":"7 Defining the inquiry","heading":"7.5 How should you select inquiries?","text":"’s hard know start picking research question. want pick one interesting right one facilitate real-world decision. want pick research questions can learn answer someday, lot effort. Unfeasible research questions abandoned soon possible, course, ’s hard . trouble ’s hard know research questions feasible start looking , ’s really hard quit research projects learn unfeasible sunk cost fallacy. Among feasible research questions, want select ones likely obtain informative answers, terms moving priors . last criterion often help us select among related, feasible inquiries DAG know can learn less research.Sometimes, people advise students follow “theory-first” route picking research question. Read literature, find unsolved puzzle, start choosing among methodological approaches might answer problem. Others eschew theory-first approach: “earth going happen land upon unsolved – yet somehow solvable – puzzle just reading!?” advice-givers emphasize method-first route. Master technical data-gathering analysis procedures first, set find opportunities apply . theory-first people say: “know interesting theoretical question smacked face!?”Iteration two typically necessary. order select research questions, empiricists concerned entire research design. develop empirical strategies provide answers inquiries. learn lot select data answer strategies ways map inquiries models. empiricists learn models inquiries (theory) well data strategies answer strategies (empirics).first criterion subjective importance question. object importance may scientist, considering value building theoretical understanding world; policymaker, deciding collect allocate resources government; private firm, making decisions invest resources maximize profit; another individual organization. scientific enterprise designed around idea importance eye scientist objective quantity. two reasons. First, scientific practical importance discovery may understood decades later, pieces causal model put together world faces new problems. Moreover, “importance” differs different segments society, scientists must able study questions judged important groups power order discover new ways solve problems faced left-groups.Among important questions, researchers can select \\(\\) think likely able learn . start prior distribution \\(^w\\), true answer inquiry, past research, good research design substantially update prior distribution, either moving mean distribution reducing uncertainty .final criterion among important, probative research designs must feasible \\(M\\), \\(D\\), \\(\\) answer \\(\\). means picking good question \\(\\) just involve theory. study \\(M\\) understand \\(\\)s worth knowing. also study \\(D\\) \\(\\) order learn demonstrate \\(\\). central goal research methodologists expand feasible set \\(D\\) \\(\\) can provide informative answers important questions.Clearly, research designs vary important, feasible, probative , criteria provide immediate answer select design. Instead, researcher must choose weighting three, norms research community may guide weightings. disciplines, providing minimally probative answer highly important question may preferable highly probative answer less important question.\n\n\n\n\n\n\n’s best advice get started picking research question: Write \\(M\\) \\(\\) causal model think important get started thinking selecting strong \\(D\\)s \\(\\)s. goal learn map \\((M)\\) \\((D)\\). return theory find new important inquiries, write new model, inquiry, data strategy, answer strategy. process make progress \\(M\\): bring \\(M\\) closer \\(W\\), thereby making \\(M\\) truer. iteration, consider informative answers can provide: much update know world?People looking forest mushrooms often don’t see mushroom long period time. , acclimate. get “eyes ,” successful finds seem around every bend. analogy, theory-building process going walk forest methods training learning spot mushrooms – need get eyes answerable research questions worth answering.","code":""},{"path":"defining-the-inquiry.html","id":"further-reading-5","chapter":"7 Defining the inquiry","heading":"7.6 Further reading","text":"Goertz Mahoney (2012) differences across inquiries qualitative quantitative research.Dawid (2000) cause--effects questions.Yamamoto (2012) causal attribution.Zhang Rubin (2003) “truncation--death”","code":""},{"path":"crafting-a-data-strategy.html","id":"crafting-a-data-strategy","chapter":"8 Crafting a data strategy","heading":"8 Crafting a data strategy","text":"order collect information world, researchers must deploy data strategy. Depending design, data strategy include decisions following: sampling, assignment, measurement. Sampling procedure selecting units measured; assignment procedure allocating treatments sampled units; measurement procedure turning information sampled units data. three procedures parallel three elements inquiry: units, treatment conditions, outcomes.\nSampling choices confront fundamental problem generalization: want make inferences units sampled. reason, need pay special attention procedure units selected sample. might use random sampling procedure order generate design-based justification generalizing samples specific population units drawn. Nonrandom sampling procedures also possible: convenience sampling, respondent-driven sampling, snowball sampling data strategies include explicitly randomized component. Nonrandomized designs usually require model-based inference order generalize sample population.Assignment choices confront fundamental problem causal inference: want make inferences conditions units assigned. reason, experimental design focused assignment treatments. many treatment conditions ? use simple coin flip decide receives treatment, use complicated strategy like blocking? Experimenters course also concerned sampling measurement procedures, random assignment treatments make randomized experiments distinctive among research designs.Measurement choices confront fundamental problem descriptive inference: want make inferences latent values basis measured values. tools use measure critical part data strategy. many social scientific studies, main way collect information surveys. huge methodological literature survey administration developed help guide questionnaire development. Bad survey questions yield distorted noisy responses. biased question systematically misses true latent target designed measure, case question low validity. question high variance (hypothetically) obtain different answers time asked, case question low reliability. concerns validity reliability disappear move survey environment. example, information shows administrative database result many human decisions, possibility increasing decreasing distance measurement latent measurement target.three problems fundamental sense surmounted researchers humble face . Strong research design can help, can never sure sample generalizes, know happened counterfactual state world, true latent value outcome (even exists). Researchers choose good sampling, assignment, measurement techniques , combined applied world, produce analysis-ready information. discuss answer strategies – set analysis choices data collected – next chapter. data answer strategies course intimately interconnected. analyze data depends deeply collected collect data depends just deeply plan analyze . moment, thinking many choices might make part data strategy, applied research design setting, considered concert answer strategy.data strategy \\(D\\) set procedures result dataset \\(d\\). important keep two concepts straight. apply data strategy \\(D\\) world \\(w\\), produces dataset \\(d\\). say \\(d\\) “” result \\(D\\), since apply data strategy world, obtain data obtain. crafting data strategy, think many datasets data strategy produced. datasets might really excellent. example, good datasets, achieve good covariate balance across treatment control groups. might draw sample whose distribution observable characteristics looks really similar population. datasets might worse: vagaries randomization, particular realizations random assignment random sampling might less balanced. settle data strategies might produce weak datasets – control procedures choose. want choose data strategy \\(D\\) likely result high-quality dataset \\(d\\).Figure 8.1, illustrate data strategy three elements: sampling, assignment, measurement. Sampling procedure selecting units measured; assignment procedure allocating treatments sampled units; measurement procedure turning information sampled units data. empirical studies data strategy every data strategy involves sampling, assignment, measurement. Even studies researchers measure full universe cases involve sampling, since future units included. Even studies researchers apply treatments involve assignment, since forces end assigning units conditions. Even studies researchers take new measurements involve measurement, since choice measurements made others use must nevertheless made.\nFigure 8.1: DAG illustrating three elements data strategy: sampling, assignment, measurement.\nfigure, rule threats inference come implementation: failure treat (noncompliance), failure inclusion sample (attrition), causal relationships random sampling assignment well measurement latent outcome (excludability). last section chapter, discuss threats realized can mitigate risks present design.","code":""},{"path":"crafting-a-data-strategy.html","id":"p2sampling","chapter":"8 Crafting a data strategy","heading":"8.1 Sampling strategies","text":"Sampling process units selected population studied. sampling procedures involve randomization others .\n\nWhether sampling procedure randomized large implications answer strategy. Randomized designs support “design-based inference,” refers idea rely known features sampling process producing population-level estimates – much next chapter Answer strategies. randomization breaks (e.g., design encounters attrition) nonrandomized designs used, fall back model-based inference generalize sample population. Model-based inference relies researcher beliefs nature uncontrolled sampling process order make inferences population. possible, design-based inference advantage letting base inferences known rather assumed features data selection process. said, randomly sampled individuals fail respond seek make inferences new populations, must apply model-based inference even data result random process.ever content study sample full population? infinite populations, choice. finite populations first best explanation cost: ’s expensive time-consuming conduct full census population. Even well-funded research projects face problem, since money effort spent answering one question also spent answering second question. tend sample rather measure every unit population face opportunity costs well. second reason sample diminishing marginal returns additional data collection. Increasing number sampled units 1,000 2,000 greatly increase precision estimates. Moving 100,000 101,000 improve things , scale improvement much smaller.","code":""},{"path":"crafting-a-data-strategy.html","id":"randomized-sampling-designs","chapter":"8 Crafting a data strategy","heading":"8.1.1 Randomized sampling designs","text":"Owing natural appeal design-based inference, start randomized designs proceeding nonrandomized designs. Randomized sampling designs typically begin list units population, choose subset sample using random process. random processes can simple (every unit equal probability inclusion) complex (first select regions random, villages random within selected regions, households within selected villages, individuals within selected households).Table 8.1 collects kinds random sampling together offers example functions randomizr package can use conduct kinds sampling. basic form simple random sampling, also called “coin flip” Bernoulli random sampling. simple random assignment, units population probability \\(p\\) included sample. sometimes called coin flip random sampling though unit, flip weighted coin probability \\(p\\) landing heads-. quite straightforward, drawback simple random sampling can’t sure number sampled units advance. average, ’ll sample \\(N*p\\) units, sometimes slightly units sampled sometimes fewer.Table 8.1:  Kinds random samplingComplete random sampling addresses problem. complete random sampling, exactly \\(n\\) \\(N\\) units sampled. unit still inclusion probability \\(p = n/N\\), contrast simple random sampling, guaranteed final sample size \\(n\\).9 Complete random sampling represents improvement simple random sampling rules samples fewer \\(N*p\\) units sampled. One circumstance might nevertheless go simple random sampling size population known advance sampling choices made “fly.”Complete random sampling solves problem fixing total number sampled units, doesn’t address problem total number units particular characteristics fixed. Imagine population \\(N_{y}\\) young people \\(N_{o}\\) old people. sample exactly \\(n\\) population \\(N_{y} + N_{o}\\), number sampled young people (\\(n_y\\)) sampled old people (\\(n_{o}\\)) bounce around sample sample. can solve problem conducting complete random sampling within group units. procedure goes name stratified random sampling, since sampling conducted separately within strata units.10 example, strata formed dichotomous grouping people “young” “old” categories, general, sampling strata can formed information units sampled. Stratification offers least three major benefits. First, defend sampling surprisingly units stratum “bad luck.” Second (discussed chapter answer strategies) stratification tends produce lower variance estimates inquiries. Finally, stratification allows researchers “oversample” subgroups particular interest.Stratified sampling confused cluster sampling. Stratified sampling means fixed number units particular group drawn sample. Cluster sampling means units particular group brought sample together. example, cluster sample households, interview individuals living sampled household. Clustering introduces dependence sampling procedure – one member household sampled, members also always sampled. Relative complete random sample size, cluster samples tend produce higher variance estimates. Just individual sampling designs, cluster sampling comes simple, complete, stratified varieties exactly parallel logics motivations.Lastly, turn multi-stage random sampling, conduct random sampling multiple levels hierarchically-structured population. example, might first sample regions, villages within regions, households within villages, individuals within households. sampling steps might stratified clustered depending researcher’s goals. purpose multi-stage approach typically balance logistical difficulties visiting many geographic areas relative ease collecting additional data arrived.Figure 8.2 gives graphical interpretation kinds random sampling. , imagine population 64 units two levels hierarchy. concreteness, can imagine units individuals nested within 16 households four people 16 households nested within four villages four people . Starting top left, simple random sampling individual level. inclusion probability set 0.5, average, sample 32 people, particular draw, actually sampled 29. Complete random sampling (top center), fixes problem, exactly 32 people sampled – 32 unevenly spread across four villages. addressed stratified sampling – sample exactly 8 people random village 16 total people.Moving middle row figure, three approaches clustered random sampling. simple sampling cluster, cluster probability \\(p\\) inclusion sample, average sample eight clusters. time, sampled seven, problem can fixed complete random sampling (center facet), uneven distribution across villages. Stratified cluster sampling ensures exactly two households village sampled.bottom row figure illustrates approaches multistage sampling. bottom left panel, conduct simple random sample individuals sampled cluster. bottom center, draw complete random sample individuals sampled household. bottom left, stratify individual level characteristic – always draw one individual row household. “Row” refer age household members. doubly-stratified multistage random sampling procedure ensures sample two households village within households, one older member one younger member.\nFigure 8.2: Nine kinds random sampling\n","code":""},{"path":"crafting-a-data-strategy.html","id":"nonrandomized-sampling-designs","chapter":"8 Crafting a data strategy","heading":"8.1.2 Nonrandomized sampling designs","text":"nonrandomized sampling procedures defined don’t – don’t use randomization – hugely varied set procedures described way. ’ll consider just common ones, since idiosyncrasies approach hard systematize.Convenience sampling refers practice gathering units population least expensive way available . Convenience sampling good choice generalizing explicit population main goal design, example sample average treatment effect theoretically-important inquiry. many decades, social science undergraduates abundant data source available academics many important theoretical claims established basis experiments conducted samples. recent years, however, online convenience samples like Mechanical Turk, Prolific, Lucid mostly supplanted undergraduates convenience sample choice. (mostly nonexperimental) circumstances, however, convenience sampling likely lead badly biased estimates. example, cable news shows often conduct viewer polls taken seriously. polls might promote viewer loyalty (might worth cable executives’ perspective) provide credible evidence population large thinks believes.Many types qualitative quantitative research involve convenience sampling. Archival research often involves convenience sample documents certain topic exist archive. question documents differ different archive, documents available archives differ ever make archive importantly shapes can learn . decline telephone survey response rates, researchers can longer rely random digit dialing obtain representative sample people many countries, instead must rely convenience samples internet panels agree phone numbers list. Sometimes, reweighting techniques answer strategy can, cases, help recover estimates population whole sampling credible model unknown sampling process can agreed upon.Next, consider purposive sampling. Purposive catch-term rule-based sampling strategies involve random draws also purely based convenience cost. common example quota sampling. Sampling purely based convenience often means end many units one type another type. Quota sampling addresses problem continuing search subjects target counts (quotas) kind subject found. Loosely speaking, quota sampling convenience sampling stratified random sampling complete random sampling: fixes problem enough (many) subjects particular types sampled employing specific quotas. Importantly, however, guarantee sampled units within type representative type overall: quota samples within-stratum convenience samples.second common form purposive sampling respondent-driven sampling, used sample hard--reach populations HIV-positive needle users. RDS methods often begin convenience sample systematically obtain contacts units share characteristic order build large sample.three nonrandom sampling procedures – convenience, quota, respondent-driven – illustrated Figure 8.3. Imagining village easier reach, obtain convenience sample contacting everyone can reach village moving village B. process doesn’t yield good coverage across villages , can turn quota sampling. quota sampling scheme, talk five people easiest reach four villages. Finally, conduct respondent-driven sample, select one seed unit village, person recruits four closest friends (may may reside village).\nFigure 8.3: Three forms non-random sampling.\n","code":""},{"path":"crafting-a-data-strategy.html","id":"sampling-designs-for-qualitative-research","chapter":"8 Crafting a data strategy","heading":"8.1.3 Sampling designs for qualitative research","text":"Case selection another term sampling strategy. case study research, whether qualitative quantitative, way select (typically small) set cases great importance, considerable attention paid developing case selection methods.John Stuart Mill (1884) elaborated methods induction inspired two popular case selection methods based whether outcomes “different” “agree.” method difference involves selecting cases divergent outcomes otherwise look similar. one characteristic covaries outcome, becomes candidate cause. example, Skocpol (1979) compares historical periods France, Russia, United Kingdom, Germany look similar many regards. first two, however, social revolutions, second two . presence agrarian institutions provided degree political autonomy peasants France Russia absence UK Germany becomes possible clue understanding underlying causal structure social revolutions. discuss debate around answer strategy section answer strategies. contrast, method agreement involves selecting cases share outcome diverge host characteristics. characteristics common cases become candidates causal attribution.11 strategies, selection cases guided inquiry answer strategy.qualitative case selection strategies borrow directly toolkit quantitative, large-\\(N\\) analysis. datasets large number cases available, Lieberman (2005) proposes using predicted values regression model—often referred “regression line”—initial quantitative analysis order select cases -depth analysis.12 , inquiry answer strategy guide case selection. inquiry focused uncovering causal relationship sought quantitative analysis, Lieberman (2005) suggests selecting cases relatively well-predicted maximize variation causal variable. points Martin (1992) Swank (2002) examples designs employing strategy. However, Lieberman (2005) advocates different case selection strategy goal expand upon theory initially tested quantitative analysis. instance, recommends choosing cases lying far regression line, well-predicted may therefore lead insights alternative mechanisms left initial regression.Seawright Gerring (2008a) use regression line analogy describe seven different sampling strategies tailored suit different inquiries, depicted Figure 8.4.13 horizontal axis represents characteristic cases, \\(X\\), degree labor union strength. vertical axis represents outcome, \\(Y\\), degree welfare state generosity. line represents predicted cross-case relationship, dark grey squares represent possible cases population, blue squares represent sample chosen strategy.\nFigure 8.4: Case selection Strategies\nTypical cases typify cross-case relationship can chosen order explore validate mediating mechanisms. researcher’s model implies union membership increases welfare spending democracies effects negotiations government, example, researcher might look evidence processes cases well-predicted theory. Diverse cases maximize variation \\(X\\) \\(Y\\), extreme cases located maximal distance cases just one dimension—example, researcher chooses two cases highest degree union strength. diverse extreme cases might lie regression line, deviant cases defined distance . Influential cases whose exclusion noticeably change imaginary regression line (.e., highest leverage regression). Thus, diverse extreme case selection can useful exploratory work, like updating theoretical model finding new ways pose inquiry, deviant influential case selection can useful understanding scope conditions theory. Confusingly, “similar systems” design employs Mill’s method difference order select cases similar \\(X\\) different \\(Y\\), “different systems” design employs method agreement select cases agree \\(Y\\) different characteristics.unlikely every strategy available provide equally good answers every research design, one method always fare better others. Declaring simulating design simplifying assumptions model, inquiry, answer strategy clarifies “cases choose affect answers get” (Geddes 2003).example, Herron Quinn (2016) used Monte Carlo simulations study well seven strategies depicted perform model stipulates cases four causal types,14 inquiry average treatment effect population, answer strategy involves, perhaps optimistically, perfectly observing selected cases’ causal type. simplifying assumptions, uncover clear hierarchy set prescriptions: extreme deviant case selection fare much worse methods terms three diagnosands considered (root mean square error, variance, bias mean posterior distribution). contrast, influential case selection outperforms strategies, followed closely diverse simple random sampling. authors acknowledge, however, hierarchy might look different inquiry aimed different, exploratory quantity (discovering number causal types exist). Humphreys Jacobs (2015) provide simulations incorporate process tracing inferential procedure highlight importance “probative value” case selection. point rarely case selection strategy fits problems equally well—best strategy one optimizes particular diagnosand given stipulations inquiry, model, answer strategy. can justify stipulations importance diagnosand, defending choice sampling strategy straightforward.point speaks two greatest controversies qualitative case selection: whether randomize whether select using information dependent variable.Since qualitative case analysis labor-intensive, sample sizes typically small (1-30 case range). Scholars like Seawright Gerring (2008a) point random sampling therefore produces high variance can even lead aberrant samples variation outcomes explanatory variables, share idiosyncratic features locations. , advocate purposive selection. Fearon Laitin (2008) point , however, convenience samples purposive selection can lead severe bias, especially goal make population-level inference. advocate use stratified random sampling. Yet, question whether randomly sample resolved deciding claims correct, since correct different diagnosands: nonrandom sampling might minimize variance maximizing bias, random sampling might increase variance minimizing bias. even diagnosand-specific claims conditional specific model, inquiry, answer strategy authors mind.reasoning also applies question whether incorporate information dependent variable sampling decisions. Geddes (2003) famously critiqued Skocpol (1979) making inference role foreign threat social revolution based selection cases experienced revolutions.15 essence, Geddes criticizes resultant lack variation outcome, makes difficult assess whether revolutions failed occur, example, countries face foreign threats.16 However, critique assumes Skocpol’s answer strategy based Mill’s method difference17 inquiry focuses something like average effect foreign threat revolution—.e., effects--cause question. design library, declare process-tracing design inquiry cause--effects question: , average effect \\(X\\) \\(Y\\), , given \\(Y\\) happened, likelihood happened \\(X\\)? inquiries, selecting dependent variable essential (’s hard study \\(Y\\) happened didn’t happen!), advantageous: cuts space competing causal hypotheses half, drastically simplifies answer strategy. Note prescription necessarily hold quantitative answer strategies: Yamamoto (2012) shown “negative” cases necessary estimate cause--effects questions, even necessary define inquiry.Thus, many data strategy choices, whether makes sense sample using information outcomes depend , M, , mention diagnosands, \\(\\phi\\), judge design performance.","code":""},{"path":"crafting-a-data-strategy.html","id":"choosing-among-sampling-designs","chapter":"8 Crafting a data strategy","heading":"8.1.4 Choosing among sampling designs","text":"short, choice sampling strategy depends features model inquiry, different sampling strategies can compared terms power RMSE design diagnosis. model defines population units interested making inferences , target population sampling strategy match much possible. model also points us important subgroups (defined nodes endogenous variables) may wish stratify , depending variability within subgroups. Whether select convenience, random, purposive sampling depends budget logistical constraints well efficiency (power RMSE) design. little bias convenience sampling, often want select cost reasons; obtain convenience sample right composition, may choose purposive method ensures . choice simple stratified sampling comes inquiry diagnosis RMSE: inquiry involves comparison subgroups, often select stratified sampling. either, diagnosis alternative designs terms power RMSE guide selection.Sampling, like data strategies, intervention world researchers. result, may independent causal effects outcomes. see next section, deep difference sampling treatment assignment, case randomly sampling two potential outcomes. treatment assignment, assigning units treatment group control group, obtaining sample treated potential outcome second random sample control potential outcome. random sampling, assigning units sampled sampled. obtain one sample sampled potential outcome. However, non-sampled potential outcome exists conceptually, may differ sampled potential outcome. act including units sample may change outcomes, example, selected participate medical trial believe going receive better care placebo effect changes health condition. research design conduct experiment sampled units, also unobtrusively measure outcomes nonsampled units used estimate effect inclusion sample. design provide random sample nonsampled potential outcome.Design-based inference can help extrapolate sample data quantities population, using details sampling design. Whether extrapolations unbiased efficient depend sampled. sometimes also interested extrapolating outside population sampled populations. Model-based inference can used extrapolate populations, assumptions population sample differs population wish extrapolate .","code":""},{"path":"crafting-a-data-strategy.html","id":"p2assignment","chapter":"8 Crafting a data strategy","heading":"8.2 Treatment assignment","text":"many studies, researchers intervene world set level causal variable interest. procedures used assign units treatment tightly analogous procedures explored previous section sampling. Like sampling, assignment procedures fall two classes, randomized nonrandomized. Among randomized procedures, can distinguish host two arm trial designs, multiarm designs, set designs advanced designs.","code":""},{"path":"crafting-a-data-strategy.html","id":"two-arm-trials","chapter":"8 Crafting a data strategy","heading":"8.2.1 Two arm trials","text":"analogy sampling assignment runs deep. sampling designs discussed previous section directly equivalent assignment designs. Simple random sampling analogous Bernoulli random assignment, stratified random sampling analogous blocked random assignment . Many design tradeoffs hold well: just like cluster sampling generates higher variance estimates individual sampling, clustered assignment generates higher variance estimates individual assignment. usually think randomized assignment designs , nonrandomized designs research applies treatments also occur. example, researchers sometimes treat convenience sample, search different convenience sample serve control group. Within-subject designs subjects measured, treated, measured second example nonrandomized application treatment.analogy sampling assignment runs deep , sense, assignment sampling. Instead sampling units study, sample alternative possible worlds. treatment group represents sample alternative world units treated control group represents sample alternative world units untreated.18 can reencounter fundamental problem causal inference lens – unit sampled one possible world, can’t sampled possible world. Table 8.2 collects together common forms random assignment.Table 8.2:  Kinds random assignmentFigure 8.5 visualizes nine kinds random assignment, arranged according whether assignment procedure simple, complete, blocked according whether assignment procedure carried individual, cluster, saturation level. top left facet, simple (Bernoulli) random assignment, units 50% probability treatment, total number treated units bounce around assignment assignment. top center, problem fixed: complete random assignment, exactly \\(m\\) \\(N\\) units assigned treatment \\(N - m\\) assigned control. complete random assignment fixes number units treated exactly \\(m\\), number units treated within particular group units (defined pre-treatment covariate) bounce around. block random assignment, conduct complete random assignment within block separately, directly control number treated within block. Moving simple complete random assignment tends decrease sampling variability bit, ruling highly unbalanced allocations. Moving complete blocked can help , long blocking variable correlated outcome. Blocking rules assignments many units particular subgroup treated. build intuition correlation blocking variable outcome important, consider forming blocks random; none assignments complete random assignment ruled , sampling distributions two assignment procedures equivalent.second row Figure 8.5 shows clustered designs units within cluster receive treatment assignment. Clustered designs common household-level, school-level, village-level designs, impractical infeasible conduct individual level assignment. units within cluster alike units different clusters (cases), clustering increases sampling variability relative individual level assignment. Just like individual level designs, moving simple complete complete blocked tends result lower sampling variability.final row Figure 8.5 shows series designs analogous multi-stage sampling designs shown Figure 8.2 – purpose subtly different spirit. Multi-stage sampling designs employed reduce costs – first clusters sampled units within cluster sampled. saturation randomization design (sometimes called “partial population design”) uses similar procedure contain learn spillover effects. clusters chosen treatment, units within clusters treated. Units untreated treated clusters can compared units untreated untreated clusters order suss intra-cluster spillover effects (Sinclair, McConnell, Green 2012). figure shows saturation design comes simple, complete, blocked varieties.\nFigure 8.5: Nine kinds random assignment. first row individuals sampling units, second row clusters sampled, third clusters sampled individuals within clusters sampled. first column units sampled independently, second units sampled hit target, third units sampled hit targets within strata.\n","code":""},{"path":"crafting-a-data-strategy.html","id":"multiarm-and-factorial-trials","chapter":"8 Crafting a data strategy","heading":"8.2.2 Multiarm and factorial trials","text":"Thus far considered assignment strategies allocate subjects either treatment control. designs considered far generalize quite nicely multiarm trials. Trials three, four, many arms can course simple, complete, blocked, clustered, feature variable saturation. Figure 8.6 shows blocked versions three-arm trial, factorial trial, four-arm trial.three-arm trial left, subjects can assigned control condition one two treatments. design enables three comparisons: comparison treatment control condition, also comparison two treatment conditions . four-arm trial right, subjects can assigned control condition one three treatments. design supports six comparisons: treatments control, three pairwise comparisons across treatments.two--two factorial design center panel shares similarities three-arm four-arm trials. Like three arm, considers two treatments T1 T2, also includes fourth condition treatments applied. Factorial designs can analyzed like four-arm trial, structure design also enables subtle analyses. particular, factorial structure allows researchers investigate whether effects one treatment depend level treatment.\nFigure 8.6: Multi-arm random assignment.\n","code":""},{"path":"crafting-a-data-strategy.html","id":"complex-designs","chapter":"8 Crafting a data strategy","heading":"8.2.3 Complex designs","text":"written:Patient preference trialsStepped-wedge assignment (see Figure 8.7).Multiarm bandits\nFigure 8.7: Step-wedge random assignment.\n","code":""},{"path":"crafting-a-data-strategy.html","id":"non-randomized-assignment","chapter":"8 Crafting a data strategy","heading":"8.2.4 Non randomized assignment","text":"written:Alphabetical assignmentRDDBayesian optimal assignment","code":""},{"path":"crafting-a-data-strategy.html","id":"measurement-1","chapter":"8 Crafting a data strategy","heading":"8.3 Measurement","text":"Measurement part data strategy variables collected population units enable sampling, variables collected sample treatment assignment including used treatment assignment, outcomes collected treatment assignment. variables used answer strategy collected measurement, aside treatment assignment variable assignment sample inclusion probabilities.fundamental problem description can never measure latent variables interested , \\(Y^*\\), fear, support political candidate, economic well-. Instead, use measurement technology imperfectly observe , represent function \\(Q\\) yields observed outcome \\(Y^{\\mathrm obs}\\): \\(Q(Y^*) = Y^{\\mathrm obs}\\). measurement strategy set functions \\(Q\\) variable measure.two basic ways assess quality function \\(Q\\): bias, difference observed latent outcome, \\(Y^{\\mathrm obs} - Y^*\\), given special label measurement validity; measurement reliability, variance across multiple outcomes given individual, \\(\\mathbb{V}(Y_1^{\\mathrm obs}, Y_2^{\\mathrm obs}, Y_3^{\\mathrm obs})\\).Researchers select several characteristics \\(Q\\): collects measures; mode measurement; often measures taken; many different observed measures \\(Y^*\\) collected summarized single measure; information provided participants measured (). design characteristics may affect validity, reliability, .Data may collected researcher , participant, third party. forms qualitative research participant-observation interview-based research, researcher may primary data collector. survey research, interviewer typically hired agent researcher, many cases, multiple interviewers hired. interviewers may ask questions differently, leading less reliable (variable) answers cases validity problems ask questions way leads biased measures \\(Y^*\\). Participants often asked collect data , either self-administered surveys, journaling, taking measurements using thermometers scales. primary concern self-reports validity: respondents report measurements truthfully. parallel concern raised participants collect data made aware fact measured others. Finally, data may collected agents government organizations, yielding -called administrative data. difference administrative data forms data identity data collector.variety measurement strategies data collectors obtain data. Humans can code data observation five senses sight, hearing, touch, smell, taste, asking humans self-reports surveys. Measurement instruments can also used record waves light (e.g., photos), sound (e.g., audio seismic recordings), electromagnetism (e.g., EKGs x-rays), combinations one (e.g., video); characteristics atmosphere (e.g., temperature pressure), water (e.g., salinity pollution), soil (.e., mercury pollution); human animal health (e.g., blood tests). Considerable recent progress made taking advantage measurement modes due increasing computing power machine learning techniques can code streams raw data photos, videos, sources translate usable data. translation raw data coded data can used analysis part \\(Q\\) measurement strategy.data collected, often, can also affect validity reliability. inquiry guide data collected relation events election holiday period time treatment delivered research participants. inquiry defines whether effect interest month treatment case long-term effects year . However, data need collected single time period. model encodes beliefs autocorrelation (correlation time) data varies time, can help guide whether collect multiple measurements just one whether measure data baseline well endline. data expected highly variable (low autocorrelation), taking multiple measurements averaging may provide efficiency gains. exhibit high autocorrelation, multiple measures waste resources — approximately measure returned time. However, high autocorrelation, large gains collecting baseline measure treatment experiment, controlling baseline outcomes major change response treatment. Thus, estimates treatment effects much efficient (T cite). Beliefs autocorrelation can guide decisions tradeoff including participants measures smaller set participants.Parallel considerations apply choice measure latent \\(Y_i^*\\) multiple measurement tools time period. possible, taking multiple, different measures averaging typically yield efficiency improvements. (cases, may tradeoff budget constraint terms survey length number subjects.) multiple measures can combined construct single measure \\(Y_i^*\\). tools produce answers highly correlated, taking multiple measures unlikely worth cost, correlation low, worth taking multiple measurements averaging improve efficiency.Beyond , two important features data collection strategy often. experiments, data often collected treatment delivered, sometimes baseline enable block randomization -comparisons. difference--difference studies, data always collected , buttress claims parallel trends treated control often multiple periods treatment. process tracing studies, data typically collected many periods change independent variable change dependent variable demonstrate connection two. often data collected treatment (change independent variable) influences reliability: measuring highly variable outcomes multiple times averaging can increase measurement reliability. flip side variables fast change, taking measurements baseline compare endline measurements likely increase reliability. outcomes change slowly, however, baseline measurements likely substantially improve efficiency estimates treatment effects (discussion, see cite T).One common pitfall measurement sensitivity bias affects measures self-reported participants presence social pressure risk sanction authorities prefer hide wish detect sensitive characteristics population (Blair, Coppock, Moor 2020). Sensitivity bias represents excludability violation: latent outcome \\(Y_i^*\\) depends measurement tool used. generally, excludability violations measurement imply act measurement affects latent outcome, often known Hawthorne effect. Typically, want avoid excludability violations, trying measure latent quantity exists independent measured.Selecting among measurement modes, data collectors, time periods, frequency, number measurements reduces tradeoffs validity reliability. want largest set valid, measures combine measurement \\(Y_i^*\\) meet budget logistical constraints. However, measurement techniques vary validity reliability, often must decide weighting two concerns given research goals. cases, validity important even measures unreliable, others, may want weight approximately equally.Learning measurement tools valid reliable ultimately guesswork, though can informed guesswork. measure true \\(Y_i^*\\), truly “validate” measurement technique. Often studies present validation studies comparing proposed measure “ground truth,” measured administrative data second technique reduce measurement error. However, neither measurement known exactly \\(Y_i^*\\), ultimately studies comparisons multiple techniques advantages disadvantages. make useless, rather used understand measurements differ average variability. may also useful informing combine multiple measures.","code":""},{"path":"crafting-a-data-strategy.html","id":"threats-to-implementation","chapter":"8 Crafting a data strategy","heading":"8.4 Threats to implementation","text":"\nFigure 8.8: DAG exclusion restrictions.\nData strategies plans sample units, assign treatments, measure outcomes. studies always go according plan. section, explore threats inferences emerge implementing data strategies. Anticipating planning threats presents two benefits: can assess properties designs actually run, obtaining realistic guesses power bias study given problems like attrition noncompliance; can incorporate procedures mitigate risk threats part designs., adapt figure first present chapter’s introduction introduce threats come noncompliance (failure treat), attrition (failure included sample provide measures), excludability violations (causal effects random sampling, random assignment, measurement latent outcome).","code":""},{"path":"crafting-a-data-strategy.html","id":"noncompliance","chapter":"8 Crafting a data strategy","heading":"8.4.1 Noncompliance","text":"first type threat implementation noncompliance: random assignment \\(Z\\) imperfectly manipulates treatment variable \\(D\\). absence noncompliance, \\(D_i = Z_i\\). One-sided noncompliance setting treated units fail treated (receive control condition instead). Two-sided noncompliance units fail treated, addition units assigned control group receive treatment.randomized experiments, noncompliance may happen due administrative error, miscommunications researcher partners, shortages materials staff, transportation problems, participants refusing treatment, inability researchers find participant treat, among problems.Noncompliance also affects observational designs causal inference nature non-random administrative process affects treatment threshold cut-. instrumental variables designs, noncompliance core: instrument \\(Z\\) causally affects treatment \\(D\\), factors also affect \\(D\\). regression discontinuity design, threshold determines treatment status certain value scale unit treated . noncompliance can also occur, leading fuzzy regression discontinuity designs, threshold imperfect units receive treatment units .two-sided noncompliance, four types participants, determined potential outcomes \\(D\\) function condition unit randomly assigned two. two-arm trial, two treatment potential outcomes: whether receive treatment assigned control (\\(D_i(Z_i = 0)\\)) whether receive treatment assigned treatment (\\(D_i(Z_i = 1)\\)). four types enumerated labeled Table 8.3 .Table 8.3:  Compliance typesNever-takers never take treatment, matter treatment assigned. Compliers take exactly treatment assigned. Defiers exactly opposite. assigned treatment, refuse , assigned control, take treatment. Like name suggests, always-takers take treatment regardless treatment condition assigned . settings, can rule types basis design information. Canvassing experiments, example, feature always takers way “take treatment” unless assigned treatment, never-takers subjects might home canvassers come knocking. experiments said experience one-sided noncompliance.presence noncompliance, change inquiry inevitable. average treatment effect estimated comparing assigned treatment assigned control, assignment differs whether treatment received. average difference assigned two conditions can relabeled intent--treat effect. Comparing received treatment also option, without additional herculean assumptions, unobserved heterogeneity now jointly affects \\(D\\) \\(Z\\). randomized experiment broken: treatment group longer comparable control group expectation. Instead, complier average treatment may obtained using instrumental variables estimation, implies switching local inquiry among complier types. effect may differ average treatment effect compliers differ systematically types. Estimating complier average treatment effect requires addition assumptions top randomized experiments, including ignorability treatment assignment , case two-sided noncompliance, monotonicity assumption rules defiers.case randomized experiments, spending budget time carefully design treatment delivery protocols avoid noncompliance help avoid minimize threat noncompliance.parallel set decisions faces designer observational study noncompliance treatments. Instrumental variables designs imply noncompliance inquiry complier average treatment effect (cases, intent--treat effect reduced form effect also interest). Researchers adopt regression discontinuity designs also focus local effect among units near threshold, case fuzzy regression discontinuity design noncompliance must switch complier local average treatment effect.Compliance need binary: assigned treatment, may receive partial treatment none . However, typically difficult measure partial compliance, difficult still account possibility analysis (separate effects partial compliance full compliance without treatments attempt manipulate treatment less strongly).multi-arm trials continuous rather binary instruments, noncompliance becomes complex problem define address data strategy answer strategy. must define complier types according (potentially-infinite) possible treatment conditions. multiarm trials, complier types first treatment may second treatment; words, units comply different rates different treatments. Apparent differences complier average treatment effects intent--treat effects, result, may reflect differences treatment effects different rates compliance.","code":""},{"path":"crafting-a-data-strategy.html","id":"attrition","chapter":"8 Crafting a data strategy","heading":"8.4.2 Attrition","text":"Attrition occurs measures units sampled. two types missing data may result: single measure missing, commonly known item nonresponse; measures missing participant, known survey nonresponse. Though terms coined survey researchers, problems identical collected surveys passive measurement instrumentation. Item nonresponse may result many causes, often respondents wish answer specific questions sensitive intrusive. Survey nonresponse questions answered may instead due lack interest measurement presented, inability contact subject, inability data collector build rapport gain trust participant.Attrition occurs least one unit respond least one measurement item. Whether attrition problem depends whether participant response \\(R\\) causally affected variables sampling completely random. Though missingness completely random rare, possible, often due administrative computer error. attrition completely random, effect variable \\(R\\), loss sample size distortion estimates.descriptive inquiries, attrition completely random, types units respond different way respond. Model-based inference required adjust estimates match characteristics original sample, upweighting types respondents less likely respond downweighting likely respond. However, guess wrong set variables reweight model \\(M\\), miss target.causal inquiries, also need know whether random assignment \\(Z\\) affects \\(R\\), well \\(U\\). \\(Z\\) affects \\(R\\) (lower/higher rate attrition one treatment group ), \\(U\\) affect \\(R\\), just loss sample size distortion estimates. simply (less) precise estimate potential outcomes one group . However, \\(Z\\) \\(U\\) affect \\(R\\), random sample either treated potential outcome control potential outcome treatment control group. Therefore, resort model-based inference groups, reweighting group based assumptions (\\(M\\)) \\(U\\) \\(Z\\) affect \\(R\\). Alternatively, can construct bounds estimates causal effect, incorporate uncertainty due non-random attrition.case descriptive causal inquiries, also design-based way estimate effects presence uncertainty rely assumptions \\(M\\): double sampling. missing outcomes, can take second sample respond conduct additional measurement try obtain units’ outcomes. spend resources making sure can obtain measures randomly-sampled units, can use random sample non-responders combination original sample construct full sample. still loss sample size, take subsample second round, attempt interview nonresponders. can also use double sampling causal inquiries, conducting random sample nonresponders treatment group stand missing observations second control group.","code":""},{"path":"crafting-a-data-strategy.html","id":"excludability","chapter":"8 Crafting a data strategy","heading":"8.4.3 Excludability","text":"final type threat implementation research excludability violations. Excludability means define potential outcomes, can exclude variable definition potential outcome. define treated potential outcome latent outcome \\(Y_i^*(D_i = 0)\\), invoke four important excludability assumptions: effect sampling \\(S\\), treatment assignment \\(Z\\) (except treatment \\(D\\)!), measurement \\(Q\\) latent outcome. invoke assumptions, must define treated potential outcome \\(Y_i^*(S_i, Z_i, D_i = 0, Q_i)\\).problem simply one definitions: order define inquiry average treatment effect, average possible values \\(S_i\\), \\(Z_i\\), \\(Q_i\\). Many potential outcomes, nonsampled units (\\(S_i = 0\\)), impossible obtain known complex potential outcomes. average impossible--observe outcomes, thus identify even infinite data quantity like average treatment effect absence excludability assumption.Excludability assumptions required identification causal descriptive claims data, order interpret effects way want . can often identify average treatment effect separate (exclude!) effects treatment assignment measurement.three excludability assumptions \\(S_i\\), \\(Z_i\\), \\(Q_i\\) represented gray dotted lines Figure 8.8, strong assumptions often met practice.first excludability assumption causal effect sampling \\(Y_i^*\\). assumption violated fact included sample changes attitudes. asked focus group, even assigned treatment asked survey questions, reflect political beliefs change mind, sampling excludability assumption may violated.excludability \\(Z\\) \\(Y^*\\) subtle. studying causal effects, certainly believe treatment assignment effect outcomes. treatment assignment excludability assumption says effect works received treatment \\(D\\). words, independent effect assigned apart actually treated. many ways assumption might violated. observational studies using instrumental variables, excludability \\(Z\\) \\(Y^*\\) assumption alternative channels instrument affects outcomes except endogenous variable. studies effect economic growth civil conflict using rainfall instrument economic growth, must invoke excludability assumption rainfall independent effect civil conflict besides economic growth.Equally worrying excludability measurement assumption, \\(Q\\) affect \\(Y^*\\). Hawthorne effects, fact measured changes outcomes, another word violation excludability assumption. outcomes depend whether subjects know measured , exclude effect measurement effects.final excludability assumption represent DAG \\(Z\\) must effect \\(Q\\). whether measure outcomes depend whether unit assigned treatment. excludability assumption commonly referred requirement measurement parallel across treatment conditions. measure outcomes using survey treatment group using remote measurement device control, separate (exclude!) effect measurement effect treatment.Researchers avoid excludability violations possible report . cases, best can produce unbiased estimates effect exclude multiple causes. Future research can used disentangle credibly invoke excludability assumption.","code":""},{"path":"crafting-a-data-strategy.html","id":"interference","chapter":"8 Crafting a data strategy","heading":"8.4.4 Interference","text":"four endogenous outcomes DAG research design : \\(R\\), whether participant responds data collection; \\(D\\), whether respondent receives treatment; \\(Y^*\\), latent outcome; \\(Y\\), observed outcome. Setting aside attrition noncompliance moment, \\(R\\) function sampling; \\(D\\) treatment assignment; \\(Y^*\\) \\(D\\); \\(Y\\) measurement strategy \\(Q\\). Interference problem endogenous variables depend whether sampled, assigned treatment, measured, whether units sampled, assigned treatment, measured. assume, example, \\(Y_i(Z_i) = Y_i(\\mathbf{Z})\\). words, \\(Y_i\\) outcome unit \\(\\), function treatment assignment status \\(Z_i\\) units (\\(\\mathbf{Z}\\). noncompliance assignment, assume instead \\(Y_i(Z_i, D_i) = Y_i(\\mathbf{Z}, \\mathbf{D})\\) \\(D_i(Z_i) = D_i(\\mathbf{Z})\\), need know assignment received treatment status individual \\(\\) know outcome need know whether unit assigned treatment know received treatment.interference assumption strong. Interference common, perhaps even ubiquitous, often referred spillovers. Interference may occur participants tell treatment status, also may occur subtle ways. treatments displace outcomes crime, reduce crime treated locales crime simply moved localities. overall crime rate may change. Displacement form interference. Resource allocation decisions affected treatment also often suffer interference. citizens treated information report potholes , roads agency may move resources untreated areas treated areas, increasing number potholes control areas.Interference can also induced sampling. potential outcomes depend whether units included sample, sampling interference, .e. \\(Y_i(S_i = 1) \\neq Y_i(\\mathbf{S})\\). subjects asked questions induce consider relative status respect people, people sample may affect comparison group potential outcomes.Interference can affect measurement, just affects sampling treatment assignment. Measurement interference occurs \\(Y_i^*\\) depends whether units measured. participants experiment discuss measurement technique researchers asking participants, may lead violation. Psychology researchers academic campuses worry possibility encourage subjects discuss lab potential participants. worried interference treatment assignment also measurement, often implicit measurement techniques can defeated (measure something \\(Y_i^*\\)) concept known session. researchers worry measuring sensitive questions sensitive topic revealed participants either earlier participants local leaders discover purpose.presence interference, unable obtain random sample potential outcome \\(Y_i(Z_i = 1)\\). reason , even simplified setting, units longer either receive treatment , may directly treated, indirectly treated near directly treated, far others truly receive treatment. Thus, group assigned control made units receive indirect treatment treatment. receive purely determined randomization scheme: whether indirectly treated depends close distance social connections units. obtain random sample one potential outcomes, produce unbiased estimates quantities like average treatment effect even average effect depend . may, however, able change inquiry, switching set units can credibly assume interference.cases, interference individuals within household may highly likely (read family’s mail), interference households may less likely. others, interference may likely across households neighborhood neighborhoods. case, may able random sample \\(Y_j(Z_j = 0)\\), potential outcome neighborhood \\(j\\), randomly assigning households within neighborhood receive treatment condition (.e., cluster random assignment). represents change inquiry – potential outcomes individuals potential outcomes neighborhood – presence interference change may required. Similar solutions available presence sampling measurement interference.Another way change inquiry sample units inclusion study separated physical social distance interference unlikely. might restrict sample include people separated least one mile least two social connections (friends friends--friends ruled ). , sample individual potential outcomes, research design complicated heterogeneous sampling probabilities. high density areas, low sampling inclusion probabilities low density areas high probabilities, easier find samples people separated distance areas low density. fixed problem interference, still units indirectly treated, constructed sampling procedure prevent sampling units design. sample units either randomly assigned treatment control separating units none sample indirectly treated.Interference can also occur time. Potential outcomes can depend whether unit treated present time period, also past. Time interference occurs contemporaneous potential outcome depends sampling, treatment, measurement past. Design can help avoid sampling partially treated units introducing washout periods researcher assumes subjects forget (longer affected ) past treatments research activities.","code":""},{"path":"crafting-a-data-strategy.html","id":"further-reading-6","chapter":"8 Crafting a data strategy","heading":"8.5 Further reading","text":"Lohr (2010) sampling.Shadish, Cook, Campbell (2002), Alan S. Gerber Green (2012) experimental designSeawright Gerring (2008b) Lieberman (2005) case selectionCollier (2011), Fairfield (2013), Humphreys Jacobs (2015) process tracing","code":""},{"path":"choosing-an-answer-strategy.html","id":"choosing-an-answer-strategy","chapter":"9 Choosing an answer strategy","heading":"9 Choosing an answer strategy","text":"answer strategy plan information gathered world order generate answer inquiry. Qualitative quantitative methods courses overflow advice answer strategies choose. conditions use Ordinary Least Squares, use logit? machine learning algorithm appropriate choice comparative case study informative? answer strategy worth pursuing fundamental limitations data strategy?perspective answer strategies informed three elements research design: model, inquiry, data strategy. Sometimes answer strategy advice offered basis realized data \\(d\\) , without particular attention paid important features question answered manner data collected generated. example, bit received methodological wisdom holds whenever dependent variable binary, binary choice statistical model like logit probit must used. advice based knowledge \\(d\\) contains binary outcome; whether appropriate depends features \\(\\) features \\(D\\). example, \\(\\) average treatment effect \\(D\\) includes randomization treatment, answer strategies beyond binary choice models may serve just well better. student asks, “estimator use?” response always, “depends.” depend ? model, inquiry, data strategy.Choosing “design-aware” answer strategies sounds straightforward enough, precise way approach applied real empirical setting course differ case case. highest-level advice want pick answer strategy \\(\\) answer provides, \\((d) = ^d\\), close answer model world, \\((m) = ^m\\). implication strive parallelism across \\(\\) \\(\\). idea sometimes described “plug-principle.” function \\(\\) close function \\(\\), can “plug ” \\(d\\) \\(m\\). Following plug-principle leads straightforward estimation procedures. inquiry population mean, can use sample mean estimator estimate , long data strategy produces data \\(d\\) sufficiently representative \\(m\\). extent possible, want choose data strategies enable plug-estimator answer strategies.level, answer strategies rely unverifiable hopefully plausible assumptions. descriptive inference, assume sample represents population well. causal inference, assume treated untreated units similar respects beyond treatment status. kinds inference, assume measurements close latent constructs wish measure. Answer strategies model-based consequential assumptions part \\(M\\). Answer strategies design-based extent can pull assumptions \\(M\\) assure design, .e., choosing \\(D\\) way can confident assumption true. Observational causal inference often relies assumption “selection observables,” claim within groups units observed characteristics, treatments -randomly assigned. Observational causal inference model-based sense selection observables assumption grounded researchers’ theoretical model world, course might wrong. Experimental causal inference design-based sense can sure treatments indeed randomly assigned random assignment part data strategy. observational experimental settings, need make assumption, one case, rely theoretical model , rely data strategy. possible, prefer assuring assumptions design asserting basis theoretical model.Answer strategies come number varieties. familiar point-estimators produce estimates parameters. Ordinary least squares, difference--means, logit, random forests, long list others point-estimator answer strategy class. second class comprised tests. Tests return binary decision (True False). Null hypothesis significance tests common form test quantitative research. Qualitative researchers also employ tests; go names like “hoop test” “straw---wind” test. Bayesian answer strategies return full posterior distributions. contrast point-estimators, answer strategies interval-estimators. class estimator choose – particular estimator within class select – depend features model, inquiry, data strategy. sections follow walk important ways design features influence answer strategy, though means offer full accounting. range estimation approaches vast book place tally strengths weaknesses. Instead, goal provide framework thinking go choosing one answer strategy another.","code":""},{"path":"choosing-an-answer-strategy.html","id":"following-the-model","chapter":"9 Choosing an answer strategy","heading":"9.1 Following the model","text":"described chapter 6, can (non-parametrically) express model world directed acyclic graph, DAG. DAGs express parts model, precisely nonparametric. don’t encode variables cause , just whether . Even , writing theoretical model parsimonious form nonparametric structural causal model can guide answer strategies enormously powerful way. Given DAG, can learn whether answer strategy sufficient estimating causal effect. , can learn variables answer strategy must condition , must left alone.want estimate particular causal relationship, average causal effect \\(D\\) \\(Y\\), can read DAG whether relationship identified —informally, whether quantity care can distinguished quantities. common (useful) criterion identification “backdoor criterion.” exists unblocked “backdoor path” D Y, relationship identified.19 backdoor path causal path begins arrow D ends arrow Y. Backdoor paths can blocked two ways: conditioning analysis variable along path conditioning “collider” along path.reproduce definition backdoor criterion given Pearl (2009) (p. 79), swapping variable names using book:Given ordered pair variables D, Y DAG G, set variables X, satisfies backdoor criterion relative D, Y node X descendant D, X blocks every path D Y contains arrow D.definition hand, can inspect DAGs find “adjustment sets.” adjustment set set variables may conditioned upon answer strategy. “Conditioned upon” sufficiently vague phrase include conditioning procedures controlling variable regression setting stratifying analysis according variables.general, \\(X\\) adjustment set satisfies backdoor criterion, can estimate conditional probability distributions \\(Y\\) level \\(D\\) using expression.\\[\\Pr(Y = y \\mid (D=d)) = \\sum_x \\Pr(Y = y \\mid D = d, X = x) \\Pr(X = x)\\]can write expression using potential outcomes notation:\\[\\Pr(Y_i(D_i = 1) = y) = \\sum_x \\Pr(Y_i(D_i = 1) = y \\mid X_i = x) \\Pr(X_i = x)\\]Figure 9.1 shows three DAGs three variables D, X, Y, well additional unmeasured variable \\(U\\) (consider \\(U\\) later sections). three cases, inquiry average treatment effect D Y, three cases, X, D, Y correlated. first case, \\(X\\) confounds causal relationship D Y, say simply compared units different levels D, estimates causal effect prone bias. However, conduct analysis separately within levels X (say, condition X), combine separate analyses, overall estimate unbiased. first DAG setting analysts mind controlling observables order estimate causal effects.dangerous possibility, however, represented second DAG. causal graph, X doesn’t confound relationship D Y – instead, downstream consequence variables. analyst mistakenly conditions X, noncausal confounding path opens D Y, biasing estimates average effect D Y. contrast first second graphs illustration general principle theoretical models guide analytic choices. first case, estimates unbiased control \\(X\\), second case control X.third case describes setting D direct effect Y indirect effect travels X – case, X mediator. condition X, estimate effect D Y biased (X descendant D). Like collider case, effect identified control X, identified condition. intuition problems associated controlling mediators: controlling X “controls away” portion effect.example illustrates small way model world guides answer strategy. three cases, principle dataset D, X, Y. followed common regression advice, control X three cases correlated D Y – approach works X confounder. X collider mediator, control strategy induce bias. Without changes design, empirical tests can distinguish three DAGs, real way, theoretical assumptions Model must relied upon correctly choose control strategy.\nFigure 9.1: Three roles variable X.\nconsider three possible paths estimating effect treatment \\(D\\) \\(Y\\). First, consider conditioning \\(X\\) without making additional assumptions DAG; second, consider invoking ignorability assumption, often known selection--observables; third, consider randomizing treatment measuring \\(X\\) treatment. Finally, consider role estimation narrowing set possible DAGs describe world.","code":""},{"path":"choosing-an-answer-strategy.html","id":"identification","chapter":"9 Choosing an answer strategy","heading":"9.1.1 Identification","text":"can learn answer inquiry certainty inquiry identified, meaning infinite data distinguish true value alternatives. Identification can obtained backdoor criterion met, frontdoor criterion met, causal relationships \\(D\\) \\(Y\\). conditions violated several circumstances, including observed confounder \\(X\\) confounds relationship \\(D\\) \\(Y\\) left unadjusted unobserved confounder \\(U\\).reason causal identification, turn back DAGs. start simple DAG four variables: treatment \\(D\\), outcome \\(Y\\), observed variable \\(X\\) (may may confound relationship \\(D\\) \\(Y\\)), unobserved variable \\(U\\). six edges variables, three possible relations: variable may cause (e.g., \\(D \\rightarrow Y\\)), caused (e.g., \\(D \\leftarrow Y\\)), causally related every variable. three possible relationships six edges, \\(3^6 = 729\\) conceptually possible DAGs. rule DAGs variables cause \\(U\\), defined \\(U\\) unobserved confounder. leaves 216 possible combinations. know one! goal researchers narrow possible DAGs., collect data three observable variables, Table 9.1. faced choice answer strategy. presence possible confounding \\(X\\) /\\(U\\), several options research design: can estimate causal effect \\(D\\) \\(Y\\) controlling observed confounder \\(X\\) (), invoking additional conditional independence assumptions, randomizing treatment. case, aim rule DAGs causal effect identified.\nTable 9.1: Simulated data DAG variables \\(X\\), \\(D\\), \\(Y\\).\n","code":""},{"path":"choosing-an-answer-strategy.html","id":"consequences-of-conditioning","chapter":"9 Choosing an answer strategy","heading":"9.1.1.1 Consequences of conditioning","text":"first strategy can control observed variable \\(X\\) . Whether controlling \\(X\\) enables, prevents, affect causal identification effect \\(D\\) \\(Y\\) depends beliefs DAG true DAG.illustrate possible beliefs edges visualizing 216 DAGs \\(U\\) unobserved confounder (see Figure 9.2). large groups three squares nine group DAGs based \\(U\\) affects variables model: top left grouping 27 DAGs represents \\(U\\) affects \\(D\\), \\(X\\) \\(Y\\). Within grouping, three squares nine squares. group nine squares represents set DAGs single relationship \\(D\\) \\(Y\\). top left group nine DAGs \\(D\\rightarrow Y\\).can rule several 216 acyclicity: gray squares graphs cyclical. (operate view cycling possible world, imply variables simultaneously cause .)color rest squares terms whether controlling \\(X\\) identify causal effect \\(D\\) \\(Y\\).cases, effect \\(D\\) \\(Y\\) identified regardless whether control \\(X\\) (white squares). example, left DAG, \\(X\\) affects \\(D\\) \\(U\\) affects \\(Y\\) neither affects . causal effect identified path \\(D\\) \\(Y\\) except direct effect \\(D\\rightarrow Y\\). words, neither \\(X\\) \\(U\\) confounds relationship, potential outcomes \\(Y\\) independent \\(D\\).cases, effect \\(D\\) \\(Y\\) identified control \\(X\\) (blue squares). situations \\(X\\) confounds relationship additional confounding unmeasured confounders. cases potential outcomes \\(Y\\) independent \\(D\\) conditional \\(X\\).However, conditioning \\(X\\) risky, one blue DAGs might purple DAGs effect \\(D\\) \\(Y\\) identified condition \\(X\\) (purple squares). DAGs \\(X\\) collider, opening backdoor path \\(D\\) \\(Y\\) aside direct effect \\(X\\) conditioned . knew one DAGs, control \\(X\\).also many situations — majority fact, pink squares — relationship \\(D\\) \\(Y\\) never identified, regardless whether control \\(X\\). result confounding unobservable confounders \\(U\\). upper left quadrant contains cases \\(U\\) affects variables, middle left \\(U\\) affects \\(D\\) \\(Y\\). cases, identification require minimum ability control unmeasured confounders. situations causal identification fails causal order \\(D\\) \\(Y\\) reversed, .e. \\(Y\\) causes \\(D\\). Without additional assumptions control order variables collected data strategy, rule possibility, first fourth columns subgraphs labeled \\(D\\leftarrow Y\\).Without additional assumptions manipulation data strategy, know among 200 acyclic DAGs represented plot. result, danger either effect \\(D\\) \\(Y\\) unidentified regardless , making wrong choice control control.\nFigure 9.2: Consequences conditioning variable X 216 possible DAGs.\n","code":""},{"path":"choosing-an-answer-strategy.html","id":"adding-model-based-assumptions","chapter":"9 Choosing an answer strategy","heading":"9.1.1.2 Adding model-based assumptions","text":"can address problem unmeasured confounding invoking conditional independence assumptions. ``selection--observables’’ answer strategy invokes assumption \\(D\\) statistically independent potential outcomes \\(Y\\) given X, .e. adjusting \\(X\\). words, controlling \\(X\\) blocks backdoor paths \\(D\\) \\(Y\\). assumption also known conditional independence ignorability assumption.Figure 9.3, display 216 possible causal graphs , ruling gray cyclical. Among 200 remain, color way controlling \\(X\\) add fourth color: lavender DAGs rule based conditional independence assumption. ruled , upper left quadrants, unobserved confounding \\(U\\) addressed adjusting \\(X\\).Analysts invoke conditional independence assumption assure many fewer circumstances identification possible either controlling \\(X\\) controlling \\(X\\). However, strong assumption possible test directly. Instead, analyst must justify assumption based circumstantial qualitative quantitative evidence. task rule evidence either relationship \\(U\\) \\(D\\) relationship \\(U\\) \\(Y\\) . first case, evidence might take form background knowledge values \\(D\\) determined. treatment might assigned using cut-rule, case cut-assigned treatment (e.g., admitted college) . case, relationship unobserved variables \\(U\\) treatment \\(D\\), relationship \\(X\\) (score) \\(D\\). Controlling \\(X\\) enable causal identification even \\(Y\\) affected \\(U\\). However, assumption conditional independence \\(U\\) \\(D\\) given \\(X\\) strong assumption: analyst must sure unobserved variable \\(U\\) directly affects \\(D\\), legacy applicants may “pushed” threshold cut-close enough .selection--observables, still many DAGs raise problems us. still many DAGs \\(D\\leftarrow Y\\), \\(D\\) caused \\(Y\\) instead way around. rule assumption, never identify causal effect \\(D\\) \\(Y\\) regardless whether control \\(X\\). DAGs blue (effect identified condition \\(X\\) otherwise) purple (opposite, achieve identification condition \\(X\\)) still remain. blue involve \\(X\\) confounding relationship \\(D\\) \\(Y\\) purple \\(X\\) collider conditioning opens backdoor path \\(U\\). words, conditional independence \\(D\\) \\(U\\) given \\(X\\) insufficient identify effect, without ruling scenarios.\nFigure 9.3: Consequences conditional ignorability assumption 216 possible DAGs.\n","code":""},{"path":"choosing-an-answer-strategy.html","id":"design-based-identification","chapter":"9 Choosing an answer strategy","heading":"9.1.1.3 Design-based identification","text":"unable rule confounding assumption adjustment, can randomly assign \\(D\\) sever connections unobserved variables \\(U\\) \\(D\\) design. Ruling confounders assumption adjustment requires model correct, often known model-based inference, whereas ruling confounders design labeled design-based inference.20 addition, can measure \\(X\\) treatment rule situations \\(D\\) causes \\(X\\), can lead collider bias opening backdoor paths \\(D\\) \\(Y\\)., dramatically reduce set possible DAGs, set causal order \\(X\\) \\(D\\), dramatically expand number settings effect \\(D\\) \\(Y\\) identified due restriction causal order randomization \\(D\\) guarantees ignorability \\(U\\).Figure 9.4, swap colors now indicate DAGs ruled measuring \\(X\\) treatment (salmon squares), ruled random assignment (blue squares). remaining white squares, effect \\(D\\) \\(Y\\) causally identified, regardless whether adjust \\(X\\) . remove conditionality inference depending whether control. good, ultimately even presence strong ignorability assumptions outlined last section many possible DAGs controlling failing control lead bias. Now, inferences depend guessing correct DAG.\nFigure 9.4: Consequences randomizing Z measuring X treatment 216 possible DAGs.\nshort, controlling timing measurement \\(X\\) randomizing \\(D\\) move assumptions conditional independence \\(M\\) assumed (possibly incorrect!) model world data strategy, control can guarantee design.","code":""},{"path":"choosing-an-answer-strategy.html","id":"estimation","chapter":"9 Choosing an answer strategy","heading":"9.1.2 Estimation","text":"identification causal effect \\(D\\) \\(Y\\) either assumption design enables us undertake two tasks: estimate average treatment effect, estimate sign effect, estimate whether effect .first task, estimating magnitude effect \\(D\\) \\(Y\\), can accomplished using model-based inference selection--observables design randomized experiment. cases, apply plug-principle, replacing true unknown average potential outcomes treatment (control) sample analogues, average outcomes treatment (control) group. data randomized experiments, \\(X\\) ignorable, can either adjust (may reduce variability estimates) without bias. selection--observables, rule many DAGs assumption, including cases controlling \\(X\\), controlling \\(X\\), open leave open backdoor path \\(D\\) \\(Y\\).estimating sign effect, can calculate sign effect magnitude, conduct statistical test null hypothesis zero effect distinguish among zero, positive, negative effects.third task, determining whether effect , similarly involves statistical test null hypothesis zero effect. fail reject null, posterior belief effect, reject two-sided test leave believing effect. zero average effect null hypothesis test can help us take final step distinguishing among 216 possible DAGs representing relationships \\(D\\) \\(Y\\) confounders \\(X\\) \\(U\\). Figure 9.5, display 16 DAGs identified random assignment pretreatment measurement \\(X\\). need use data two-sided null hypothesis test distinguish top row (effect \\(D\\rightarrow Y\\)) bottom row (effect). design got us way , need use data narrow one two rows eight DAGs. Since inquiry causal relationship \\(D\\) \\(Y\\), may concerned distinguishing among eight. , need develop alternative research design order learn causal effects \\(X\\).\nFigure 9.5: sixteen DAGs consistent pretreatment measurement X randomization Z. Hypothesis testing can distinguish among eight DAGs effect Z Y (top row) eight effect Z Y (bottom row).\n","code":""},{"path":"choosing-an-answer-strategy.html","id":"robustness","chapter":"9 Choosing an answer strategy","heading":"9.1.3 Robustness","text":"Robustness checks modified answer strategies aim demonstrate “robustness” “sensitivity” answer strategy choices. Often primary analysis quantity interest effect treatment \\(D\\) outcome \\(Y\\) presented appendix set modified analyses presented. author discusses whether variation magnitude statistical significance estimates across specifications.Robustness checks implicitly response fundamental uncertainty identified last several sections true DAG. know! result, robustness check motivated particular DAG. complete set robustness checks, can make claims “robust” estimates effect \\(D\\) \\(Y\\) alternative DAGs. robustness analysis aim falsify DAG, case controlling \\(X\\). three variables \\(X_1\\), \\(X_2\\), \\(X_3\\) either confound relationship \\(D\\) \\(Y\\) play role causal system, may want present robustness checks including excluding combination three variables. , rule variable collider mediator assumption. analyses can help distinguish among DAGs give us evidence effect \\(D\\) \\(Y\\) across possible DAGs involve confounding combination \\(X_1\\), \\(X_2\\), \\(X_3\\). However, important highlight assumptions make selecting DAGs ruling relationships. controlling \\(X_2\\) sensible strategy, must invoke assumption \\(X_2\\) collider. may , clear can learn adjusting \\(X_2\\). find inclusion wipes effect \\(D\\) \\(Y\\) may causal relationship \\(X_2\\) collider.illustrate simple analysis correlation two variables y1 y2, true positive correlation. y2 also function observed covariate x measurement error. main analysis bivariate regression predicting y2 y1. compare answer strategy one run analysis, also run robustness check controlling x. , analyst, unsure true DGP wish demonstrate reviewers results dependent functional form choose.Using MIDA way thinking designs, discuss diagnosis section another notion “robustness” design. typical way think robustness checks multiple secondary analyses conditional observed data build confidence analysis fixed data. However, motivation robustness checks uncertainty true data generating process. declaring design terms MIDA, can think robustness single estimator multiple possible true data generating processes. estimator robust sense one unbiased low uncertainty regardless , say, true functional form y1 y2. determine whether estimator robust, can redefine set designs different functional forms assess rate correct decisions robustness checks strategy different model.","code":""},{"path":"choosing-an-answer-strategy.html","id":"following-the-inquiry","chapter":"9 Choosing an answer strategy","heading":"9.2 Following the inquiry","text":"Answering descriptive inquiries involves studying existence nodes values take . might study whether behavior exists world, measure frequency. might also study frequency behavior varies time across people. answer strategy, following plug-principle, often involve average value node sample estimator average value population.contrast, target causal inquiry answer strategy, interested existence (non-existence) edge two nodes. can label two \\(D\\) (treatment) \\(Y\\) (outcome).","code":""},{"path":"choosing-an-answer-strategy.html","id":"plug-in-principle","chapter":"9 Choosing an answer strategy","heading":"9.2.1 Plug-in principle","text":"plug-principle refers idea good estimates population quantities \\((m) = ^m\\) can often generated choosing \\(\\) similar \\(\\) “plugging-” \\(d\\) \\(m\\). Suppose inquiry average treatment effect among \\(N\\) units population.\\((m) = \\frac{1}{N}\\sum_1^N[Y_i(1) - Y_i(0)] = \\frac{1}{N}\\sum_1^NY_i(1) - \\frac{1}{N}\\sum_1^NY_i(0) = ATE\\)can develop plug-estimator average treatment effect replacing population means (\\(\\frac{1}{N}\\sum_1^NY_i(1)\\) \\(\\frac{1}{N}\\sum_1^NY_i(0)\\)) sample analogues:\\((d) = \\frac{1}{m}\\sum_1^m{Y_i} - \\frac{1}{N - m}\\sum_{m+1}^N{Y_i}\\),units 1 though \\(m\\) reveal treated potential outcomes remainder reveal untreated potential outcomes. difference--means estimator course depends strength analogy population mean sample mean, turn depends features Data strategy. many () forms random assignment, sample mean estimators treated untreated outcomes unbiased estimators. discussed Section 9.3, specifics random assignment strategy probabilities assignment vary unit must taken account.formally, Aronow Miller describe plug-estimator :..d. random variables \\(X_1, X_2, \\ldots, X_n\\) common CDF \\(F\\), plug-estimator \\(\\theta = T(F)\\) : \\(\\widehat\\theta = T(\\widehat F)\\).","code":""},{"path":"choosing-an-answer-strategy.html","id":"point-estimation","chapter":"9 Choosing an answer strategy","heading":"9.2.2 Point estimation","text":"written.","code":""},{"path":"choosing-an-answer-strategy.html","id":"estimating-the-variance","chapter":"9 Choosing an answer strategy","heading":"9.2.3 Estimating the variance","text":"written.","code":""},{"path":"choosing-an-answer-strategy.html","id":"tests","chapter":"9 Choosing an answer strategy","heading":"9.2.4 Tests","text":"written.","code":""},{"path":"choosing-an-answer-strategy.html","id":"partial-identification","chapter":"9 Choosing an answer strategy","heading":"9.2.5 Partial identification","text":"written.","code":""},{"path":"choosing-an-answer-strategy.html","id":"p2followdatastrategy","chapter":"9 Choosing an answer strategy","heading":"9.3 Following the data strategy","text":"However, order benefit two controlled decisions data strategy, must follow dictum due R.. Fisher “analyze randomize.” answer strategy follow data strategy. four components: make comparisons across randomly-assigned conditions; analyze data level random assignment; make comparisons groups within random assignment conducted (e.g., strata); adjust differences probabilities random assignment. parallel set rules govern answer strategies descriptive inference: draw random samples describing population; analyze data level random sampling (primary sampling unit); adjust differences probabilities sampling.Making comparisons across randomly-assigned conditions within groups random assignment conducted directly follows comparison causal identification assignment vs. design. make comparisons rely differences \\(D\\) \\(X\\), example, difference treatment effects two subgroups, susceptible confounding \\(U\\), \\(X\\) randomly-assigned. Similarly, analysis data block-randomized experiments can broken failing account blocking, blocks randomly assigned fact typically constructed outcomes units block similar within blocks different across blocks. differential probabilities assignment across blocks pool data ignoring blocking structure, unweighted comparisons may contrasting groups differ systematically. descriptive inferences, must always adjust different sampling inclusion probabilities.random sampling, sampling probability quantifies person’s likelihood sample. taking inverse probability, can generate weights allow us make really skewed sample representative target population. basic insight can extended random sampling many contexts. applies random treatment assignment, use inverse probability weighting make unbiased inferences presence spillovers heterogeneous assignment probabilities. even extends use poststratification nonrandom sampling inverse propensity weighting unbiased causal inference presence confounding. reason, understanding connection sampling probabilities inverse probability weights one important concepts descriptive causal inference. However, weights come mean can often seem confusing. fact, basic intuition simple random sampling analogy extends directly contexts.sample, select subset units stand unselected units. ’s mean say sample “representative”—units literally represent units (). can even quantify exact number units population unit sample represents. sample \\(n = 10\\) people group \\(N = 1000\\) people, make inference larger group smaller group, 10 people sample standing \\(N/n = 100\\) people population. sample people, person just represents one person, : \\(N/n = 1\\). Let’s call \\(w\\) number people population person sample represents: \\(w = N / n\\). let’s call \\(p\\) probability sample: \\(p = n / N\\). Notice \\(p\\) \\(w\\) reciprocal one another. Since number 1 divided fraction gives reciprocal, can use inverse \\(p\\) find \\(w\\): \\(\\frac{1}{p} = \\frac{1}{1} \\times \\frac{N}{n} = w\\). Now know inverse probability weight : count number people population given unit sample represents.Now suppose two samples, size \\(n_1 = n_2 = 10\\). first sample drawn population 10 people, \\(X = 1\\). second sample drawn population 20 people, \\(X = 0\\). want know true mean \\(X\\) across populations: \\(\\bar{X} = \\frac{10 + 0}{10 + 20} = \\frac{1}{3}\\). pool data take average \\(X\\), get wrong answer: \\(\\hat{\\bar{X}} = \\frac{1}{2}\\). get wrong answer people first group overrepresented: \\(1/2\\) sample group 1, \\(1/3\\) combined population group 1. terms sampling probabilities weights, every person first sample represents one person: \\(\\frac{1}{p_1} = \\frac{1}{1} \\frac{N_1}{n_1} = \\frac{1}{1} \\frac{1}{1} = 1 = w_1\\), whereas every person second sample stands two people, \\(\\frac{1}{p_2} = \\frac{1}{1} \\frac{20}{10} = 2 = w_2\\). weighting sample estimate average number people person pooled sample represents, recover right answer:\\[\\hat{\\bar{X}}_{IPW} = \\frac{\\sum X_i w_i}{\\sum w_i} = \\frac{n_1 \\times 1 \\times w_1 + n_2 \\times 0 \\times w_2}{n_1 \\times w_1 + n_2 \\times w_2} = \\frac{10 \\times 1\\times 1 + 10 \\times 0 \\times 2}{10 \\times 1 + 10 \\times 2} = \\frac{1}{3} = \\bar{X}.\\]Notice example equivalent stratified random sampling strategy, units group 1 sampled \\(p_1 = 1\\) units group 2 sampled probability \\(p_2 = 1/2\\). time use stratified random sampling probabilities vary among strata, weight estimates observation’s contribution equivalent number units represents, corrects - underrepresentation introduced sampling procedure.way thinking provides framework poststratification nonrandom samples. Suppose researcher comes across dataset 20 people. don’t know anything sampling strategy—, ’s non-random sample. However, know 10 people group 1 20 people group 2 population, able construct estimates sampling probabilities using \\(\\hat{p_1} = n_1/N_1\\) \\(\\hat{p_2} = n_2/N_2\\). , can construct weights using \\(1/p_1\\) \\(1/p_2\\) reweight data representative. ’s basic intuition behind post-stratification: use known population counts reverse engineer probability weights hypothetical stratified random sampling strategy given . See entry X design library .analogy extends even consider random treatment assignment procedures also sampling procedures. Rather sampling entire units, however, treatment assignments randomly sample different potential outcomes. assign 5/15 people treatment rest control, example, five treated potential outcomes need “represent” \\(1/(5/15) = 3\\) people’s treated potential outcomes ten people assigned control represent \\(1/(10/15) = 1.5\\) people’s control potential outcomes. tried pool different experiments get average treatment effect across people experiments, need take account fact different experiments might underrepresent treatment control potential outcomes, weight accordingly. issue arises block randomized designs treatment assignment probabilities vary block (see entry X design library), designs spillovers spatial networks induce differential assignment probabilities spillovers (see entry X design library), stepped wedge designs earlier waves overrepresent control potential outcomes later waves overrepresent treatment potential outcomes (see entry X design library). Finally, way poststratification uses covariates construct sample weights corresponding hypothetical random sampling procedure, matching selection observable designs use covariates reverse engineer probabilities sampling treatment control potential outcomes order reweight data account systematic selection certain kinds units treatment (confounding).","code":""},{"path":"choosing-an-answer-strategy.html","id":"qualitative-modes-of-inference","chapter":"9 Choosing an answer strategy","heading":"9.4 Qualitative modes of inference","text":"researchers don’t implement qualitative answer strategies writing executing code computer. Instead, use case knowledge reasoning arrive answer, weighing different sources evidence might include arguments made researchers, transcripts impressions interviews participant observation, various kinds archival records. Since much inferential work resides mind researcher, formalizing qualitative modes inference design diagnosis presents formidable task formalizing quantitative methods already available code.Yet possible often simpler might think represent qualitative modes inference using code. task also becoming easier due increasing use formal notation software represent qualitative inference describe . course, representation answer strategy code involve unrealistic simplifications. unrealistically simplistic representations researcher’s process still shed light stake often bewildering set design choices make.","code":""},{"path":"choosing-an-answer-strategy.html","id":"boolean-formalizations","chapter":"9 Choosing an answer strategy","heading":"9.4.1 Boolean formalizations","text":"One simplest ways formalize qualitative strategy constructing “-” statements, computers well. example, Blair et al. (2020), simulate researcher employing nested mixed methods analysis (Lieberman 2005) understand average causal effect. Formalizing procedure even simplest syntax revealed many different choices facing simulated researcher: share qualitative cases need validate large-N theory, example, latter determined valid? much effort allocate put building versus testing theories? Moreover, revealed choices mattered lot likelihood getting good answer.","code":""},{"path":"choosing-an-answer-strategy.html","id":"bayesian-formalizations","chapter":"9 Choosing an answer strategy","heading":"9.4.2 Bayesian formalizations","text":"Recent developments qualitative methods sought take Bayes’ rule “metaphor analytic tool” (Bennett 2015). approach characterizes qualitative inference one prior beliefs world can specified numerically updated basis evidence observed. minimum, writing answer strategy computer requires specifying beliefs, expressed probabilities, likelihood seeing certain kinds evidence different hypotheses. provide simple example strategy design library. Herron Quinn (2016) provide one approach formalizing qualitative answer strategy focuses understanding average treatment effect. Humphreys Jacobs (2015) provide approach can used formalize answer strategies targeting causal effect causal attribution inquiries, Fairfield Charman (2017) formalize Bayesian approach approaches causal attribution problem attaching posterior probability competing alternative hypotheses. Abell Engel (2019) suggest use “Supra-Bayesian” methods aggregate multiple participant-provided narratives ethnographic studies targeting causal attribution estimands.","code":""},{"path":"choosing-an-answer-strategy.html","id":"set-theoretic-formalizations","chapter":"9 Choosing an answer strategy","heading":"9.4.3 Set-theoretic formalizations","text":"Set-theoretic approaches formalizing qualitative reasoning employ deterministic concepts causal necessity sufficiency. Variants qualitative comparative analysis (QCA), typically seeks understand minimal set combinations factors sufficient produce outcome, implemented computers, therefore straightforward declare diagnose. QCA subject numerous simulation studies. provide example Blair et al. (2020).\n","code":""},{"path":"choosing-an-answer-strategy.html","id":"games-as-formal-narratives","chapter":"9 Choosing an answer strategy","heading":"9.4.4 Games as formal narratives","text":"counterfactual accounts causality (see Dawid 2000 alternative), causal inferences often rely speculations counterfactual state affairs. average outcome control group experiment, example, represents counterfactual average outcome treatment group assigned treatment. comparative historical analyses, conjectures often focus case level. Speaking “balance rule” introduced United States Senate Missouri Compromise 1820, provided veto power coalitions Northern Southern states policy, Weingast (1998) (p.176) speculates thatHad veto engineered prevent Northern assaults slavery, assaults actually observed, might succeeded. Southern veto senate made assaults slavery relatively rare.reaches conclusion analysis formal model extensive form.","code":""},{"path":"choosing-an-answer-strategy.html","id":"answer-strategies-as-procedures","chapter":"9 Choosing an answer strategy","heading":"9.5 Answer strategies as procedures","text":"Consider randomized experiment seeks estimate causal effect treatment. answer strategy just “logistic regression covariate adjustment”. includes every step process takes raw data, cleans recodes , considers 5 alternative estimators (DIM, OLS covariate adjustment, fancy thing colleague suggested couldn’t get converge), finally settling logit.Multiple estimates. Answer strategies can account many statistical tests conducting. Often, generating answer single inquiry, may construct multiple estimates provide different types answers varying quality. present results many null hypothesis tests, rate falsely rejecting least one tests even true goes , due multiple comparisons problem. plan adjust problem, adjustments part answer strategy, typically adjust p-values report decisions readers make . may three survey items imperfectly estimate latent quantity. presenting results, present three estimates three regressions, adjust three estimates using procedure family-wise error rate correction, average three items together index present one estimate one regression. three methods select change properties answer strategy.Analysis procedures. final estimator goes paper neither beginning end answer strategy. Procedures, , explore data determine final set estimates part answer strategy. Procedures summarizing multiple estimates one example many.Commonly, final estimator selected depended exploratory procedure multiple models assessed, example comparing model fit statistics. answer strategy research design fit final model — multiple step -procedure. procedures may part prespecified analysis plan may informal, may sometimes possible declare full design data obtained. (may find different analysis procedure data dependent preferable, diagnose design fact.) reason declare procedure rather final estimator diagnosis design may differ. procedure may powerful, example assessed multiple sets covariate controls selecting specification lowest standard error estimate. procedure may also exhibit poor coverage, accounting multiple bites apple.also sometimes find model planned run analyze data estimated. cases, iterative estimation procedure first model run, changes specification made, second third model presented result. full set steps — decision tree, depending estimable — answer strategy can evaluate whether good one realized data possible realizations decision tree decisions different.fact, examples analysis procedures types research, quantitative qualitative. Many strategies causal inference observational data involve estimation strategy set falsification placebo tests. answer provided research designs depends crucial way results tests: tests fail, design provides definitive answer. qualitative research, process tracing involves set steps, results depend information gathered earlier steps. Many mixed methods strategies also multi-step procedures. Nested designs involve running quantitative analysis selecting cases basis predictions regression. designs assessed considering single step procedure isolation.\nthings go according plan. compare answer strategies, can imagine estimators possible things go well well things go wrong, missing data outliers variables. good answer strategy, might single estimator, procedure ---, can handle states world. Procedures addressing deviations expected analyses part answer strategy. Even absence preanalysis plan, often way expect analyze data things go well. — data missing, noncompliance intervention, study suspended example — answers change. procedures determine answer study provides (cases ), part answer strategy. Standard operating procedures documents systematize procedures advance (Green Lin 2016).demonstrate fact properties procedures differ properties design final estimator simple example. compare two possible estimation specifications, without covariates, procedure run models report model paper lower p-value. models exactly , properties procedure differ properties either two possible models. particular, procedure higher power either two models, exhibits poor coverage, means bias measure uncertainty.","code":""},{"path":"choosing-an-answer-strategy.html","id":"interpretation","chapter":"9 Choosing an answer strategy","heading":"9.6 Interpretation","text":"answer strategies described thus far also yield single answer, whether point estimate, set bounds, p-value. Yet take tiny proportion writeup study’s results. Much ink taken tables report , figures visualize , results discussion sections describe . Importantly, report, visualize, describe results change depending realization data. choices often depend results simple ways, scale outcomes changing, complex ways, whether result even reported paper statistically significant.report, visualize, describe results part answer strategy. answer provide readers depends components, numbers pop regression. Considering aspects answer strategy change depending data strategy turns ","code":""},{"path":"choosing-an-answer-strategy.html","id":"further-reading-7","chapter":"9 Choosing an answer strategy","heading":"9.7 Further reading","text":"Gelman Hill (2006) [stories] multilevel modeling.Aronow Samii (2016) generalizability regression estimators observational settings.Van Evera (1997) hoop tests.","code":""},{"path":"p2diagnosis.html","id":"p2diagnosis","chapter":"10 Diagnosis","heading":"10 Diagnosis","text":"Research design diagnosis process evaluating properties research design. Since “property research design” cumbersome phrase, made word “diagnosand” refer properties research design like diagnose. Many diagnosands familiar. Power probability obtaining statistically significant result. Bias average deviation estimates true value estimand. diagnosands exotic, like Type-S error rate, probability estimate incorrect sign, conditional statistically significant (Gelman Carlin 2014).Research designs strong empirical answer \\(^d\\) generated design close true answer \\(^w\\). Since can never know \\(^w\\), assess whether distribution \\(^d\\) possible realizations research design close distribution \\(^m\\), answer model. can assess close distributions ? distribution \\(^m\\) easy simulate. Given theoretical model, can easily ask computer generate distribution estimand. often , distribution \\(^M\\) degenerate – fixed population, example, theoretical models posit estimands like Average Treatment Effect just one value.distribution \\(^d\\) trickier estimate. actual research design implemented , don’t get see distribution actual answers eventuated application D world. solve problem, make small – extremely consequential – substitution \\(m\\) \\(w\\) DAG research design. Swapping \\(m\\) \\(w\\), can ask computer simulate distribution \\(^d\\) conditional model. model wrong, simulated distribution \\(^d\\) wrong – ever, garbage , garbage . Figure YY shows DAG use simulate research designs. simulate designs, \\(d\\) affected \\(m\\) (realization theoretical model), rather \\(w\\). makes sense, since computer simulations can entirely untethered reality.Design diagnosis process simulating \\((m) = ^m\\) \\((d) = ^d\\) many draws \\(M()\\) \\(D(m)\\), comparing . specific comparisons make called “diagnostic statistics.”“diagnostic statistic” function \\(g\\) \\(^d\\), answer given data, \\(^m\\), answer given model, answers. function might really simple, like identity function, \\(g(^d)=^d\\), difference two answers: \\(g(^m, ^d) = ^d - ^m\\). \\(^m\\) \\(^d\\) random variables, function random variables also random variable, diagnostic statistic also random variable. diagnosand summary random variable.\n\\[\n\\phi = f(g(^m, ^d))\n\\]\\(f()\\) statistical functional summarizes random variable. example, expectation function \\(\\mathbb{E}[X]\\) summarizes random variable \\(X\\) expectation, mean, variance function summarizes expectation squared deviation random variable mean. ’ll use Greek letter \\(\\phi\\) describe idea diagnosand general.Let’s back moment work concrete examples common diagnosands (see section 10.3 exhaustive list). Consider diagnosand “bias.” Bias average difference estimand estimate. model two potential outcomes, treated potential outcome untreated potential outcome, inquiry might average treatment effect (difference two potential outcomes averaged units population sample, abbreviated ATE). single realization \\(m\\) model \\(M\\), value ATE particular number, call \\(^m\\). data strategy simply collect data come treated versus don’t (.e., use random assignment), answer strategy difference--means, answer \\(^d\\) systematically different \\(^m\\). diagnostic statistic error \\(^d - ^m\\); error random variable draw \\(m\\) \\(M\\) slightly different. expectation random variable \\(\\mathbb{E}[^d - ^m]\\), value bias diagnosand.Answer strategies commonly rely measures uncertainty like \\(p\\)-values, standard errors, confidence intervals order make decisions interpret \\(^d\\). measure estimated data make decision \\(^d\\) can used diagnostic statistic summarized diagnosand. Like bias, statistical power expectation, time diagnostic statistic \\(\\mathbb{1}(p \\leq 0.05)\\), indicator function equals 1 \\(p\\)-value greater 0.05 0 otherwise. Power describes frequently (beliefs model) research design return statistically significant result. standard error provides another diagnostic statistic whose expectation provides expected standard error diagnosand, example. can especially informative comparison another diagnosand, \\(\\sqrt{\\mathbb{V}(^d)}\\), actual standard deviation estimates generated model.diagnosands can calculated analytically. can straightforward calculate \\(\\sqrt{\\mathbb{V}(^d)}\\) two-arm experiment willing make lot simplifying assumptions.21 diagnosands even moderately complex designs, however, require Monte Carlo computer simulation. main purpose DeclareDesign software package make simulation step easier.practice, important diagnose design multiple possible \\(M\\)’s, given fundamental uncertainty world \\(w\\). know precise distributions exogenous variables exact functional forms potential outcomes (e.g., know true effect size). Diagnosis, therefore, typically involve simulating properties fixed set inquiries, data strategies, answer strategies multiple likely models. \\(D\\) \\(\\) given \\(\\) provide good diagnosand values multiple \\(M\\)’s can said robust multiple models.\nFigure 10.1: DAG Design Simulation\n","code":""},{"path":"p2diagnosis.html","id":"p2analyticdiagnosis","chapter":"10 Diagnosis","heading":"10.1 Estimating diagnosands analytically","text":"Diagnosis can done analytic, pencil--paper methods. Indeed, research design textbooks often contain many formulas calculating power variety designs. example, Alan S. Gerber Green (2012) include following power formula:write:“illustrate power analysis, consider completely randomized experiment \\(N>2\\) \\(N\\) units selected binary treatment. researcher must now make assumptions distributions outcomes treatment control units. example, researcher assumes control group normally distributed outcome mean \\(\\mu_c\\), treatment group normally distributed outcome mean \\(\\mu_t\\), group’s outcomes standard deviation \\(\\sigma\\). researcher must also choose \\(\\alpha\\), desired level statistical significance (typically 0.05).\nscenario, exists simple asymptotic approximation power experiment (assuming significance test two-tailed):\n\\[\n\\beta = \\Phi \\bigg(\\frac{|\\mu_t - \\mu_c| \\sqrt{N}}{2\\sigma} - \\Phi^{-1} (1 - \\frac{\\alpha}{2}) \\bigg)\n\\]\n\\(\\beta\\) statistical power experiment, \\(\\Phi(\\cdot)\\) normal cumulative distribution function (CDF), \\(\\Phi^{-1}(\\cdot)\\) inverse normal CDF.”power formula makes detailed assumptions \\(M\\), \\(D\\), \\(\\). \\(M\\), assumes potential outcomes normally distributed group specific means common variance. \\(D\\), assumes particular randomization strategy (simple random assignment). \\(\\), assumes particular hypothesis testing approach (equal variance \\(t\\)-test \\(N - 2\\) degrees freedom). set assumptions may “close enough” many research settings, can difficult understand specific impacts different beliefs \\(M\\), \\(D\\) \\(\\) value diagnosand. instead normally distributed, potential outcomes measured 1 - 5 Likert scales? randomization procedure includes blocking? include covariates treatment effect estimation approach? Formulas large sources design variation derived (clustering), certainly every design variant.quickly, hope analytic design diagnosis fades. analytic formulas abstractions – abstract away design details sometimes design details important. problem confined “power” diagnosand. randomized experiments, claims bias diagnosand quite general. Many randomized designs unbiased ATE, . Designs encounter noncompliance, attrition, forms spillover may unbiased ATE. Even without complications, cluster randomized trials heterogeneous cluster sizes unbiased (Middleton 2008; Imai, King, Nall 2009).Diagnosands depend design details, conduct study matters properties. means design diagnosis must design-aware. Since designs heterogeneous can vary many dimensions, computer simulation feasible way diagnose anything beyond simplest ideal-type designs.","code":""},{"path":"p2diagnosis.html","id":"estimating-diagnosands-via-simulation","chapter":"10 Diagnosis","heading":"10.2 Estimating diagnosands via simulation","text":"Research design diagnosis usually occurs two-step, simulation-based procedure. First simulate research designs , collecting “diagnostic statistics” run simulation. Second, summarize distribution diagnostic statistics order estimate diagnosands.estimate diagnosands summarizing distribution diagnostic statistics – course raises question: diagnostic statistic? take draw model (\\(m\\)) calculate value inquiry \\((m) = ^m\\). take one draw data strategy (\\(D(m) = d\\)), calculates value answer strategy \\((d) = ^d\\). diagnostic statistic function \\(^m\\) \\(^d\\).simple diagnostic statistic “error,” difference estimate estimand: \\(\\mathrm{error} = ^d - ^m\\). bias diagnosand expectation error statistic \\(\\mathbb{E}[\\mathrm{error}]\\) possible ways study come .Usually, consider many diagnostic statistics time. ’s design declaration two-arm trial balanced (50/50) design. 100 subjects responses treatment drawn normal distribution mean 0.1 sd 0.1.One draw simulation returns following:\nTable 10.1: One simulation draw\nFigure 10.2 shows information might obtain single run simulation. filled point estimate \\(^d\\). open triangle estimand \\(^m\\). bell-shaped curve normal-approximation based estimate sampling distribution. standard deviation estimated distribution estimated standard error, expresses uncertainty. confidence interval around estimate another expression uncertainty: ’re sure \\(^d\\) , things going according plan, confidence intervals constructed way bracket \\(^d\\) 95% time.\nFigure 10.2: Visualization one draw design diagnosis.\nsingle draw, can’t yet estimate diagnosands, can estimate diagnostic statistics. estimate higher estimand draw, error 0.10 - 0.08 = 0.02. Likewise, squared error (0.10 - 0.08)^2 = 0.0004. \\(p\\)-value 0.04, just barely lower threshold 0.05, “statistical significance” diagnostic statistic equal TRUE. confidence interval stretches 0.003 0.156, value estimand (0.10) bounds, “covers” diagnostic statistic equal TRUE well.Learning distribution diagnostic statistics main barrier design diagnosis. simply write distribution diagnostic statistics, straightforward matter summarize order calculate diagnosands. distributions diagnostic statistics depend complex information four parts research design: M, , D, . example, error statistic depends \\(^d\\) \\(^m\\), details matter greatly.calculate distributions diagnostic statistics, simulate designs just , many many times . bias diagnosand average error many runs simulation. statistical power diagnosand fraction runs estimate significant. coverage diagnosand fraction runs confidence interval covers estimand.figure visualizes just 10 runs simulation (obtained simulate_design(design)). can see run, \\(^m\\) little different. might seem counterintuitive – isn’t estimand supposed fixed number? estimands fixed, others stochastic, depending specifics model. Notice design declaration, drew potential outcomes distribution rather fixed numbers. choice incorporates modeling uncertainty. treatment effects unit close 0.1, ’re sure close particular unit. can also see draws produce statistically significant estimates (shaded areas small confidence intervals don’t overlap zero), . get sense true standard error seeing point estimates bounce around. get feel difference estimates standard error true standard error. Design diagnosis process learning ways study might come , just one way .\nFigure 10.3: Visualization ten draws design diagnosis.\nline code one. simulate design 1000 times calculate diagnostic statistics, summarize terms bias, true standard error (standard deviation sampling distribution), RMSE, power, coverage.\nTable 10.2: Diagnosand estimates bootstrapped standard errors.\n","code":"\ndesign <-\n  declare_population(N = 100, \n                     tau = rnorm(N, mean = 0.1, sd = 0.1),\n                     U = rnorm(N, 0, 0.2)) +\n  declare_potential_outcomes(Y ~  tau * Z + U) + \n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_assignment(prob = 0.5) +\n  declare_estimator(Y ~ Z, estimand = \"ATE\")\nrun_design(design)\ndiagnosis <- \n  diagnose_design(\n    design, sims = 1000, \n    diagnosands = declare_diagnosands(\n      bias = mean(estimate - estimand),\n      true_se = sd(estimate),\n      power = mean(p.value <= 0.05),\n      coverage = mean(estimand <= conf.high & estimand >= conf.low)\n    )\n  )"},{"path":"p2diagnosis.html","id":"interpreting-diagnosands","chapter":"10 Diagnosis","heading":"10.2.1 Interpreting diagnosands","text":"Say decide simulate design 1000 times see ’s biased. seconds, computer spits bias .02. Unsure whether concerned, simulate design another 1000 times. Now computer says bias .01. Since simulation randomly generated, expect get estimates bias vary. Given random variation simulations large estimate estimand , maybe even quite likely get .01 .02 bias even true bias zero. maybe design biased.frequentist statistics, separate signal noise. estimate (, bias diagnosand) varies depending simulation turns . want quantify variation order make decision whether estimated bias large enough significant. simulation standard error (, Monte Carlo standard error) estimate variation created simulation procedure. estimated variation high relative estimated diagnosand, need increase number simulations.\nMorris, White, Crowther (2019) provide range helpful formulas estimate simulation standard errors diagnosands. Define \\(^d_i\\) point estimate \\(\\)’th simulation, \\(n_{sim}\\) total number simulations, average point estimates across simulations \\(\\bar{^d} = \\frac{1}{n_{sim}}\\sum ^d_i\\). , estimand constant \\(^d_i\\) independently normally distributed, simulation standard error bias can calculated \\(\\sqrt{1/(n_{sim}(n_{sim} - 1)) \\sum^{n_{sim}}_{= 1} (^d_i - \\bar{^d})^2}\\).course, estimate variance relies assumptions won’t work soon estimand constant estimates normally distributed. Rather using analytic formulas, recommend using approach called nonparametric bootstrapping estimate simulation standard errors. Nonparametric bootstrapping can used estimate standard error diagnosand whose diagnostic statistic independently identically distributed, don’t limit classic diagnosands formula. Nonparametric bootstrapping quite simple : randomly resample \\(n_{sims}\\) diagnostic statistics replacement large number times re-estimate diagnosand resampled collection diagnostic statistics. standard deviation resulting distribution diagnosand estimates gives estimate simulation standard error. DeclareDesign nonparametric bootstrapping default whenever diagnose_design() function called. numbers reported parentheses diagnosand estimates table .","code":""},{"path":"p2diagnosis.html","id":"how-many-simulations-to-run","chapter":"10 Diagnosis","heading":"10.2.2 How many simulations to run","text":"Unlike increasing sample size real world, increasing precision diagnosand estimates simple waiting little longer computer runs higher number simulations. However, running 100 simulations may take matter seconds, running 10,000,000 simulations may take days. know ’ve done enough?rule thumb, multiplying simulation standard errors two head order make decisions whether simulations, since gets close upper lower bounds 95% confidence interval (properly calculated multiplying standard error 1.96). , change randomization strategy power increases .78 .82 simulation standard error .015, increase number simulations: upper bound first estimate \\((.78 + 0.015*1.96 = .81)\\) overlaps lower bound second \\((.82 - 0.015*1.96)\\), apparent improvement power may artifact simulation.","code":""},{"path":"p2diagnosis.html","id":"diagnosands","chapter":"10 Diagnosis","heading":"10.3 How to choose among diagnosands","text":"diagnostic statistic summary function \\(^m\\) \\(^d\\), diagnosand summary diagnostic-statistics. result, great many choose . Table 10.3, introduce set diagnostic statistics (far complete!), Table 10.4 set diagnosands including commonly less commonly considered.Table 10.3:  Diagnostic statistics.Table 10.4:  Diagnosands.diagnosands relevant every study. example, descriptive study whose goal estimate fraction people France left-handed, statistical power irrelevant. hypothesis test null hypothesis zero percent people France left-handed preposterous. know sure fraction zero, just don’t know precise value. much important diagnosand study RMSE (root-mean-squared-error), measure well-estimated estimand incorporates bias variance.Often, need look several diagnosands order understand might going wrong. design exhibits “undercoverage” (e.g., coverage less \\(1 - \\alpha\\)), might standard errors small (-estimation variance sampling distribution) point estimate biased, combination two. really perverse instances, might biased point estimate , thanks overly-wide confidence intervals, just happens get covered 95% time. , assessing coverage ’s important look point estimate bias, also check average estimated standard error lines true standard error. Alternatively, look “bias-eliminated coverage,” assesses coverage purely terms confidence interval width, ignoring bias.Many research design decisions involve trading bias variance. trade-settings, may need accept higher variance order decrease bias. Likewise, may need accept bit bias order achieve lower variance. tradeoff captured mean-squared error – average squared distance \\(^d\\) \\(^m\\). course, ideally like low mean-squared error possible. like achieve low variance low bias simultaneously.illustrate, consider following three designs represented three targets. inquiry bullseye target. data answer strategies combine generate process arrows shot towards target. left, bad archer: even though estimates unbiased sense hit bullseye “average”, arrows target. middle, Katniss Everdeen (heroine Hunger Games novels good bow) data answer strategies: target low variance. right, archer consistent (low variance) biased. mean squared error highest left lowest middle.archery metaphor common research design textbooks effectively conveys difference variance bias, elide important point. really matters target archer shooting . Figure 10.4 shows bizarre double-target representing two inquiries. empirical strategy unbiased precise left inquiry, clearly biased right inquiry. describing properties \\(^d\\), clear \\(^\\) associated .\nFigure 10.4: bias, variance, RMSE answer strategy depend inquiry.\nRMSE exactly equal weighting variance bias. Yet many weightings two diagnosands possible, different researchers vary weightings. weights may also depend research question researcher studying, career stage, strength priors, size effects, features.evaluating research design diagnosis, need know researcher’s weighting relevant diagnosands. can think utility unction. utility function includes important study big questions, shift beliefs research field, overturn established findings, obtain unbiased answers, get sign right. utility function evaluated given design yield utility can compared across designs (process redesign, described detail next section).often consider diagnosand power . diagnosand probability getting statistically significant result, course depends many things design including, crucially, unknown magnitude parameter estimated. can think statistical power probability success, success defined getting significant results. conventional power target 80% power. One imagine redefining statistical power “null risk,” probability obtaining null result. terms, conventional power target 20% null risk, one five chance “failure.” odds aren’t great, recommend designing studies lower null risk. considering power alone also misleading: researcher wants design study 80% powered returns highly biased estimates 2-3x true estimate. Another way saying researchers always carry power bias. much care feature determines weight power bias utility function.Diagnosands need hypothesis testing even statistical analysis data . often tradeoff much learn research design cost terms money time. financial time budgets provide hard constraints designs, also margin many researchers wish select cheaper (shorter) designs order carry studies finish degree sooner. Time cost also diagnostic statistics! may wish explore maximum cost study maximum amount time take.Ethical considerations also often enter process assessing research designs, implicitly. can explicitly incorporate utility function valuing minimizing harm maximizing degree informed consent requested subjects. collecting, researchers often believe face tradeoff informing subjects subject data collection (ethical consideration, requirement IRB) one hand bias comes Hawthorne demand effects. can incorporate considerations research design diagnosis specifying diagnostic statistics related amount disclosure purposes research number subjects harmed research.","code":""},{"path":"p2diagnosis.html","id":"diagnosing-with-respect-to-variations-in-m","chapter":"10 Diagnosis","heading":"10.4 Diagnosing with respect to variations in M","text":"always uncertain M – certain M (real dispute ), need conduct new empirical research . Research design diagnosis can account uncertainty evaluating performance design alternative models. unsure exact value intra-class correlation outcomes encounter, simulate variance estimator range plausible ICC values. unsure true average treatment effect, simulate power study range plausible effect sizes. Uncertainty model inputs like means, variances, covariances data eventually collected major reason simulate range plausible values.","code":""},{"path":"p2diagnosis.html","id":"estimating-the-minimum-detectable-effect-size","chapter":"10 Diagnosis","heading":"10.4.1 Estimating the minimum detectable effect size","text":"written.","code":""},{"path":"p2diagnosis.html","id":"adjudicating-between-competing-models","chapter":"10 Diagnosis","heading":"10.4.2 Adjudicating between competing models","text":"can apply principle competing models. Imagine believe \\(M_1\\) true scholarly rival believes \\(M_2\\). spirit scientific progress, design study together. design () demonstrate \\(M_1\\) true true (B) demonstrate \\(M_2\\) true true. order come agreement properties design, need simulate design models.","code":""},{"path":"p2diagnosis.html","id":"further-reading-8","chapter":"10 Diagnosis","heading":"10.5 Further reading","text":"Gelman Carlin (2014) Type M Type S errorsHerron Quinn (2016) case selection/sampling biasBaumgartner Thiem (2017) Rohlfing (2018) diagnosands qualitative researchRubin (1984) diagnosands Bayesian research","code":""},{"path":"redesign-2.html","id":"redesign-2","chapter":"11 Redesign","heading":"11 Redesign","text":"Diagnosis process learning value diagnosand utility research design. Often diagnosis undertaken range specifications M assess robustness alternative models.Redesign process changing parts D (, perhaps, ) order learn diagnosand values researcher utility change. redesign process complete researcher settles choice D among feasible set.example, can compare distribution errors changes use different data strategy \\(D'\\): \\(P_M(^d - ^m|D')\\) different answer strategy \\('\\): \\(P_M(^{d'} - ^m|D)\\). case, examine variation D , diagnoses can assess distribution errors differs alternative model \\(M'\\): \\(P_M(^{d} - ^{m'}|D)\\).Sometimes redesign fixing problems. diagnose design find estimator shooting altogether different quantity mind. cases, redesign fixing errors.Often however researchers wish find optimal design question subject financial logistical constraints. Even simple designs many variations, search space must limited. example, researcher might want find sample size \\(N\\) number treated units \\(N_t\\) minimize design’s error subject fixed budget experiment data collection unit costs $25 treating one unit costs $5. solve optimization problem:\\[\\begin{equation*}\n\\begin{aligned}\n& \\underset{N, N_t}{\\text{argmin}}\n& & E_M(L(^{d} - ^{m}|D_{N, N_t})) \\\\\n& \\text{s.t.}\n& & 25  N + 5  N_t \\leq 5000\n\\end{aligned}\n\\end{equation*}\\]\\(L\\) loss function, increasing difference \\(^{d}\\) \\(^{m}\\).reason however optimize respect single diagnosand . redesign optimizing power diagnosand, likely find design highly powered highly biased — targeting wrong estimand. can solve switching root mean-squared error diagnosand, reflects bias efficiency design. generally, choice optimal design requires researcher select weighting diagnosands. must balance bias, efficiency, risk imprecise null results, risk getting sign effect wrong, diagnosands given research goals.full evaluation design — declaration, diagnosis, redesign — depends assessment one diagnosands, comparing diagnoses achieved alternative designs.","code":""},{"path":"redesign-2.html","id":"graphical-inspection","chapter":"11 Redesign","heading":"11.1 Graphical inspection","text":"One approach examine performance along multiple diagnosands visually. may encountered figures implicitly redesign. Power curves example redesign process. power curve sample size horizontal axis statistical power vertical axis. power curve, can learn big study needs order achieve desired level statistical power. “minimum detectable effect” figure similar. sample size horizontal axis , plots smallest effect size 80% power can achieved. plots useful learning something like, “given budget, can sample 400 units. 400 units, MDE 0.5 standard deviation effect. theory says effect smaller , something closer 0.1 SDs. apply funding study something else.”highest level advice redesign decisions using approach , beginning processes least, change one design parameter time. Vary data strategy, holding model, inquiry, answer strategy constant. Change answer strategy, holding aspects design constant. Graphical representation diagnosands change designs change intuitive way .","code":""},{"path":"redesign-2.html","id":"formal-optimization","chapter":"11 Redesign","heading":"11.2 Formal optimization","text":"somewhat integrated approach generate diagnosand reflects objective function (loss function)—may combination performance cost—choose design performs best diagnosand. steps approach involve:Declaring model reflects prior beliefs underlying processes: expectations diagnosands taken respect priors.Varying D viable alternatives, recording costs different alternatives.Declaring diagnosand reflects ultimate objectives including performance cost.Consider example design place prior distribution treatment effect, \\(b\\). consider designs vary () number units studied (b) investment careful measurement. comes cost. imagine results used follows: significant treatment effect \\(b>c\\) estimated intervention implemented value \\(b\\) cost \\(c\\). diagnosand expected utility whatever policy implemented study: \\(b-c\\) policy adopted, 0 otherwise. Thus responsive different types errors: implementing weak intervention \\(b<c\\) failing implement strong intervention \\(b>c\\). Note course policy decision rule optimal insofar based significance posteriors, yet may still rule used may still need know undertake optimal design given rule.illustrate simple design explicitly provide prior distribution estimand build decision relevant parameters arguments.key step define diagnosand reflects choices, benefits, costs assess performance range designs.Payoffs across two decision dimensions shown Figure 11.1.\nFigure 11.1: Utility two design parameters.\n","code":"\n# Decision parameters\n\nN <- 1000  # Number of cases\nr <- 1     # Measurement error\nc <- .2   # Cost of policy\n\n# Design\ndesign <- \n  declare_population(\n    b = add_level(N = 1, b = rnorm(1,0,1)),\n    i = add_level(N = N, X = rnorm(N), Y = 1+b*X + rnorm(N))) +\n  declare_measurement(Y_seen = Y + rnorm(N, 0, r)) +\n  declare_estimand(b=b[1]) +\n  declare_estimator(Y_seen ~ X, model = lm_robust) \n# Utility as a diagnosand\n\ndiagnosands <- declare_diagnosands(\n  utility = mean((p.value <= 0.05)*(estimate > c)*(estimand - c) - N/100000+ r/250))\n                                  \ndesign <- set_diagnosands(design, diagnosands)\n\ndesigns <- redesign(design, N = 100*2^(0:6), r = (0:8)/2)\ndiagnosis <- diagnose_design(designs, sims = 20000)"},{"path":"part-ii-exercises.html","id":"part-ii-exercises","chapter":"12 Part II Exercises","heading":"12 Part II Exercises","text":"written.","code":""},{"path":"research-design-library.html","id":"research-design-library","chapter":"13 Research Design Library","heading":"13 Research Design Library","text":"section book enumerates series common social science research designs. entry include description design terms M, , D, also declaration design code. ’ll often diagnose designs range values design parameters order point especially interesting unusual features design.goal section provide comprehensive accounting empirical research designs. ’s also describe particular designs exhaustive detail, quite sure order designs useful practical purpose, need modified. entries design library recipes automatically produce high-quality research. Instead, hope entries provide inspiration tailor particular class designs – blocked--clustered randomized trial catch--release design – research setting. basic structure design library entry useful, specifics plausible ranges outcomes, sample size constraints, etc, different particular setting.’ve split designs inquiry data strategy. Inquiries can descriptive causal data strategies can observational experimental. describe five categories research design: observational descriptive, experimental descriptive, observational causal, experimental causal “multistudy.” Across categories include qualitative quantitative strategies highlighting similar kinds concerns often arise .Table 13.1:  Research design types examples","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"observational-designs-for-descriptive-inference","chapter":"14 Observational designs for descriptive inference","heading":"14 Observational designs for descriptive inference","text":"observational design descriptive inference inquiry like population mean, covariance, distribution main research goal. observational research design, data strategy includes sampling measurement components, treatments allocated. Put differently, observational design descriptive inference, researchers seek measure summarize world, change . class research design encompasses huge portion research activity – surveys fall class, large-scale data collections economic sociopolitical indicators. examples observational designs descriptive inference include classic case studies focused “thick description” many text analysis projects.","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"random-sampling","chapter":"14 Observational designs for descriptive inference","heading":"14.1 Random sampling","text":"Often, interested features population, data entire population prohibitively expensive collect. Instead, researchers obtain data small fraction population use measurements taken sample draw inferences population.Imagine seek estimate average political ideology adult residents small town Portola, California (population 2,100). Households include 1 3 adults; adults living households typically similar ideologies, differ somewhat. latent ideology subject therefore composed household-level shock well idiosyncratic individual-level shock. data strategy involve administering survey asks subjects place left-right scale varies 1 (liberal) 7 (conservative). approximate measurement procedure function “cuts” latent ideology 7 separate groups. Note define measurement sampling let us simulate measure member population sample .inquiry “data-dependent”, since interested population mean measured variable \\(Y\\): \\(\\frac{1}{N} \\sum_1^N Y_i = \\bar{Y}\\) (, measure able measure population). first sampling strategy complete random sampling. draw sample exactly \\(n = 100\\), every member population equal probability inclusion sample, \\(\\frac{n}{N}\\). answer strategy sample mean estimator: \\(\\widehat{\\overline{Y}} = \\frac{1}{n} \\sum_1^n Y_i\\), implemented ordinary least squares regression facilitate easy calculation auxiliary statistics like standard error estimate 95% confidence interval.","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"declaration-5","chapter":"14 Observational designs for descriptive inference","heading":"14.1.1 Declaration","text":"","code":"\nportola <-\n  fabricate(\n    households = add_level(N = 500, \n                           n_adults = sample(1:3, N, replace = TRUE),\n                           household_shock = rnorm(N, mean = 1)),\n    adults = add_level(N = n_adults, \n                       individual_shock = rnorm(N, sd = 0.1),\n                       Ystar = household_shock + individual_shock)\n    )\n\ndesign <- \n  declare_population(data = portola) + \n  declare_measurement(Y = as.numeric(cut(Ystar, 7))) + \n  declare_estimand(Y_bar = mean(Y)) + \n  declare_sampling(n = 100) + \n  declare_estimator(Y ~ 1, model = lm_robust, estimand = \"Y_bar\")"},{"path":"observational-designs-for-descriptive-inference.html","id":"dag-5","chapter":"14 Observational designs for descriptive inference","heading":"14.1.2 DAG","text":"","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"diagnosis","chapter":"14 Observational designs for descriptive inference","heading":"14.1.2.1 Diagnosis","text":"Two main diagnosands simple random sampling design bias rmse. want know get right answer average want know, average, far truth .\nTable 14.1: Complete random sampling design diagnosis\ndiagnosis table 14.1 indicates complete random sampling, sample mean estimator population mean unbiased root mean squared error manageable 0.11.","code":"\ndiagnosands <- declare_diagnosands(\n  bias = mean(estimate - estimand),\n  rmse = sqrt(mean((estimate - estimand) ^ 2))\n)\ndiagnosis <- diagnose_design(design, diagnosands = diagnosands) "},{"path":"observational-designs-for-descriptive-inference.html","id":"exercises-1","chapter":"14 Observational designs for descriptive inference","heading":"14.1.2.2 Exercises","text":"Can modify design define estimand mean latent variable Y_star? happens diagnose design?Can modify design allow possibility measurement error?Can modify design allow possibility sampling error?","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"clustered-random-sampling","chapter":"14 Observational designs for descriptive inference","heading":"14.1.3 Clustered random sampling","text":"Researchers often randomly sample individual level may, among reasons, costly logistically impractical. Instead, may choose randomly sample households, political precincts, group individuals order draw inferences population. strategy may cheaper simpler may also introduce risks less precise estimates.Let’s modify first design sample household level, rather individual level. ’ll also add additional estimator clusters standard errors level sampling, household compare estimator ignores clustering. ’ll also add new diagnosand, coverage, indicate whether confidence intervals construct (depend flavor standard errors estimate) indeed include true value inquiry 95% time.\nTable 14.2: Cluster random sampling design diagnosis\ndiagnosis shows effect clustering quality estimates. design remains unbiased population mean, RMSE gone . still interview (average) 100 people sample, since clustered design, interview adults household, sampling distribution estimates higher variance. diagnosis also highlights importance matching answer strategy data strategy. sampling clustered household. answer strategy fails account clustering yields confidence intervals small, evidenced coverage rate well nominal 95%. estimate clustered standard errors instead, coverage improves.","code":"\ndesign <-\n  declare_population(data = portola) +\n  declare_measurement(Y = as.numeric(cut(Ystar, 7))) +\n  declare_estimand(Y_bar = mean(Y)) +\n  declare_sampling(clusters = households, n = 50) +\n  declare_estimator(Y ~ 1,\n                    model = lm_robust,\n                    estimand = \"Y_bar\",\n                    label = \"Standard errors not clustered\") +\n  declare_estimator(Y ~ 1,\n                    clusters = households,\n                    model = lm_robust,\n                    estimand = \"Y_bar\",\n                    label = \"Standard errors clustered\")\n\ndiagnosands <- declare_diagnosands(\n  bias = mean(estimate - estimand),\n  rmse = sqrt(mean((estimate - estimand) ^ 2)),\n  coverage = mean(estimand <= conf.high & estimand >= conf.low)\n)\ndiagnosis <- diagnose_design(design, diagnosands = diagnosands) "},{"path":"observational-designs-for-descriptive-inference.html","id":"exercises-2","chapter":"14 Observational designs for descriptive inference","heading":"14.1.3.1 Exercises","text":"clusters defined part model, defined part data strategy. Can modify design generate sampling clusters putting neighboring households cluster. data household identifier households try making single cluster pair (instance households 001 002). sampling using pairs households together affect estimated rmse?","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"poststratification","chapter":"14 Observational designs for descriptive inference","heading":"14.2 Poststratification","text":"Poststratification tool learning population sample whose units drawn unequal (usually unknown) probabilities. essence, know features population can measure sample, can reweight sample look like population—least along dimensions can measure. simplest form, poststratification just involves counting dividing. Say want poststratify sample , terms gender age, looks like people twenties United States population 2010. gives twenty groups—one age gender—whose size need count 2010 Decennial census sample. , simply weight person sample size group population divided size group sample. illustrate declaration , turns weights equivalent inverse sampling probabilities one obtain one construct sample using groups strata. ’s “stratification” poststratification comes —’s “post” weights constructed sampling already taken place. online example, illustrate multi-level regression poststratification (MRP) can help address problem , number group-defining features increases, sample may contain information groups defined combination features. multi-level regression model helps partially pooling across strata combat sparsity problem.","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"declaration-6","chapter":"14 Observational designs for descriptive inference","heading":"14.2.1 Declaration","text":"Model: fixed, finite population comprised two groups defined binary variable, \\(X\\). 100 people \\(X = 0\\) 50 \\(X = 1\\). outcome, \\(Y\\), takes one three values, 0, 1, 2, function \\(X\\).Model: fixed, finite population comprised two groups defined binary variable, \\(X\\). 100 people \\(X = 0\\) 50 \\(X = 1\\). outcome, \\(Y\\), takes one three values, 0, 1, 2, function \\(X\\).Inquiry: researcher wants know true average \\(Y\\) population.Inquiry: researcher wants know true average \\(Y\\) population.Data strategy: Ten individuals group included sample. Thus, whereas 2/3 population belong \\(X = 0\\) group, 1/2 sample . Conversely, 1/3 population belongs \\(X = 1\\) group, 1/2 sample .Data strategy: Ten individuals group included sample. Thus, whereas 2/3 population belong \\(X = 0\\) group, 1/2 sample . Conversely, 1/3 population belongs \\(X = 1\\) group, 1/2 sample .Answer strategy: answer strategy involves taking weighted average \\(Y\\) sample, unit’s weight equal size group population divided size group sample. Specifically, underrepresented group, \\(X = 0\\), get weight 100/10 = 10, overrepresented group, \\(X = 1\\), gets weight 50 / 10 = 5. poststratification groups perfectly line sample strata, , approach equivalent weighting inverse sampling probability: 1/(10/100) = 10 1/(10/50) = 5. cases, weights tell us many units population unit sample represents.Answer strategy: answer strategy involves taking weighted average \\(Y\\) sample, unit’s weight equal size group population divided size group sample. Specifically, underrepresented group, \\(X = 0\\), get weight 100/10 = 10, overrepresented group, \\(X = 1\\), gets weight 50 / 10 = 5. poststratification groups perfectly line sample strata, , approach equivalent weighting inverse sampling probability: 1/(10/100) = 10 1/(10/50) = 5. cases, weights tell us many units population unit sample represents.","code":"\nfixed_population <-\n  fabricate(\n    group = add_level(\n      N = 2,\n      X = c(0, 1),\n      population_n = c(100, 50)\n    ),\n    individual = add_level(N = population_n,\n                           Y = X + sample(0:1, N, replace = TRUE))\n  )\n\ndesign <-\n  declare_population(data = fixed_population) +\n  declare_estimand(Ybar = mean(Y)) +\n  declare_sampling(strata_n = c(10, 10), strata = X) +\n  declare_step(handler = group_by, groups = X) +\n  declare_step(handler = mutate,\n               sample_n = n(),\n               weight = population_n / sample_n) +\n  declare_estimator(\n    Y ~ 1,\n    term = \"(Intercept)\",\n    model = lm_robust,\n    weights = weight,\n    estimand = \"Ybar\"\n  )"},{"path":"observational-designs-for-descriptive-inference.html","id":"dag-6","chapter":"14 Observational designs for descriptive inference","heading":"14.2.2 Dag","text":"","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"exercises-3","chapter":"14 Observational designs for descriptive inference","heading":"14.2.3 Exercises","text":"Add unweighted estimator design diagnose . Explain unweighted estimator biased, well direction bias.Remove X definition Y. Explain bias now equivalent two approaches.Change strata_n 4 people sampled first group 25 sampled second group.\nCalculate poststratification weight group hand (hint: can check answer using draw_data(design)$weight).\nCalculate sample inclusion probability group hand (hint: can check answer using draw_data(design)$S_inclusion_prob).\nCalculate inverse sampling probability group. equal poststratification weights.\nCalculate poststratification weight group hand (hint: can check answer using draw_data(design)$weight).Calculate sample inclusion probability group hand (hint: can check answer using draw_data(design)$S_inclusion_prob).Calculate inverse sampling probability group. equal poststratification weights.","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"example-4","chapter":"14 Observational designs for descriptive inference","heading":"14.2.4 Example","text":"MRP based Lax Philips APSR 2009","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"inference-about-unobserved-variables","chapter":"14 Observational designs for descriptive inference","heading":"14.3 Inference about unobserved variables","text":"Oftentimes researchers seek measure latent variable (Y_star) access proxies (Y_1, Y_2, Y_3). cases, proxies sometimes combined index thought capture Y_star used analysis.difficult feature problems access scale Y_star measured may seem like hopeless exercise try assess whether got good bad estimates Y_star combine measured data.One way around normalize scale latent variable measured variable mean 0 unit standard deviation. case, guaranteed estimate mean normalized variable unbiased! certainly estimate mean 0! may show declaration , model correct approach may still useful calculating quantities — conditional means—don’t just get right construction.","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"declaration-7","chapter":"14 Observational designs for descriptive inference","heading":"14.3.1 Declaration","text":"declaration Y_star normal distribution centered zero. measured variables Y_1, Y_2, Y_3 also normally distributed scale; related Y_star, though strongly others. construct two indices. One (Y_av) constructed first scaling measured variables, averaging scaling . akin approach used Kling, Liebman, Katz (2007). second also weights components time using weights generated ‘principal components analysis’ , intuitively, seeks find weighting minimizes distance measured variables.","code":"\ndesign <-\n  declare_population(N = 250, gender = rep(0:1, N/2), Y_star = 1 + gender + 2* rnorm(N)) +\n  declare_estimand(Y_bar = mean(scale(Y_star)[gender == 1])) + \n  declare_measurement(Y_1 = 3 + 0.1 * Y_star + rnorm(N, sd = 0.5),\n                      Y_2 = 2 + 1.0 * Y_star + rnorm(N, sd = 1),\n                      Y_3 = 1 + 0.5 * Y_star + rnorm(N, sd = 0.5),\n                      Y_av = scale((scale(Y_1) + scale(Y_2) + scale(Y_3))),\n                      Y_fa  = princomp(~ Y_1 + Y_2 + Y_2, cor = TRUE)$scores[,1]) + \n  declare_estimator(Y_av ~ 1, model = lm_robust, estimand = \"Y_bar\", subset = gender ==1, label = \"Average\") +\n  declare_estimator(Y_fa ~ 1, model = lm_robust, estimand = \"Y_bar\", subset = gender ==1, label = \"principal components\")"},{"path":"observational-designs-for-descriptive-inference.html","id":"dag-7","chapter":"14 Observational designs for descriptive inference","heading":"14.3.2 Dag","text":"DAG representing design given Figure 14.1: underlying measure gives rise three observed measures (mechanically) combined produce measured index.\nFigure 14.1: Index\n","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"diagnosis-1","chapter":"14 Observational designs for descriptive inference","heading":"14.3.3 Diagnosis","text":"diagnosis given Table 14.3. show RMSE bias also average value estimand (estimate).\nTable 14.3: Estimation conditional mean normalized latent variable\nsee diagnosis quite well recovering conditional mean standardized latent variable.","code":"\ndiagnosis <- diagnose_design(design) "},{"path":"observational-designs-for-descriptive-inference.html","id":"exercises-4","chapter":"14 Observational designs for descriptive inference","heading":"14.3.4 Exercises","text":"Modify design two rather three observed components used. diagnosis affected? matter indices use?\nestimation affected wrong model linking measures latent variables? Try design measures non linear latent variable.","code":""},{"path":"observational-designs-for-causal-inference.html","id":"observational-designs-for-causal-inference","chapter":"15 Observational designs for causal inference","heading":"15 Observational designs for causal inference","text":"section introductioncite Dunning (2012), Angrist Pischke (2008)","code":""},{"path":"observational-designs-for-causal-inference.html","id":"causal-inquiries-about-a-single-unit","chapter":"15 Observational designs for causal inference","heading":"15.1 Causal inquiries about a single unit","text":"often start causal investigation wanting know causal effect single unit. unit experienced event process, want know causal effect event. , need know happened event happen, counterfactual outcome opposed factual outcome actually occurred. Due fundamental problem causal inference, observe happened counterfactual case happened. event can either happen, happen, unit. result, guess happened. Social scientists developed large array tools guessing, imputing, missing counterfactual outcome — happened counterfactual case, event happened.22In section, inquiry treatment effect treated case (TET). TET difference treated potential outcome control potential outcome posttreatment period treated unit interest.","code":""},{"path":"observational-designs-for-causal-inference.html","id":"process-tracing","chapter":"15 Observational designs for causal inference","heading":"15.1.1 Process tracing","text":"","code":""},{"path":"observational-designs-for-causal-inference.html","id":"before-after-comparison","chapter":"15 Observational designs for causal inference","heading":"15.1.2 Before-after comparison","text":"natural idea guessing happened event occur single unit look within unit happened event occur. Within-unit -time comparisons use outcomes pretreatment periods impute posttreatment counterfactual outcome.order pretreatment outcome good stand-posttreatment control potential outcome, must invoke two assumptions: excludability time; interference time periods. excludability assumption says change pre post fact treatment administered. rules time trends form time-varying heterogeneity aside treatment status. second assumption unit assigned control pretreatment period affect outcomes posttreatment period, fact unit treated second period affect outcomes first period. often called carryover assumption, importantly case rules possibility effects anticipating treatment next period. unit expects treated period 1, may affect outcome period 0 even though treatment yet administered. Often anticipation effects wash treatment effects; treatment already taken place.declare design, consider violations excludability assumption. inquiry TET, designs section. model, consider two sources heterogeneity outcome: time trends (effect time) time-varying heterogeneity (effect \\(U_{\\rm time}\\)). Potential outcomes function two variables treatment variable. data strategy measure \\(Y\\) periods, reveal control potential outcome first period, treatment, control potential outcome second period, treatment. answer strategy take difference outcome post- pretreatment periods.diagnose design four settings, two--two : without time trends, without time-specific effects. see -time within-unit design unbiased neither. time trends, bias, due violation excludability time assumption. time-varying heterogeneity aside treatment, excludability assumption also violated. test two conditions lead bias, diagnostic like pretrends comparison difference--difference design.","code":"\ndesign <- \n  declare_population(\n    N = 2, \n    time = 0:1,\n    U_time = rnorm(N),\n    potential_outcomes(\n      Y ~ time_trend * time + time_specific_effect * U_time + 0.5 * Z\n    ),\n    Z = 1\n  ) + \n  declare_estimand(TET = Y_Z_1 - Y_Z_0, subset = time == 1) + \n  declare_measurement(Y = ifelse(time == 0, Y_Z_0, Y_Z_1)) + \n  declare_estimator(\n    estimate = Y[time == 1] - Y[time == 0], \n    estimand_label = \"TET\", handler = summarize)\n\ndesigns <- redesign(\n  design, time_trend = c(0, 0.5), time_specific_effect = c(0, 1))\ndiagnosis <- diagnose_design(designs, sims = sims, bootstrap_sims = b_sims)"},{"path":"observational-designs-for-causal-inference.html","id":"posttreatment-comparison-to-an-untreated-case","chapter":"15 Observational designs for causal inference","heading":"15.1.3 Posttreatment comparison to an untreated case","text":"second natural design compare outcomes treated case treatment outcomes another unit time period receive treatment. Instead filling control potential outcome treated unit outcome pretreatment period, fill outcomes second “control” unit.can invoke exactly parallel set assumptions time within unit design: excludability unit difference (rather time), .e., difference two units treatment status unit-specific heterogeneity. strong assumption often relaxed favor “selection--observables” assumption accounting observable differences units selecting comparison unit similar observable ways.declaring design, target TET inquiry, swap model time heterogeneity -unit heterogeneity (\\(U_{\\rm unit}\\)) (parallel time trends). data strategy measure Y treated unit comparison unit posttreatment time period; treated unit reveals treated potential outcome, comparison unit reveals control potential outcome. answer strategy posttreatment comparison treated control unit.diagnose two-unit posttreatment comparison design two settings: without unit-specific differences accounted observables, aside treatment. differences, excludability assumption violated. see design unbiased TET unit-specific differences besides treatment.","code":"\ndesign <- \n  declare_population(\n    N = 2, \n    time = 1,\n    U_unit = rnorm(N),\n    potential_outcomes(Y ~ 0.5 * Z + unit_specific_effect * U_unit),\n    Z = if_else(U_unit == max(U_unit), 1, 0)\n  ) + \n  declare_estimand(TET = Y_Z_1 - Y_Z_0, subset = time == 1) + \n  declare_measurement(Y = ifelse(Z == 0, Y_Z_0, Y_Z_1)) + \n  declare_estimator(\n    estimate = Y[Z == 1] - Y[Z == 0], \n    estimand_label = \"TET\", handler = summarize)\n\ndesigns <- redesign(design, unit_specific_effect = c(0, 1))\ndiagnosis <- diagnose_design(designs, sims = sims, bootstrap_sims = b_sims)"},{"path":"observational-designs-for-causal-inference.html","id":"difference-in-differences-1","chapter":"15 Observational designs for causal inference","heading":"15.1.4 Difference-in-differences","text":"-time excludability treatment selection--observables assumptions unreasonable, alternative difference--differences design. difference unit characteristics, whether observable , vary time comparing outcomes treatment. left time trends unit-invariant time-varying factors, difference comparing change time treated unit change time comparison unit. difference takes one class factors violate excludability assumptions -time within unit designs alone posttreatment comparison across-unit designs alone.difference--difference design rely assumptions earlier two designs . adds new assumption: parallel trends assumption. order difference--difference design work, change treatment control potential outcomes must equal (.e., parallel). assumption depends change values unrealized (thus unobservable) control potential outcome treated unit, tested. widely-used diagnostic difference observed trends treatment […]Declaring design combines elements within-unit time -unit posttreatment designs. two units (treated unit comparison unit), two time periods, (0) treatment (1). unit-specific, time invariant heterogeneity (\\(U_{\\rm unit}\\)), unit-invariant time heterogeneity (\\(U_{\\rm time}\\)). potential outcomes function treatment, heterogeneity variables, time trends. target TET inquiry designs. measure outcome way combines two previous designs: control potential outcome revealed periods comparison untreated unit pretreatmetn period treated unit, treated potential outocme revealed posttreatment period treated unit. answer strategy difference--differences, first differencing within-unit changes first second period across-unit changes comparison unit.diagnose difference--difference design eight cases, combinations : without time trends outcomes; without , time-varying unit-invariant heterogeneity; without unit-specific time-invariant heterogeneity. see power difference--difference design: alternatives, design unbiased TET inquiry. -time within-unit design posttreatment comparison across units design, design unbiased relevant one conditions holds. Unfortunately, tests validity assumptions.","code":"\ndesign <- \n  declare_population(\n    unit = add_level(N = 2, U_unit = rnorm(N, sd = 0.5), Z = if_else(U_unit == max(U_unit), 1, 0)),\n    period = add_level(N = 2, time = 0:1, U_time = rnorm(N), nest = FALSE),\n    unit_period = cross_levels(\n      by = join(unit, period), \n      U = rnorm(N, sd = 0.01),\n      potential_outcomes(\n        Y ~ time_trend * 0.5 * time + \n          time_specific_effect * U_time + \n          unit_specific_effect * U_unit + \n          1.25 * Z + U)\n    )\n  ) + \n  declare_estimand(TET = Y_Z_1 - Y_Z_0, subset = time == 1) + \n  declare_measurement(Y = if_else(Z == 0 | period == 1, Y_Z_0, Y_Z_1)) + \n  declare_estimator(\n    estimate = \n      (mean(Y[Z == 1 & time == 2]) - mean(Y[Z == 1 & time == 1])) - \n      (mean(Y[Z == 0 & time == 2]) - mean(Y[Z == 0 & time == 1])), \n    estimand_label = \"ATT\", handler = summarize)\n\ndesigns <- redesign(design, \n                    time_trend = c(0, 0.5), \n                    time_specific_effect = c(0, 1), \n                    unit_specific_effect = c(0, 1))\ndiagnosis <- diagnose_design(designs, sims = sims, bootstrap_sims = b_sims)"},{"path":"observational-designs-for-causal-inference.html","id":"synthetic-controls-1","chapter":"15 Observational designs for causal inference","heading":"15.1.5 Synthetic controls","text":"","code":"\ndesign <- \n  declare_population(\n    units = add_level(N = 10, unit_ID = 1:10, U_unit = rnorm(N), X = rnorm(N), Z = if_else(unit_ID == 1, 1, 0)), # if_else(U_unit == max(U_unit), 1, 0)),\n    periods = add_level(N = 3, time = -1:1, U_time = rnorm(N), nest = FALSE),\n    unit_periods = cross_levels(\n      by = join(units, periods), \n      U = rnorm(N),\n      potential_outcomes(Y ~ time_trend * 0.5 * time + time_specific_effect * U_time + \n          unit_specific_effect * U_unit + \n            1.25 * Z + X + U),\n      Y = if_else(Z == 0 | time <= 0, Y_Z_0, Y_Z_1)\n    )\n  ) + \n  declare_estimand(ATT = mean(Y_Z_1 - Y_Z_0), subset = time == 1) + \n  declare_measurement(predictors = \"X\",\n                    time.predictors.prior = -1:0,\n                    dependent = \"Y\",\n                    unit.variable = \"unit_ID\",\n                    time.variable = \"time\",\n                    treatment.identifier = 1,\n                    controls.identifier = 2:10, \n                    handler = synth_weights_tidy) +\n  declare_estimator(Y ~ Z, subset = time == 1, weights = synth_weights, \n                    model = lm_robust, label = \"synth\")\ndesigns <- redesign(design, \n                    time_trend = c(0, 0.5), \n                    time_specific_effect = c(0, 1), \n                    unit_specific_effect = c(0, 1))\n\ndiagnosis <- diagnose_design(designs, sims = 5000, diagnosands = declare_diagnosands(bias = mean(estimate - estimand, na.rm = TRUE)))\n\nget_diagnosands(diagnosis) %>% select(time_trend, time_specific_effect, unit_specific_effect, bias, `se(bias)`) %>% round(2)"},{"path":"observational-designs-for-causal-inference.html","id":"observational-causal-inference-for-multiple-units","chapter":"15 Observational designs for causal inference","heading":"15.2 Observational causal inference for multiple units","text":"","code":""},{"path":"observational-designs-for-causal-inference.html","id":"over-time-change-within-units","chapter":"15 Observational designs for causal inference","heading":"15.2.1 Over-time change within units","text":"","code":""},{"path":"observational-designs-for-causal-inference.html","id":"posttreatment-differences","chapter":"15 Observational designs for causal inference","heading":"15.2.2 Posttreatment differences","text":"","code":""},{"path":"observational-designs-for-causal-inference.html","id":"difference-in-differences-2","chapter":"15 Observational designs for causal inference","heading":"15.2.3 Difference-in-differences","text":"","code":""},{"path":"observational-designs-for-causal-inference.html","id":"synthetic-control","chapter":"15 Observational designs for causal inference","heading":"15.2.4 Synthetic control","text":"","code":""},{"path":"observational-designs-for-causal-inference.html","id":"p3iv","chapter":"15 Observational designs for causal inference","heading":"15.3 Instrumental variables","text":"unwilling unable credibly assert blocked back-door paths treatment \\(D\\) outcome \\(Y\\) – , selection--observables assumption fails – might give goal drawing causal inferences.occasionally, world yields opportunity sidestep unobserved confounding generating -random variation variable affects treatment variable. call variable -randomly assigned world “instrument.”delve intricacies instrumental variables design – many – ’s worth considering special thing instrument . Instruments variables randomly assigned nature, government, powers--. Usually, genuine random assignments crafted researchers part deliberate data strategy. Experimenters go great lengths randomly expose units others treatments. world provides bonafide random variation something, ’s rare valuable opportunity. virtue -random assignment, can learn average causal effects instrument without consternation. Conditional geography season, weather conditions -randomly assigned, can learn average effects rainfall many outcomes, like crop yields, voter turnout, attendance sporting events. Conditional gender birth year, draft numbers randomly assigned government, can learn average effects drafted educational attainment, future earnings, public policy preferences. argot instrument variables, effect instrument called “reduced form” “intention--treat” (ITT) effect. ’s really true instrument randomly assigned world, estimation reduced form effect straightforward.trouble comes want leverage random assignment instrument (\\(Z\\)) learn average causal effects treatment variable (\\(D\\)) outcome (\\(Y\\)). , first need change inquiry average treatment effect \\(D\\) \\(Y\\) “local” average treatment effect, LATE. name inquiry reflects idea effect applies specific group units whose treatment status changes result instrument. group often called “compliers.” LATE \\(\\mathbb{E}[Y_i(D_i = 1) - Y_i(D_i = 0) | D_i(Z_i = 1) > D_i(Z_i = 0)]\\) – average treatment effect among group people whose value treatment higher result treatment. LATE different ATE average units whose value treatment depend instrument. Secondly, need assure five (5) instrumental variables assumptions. case binary instrument binary treatment, :Exogeneity instrument: \\(Y_i(D_i = 1), Y_i(D_i = 0), D_i(Z_i = 1), D_i(Z_i = 0) \\perp \\!\\!\\! \\perp Z_i | X_i\\). Substantively, assumption requires (possibly conditional observed covariates \\(X\\)) instrument -randomly, jointly independent treated untreated potential outcomes well potential values treatment variable \\(D\\) take depending values instrument \\(Z\\). Exogeneity usually justified basis qualitative knowledge -random assignment reasonable assumption make. assumption can bolstered design checks like balance pre-treatment values covariates according levels instrument.Exogeneity instrument: \\(Y_i(D_i = 1), Y_i(D_i = 0), D_i(Z_i = 1), D_i(Z_i = 0) \\perp \\!\\!\\! \\perp Z_i | X_i\\). Substantively, assumption requires (possibly conditional observed covariates \\(X\\)) instrument -randomly, jointly independent treated untreated potential outcomes well potential values treatment variable \\(D\\) take depending values instrument \\(Z\\). Exogeneity usually justified basis qualitative knowledge -random assignment reasonable assumption make. assumption can bolstered design checks like balance pre-treatment values covariates according levels instrument.Excludability instrument: \\(Y_i(D_i(Z_i), Z_i) = Y_i(D_i(Z_i))\\). can “exclude” instrument potential outcomes function \\(Y_i()\\) – relevant argument value treatment variable. Another way thinking exclusion restriction conceive “total mediation assumption,” mean \\(Z\\) exactly effect \\(Y\\) except changing value \\(D\\). exclusion restriction, effect instrumental variable wholly mediated treatment variable. exclusion restriction demonstrated empirically typically must asserted qualitative grounds. Since reduced form instrument can estimated many different outcome variables, one piece evidence can bolster exclusion restition show instrument affect plausible causal precedents outcome variable. affect variables might, turn, affect outcome, doubt may cast exclusion restriction.Excludability instrument: \\(Y_i(D_i(Z_i), Z_i) = Y_i(D_i(Z_i))\\). can “exclude” instrument potential outcomes function \\(Y_i()\\) – relevant argument value treatment variable. Another way thinking exclusion restriction conceive “total mediation assumption,” mean \\(Z\\) exactly effect \\(Y\\) except changing value \\(D\\). exclusion restriction, effect instrumental variable wholly mediated treatment variable. exclusion restriction demonstrated empirically typically must asserted qualitative grounds. Since reduced form instrument can estimated many different outcome variables, one piece evidence can bolster exclusion restition show instrument affect plausible causal precedents outcome variable. affect variables might, turn, affect outcome, doubt may cast exclusion restriction.Non-interference: \\(Y(D_i(Z_i), D_{-}, Z_{-}) = Y(D_i(Z_i))\\). Like non-interference assumption, assert particular unit, units’ values instrument treatment affect outcome.Non-interference: \\(Y(D_i(Z_i), D_{-}, Z_{-}) = Y(D_i(Z_i))\\). Like non-interference assumption, assert particular unit, units’ values instrument treatment affect outcome.Monotonicity: \\(D_i(Z_i = 1) \\geq D_i(Z_i = 0), \\forall_i\\). assumption states effect instrument treatment either zero positive units. Monotonicity rules odd types (called “defiers”) \\(D = 1\\) \\(Z = 0\\) \\(D = 0\\) \\(Z = 1\\). Monotonicity usually quite plausible (’s tough imagine person serve military drafted serve drafted!), ’s possible affirm empirically. empirical test demonstrates positive effect instrument treatment one group negative effect different group , however, falsify monotonicity assumption.Monotonicity: \\(D_i(Z_i = 1) \\geq D_i(Z_i = 0), \\forall_i\\). assumption states effect instrument treatment either zero positive units. Monotonicity rules odd types (called “defiers”) \\(D = 1\\) \\(Z = 0\\) \\(D = 0\\) \\(Z = 1\\). Monotonicity usually quite plausible (’s tough imagine person serve military drafted serve drafted!), ’s possible affirm empirically. empirical test demonstrates positive effect instrument treatment one group negative effect different group , however, falsify monotonicity assumption.Non-zero effect instrument treatment. instrument affect treatment, useless learning effects treatment outcome, simply generates compliers. compliers, LATE undefined.Non-zero effect instrument treatment. instrument affect treatment, useless learning effects treatment outcome, simply generates compliers. compliers, LATE undefined.five assumptions met, can shown \\(LATE = \\frac{\\text{Reduced Form}}{\\text{First Stage}}\\). expression underlines importance assumption 5 – instrument doesn’t affect treatment, first stage equal zero ratio undefined. plug-estimator LATE difference--means outcome according instrument divided difference--means treatment according instrument. Equivalently, can use two-stage least squares, yield identical answer ratio difference--means estimates covariate-adjustment case.instrumental variables setup perfectly analogous randomized experiment noncompliance. instrument equivalent random assignment. units comply assignment (treatment group don’t take treatment control group take treatment), comparison groups according treatment variable biased unobserved confounding. best can noncompliance redefine estimand complier average causal effect (equivalent LATE), estimate via two-stage least squares. required excludability assumption assignment treatment can’t affect outcome except realized treatment variable, may may hold given experimental setting.","code":""},{"path":"observational-designs-for-causal-inference.html","id":"declaration-8","chapter":"15 Observational designs for causal inference","heading":"15.3.1 Declaration","text":"","code":"\ndesign <-\n  declare_population(N = 100, U = rnorm(N)) +\n  declare_potential_outcomes(D ~ if_else(Z + U > 0, 1, 0), \n                             assignment_variables = Z) + \n  declare_potential_outcomes(Y ~ 0.1 * D + 0.25 + U, \n                             assignment_variables = D) +\n  declare_estimand(LATE = mean(Y_D_1[D_Z_1 == 1 & D_Z_0 == 0] - \n                                 Y_D_0[D_Z_1 == 1 & D_Z_0 == 0])) +\n  declare_assignment(prob = 0.5) +\n  reveal_outcomes(D, Z) + \n  reveal_outcomes(Y, D) + \n  declare_estimator(Y ~ D | Z, model = iv_robust, estimand = \"LATE\") "},{"path":"observational-designs-for-causal-inference.html","id":"dag-8","chapter":"15 Observational designs for causal inference","heading":"15.3.2 DAG","text":"","code":""},{"path":"observational-designs-for-causal-inference.html","id":"redesign-3","chapter":"15 Observational designs for causal inference","heading":"15.3.3 Redesign","text":"excludability violationsif , go \\(ITT_d\\) \\(ITT_y\\)","code":""},{"path":"observational-designs-for-causal-inference.html","id":"example-5","chapter":"15 Observational designs for causal inference","heading":"15.3.4 Example","text":"Instrument gender match second child first child\ntreatment family size\noutcome variable labor force participation.treatment causes decreases labor participation women parents men parents.","code":""},{"path":"observational-designs-for-causal-inference.html","id":"further-reading-9","chapter":"15 Observational designs for causal inference","heading":"15.3.5 Further reading","text":"– discussion Deaton Cartwright’s critique IV (reiterpretation saying IV biased ATE; response IV targets LATE unbiased late; clear inquiry data+answer strategy can clear debate)ApplicationsMo, Cecilia Hyunjung, Katherine Conn, Georgia Anderson-Nilsson. 2019. “Can National Service Activism Activate Women’s Political Ambition? Evidence Teach America.” Politics, Groups, Identities 7(4): 864-877.Methodological literatureAngrist Pischke (2008) ch. 4 instrumental variables estimationAlan S. Gerber Green (2012) ch. 5 ch. 6 connection IV noncompliance experiment\nDeaton (2010) critique LATE estimand Imbens (2010) rejoinder.","code":""},{"path":"observational-designs-for-causal-inference.html","id":"RDD","chapter":"15 Observational designs for causal inference","heading":"15.4 Regression Discontinuity","text":"","code":""},{"path":"observational-designs-for-causal-inference.html","id":"declaration-9","chapter":"15 Observational designs for causal inference","heading":"15.4.1 Declaration","text":"Regression discontinuity designs exploit substantive knowledge treatment assigned particular way: everyone threshold assigned treatment everyone . Even though researchers control assignment, substantive knowledge threshold serves basis strong identification claim.Thistlewhite Campbell introduced regression discontinuity design 1960s study impact scholarships academic success. insight students test score just scholarship cutoff plausibly comparable students whose scores just cutoff, differences future academic success attributed scholarship .Regression discontinuity designs identify local average treatment effect: average effect treatment exactly cutoff. main trouble design vanishingly little data exactly cutoff, answer strategy needs use data distance away cutoff. away cutoff move, larger threat bias.’ll consider application regression discontinuity design examines party incumbency advantage – effect party winning election vote margin next election.","code":""},{"path":"observational-designs-for-causal-inference.html","id":"declaration-10","chapter":"15 Observational designs for causal inference","heading":"15.4.1.1 Declaration","text":"Model: Regression discontinuity designs four components: running variable, cutoff, treatment variable, outcome. cutoff determines units treated depending value running variable.\nexample, running variable \\(X\\) Democratic party’s margin victory time \\(t-1\\); treatment, \\(D\\), whether Democratic party won election time \\(t-1\\). outcome, \\(Y\\), Democratic vote margin time \\(t\\). ’ll consider population 1,000 pairs elections.\nmajor assumption required regression discontinuity conditional expectation functions treatment control potential outcomes continuous cutoff.23 satisfy assumption, specify two smooth conditional expectation functions, one potential outcome. figure plots \\(Y\\) (Democratic vote margin time \\(t\\)) \\(X\\) (margin time \\(t-1\\)). ’ve also plotted true conditional expectation functions treated control potential outcomes. solid lines correspond observed data dashed lines correspond unobserved data.Model: Regression discontinuity designs four components: running variable, cutoff, treatment variable, outcome. cutoff determines units treated depending value running variable.example, running variable \\(X\\) Democratic party’s margin victory time \\(t-1\\); treatment, \\(D\\), whether Democratic party won election time \\(t-1\\). outcome, \\(Y\\), Democratic vote margin time \\(t\\). ’ll consider population 1,000 pairs elections.major assumption required regression discontinuity conditional expectation functions treatment control potential outcomes continuous cutoff.23 satisfy assumption, specify two smooth conditional expectation functions, one potential outcome. figure plots \\(Y\\) (Democratic vote margin time \\(t\\)) \\(X\\) (margin time \\(t-1\\)). ’ve also plotted true conditional expectation functions treated control potential outcomes. solid lines correspond observed data dashed lines correspond unobserved data.Inquiry: estimand effect Democratic win election Democratic vote margin next election, Democratic vote margin first election zero. Formally, difference conditional expectation functions control treatment potential outcomes running variable exactly zero. black vertical line plot shows difference.Inquiry: estimand effect Democratic win election Democratic vote margin next election, Democratic vote margin first election zero. Formally, difference conditional expectation functions control treatment potential outcomes running variable exactly zero. black vertical line plot shows difference.Data strategy: collect data Democratic vote share time \\(t-1\\) time \\(t\\) 1,000 pairs elections. sampling random assignment.Data strategy: collect data Democratic vote share time \\(t-1\\) time \\(t\\) 1,000 pairs elections. sampling random assignment.Answer strategy: approximate treated untreated conditional expectation functions left right cutoff using flexible regression specification estimated via OLS. particular, fit regression using fourth-order polynomial. Much literature regression discontinuity designs focuses tradeoffs among answer strategies, many analysts recommending higher-order polynomial regression specifications. use one highlight well answer strategy matches functional form model. discuss alternative estimators exercises.Answer strategy: approximate treated untreated conditional expectation functions left right cutoff using flexible regression specification estimated via OLS. particular, fit regression using fourth-order polynomial. Much literature regression discontinuity designs focuses tradeoffs among answer strategies, many analysts recommending higher-order polynomial regression specifications. use one highlight well answer strategy matches functional form model. discuss alternative estimators exercises.","code":"\ncutoff <- 0.5\ncontrol <- function(X) {\n  as.vector(poly(X, 4, raw = TRUE) %*% c(.7, -.8, .5, 1))}\ntreatment <- function(X) {\n  as.vector(poly(X, 4, raw = TRUE) %*% c(0, -1.5, .5, .8)) + .15}\n\ndesign <-\n  declare_population(\n    N = 1000,\n    U = rnorm(N, 0, 0.1),\n    X = runif(N, 0, 1) + U - cutoff,\n    D = 1 * (X > 0)\n  ) +\n  declare_potential_outcomes(\n    Y ~ D * treatment(X) + (1 - D) * control(X) + U, \n    assignment_variable = D\n  ) +\n  declare_estimand(LATE = treatment(0) - control(0)) +\n  reveal_outcomes(Y, D) +\n  declare_estimator(\n    Y ~ poly(X, 4) * D, \n    model = lm_robust, \n    estimand = \"LATE\"\n  )"},{"path":"observational-designs-for-causal-inference.html","id":"dag-9","chapter":"15 Observational designs for causal inference","heading":"15.4.2 DAG","text":"","code":""},{"path":"observational-designs-for-causal-inference.html","id":"example-6","chapter":"15 Observational designs for causal inference","heading":"15.4.3 Example","text":"","code":""},{"path":"observational-designs-for-causal-inference.html","id":"exercises-5","chapter":"15 Observational designs for causal inference","heading":"15.4.4 Exercises","text":"Gelman Imbens (2017) point higher order polynomial regression specifications lead extreme regression weights. One approach obtaining better estimates select bandwidth, \\(h\\), around cutoff, run linear regression. Declare sampling procedure subsets data bandwidth around threshold, well first order linear regression specification, analyze power, bias, RMSE, coverage design vary function bandwidth.Gelman Imbens (2017) point higher order polynomial regression specifications lead extreme regression weights. One approach obtaining better estimates select bandwidth, \\(h\\), around cutoff, run linear regression. Declare sampling procedure subsets data bandwidth around threshold, well first order linear regression specification, analyze power, bias, RMSE, coverage design vary function bandwidth.rdrobust estimator rdrobust package implements local polynomial estimator automatically selects bandwidth RD analysis bias-corrected confidence intervals. Declare another estimator using rdrobust function add design. coverage bias estimator compare regression approaches declared ?rdrobust estimator rdrobust package implements local polynomial estimator automatically selects bandwidth RD analysis bias-corrected confidence intervals. Declare another estimator using rdrobust function add design. coverage bias estimator compare regression approaches declared ?Reduce number polynomial terms treatment() control() functions assess bias design changes potential outcomes become increasingly linear function running variable.Reduce number polynomial terms treatment() control() functions assess bias design changes potential outcomes become increasingly linear function running variable.Redefine population function units higher potential outcome likely locate just cutoff . Assess whether affects bias design.Redefine population function units higher potential outcome likely locate just cutoff . Assess whether affects bias design.","code":""},{"path":"observational-designs-for-causal-inference.html","id":"further-reading-10","chapter":"15 Observational designs for causal inference","heading":"15.4.5 Further Reading","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"experimental-designs-for-causal-inference","chapter":"16 Experimental designs for causal inference","heading":"16 Experimental designs for causal inference","text":"inquiry causal involves comparison counterfactual states world data strategy experimental involves explicit assignment units treatment conditions. Experimental designs causal inference combine two elements. goal research estimate causal effects procedure involves actively allocating treatments.vast majority experimental designs causal inference social sciences take advantage researcher control assignment treatments assign treatments . archetypal two-arm randomized trial, group \\(N\\) subjects recruited, \\(m\\) chosen random receive treatment remaining \\(N-m\\) receive treatment serve controls. inquiry average treatment effect, answer strategy difference--means estimator. strength design can appreciated analogy random sampling. \\(m\\) outcomes treatment group represent random sample treated potential outcomes among \\(N\\) subjects, sample mean treatment group good estimator true average treated potential outcome; analogous claim holds control group.randomization treatments estimate average causal effects relatively recent human invention. glimmers idea appeared earlier, wasn’t least 1920s explicit randomization appeared agricultural science, medicine, education, political science (Jamison 2019). generations scientists access tool. Sometimes critics experiments charge “can’t randomize [causal variable care ]” – course practical constraints treatments researchers can control (ethical, financial, otherwise), think main constraint researcher creativity. scientific history randomized experiments short – just hasn’t randomized  doesn’t mean can’t . (Though similar token, just  randomized doesn’t mean .)Great television situation comedy shows said “refillable” sense basic structure show can “refilled” huge number scenarios – end nine full seasons Seinfeld. Unlike observational studies require bespoke identification justifications new setting, randomized experiment design refillable. follow standardized algorithm – sample units, randomize treatments, measure outcomes – reliably generates causal inferences .Randomized experiments rightly praised desirable inferential properties, course can go wrong many ways designers experiments anticipate minimize. problems include problems data strategy (randomization implementation failures, excludability violations, noncompliance, attrition, interference units), problems answer strategy (conditioning post-treatment variables, failure account clustering, p-hacking), even problems inquiry (estimate-estimand mismatches). course problems apply fortiori non experimental studies, important emphasize experimental studies since often characterized “unbiased” without qualification.designs chapter proceed simplest experimental design – two arm trial – complex designs like randomized saturation design. chapter can profitably read alongside Alan S Gerber Green (2012), foundational text experimental design.","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"individually-randomized-designs","chapter":"16 Experimental designs for causal inference","heading":"16.1 Individually-randomized designs","text":"two-arm randomized trials common subjects can randomly assigned one two conditions, typically, one treatment condition one control condition. two-arm trials eschew pure control condition favor placebo control condition, even second treatment condition. uniting feature designs model includes two two potential outcomes unit data strategy randomly assigns potential outcomes revealed.key choice design two arm trials random assignment procedure. use simple (coin flip, Bernoulli) random assignment use complete random assignment? randomization blocked clustered? “restrict” randomization randomizations generate acceptable levels balance pre-treatment characteristic permitted? explore implications choices coming sections, moment, main point saying “treatments assigned random” insufficient – need describe randomization procedure detail order know analyze resulting experiment. See Section 8.2 description many different random assignment procedures.remainder section, ’ll consider canonical two arm-trial design described Alan S Gerber Green (2012). short, canonical design conducts complete random assignment fixed population, uses difference--means estimate average treatment effect. ’ll now unpack shorthand components M, , D, .model specified fixed sample \\(N\\) subjects. aren’t imagining sampling larger population first – mind fixed set units among conduct experiment. model, unit endowed two latent potential outcomes: treated potential outcome untreated potential outcome. potential outcomes correlation \\(\\rho\\). units higher untreated potential outcomes also higher treated potential outcomes, \\(\\rho\\) positive. Reflecting treatment effects might vary unit unit gives another way think plausible values \\(rho\\). treatment effects similar unit unit, \\(rho\\) close 1. limiting case exactly constant effects (difference treated untreated potential outcome exactly every unit), \\(rho\\) equal 1. difficult (impossible) imagine settings \\(rho\\) negative. potential outcomes negatively correlated, units higher treated potential outcomes lower untreated potential outcomes (large, positive effects) units lower treated potential outcomes higher untreated potential outcomes (large, negative effects).Developing intuitions \\(rho\\) frustrated fundamental problem causal inference: since can ever observe unit treated untreated state (), can’t directly observe correlation potential outcomes. order make guess \\(rho\\), need reason treatment effect heterogeneity. effects close homogeneous, \\(rho\\) positive. patterns treatment effect heterogeneity cause \\(rho\\) negative, . example might “surprising” partisan cue. Imagine control condition, Democratic subjects tend support policy (\\(Y_i(0)\\) high) Republicans tend oppose (\\(Y_i(0)\\) low). treatment “surprise” endorsement Republican elite: treatment group Republicans find supporting policy (\\(Y_i(1)\\) high) whereas treatment group Democrats infer Republican endorsement policy must good one (\\(Y_i(1)\\) low.) Treatments extreme heterogeneity like example principle cause negatively correlated potential outcomes.model specifies fixed sample, inquiries also defined sample level. common inquiry two-arm trial sample average treatment effect, SATE. equal average difference treated untreated potential outcomes units sample: \\(\\mathbb{E}_{\\N}[Y_i(1) - Y_i(0)]\\). Two-arm trials can also support inquiries like SATE among subgroup (called conditional average treatment effect, CATE), ’ll leave inquiries side moment.data strategy uses complete random assignment exactly \\(m\\) \\(N\\) units assigned treatment (\\(Z = 1\\)) remainder assigned control (\\(Z = 0\\)). measure observed outcomes way measure treated potential outcome treatment group untreated potential outcomes control group: \\(Y = Y_i(1) * Z + Y_i(0)*(1 - Z)\\). expression sometimes called “switching equation” way “switches” potential outcome revealed treatment assignment. also embeds crucial assumption – indeed units reveal potential outcomes assigned . experiment encounters noncompliance, assumption violated. ’s also violated violate “excludability,” .e., something treatment moves assignment treatment. example, treatment group measured differently control group, excludability violated.answer strategy difference--means estimator Neyman standard errors:\\[\\begin{align}\n\\widehat{DIM} &= \\frac{\\sum_1^mY_i}{m} - \\frac{\\sum_{m + 1}^NY_i}{N-m} \\\\\n\\widehat{se(DIM)} &= \\sqrt{\\frac{\\widehat{Var}(Y_i|Z = 1)}{m} - \\frac{\\widehat{Var}(Y_i|Z = 0)}{N-m}}\\\\\n\\end{align}\\]estimated standard error can used input two statistical procedures: null hypothesis significance testing via \\(t\\)-test construction 95% confidence interval.DAG corresponding two-arm randomized trial simple. outcome \\(Y\\) affected unknown factors \\(U\\) treatment \\(Z\\). measurement procedure \\(Q\\) affects \\(Y\\) sense measures latent \\(Y\\) records dataset. arrows lead \\(Z\\) randomly assigned. arrow leads \\(Z\\) \\(Q\\), assume excludability violations wherein treatment changes units measured. simple DAG confirms average causal effect Z Y nonparametrically identified back-door paths lead Z Y.","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"analytic-design-diagnosis","chapter":"16 Experimental designs for causal inference","heading":"16.1.1 Analytic design diagnosis","text":"statistical theory canonical two-arm design well explored, analytic expressions many diagnosands available.Bias difference--means estimator. Equation 2.14 Alan S Gerber Green (2012) demonstrates regardless values (except degenerate cases) \\(m\\), \\(N\\), \\(rho\\), bias diagnosand equal zero. “unbiasedness” property many randomized experimental designs. average, difference--means estimates canonical design equal average treatment effect. ’ll explore later chapter, every experimental design yields unbiased estimates. (like blocked experiments differential probabilities assignments) require fix-ups answer strategy others (like clustered experiments unequal cluster sizes) require fix-ups data strategy.Bias difference--means estimator. Equation 2.14 Alan S Gerber Green (2012) demonstrates regardless values (except degenerate cases) \\(m\\), \\(N\\), \\(rho\\), bias diagnosand equal zero. “unbiasedness” property many randomized experimental designs. average, difference--means estimates canonical design equal average treatment effect. ’ll explore later chapter, every experimental design yields unbiased estimates. (like blocked experiments differential probabilities assignments) require fix-ups answer strategy others (like clustered experiments unequal cluster sizes) require fix-ups data strategy.true standard error difference--means estimator. Equation 3.4 Alan S Gerber Green (2012) provides exact expression true standard error canonical two-arm trial.true standard error difference--means estimator. Equation 3.4 Alan S Gerber Green (2012) provides exact expression true standard error canonical two-arm trial.\\[\nSE(DIM)= \\sqrt{\\frac{1}{n-1}\\left\\{\\frac{m\\mathbb{V}(Y_i(0))}{n-m} + \\frac{(N-m)\\mathbb{V}(Y_i(1))}{m} + 2Cov(Y_i(0), Y_i(1))\\right\\}}\n\\]equation contains many design lessons. shows standard error decreases sample size (\\(N\\)) increases variances potential outcomes decrease. provides justification “balanced” designs assign proportion subjects treatment control: variances \\(Y_i(0)\\) \\(Y_i(1)\\) equal, balanced split subjects across conditions yield lowest standard error. variances potential outcomes equal, expression suggests allocating units condition higher variance.Bias standard error estimator. Equation 3.4 true standard error. also learn analytic design diagnosis standard error estimator upwardly biased, say conservative. intuition bias can’t directly estimate covariance term Equation 3.4, bound variance worst-case assumption.24 amount bias standard error estimator depends wrong worst case assumption . \\(rho\\) equal 1, bias goes zero.Bias standard error estimator. Equation 3.4 true standard error. also learn analytic design diagnosis standard error estimator upwardly biased, say conservative. intuition bias can’t directly estimate covariance term Equation 3.4, bound variance worst-case assumption.24 amount bias standard error estimator depends wrong worst case assumption . \\(rho\\) equal 1, bias goes zero.Coverage. Since standard errors upwardly biased – “big” – statistics built inherit bias well. 95% confidence intervals also “big,” coverage diagnosand nominal, , 95% confidence intervals cover true parameter frequently 95% time.Coverage. Since standard errors upwardly biased – “big” – statistics built inherit bias well. 95% confidence intervals also “big,” coverage diagnosand nominal, , 95% confidence intervals cover true parameter frequently 95% time.Power. answer strategy involves conducting statistical significance test null hypothesis average outcome control group equal average outcome treatment group. test also built estimated standard error, upward bias standard error estimator put downward pressure statistical power. section 10.1, reproduced formula given Alan S Gerber Green (2012) statistical power makes two restrictions canonical design: equally-sized treatment groups equal variances potential outcomes.Power. answer strategy involves conducting statistical significance test null hypothesis average outcome control group equal average outcome treatment group. test also built estimated standard error, upward bias standard error estimator put downward pressure statistical power. section 10.1, reproduced formula given Alan S Gerber Green (2012) statistical power makes two restrictions canonical design: equally-sized treatment groups equal variances potential outcomes.Analytic design diagnosis tremendously useful, two reasons. First, obtain guarantees large class designs. experiment fits canonical design properties. Second, learn analytic design diagnosis important design parameters . model, need think treatment effect heterogeneity order develop expectations variances covariances potential outcomes. inquiry, need thinking specific average causal effects – SATE, PATE CATE LATE. data strategy canonical design complete random assignment, need think many units assign treatment (\\(m\\)) relative control (\\(N-m\\)). answer strategy difference--means neyman standard errors – difference means unbiased ATE, Neyman standard error estimator upwardly biased. means coverage conservative ’ll take small hit statistical power. can learn theory, simulation.","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"design-diagnosis-through-simulation","chapter":"16 Experimental designs for causal inference","heading":"16.1.2 Design diagnosis through simulation","text":"course can declare design conduct design diagnosis using simulation. process confirm analytic results, well provide estimates diagnosands statisticians yet derived analytic expressions. code produces “designer” allows us easily vary important components design (course learned studying statistical theory!).simulation investigates much sample allocate treatment group treatment group variance twice large control group variance. diagnosis confirms bias zero, true standard errors Equation 3.4 predicts, coverage nominal, 80% power target middle range \\(m\\). learn power maximized (true standard error minimized) allocate 60 70 units (100 total) treatment. also learn gains choosing unbalanced design (relative 50/50 allocation) small. Even variance treatment group twice large variance control group, don’t lose much sticking balanced design. Since can never sure relative variances treatment control groups ex ante, exercise provides support choosing balanced designs many design settings.","code":"\neq_3.4_designer <-\n  function(N, m, var_Y0, var_Y1, cov_Y0_Y1, mean_Y0, mean_Y1) {\n    \n    fixed_sample <-\n      MASS::mvrnorm(\n        n = N,\n        mu = c(mean_Y0, mean_Y1),\n        Sigma = matrix(c(var_Y0, cov_Y0_Y1, cov_Y0_Y1, var_Y1), nrow = 2),\n        empirical = TRUE # this line makes the means and variances \"exact\" in the sample data\n      ) %>%\n      magrittr::set_colnames(c(\"Y_Z_0\", \"Y_Z_1\"))\n    \n    declare_population(data = fixed_sample) +\n      declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +\n      declare_assignment(m = m) +\n      reveal_outcomes() +\n      declare_estimator(Y ~ Z, estimand = \"ATE\")\n    \n  }\ndesigns <- \n  expand_design(designer = eq_3.4_designer,\n                N = 100,\n                m = seq(10, 90, 10),\n                var_Y0 = 1,\n                var_Y1 = 2,\n                cov_Y0_Y1 = 0.5,\n                mean_Y0 = 1.0,\n                mean_Y1 = 1.75)\n\ndx <- diagnose_designs(designs, sims = 100, bootstrap_sims = FALSE)\ndesigns <- \n  expand_design(designer = eq_3.4_designer,\n                N = 100,\n                m = seq(10, 90, 10),\n                var_Y0 = 1,\n                var_Y1 = 2,\n                cov_Y0_Y1 = 1,\n                mean_Y0 = 1.0,\n                mean_Y1 = 1.75)\n\ndx <- diagnose_designs(designs, sims = 200, bootstrap_sims = FALSE)##   design_label   N  m var_Y0 var_Y1 cov_Y0_Y1 mean_Y0 mean_Y1 estimand_label\n## 1     design_6 100 60      1      2         1       1    1.75            ATE\n##   estimator_label term       bias      rmse power coverage mean_estimate\n## 1       estimator    Z 0.01387209 0.2093836 0.915    0.965     0.7638721\n##   sd_estimate   mean_se type_s_rate mean_estimand n_sims\n## 1   0.2094478 0.2406971           0          0.75    200"},{"path":"experimental-designs-for-causal-inference.html","id":"increasing-precision-through-blocking","chapter":"16 Experimental designs for causal inference","heading":"16.1.3 Increasing precision through blocking","text":"Block random assignment almost always represents precision improvement complete random assignment. block random assignment, conduct “mini-experiments” within separate subsets sample (called blocks), defined pre-treatment covariates. Within block, conduct complete random assignment.25One way thinking blocking reduces sampling variability rules (design) random assignments many units particular subgroup assigned treatment: exactly \\(m_B\\) units treated block \\(B\\). potential outcomes correlated blocking variable, “extreme” assignments produce estimates tails sampling distribution associated complete random assignment.inuition behind blocking illustrated Figure 16.1, shows sampling distribution difference--means estimator complete random assignment. histogram shaded according whether random assignment happened perfectly balance pre-treatment covariate \\(X\\). sampling distribution estimator among set assignments happen balanced tightly distributed around true average treatment effect estimates associated assignments perfectly balanced. can see value blocking procedure – rules design assignments perfectly balanced.\nFigure 16.1: Sampling distribution complete random assignment, covariate balance\n","code":"\nfixed_pop <-\n  fabricate(\n    N = 12,\n    X = complete_ra(N),\n    U = rnorm(N, sd = 0.25)\n  )\n\ndesign <-\n  declare_population(data = fixed_pop) +\n  declare_potential_outcomes(Y ~ 0.2*Z + X + U) +\n  declare_assignment(Z = complete_ra(N = N), handler = fabricate) + \n  declare_reveal() +\n  declare_estimator(Y ~ Z, label = \"DIM\") +\n  declare_estimator(X ~ Z, label = \"balance\") +\n  declare_estimator(Y ~ Z + X, model = lm_robust, label = \"OLS\")\n\nsimulations <- simulate_design(design)"},{"path":"experimental-designs-for-causal-inference.html","id":"increasing-precision-through-covariate-adjustment","chapter":"16 Experimental designs for causal inference","heading":"16.1.4 Increasing precision through covariate adjustment","text":"Choosing block random assignment complete random assignment method incorporating covariate information data strategy purpose decreasing sampling variability. can also incorporate covariate information answer strategy purpose, controlling covariates otherwise conditioning estimating average treatment effect. observational settings like one explore [selection observeables], conditioning covariates used block back-door paths address confounding. , confounding problem – treatment assigned random design, need control covariates order decrease bias. Instead, control covariates order reduce sampling variability.Figure 16.2 illustrates point. sampling distribution difference means shown top line sampling distribution ordinary least squares (Y ~ Z + X) shown bottom line. Estimates extreme ends distribution difference--means pulled tightly center ATE. One interesting wrinkle graph reveals covariate adjustment tighen estimates assignments exactly balance X – help assignments slightly imbalanced.\nFigure 16.2: Impact covariate adjustment sampling distribution\n","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"simulation-comparing-blocking-to-covariate-adjustment","chapter":"16 Experimental designs for causal inference","heading":"16.1.5 Simulation comparing blocking to covariate adjustment","text":"Adjusting pre-treatment covariates predictive outcome almost always increases precision; blocking covariates predictive outcome almost always increases precision . Another way putting covariate information can incorporated answer strategy (covariate adjustment) data strategy (blocking) way, two procedures approximately equivalent.’ll now declare diagnose four closely-related experimental designs. begin, describe fixed population 100 units binary covariate \\(X\\) unobserved heterogeneity \\(U\\). Potential outcomes function treatment \\(Z\\) correlated \\(X\\). Throughout exercise, inquiry ATE.two answer strategies two data strategies ’ll mix--match.combine create four designs, diagnose.diagnosis shows incorporating covariate information either data strategy answer strategy yields similar gains true standard error. Relative canonical design (complete random assignment difference--means), alternatives represents improvement. Blocking \\(X\\) data strategy decreases sampling variability. Controlling \\(X\\) answer strategy decreases sampling variability. – blocking \\(X\\) controlling \\(X\\) – yield additional gains, controlling \\(X\\) nevertheless appropriate using blocked design. reason can seen “average estimated standard error” diagnosand. block, still use difference--means estimator, estimated standard errors decrease relative complete random assignment. usual Neyman variance estimator doesn’t “know” blocking. number fixes problem available. can, simulation, control blocking variable OLS regression. Alternatively, can use “stratified” estimator obtains block-level ATE estimates, averages together, weighting block size. stratified estimator associated standard error estimator – see gerber green page 73-74. stratified estimator instance “analyze randomize” principle. Respecting data strategy answer strategy (adjusting blocking) brings estimated standard error well.","code":"\nfixed_pop <-\n  fabricate(\n    N = 100,\n    X = rbinom(N, 1, 0.5),\n    U = rnorm(N)\n  )\n\ndesign <-\n  declare_population(data = fixed_pop) +\n  declare_potential_outcomes(Y ~ 0.2*Z + X + U) +\n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0))\n# Data strategies\ncomplete_assignment <- \n  declare_assignment(Z = complete_ra(N = N), handler = fabricate) + \n  declare_reveal()\nblocked_assignment <- \n  declare_assignment(Z = block_ra(blocks = X), handler = fabricate) + \n  declare_reveal()\n\n# Answer strategies\nunadjusted_estimator <- declare_estimator(Y ~ Z, estimand = \"ATE\")\nadjusted_estimator <- declare_estimator(Y ~ Z + X, model = lm_robust, estimand = \"ATE\")\ndesign_1 <- design + complete_assignment + unadjusted_estimator\ndesign_2 <- design + blocked_assignment + unadjusted_estimator\ndesign_3 <- design + complete_assignment + adjusted_estimator\ndesign_4 <- design + blocked_assignment + adjusted_estimator\ndiagnose_designs(list(design_1, design_2, design_3, design_4))\nsimulations <- simulate_designs(list(design_1, design_2, design_3, design_4))"},{"path":"experimental-designs-for-causal-inference.html","id":"randomization-checks","chapter":"16 Experimental designs for causal inference","heading":"16.1.6 Randomization checks","text":"Confirm number units conditionCheck balance. Covariate covariate. Omnibus test.sub sub sub bullet point: ’re really worried , can rerandomization, ’s really waste time, just block.","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"what-can-go-wrong","chapter":"16 Experimental designs for causal inference","heading":"16.1.7 What can go wrong","text":"Even simplest two-arm trial design, many things can go wrong. Naturally, sorts problems can described terms M, , D, .crucial assumption made model exactly two potential outcomes unit. violated “spillovers” units, say housemates. unit’s outcome depends treatment status housemate, imagine four potential outcomes unit: unit \\(\\) treated, unit \\(\\)’s housemate treated, treated, neither treated. indeed spillovers, ignored, definition inquiry malformed. units don’t even clearly defined “treated” “untreated” potential outcomes spillovers others, can’t define ATE usual way. solution problem elaborate model account potential outcomes, redefine inquiry respect potential outcomes.ways two arm-trial go wrong concern data strategy. think using complete random assignment fact , bias may creep . nonrandom assignment procedure might something like “first-come, first-served.” assign “first” \\(m\\) units treatment remainder control, assignment procedure randomized. Bias occur potential outcomes first \\(m\\) unlike potential outcomes remaining \\(N-m\\) units.Sometimes researchers successfully conduct random assignment, random assignment happened produce treatment control groups unlike observable ways. unbiasedness property applies whole procedure – many hypothetical iterations experiment, average estimate equal value inquiry. particular estimate can close far true value. solution problem change answer strategy adjust estimates covariates, though recommend adjusting covariates regardless whether treatment control groups appear imbalanced (see following chapters).data strategy problems include noncompliance attrition. Noncompliance occurs units’ treatment status differs treatment assignment. big enough problem devote entire library entry (see “encouragement designs”.). Attrition occurs fail measure outcomes units.","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"example-7","chapter":"16 Experimental designs for causal inference","heading":"16.1.8 Example","text":"HUNT FORa two arm trial complete random assignment. blocking, clustering, multiple treatment arms.[Lauren’s study?]","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"cluster-randomized-designs","chapter":"16 Experimental designs for causal inference","heading":"16.2 Cluster-randomized designs","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"declaration-11","chapter":"16 Experimental designs for causal inference","heading":"16.2.1 Declaration","text":"","code":"\ndesign <-\n  declare_population(\n    V = add_level(\n      N = 100,\n      X = rbinom(N, 1, 0.3),\n      Q = rnorm(N)\n    ),\n    I = add_level(N = 5,\n                  U = rnorm(N))) +\n  declare_potential_outcomes(Y ~ Z * X + U + Q) +\n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_assignment(clusters = V,\n                     blocks = X,\n                     block_prob = c(0.1, 0.5)) +\n  declare_estimator(Y ~ Z,\n                    model = difference_in_means,\n                    estimand = \"ATE\",\n                    label = \"Naive DIM\") +\n  declare_estimator(\n    Y ~ Z,\n    clusters = V,\n    blocks = X,\n    model = difference_in_means,\n    estimand = \"ATE\",\n    label = \"Blocked and Clustered DIM\"\n  ) +\n  declare_estimator(\n    Y ~ Z,\n    clusters = V,\n    fixed_effects = X,\n    model = lm_robust,\n    estimand = \"ATE\",\n    label = \"Naive FE\"\n  )"},{"path":"experimental-designs-for-causal-inference.html","id":"dag-10","chapter":"16 Experimental designs for causal inference","heading":"16.2.2 DAG","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"example-8","chapter":"16 Experimental designs for causal inference","heading":"16.2.3 Example","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"treatment-effect-heterogeneity","chapter":"16 Experimental designs for causal inference","heading":"16.3 Treatment effect heterogeneity","text":"treatment subgroup interactionsmulti-arm trials varying aspects intervention","code":"\nlibrary(DeclareDesign)\nlibrary(tidyverse)\nlibrary(reshape2)\n\n# Declare the design ------------------------------------------------------\n\nATE <- 0.0\n\ndesign <- \n  declare_population(N = 1000,\n                     binary_covariate = rbinom(N, 1, 0.5),\n                     normal_error = rnorm(N)) +\n  # crucial step in POs: effects are not heterogeneous\n  declare_potential_outcomes(Y ~ ATE*Z + normal_error) +\n  declare_assignment(prob = 0.5) +\n  # Three estimators\n  declare_estimator(Y ~ Z, subset = (binary_covariate == 0), label = \"CATE_0\") + \n  declare_estimator(Y ~ Z, subset = (binary_covariate == 1), label = \"CATE_1\") +\n  declare_estimator(Y ~ Z*binary_covariate, \n                    model = lm_robust, term = \"Z:binary_covariate\", label = \"interaction\")\n\n\n# Simulations -------------------------------------------------------------\n\n# sweep across all ATEs from 0 to 0.5\ndesigns <- redesign(design, ATE = seq(0, 0.5, 0.05))\nsimulations <- simulate_design(designs, sims = 500)\n\n\n# Summarize simulations ---------------------------------------------------\n\nreshaped_simulations <-\n  simulations %>%\n  transmute(ATE,\n            sim_ID,\n            estimator_label,\n            estimate,\n            conf.high,\n            conf.low,\n            significant = p.value < 0.05) %>%\n  melt(measure.vars = c(\"estimate\", \"conf.high\", \"conf.low\", \"significant\")) %>%\n  dcast(ATE + sim_ID  ~ estimator_label + variable)\n\n\n# Plot 1 ------------------------------------------------------------------\n\ngg_df <- \n  reshaped_simulations %>%\n  group_by(ATE) %>%\n  summarize(`Significant for one group but not the other` = mean(xor(CATE_0_significant, CATE_1_significant)),\n            `Difference in subgroup effects is significant` = mean(interaction_significant)) %>%\n  gather(condition, power, -ATE)\n\ng1 <-\n  ggplot(gg_df, aes(ATE, power, color = condition)) +\n  geom_point() +\n  geom_line() +\n  geom_label(data = (. %>% filter(ATE == 0.2)),\n             aes(label = condition),\n             nudge_y = 0.02) +\n  theme_bw() +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  theme(legend.position = \"none\") +\n  labs(\n    x = \"True constant effect size\",\n    y = \"Probability of result (akin to statistical power)\",\n    title = \"Research question: Are treatment effects different in group A versus group B?\",\n    subtitle = \"True answer: treatment effects are the same for both groups\",\n    caption = \"Code: https://gist.github.com/acoppock/803e7ca56ca1e6cbf757bdd9d46a2eb5\"\n  )\n\n\n# Plot 2 ------------------------------------------------------------------\n\ng2 <-\n  ggplot(reshaped_simulations, aes(CATE_0_estimate, CATE_1_estimate)) +\n  geom_point(alpha = 0.1) +\n  geom_errorbarh(aes(xmin = CATE_0_conf.low, xmax = CATE_0_conf.high), alpha = 0.1) +\n  geom_errorbar(aes(ymin = CATE_1_conf.low, ymax = CATE_1_conf.high), alpha = 0.1) +\n  geom_vline(xintercept = 0,\n             color = \"red\",\n             linetype = \"dashed\") +\n  geom_hline(yintercept = 0,\n             color = \"red\",\n             linetype = \"dashed\") +\n  facet_wrap( ~ ATE, labeller = label_both) +\n  theme_bw() +\n  theme(strip.background = element_blank()) +\n  labs(x = \"Effect size estimate in group A\",\n       y = \"Effect size estimate in group B\",\n       title = \"Joint distribution of subgroup treatment effect estimates under constant effects\",\n       caption = \"Code: https://gist.github.com/acoppock/803e7ca56ca1e6cbf757bdd9d46a2eb5\")\n\n\n# Save plots --------------------------------------------------------------\n\nggsave(filename = \"het_fx_1.png\", g1, height = 7, width = 7)\nggsave(filename = \"het_fx_2.png\", g2, height = 7, width = 7)"},{"path":"experimental-designs-for-causal-inference.html","id":"declaration-12","chapter":"16 Experimental designs for causal inference","heading":"16.3.1 Declaration","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"dag-11","chapter":"16 Experimental designs for causal inference","heading":"16.3.2 DAG","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"example-9","chapter":"16 Experimental designs for causal inference","heading":"16.3.3 Example","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"designs-encountering-noncompliance","chapter":"16 Experimental designs for causal inference","heading":"16.4 Designs encountering noncompliance","text":"designs noncomplianceencouragement designsplacebo-controlled design (note one part measurement one half causal)Experiments encounter noncompliance units’ treatment status differs treatment assignment. Noncompliance occurs units assigned treatment fail take treatment units assigned control find way take treatment. section describe range strategies experimenters use address noncompliance, along way, ’ll point common pitfalls.time data strategy entails contacting subjects order deliver treatment like bundle information good, noncompliance potential problem. Emails go undelivered, unopened, unread. Letters get lost mail. Phone calls screened, text messages get blocked, DMs ignored. People don’t come door knock, either aren’t home don’t trust strangers. Noncompliance can affect noninformational treatments well: goods may difficult deliver remote locations, subjects may refuse participate assigned experimental activities, research staff might simply fail respect realized treatment schedule laziness incompetence.can done? Experimenters anticipate noncompliance make compensating adjustments research designs (relative canonical two arm design). adjustments ripple M, , D, .biggest change M developing beliefs compliance types, also called “principal strata” (Frangakis Rubin 2002). two-arm trial, subjects can one four compliance types, depending treatment status responds treatment assignment. four types described Table 16.1. \\(D_i(Z = 0)\\) potential outcome – treatment status unit \\(\\) express assigned control. Likewise, \\(D_i(Z = 1)\\) treatment status unit \\(\\) express assigned treatment. potential outcomes can take take value 0 1, intersection allows four types. Always-takers, \\(D_i\\) equal 1 regardless value \\(Z\\) – always take treatment. Never-takers opposite – \\(D_i\\) equal 0 regardless value \\(Z\\). Always-takers Never-takers, assignment treatment change whether take treatment.Compliers units take treatment assigned treatment take treatment assigned control. name “compliers” connotes something disposition subjects makes “compliant” otherwise docile, connotation misleading. Compliance types generated confluence subject behavior data strategy choices. Whether subject answers door canvasser comes calling least part function whether subject home. Data strategies attempt deliver treatment evenings weekends might generate (different) compliers attempt treatment working hours.Table 16.1:  Compliance typesThe last compliance type describe defiers. strange birds refuse treatment assigned treatment, find way obtain treatment assigned control. Whether “defiers” exist turns consequential assumption must made model. good reason believe defiers rare – assignment treatment almost always positive average effect treatment take-, aware cases assignment caused decrease take-, even among subgroup.Without assumptions, can never sure unit’s compliance type. Subjects assigned control group take take treatment (\\(D_i(0) = 1\\)) defiers always-takers. Subjects assigned treatment group take treatment (\\(D_i(1) = 0\\)) defiers never-takers. inability sure compliance types another facet fundamental problem causal inference. Even though subject’s compliance type (respect given design) stable trait, defined subject act multiple counterfactual worlds. can’t tell type unit need see whether take treatment assigned treatment also assigned control.inclusion noncompliance compliance types model also necessitate changes inquiry. Always-takers Never-takers present real problem causal inference. Even power randomly assign, can’t change treatments units take. result, don’t get learn effects treatment among groups. Even inquiry average effect treatment among never-takers, experiment (designed) able generate empirical estimates .26 inquiry fall back average effects among units whose treatment status can successfully manipulate – compliers.call inquiry complier average causal effect (CACE). defined \\(\\mathbb{E}[Y_i(1) - Y_i(0) | d_i(1) > d_i(0)]\\). Just like average treatment effect, refers average individual causal effects, average taken specific subset units, compliers. Compliers units \\(d_i(1) > d_i(0)\\), compliers, \\(d_i(1) = 1\\) \\(d_i(0) = 0\\). assignments treatments binary, CACE mathematically identical local average treatment effect (LATE) described chapter XXX. Whether write CACE LATE sometimes depends academic discipline, LATE common among economists. advantage “CACE” “LATE” specific units effect “local” – local compliers.experiments encounter noncompliance, CACE usually important inquiry theory, since refers average effect causal variable study, least subset units study. However, two common inquiries important address well.first intention--treat (ITT), defined \\(\\mathbb{E}[Y_i(D_i(Z = 1), Z = 1) - Y_i(D_i(Z = 0), Z = 0)]\\). Assignment treatment \\(Z\\) total effect \\(Y\\) mediated whole part treatment status. Sometimes ITT policy-relevant estimand, since describes happen policy maker implemented policy way experiment, inclusive noncompliance. Consider randomized controlled trial effectiveness tax webinar tax compliance. Even webinar effective among people willing watch (CACE large), main trouble faced policy maker getting people sit webinar. ITT describes average effect inviting people webinar, quite small people willing join.second additional inquiry compliance rate, sometimes referred \\(ITT_D\\). describes average effect assignment treatment, written \\(\\mathbb{E}[(D_i(Z = 1) - D_i(Z = 0)]\\). small bit algebra shows \\(ITT_D\\) equal fraction sample compliers minus fraction defiers.three inquiries tightly related. five important assumptions (described ), can write:\\[\n\\begin{align*}\nCACE = \\frac{ITT}{ITT_D}\n\\end{align*}\n\\]derivation relationship given section instrumental variables (section XXX). five assumptions described section identical assumptions required . experimental setting, “exogeneity instrument” guaranteed features data strategy. Since use random assignment, know sure “instrument” (assignment) exogenous. Excludability instrument refers idea effect assignment variable outcome fully mediated treatment. assumption violated mere act assignment changes outcomes. Stated differently, never-takers always-takers reveal different potential outcomes treatment control (\\(Y_i(D_i(Z = 1), Z = 1) \\neq Y_i(D_i(Z = 0), Z = 0)\\)), must assignment changes outcomes. Non-interference setting means units’ treatment status outcomes depend assignment treatment status units. experimental context, assumption monotonicity rules existence defiers. assumption often made plausible features data strategy (perhaps impossible assigned treatment obtain treatment) features model (“defiant” responses assignment behaviorially unlikely). final assumption – nonzero effect instrument treatment – can also assured features data strategy. order learn effects treatment, data strategies must successfully cause least units assigned treatment take treatment.Experimental designs rely heavily data strategies. First, must choose random assignment procedure accommodates important social structures (clustering right level) generates tight sampling distributions (blocking covariates correlated outcome). Second, ensure units comply assignments determined carefully calibrated procedure. experimenters expect noncompliance problem, take steps mitigate problem data strategy. Sometimes means quite simply trying harder: investigating patterns noncompliance, attempting deliver treatment multiple occasions, offering subjects incentives participation. “Trying harder” turning subjects compliers choosing data strategy encounters less noncompliance.second important change data strategy explicit measurement treatment status distinct treatment assignment. designs, measuring treatment status easy: simply record units treatment group able treat unable treat. , measuring compliance trickier. example, treatments emailed, might never know subjects read email. Perhaps email service track read receipts, case one facet measurement problem solved. won’t know, however, many subjects read subject line – subject line contains treatment information, even subjects don’t click email may “partially” treated. main advice measure compliance conservative way: treatment emails bounce altogether, subjects treated. [REWRITE]Estimation CACE straightforward subseting analysis compliers. plug-estimator CACE good properties takes ratio \\(ITT\\) estimate \\(ITT_d\\) estimate. Since \\(ITT_d\\) must number zero one, estimator “inflates” \\(ITT\\) compliance rate. Another way thinking \\(ITT\\) deflated never-takers always-takers, among \\(ITT\\) construction 0, instead “inflating” “re-inflating” ITT level CACE. Two-stage least squares instrument treatment random assignment numerically equivalent procedure treatment assignments binary. Two-stage least squares advantage able seamlessly incorporate covariate information increase precision.","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"declaration-13","chapter":"16 Experimental designs for causal inference","heading":"16.4.1 Declaration","text":"design declaration show two-stage least squares provides good estimates CACE ATE, can’t learn , since always-takers never-takers don’t respond treatment assignment. Alternative estimators (per-protocol treated) biased CACE ATE.","code":"\ndirect_effect_of_encouragement <- 0.0\nproportion_defiers <- 0.0\n\ndesign <-\n  declare_population(\n    N = 100,\n    type = sample(\n      x = c(\"Always-Taker\", \"Never-Taker\", \"Complier\", \"Defier\"),\n      prob = c(0.1, 0.1, 0.8, 0.0),\n      size = N, replace = TRUE\n    ),\n    U = rnorm(N)\n  ) +\n  declare_potential_outcomes(\n    D ~ case_when(\n      Z == 1 & type %in% c(\"Always-Taker\", \"Complier\") ~ 1,\n      Z == 1 & type %in% c(\"Never-Taker\", \"Defier\") ~ 0,\n      Z == 0 & type %in% c(\"Never-Taker\", \"Complier\") ~ 0,\n      Z == 0 & type %in% c(\"Always-Taker\", \"Defier\") ~ 1\n    )\n  ) +\n  declare_potential_outcomes(\n    Y ~ 0.5 * (type == \"Complier\") * D +\n      0.25 * (type == \"Always-Taker\") * D +\n      0.75 * (type == \"Defier\") * D +\n      # Building in NO excludability violation\n      0 * Z + U,\n    assignment_variables = c(\"D\", \"Z\")\n  ) +\n  declare_estimand(CACE = mean(Y_D_1_Z_1 - Y_D_0_Z_0),\n                   subset = type == \"Complier\") +\n  declare_assignment(prob = 0.5) +\n  reveal_outcomes(D, assignment_variable = \"Z\") +\n  reveal_outcomes(Y, assignment_variables = c(\"D\", \"Z\")) +\n  declare_estimator(Y ~ D | Z, model = iv_robust, estimand = \"CACE\")"},{"path":"experimental-designs-for-causal-inference.html","id":"dag-12","chapter":"16 Experimental designs for causal inference","heading":"16.4.2 DAG","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"example-10","chapter":"16 Experimental designs for causal inference","heading":"16.4.3 Example","text":": demonstrate violations defiers excludability leads bias.","code":"\ntypes <- c(\"Always-Taker\", \"Never-Taker\", \"Complier\", \"Defier\")\ndirect_effect_of_encouragement <- 0.0\nproportion_defiers <- 0.0\n\ndesign <-\n  declare_population(\n    N = 500,\n    type = sample(\n      types,\n      N,\n      replace = TRUE,\n      prob = c(0.1, 0.1, 0.8 - proportion_defiers, proportion_defiers)\n    ),\n    noise = rnorm(N)\n  ) +\n  declare_potential_outcomes(\n    D ~ case_when(\n      Z == 0 & type %in% c(\"Never-Taker\", \"Complier\") ~ 0,\n      Z == 1 & type %in% c(\"Never-Taker\", \"Defier\") ~ 0,\n      Z == 0 & type %in% c(\"Always-Taker\", \"Defier\") ~ 1,\n      Z == 1 & type %in% c(\"Always-Taker\", \"Complier\") ~ 1\n    )\n  ) +\n  declare_potential_outcomes(\n    Y ~ 0.5 * (type == \"Complier\") * D +\n      0.25 * (type == \"Always-Taker\") * D +\n      0.75 * (type == \"Defier\") * D +\n      direct_effect_of_encouragement * Z + noise,\n    assignment_variables = c(\"D\", \"Z\")\n  ) +\n  declare_estimand(CACE = mean((Y_D_1_Z_1 + Y_D_1_Z_0) / 2 -\n                                 (Y_D_0_Z_1 + Y_D_0_Z_0) / 2),\n                   subset = type == \"Complier\") +\n  declare_assignment(prob = 0.5) +\n  reveal_outcomes(D, assignment_variable = \"Z\") +\n  reveal_outcomes(Y, assignment_variables = c(\"D\", \"Z\")) +\n  declare_estimator(Y ~ D | Z, model = iv_robust, estimand = \"CACE\")\ndesigns <- redesign(\n  design,\n  proportion_defiers = seq(0, 0.3, length.out = 5),\n  direct_effect_of_encouragement = seq(0, 0.3, length.out = 5)\n)\n\nsimulations_df <- simulate_design(designs, sims = sims)"},{"path":"experimental-designs-for-causal-inference.html","id":"multiperiod-designs","chapter":"16 Experimental designs for causal inference","heading":"16.5 Multiperiod Designs","text":"Baseline-endline (connection covariate control, e.g., controlling Y_pre)Baseline-endline (connection covariate control, e.g., controlling Y_pre)multiple rounds post-treatment measurement.\nOne justification ’re interested persistence.\nmultiple measurements decrease variance.\nInquiries: “persistence ratio” ATE_t2 / ATE_t1 versus “average persistence” (Y_{t2}(1) - Y_{t2}(0))/(Y_{t1}(1) - Y_{t1}(0))\nmultiple rounds post-treatment measurement.One justification ’re interested persistence.multiple measurements decrease variance.Inquiries: “persistence ratio” ATE_t2 / ATE_t1 versus “average persistence” (Y_{t2}(1) - Y_{t2}(0))/(Y_{t1}(1) - Y_{t1}(0))CrossoverCrossoverStepped WedgeStepped WedgeDownstream experimentation. add ONE PERIOD!Downstream experimentation. add ONE PERIOD!Literature: “T”, Durably reducing Transphobia. Nollywood stepped wedge. Progressa baseline endline?Literature: “T”, Durably reducing Transphobia. Nollywood stepped wedge. Progressa baseline endline?massive point DONT use time infer causal effects, still use random assignment.massive point DONT use time infer causal effects, still use random assignment.stepped wedge design, individuals randomly assigned enter treatment different stages stage, outcomes remeasured. Figure 16.3 illustrates study gets name: two units randomly added treatment group three waves, treatment group increases “steps” control group diminishes.Often, stepped wedge designs vaunted policy appeal allow everyone (eventually) treated context experiment. However, achieve goal regular two-arm trial treating everyone final wave measurement, stepped wedge designs involve treating everyone. Beyond ethical logistical appeal, design can squeeze power small sample treating wave though study.","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"declaration-14","chapter":"16 Experimental designs for causal inference","heading":"16.5.1 Declaration","text":"Model: Eight units randomized remeasured three time points. Importantly, unit given point time reveals one two potential outcomes — treated untreated. untreated potential outcomes consist unit- unit-period-specific shock. Treated potential outcomes increase relative control potential outcome rate 1 period — words, bigger treatment effects later periods.Model: Eight units randomized remeasured three time points. Importantly, unit given point time reveals one two potential outcomes — treated untreated. untreated potential outcomes consist unit- unit-period-specific shock. Treated potential outcomes increase relative control potential outcome rate 1 period — words, bigger treatment effects later periods.Inquiry: estimand average treatment effect units periods sample. Averaged three periods, ATE (1 + 2 + 3) / 3 = 2.Inquiry: estimand average treatment effect units periods sample. Averaged three periods, ATE (1 + 2 + 3) / 3 = 2.Data strategy: assignment strategy adds two units (one-quarter total sample) treatment random wave, leaving two units treated wave —see remaining two orange squares top-right corner Figure 16.3. Notice , every unit assigned treatment wave equal probability, wave reveals treated untreated potential outcomes unit-periods different probabilities. first wave, one-quarter units reveal treated potential outcomes; second wave, one-half units reveal treated potential outcomes, final wave, three-quarters units reveal treated potential outcomes. essence, experiment like block-randomized trial, unit-periods grouped period-specific blocks differential probabilities assignment.Data strategy: assignment strategy adds two units (one-quarter total sample) treatment random wave, leaving two units treated wave —see remaining two orange squares top-right corner Figure 16.3. Notice , every unit assigned treatment wave equal probability, wave reveals treated untreated potential outcomes unit-periods different probabilities. first wave, one-quarter units reveal treated potential outcomes; second wave, one-half units reveal treated potential outcomes, final wave, three-quarters units reveal treated potential outcomes. essence, experiment like block-randomized trial, unit-periods grouped period-specific blocks differential probabilities assignment.Answer strategy: Just block-randomized trial differential probabilities, need take account fact assignment strategy “-represents” treated potential outcomes first wave “-represents” last wave. Inverse-propensity weights (IPWs) one way correct disparities. unit’s IPW represented Figure 16.3. Using weights, get “representative” estimate average control treatment potential outcomes wave. didn’t apply weights, wouldn’t get representative view unobserved potential outcomes ’re sampling, estimates biased. buy us? also declare “two-arm” estimator focuses wave 2 , half units control half treatment. Diagnosis shows remeasuring randomizing gets us (.52 - .41) / .41 = 27% higher power.Answer strategy: Just block-randomized trial differential probabilities, need take account fact assignment strategy “-represents” treated potential outcomes first wave “-represents” last wave. Inverse-propensity weights (IPWs) one way correct disparities. unit’s IPW represented Figure 16.3. Using weights, get “representative” estimate average control treatment potential outcomes wave. didn’t apply weights, wouldn’t get representative view unobserved potential outcomes ’re sampling, estimates biased. buy us? also declare “two-arm” estimator focuses wave 2 , half units control half treatment. Diagnosis shows remeasuring randomizing gets us (.52 - .41) / .41 = 27% higher power.\nFigure 16.3: Illustration random assignment stepped-wedge design.\n","code":"\ndesign <-\n  declare_population(\n    unit = add_level(N = 8,\n                     X = rnorm(N)),\n    period = add_level(\n      N = 3,\n      time = as.numeric(period),\n      p = c(1 / 4, 1 / 4 + 1 / 4, 1 / 4 + 1 / 4 + 1 / 4),\n      nest = FALSE\n    ),\n    obs = cross_levels(by = join(unit, period),\n                       U = rnorm(N))\n  ) +\n  declare_potential_outcomes(Y ~ X + U + Z * time) +\n  declare_assignment(\n    clusters = unit,\n    conditions = 1:4,\n    assignment_variable = \"wave\"\n  ) +\n  declare_assignment(Z = as.numeric(time >= wave),\n                     ipw = 1 / (Z * p + (1 - Z) * (1 - p)),\n                     handler = fabricate) +\n  reveal_outcomes(Y, Z) +\n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_estimator(\n    Y ~ Z,\n    model = lm_robust,\n    estimand = \"ATE\",\n    label = \"1: Stepped Wedge\",\n    weights = ipw,\n    clusters = unit\n  ) +\n  declare_estimator(\n    Y ~ Z,\n    model = lm_robust,\n    estimand = \"ATE\",\n    label = \"2: Wave 2 Only\",\n    subset = period == 2\n  )"},{"path":"experimental-designs-for-causal-inference.html","id":"dag-13","chapter":"16 Experimental designs for causal inference","heading":"16.5.2 DAG","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"example-11","chapter":"16 Experimental designs for causal inference","heading":"16.5.3 Example","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"exercises-6","chapter":"16 Experimental designs for causal inference","heading":"16.5.4 Exercises","text":"Add answer strategy design doesn’t include IPWs diagnose design.affect bias estimates? Explain answer.Now change potential outcomes function treatment effect constant across periods. affect bias? Explain answer.relative power stepped-wedge single-period, two-arm trial change treatment effect restricted constant across periods? Explain answer.design makes subtle crucial assumption potential outcomes: namely, treated potential outcome revealed one period irrespective whether unit treated previous (subsequent) periods. violation assumption look like?","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"spillover-designs","chapter":"16 Experimental designs for causal inference","heading":"16.6 Spillover designs","text":"two kinds spillover designs“disconnected” graphs: e.g. connection within households across. includes “multilevel” design also “randomized saturation design”. Aronow shepherd paluck. Ichino Scheundlin. Sinclair mcconnell green.“disconnected” graphs: e.g. connection within households across. includes “multilevel” design also “randomized saturation design”. Aronow shepherd paluck. Ichino Scheundlin. Sinclair mcconnell green.“connected” graphs: stoves, lawnsigns, twitter. Assumption invariance “far enough” away. Worms.“connected” graphs: stoves, lawnsigns, twitter. Assumption invariance “far enough” away. Worms.Randomized saturation designs used measure diffusion treatment effects throughout pre-defined area network (Baird et al. (2018)). design works two phases. First, entire areas networks randomly assigned saturations, e.g. 0%, 25%, 75% saturation. , network, units individually assigned proportions determined saturations: e.g., groups randomly assigned 0%, none units assigned treatment, groups assigned 25% 75%, one-quarter three-quarters units assigned treatment, respectively. comparing untreated units 0% groups untreated counterparts higher-saturation networks, researchers can estimate indirect treatment effects result saturation-induced spillovers.","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"declaration-15","chapter":"16 Experimental designs for causal inference","heading":"16.6.1 Declaration","text":"Model: four networks comprised four units. Units’ treatment assignment function network’s saturation assignment, turn defines amount spillover receive. spillover equal proportion units treated network. Potential outcomes thus defined terms S—unit’s group’s saturation—Z—whether unit treated.Model: four networks comprised four units. Units’ treatment assignment function network’s saturation assignment, turn defines amount spillover receive. spillover equal proportion units treated network. Potential outcomes thus defined terms S—unit’s group’s saturation—Z—whether unit treated.Inquiry: two inquiries. First, want know spillover, defined average difference outcomes untreated unit high versus low saturation network: \\(\\mathbb{E}[Y_i(Z_i = 0, S_i = \\text{high})-Y_i(Z_i = 0, S_i = \\text{low})]\\). also want know “direct effect”–e.g. happens directly treated disregard spillovers. defined potential outcomes experiment reveal, since one treated low-saturation constituencies: \\(\\mathbb{E}[Y_i(Z_i = 1, S_i = \\text{low})-Y_i(Z_i = 0, S_i = \\text{low})]\\).Inquiry: two inquiries. First, want know spillover, defined average difference outcomes untreated unit high versus low saturation network: \\(\\mathbb{E}[Y_i(Z_i = 0, S_i = \\text{high})-Y_i(Z_i = 0, S_i = \\text{low})]\\). also want know “direct effect”–e.g. happens directly treated disregard spillovers. defined potential outcomes experiment reveal, since one treated low-saturation constituencies: \\(\\mathbb{E}[Y_i(Z_i = 1, S_i = \\text{low})-Y_i(Z_i = 0, S_i = \\text{low})]\\).Data strategy: assign entire networks low (0%) high (75%) saturation. randomize individuals within groups treatment control proportions dictated saturation. Thus, saturation cluster-randomized, whereas treatment block-randomized.Data strategy: assign entire networks low (0%) high (75%) saturation. randomize individuals within groups treatment control proportions dictated saturation. Thus, saturation cluster-randomized, whereas treatment block-randomized.Answer strategy: weight individual inverse probability find condition ’re . estimator conditions saturation effect direct treatment effect. Note standard errors clustered network level account clustering saturation assignment.Answer strategy: weight individual inverse probability find condition ’re . estimator conditions saturation effect direct treatment effect. Note standard errors clustered network level account clustering saturation assignment.","code":"\ndesign <-\n  declare_population(\n    group = add_level(N = 50, X = rnorm(N)),\n    unit = add_level(N = 50, U = rnorm(N))\n  ) +\n  declare_assignment(clusters = group, conditions = c(\"low\", \"high\"), assignment_variable = S) +\n  declare_step(S_prob = case_when(S == \"low\" ~ 0.25, S == \"high\" ~ 0.75), mutate) +\n  declare_assignment(blocks = group, prob_unit = S_prob) +\n  declare_step(spillover = ave(Z, group, FUN = mean),\n               handler = fabricate)  +\n  declare_potential_outcomes(\n    Y ~ Z + spillover * (S == \"low\") + Z * spillover * (S == \"high\") + X + U,\n    conditions = list(Z = c(0, 1), S = c(\"low\", \"high\"))) +\n  declare_estimand(ATE_saturation = mean(Y_Z_0_S_high - Y_Z_0_S_low),\n                   ate_no_spill = mean(Y_Z_1_S_low - Y_Z_0_S_low)) +\n  reveal_outcomes(Y, c(Z, S)) +\n  declare_estimator(Y ~ Z + S,\n                    weights = 1 / (S_cond_prob * Z_cond_prob),\n                    model = lm_robust,\n                    term = c(\"Z\", \"Shigh\"),\n                    estimand = c(\"ATE_saturation\", \"ate_no_spill\"),\n                    label = \"main effect\")"},{"path":"experimental-designs-for-causal-inference.html","id":"dag-14","chapter":"16 Experimental designs for causal inference","heading":"16.6.2 DAG","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"exercises-7","chapter":"16 Experimental designs for causal inference","heading":"16.6.3 Exercises","text":"Setting saturations back original values, remove weights estimator. happens bias? Explain answer.Setting saturations back original values, remove weights estimator. happens bias? Explain answer.potential outcomes declaration Y, add interaction high saturation condition Z, diagnose design saturations set original values 0 .75.\nestimate ATE treatment biased whereas estimate saturation effect ?\ncan researcher simply estimate interaction design?\nNow increase low saturation 25% add interaction Z S estimator. make design unbiased ?\npotential outcomes declaration Y, add interaction high saturation condition Z, diagnose design saturations set original values 0 .75.estimate ATE treatment biased whereas estimate saturation effect ?can researcher simply estimate interaction design?Now increase low saturation 25% add interaction Z S estimator. make design unbiased ?","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"experimental-designs-for-descriptive-inference","chapter":"17 Experimental designs for descriptive inference","heading":"17 Experimental designs for descriptive inference","text":"ever need experiment descriptive inference?Suppose want understand causal model M violin. particular, descriptive inquiry pitch highest string, E string. want know E string tune. Call latent pitch string \\(Y^*\\). matter hard listen string, can’t hear \\(Y^*\\) – latent. part data strategy \\(D\\), measure pitch \\(P\\) plucking : \\(Y^* -> Y <- P\\). descriptive research causal model \\(M\\), DAG violin includes four string nodes cause pitch nodes; ’d like know descriptive fact pitch nodes (frequency vibrate?).question recast causal inquiry: untreated potential outcome pitch unplucked string, defined frequency vibration. strings never perfectly still, can call untreated potential outcome \\(Y_i(0) = 0hz\\). treated potential outcome frequency string plucked \\(Y_i(1) = 650hz\\). causal effect plucking string \\(Y_i(1) - Y_i(0) = 650 - 0 = 650\\).Whether framed descriptive inquiry casual inquiry, arrive answer 650 hertz. Violinists reading know means E string flat need tuned \\(659.3hz\\) (using equal temperament).","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"audit-experiments","chapter":"17 Experimental designs for descriptive inference","heading":"17.1 Audit experiments","text":"Audit experiments used measure whether requests vary dimension interest receive equal treatment. design used commonly employment settings measure whether job applications otherwise similar come candidates different genders, races, social backgrounds receive rate job interview invitations. approach applied wide range settings, including education, housing, requests politicians.goal can thought measurement problem: measuring level discrimination. insofar acts discrimination responses stimuli can addressed using methods causal inference.example, White, Nathan, Faller (2015) seek measure discrimination assessing effects email Latino name (versus White name) whether well election officials respond requests information.","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"declaration-16","chapter":"17 Experimental designs for descriptive inference","heading":"17.1.1 Declaration","text":"Model:\nmodel two outcome variables, \\(R_i\\) \\(Y_i\\). \\(R_i\\) stands “response” equal 1 response sent, 0 otherwise. \\(Y_i\\) tone response normally distributed defined. \\(Z_i\\) treatment equals 1 email sent using Latino name 0 otherwise. table shows potential outcomes four possible types subjects, depending potential outcomes \\(R_i\\). types always respond regardless treatment D types never respond, regardless treatment. B types respond treated, whereas C types respond treated. table also includes columns potential outcomes \\(Y_i\\), showing potential outcome subjects express depending type. key thing note B, C, D types, effect treatment \\(Y_i\\) undefined messages never sent tone. last (important) feature model outcomes \\(Y_i\\) possibly correlated subject type. Even though \\(\\mathbb{E}[Y_i(1) | \\text{Type} = ]\\) \\(\\mathbb{E}[Y_i(1) | \\text{Type} = B]\\) exist, ’s reason expect .\ndesign, assume distribution types 40% , 5% B, 10% C, 45% D.Model:model two outcome variables, \\(R_i\\) \\(Y_i\\). \\(R_i\\) stands “response” equal 1 response sent, 0 otherwise. \\(Y_i\\) tone response normally distributed defined. \\(Z_i\\) treatment equals 1 email sent using Latino name 0 otherwise. table shows potential outcomes four possible types subjects, depending potential outcomes \\(R_i\\). types always respond regardless treatment D types never respond, regardless treatment. B types respond treated, whereas C types respond treated. table also includes columns potential outcomes \\(Y_i\\), showing potential outcome subjects express depending type. key thing note B, C, D types, effect treatment \\(Y_i\\) undefined messages never sent tone. last (important) feature model outcomes \\(Y_i\\) possibly correlated subject type. Even though \\(\\mathbb{E}[Y_i(1) | \\text{Type} = ]\\) \\(\\mathbb{E}[Y_i(1) | \\text{Type} = B]\\) exist, ’s reason expect .\ndesign, assume distribution types 40% , 5% B, 10% C, 45% D.Table 17.1:  Causal TypesInquiry:\ntwo inquiries. first straightforward: \\(\\mathbb{E}[R_i(1) - R_i(0)]\\) Average Treatment Effect response. second inquiry undefined inquiry answer: \\(\\mathbb{E}[Y_i(1) - Y_i(0)]\\). also consider third inquiry, defined: \\(\\mathbb{E}[Y_i(1) - Y_i(0) | \\mathrm{Type} = ]\\), average effect treatment tone among \\(\\) types.Inquiry:two inquiries. first straightforward: \\(\\mathbb{E}[R_i(1) - R_i(0)]\\) Average Treatment Effect response. second inquiry undefined inquiry answer: \\(\\mathbb{E}[Y_i(1) - Y_i(0)]\\). also consider third inquiry, defined: \\(\\mathbb{E}[Y_i(1) - Y_i(0) | \\mathrm{Type} = ]\\), average effect treatment tone among \\(\\) types.Data strategy:\ndata strategy use complete random assignment assign 250 500 units treatment.Data strategy:data strategy use complete random assignment assign 250 500 units treatment.Answer strategy:\n’ll try answer three inquiries difference--means estimator, diagnosis reveal, strategy works well inquiries others.Answer strategy:’ll try answer three inquiries difference--means estimator, diagnosis reveal, strategy works well inquiries others.","code":"\ndesign <- \n  declare_population(N = 100,\n                     U = rnorm(N)) +\n  declare_potential_outcomes(\n    R ~ if_else(Z + U > 0.5, 1, 0), conditions = list(Z = c(0, 1))) +\n  declare_potential_outcomes(\n    Q ~ if_else(R == 1, Z + U, NA_real_), \n    conditions = list(Z = c(0, 1), R = c(0, 1))) +\n  declare_estimand(ATE_R = mean(R_Z_1 - R_Z_0)) + \n  declare_estimand(CATE_ar = mean(Q_Z_1_R_1 - Q_Z_0_R_1), \n                   subset = (R_Z_1 == 1 & R_Z_0 == 0)) + \n  declare_assignment(prob = 0.5) +\n  reveal_outcomes(R, Z) +\n  reveal_outcomes(Q, c(Z, R)) +\n  declare_estimator(R ~ Z, estimand = \"ATE_R\", label = \"ATE_R\") +\n  declare_estimator(Q ~ Z, subset = (R == 1), \n                    estimand = \"CATE_ar\", \n                    label = \"CATE_ar\")"},{"path":"experimental-designs-for-descriptive-inference.html","id":"dag-15","chapter":"17 Experimental designs for descriptive inference","heading":"17.1.2 DAG","text":"","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"takeaways","chapter":"17 Experimental designs for descriptive inference","heading":"17.1.3 Takeaways","text":"effect response identified designThis “descriptive” quantity sense seek estimate fraction bureaucrats discriminate Latinos. learn treatment effect response “monotonic”","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"list-experiments","chapter":"17 Experimental designs for descriptive inference","heading":"17.2 List experiments","text":"Sometimes, subjects might tell truth certain attitudes behaviors asked directly. Responses may affected sensitivity bias, tendency survey subjects dissemble fear negative repercussions individual group learns true response (Blair, Coppock, Moor 2020). cases, standard survey estimates based direct questions biased. One class solutions problem obscure individual responses, providing protection social legal pressures. obscure responses systematically experiment, can often still identify average quantities interest. One design list experiment (introduced Miller (1984)), asks respondents count number `yes’ responses series questions including sensitive item, rather yes answer sensitive item . List experiments give subjects cover aggregating answer sensitive item responses questions.example, 2016 Presidential election U.S., observers concerned pre-election estimates support Donald Trump might downwardly biased “Shy Trump Supporters” – survey respondents supported Trump hearts, embarrassed admit pollsters. assess possibility, Coppock (2017) obtained estimates Trump support free social desirability bias using list experiment. Subjects control treatment groups asked: “list [three/four] things people people . Please tell MANY . want know ones , just many. [three/four] things:”treatment group averaged 1.843 items control group averaged 1.548 items, difference--means estimate 0.296. usual assumptions randomized experiments, difference--means unbiased estimator average treatment effect asked respond treated list versus control list. (descriptive) estimand proportion people support Donald Trump.difference--means unbiased estimator inquiry, invoke two additional assumptions (Imai 2011):design effects. count ``yes’’ responses control items must whether respondent assigned treatment control group. assumption highlights need good estimate average control item count control group (example, 1.843). use net control item count responses treated group (left sensitive item proportion).design effects. count ``yes’’ responses control items must whether respondent assigned treatment control group. assumption highlights need good estimate average control item count control group (example, 1.843). use net control item count responses treated group (left sensitive item proportion).misreporting. respondent must report truthful answer sensitive item treatment group, granted anonymity protection list experiment. assumption relies fact sensitive item aggregated among control items identifying individual responses , cases, possible, cover enough change respondent’s willingness truthfully report. However, two circumstances respondent provided cover: respondent reports “zero” treatment group, exactly identified holding sensitive trait; report highest possible count treatment group, exactly identified holding trait. describe resulting biases floor ceiling effects, respectively.misreporting. respondent must report truthful answer sensitive item treatment group, granted anonymity protection list experiment. assumption relies fact sensitive item aggregated among control items identifying individual responses , cases, possible, cover enough change respondent’s willingness truthfully report. However, two circumstances respondent provided cover: respondent reports “zero” treatment group, exactly identified holding sensitive trait; report highest possible count treatment group, exactly identified holding trait. describe resulting biases floor ceiling effects, respectively.estimate , assumptions, free sensitivity bias unbiased estimator proportion holding sensitive item. However, list experiment estimates much higher variance direct questions. Trump survey example, 95% confidence interval list experiment estimate nearly 14 percentage points wide, whereas 95% confidence interval (possibly biased!) direct question asked sample closer 4 percentage points. choice list experiments direct question therefore bias-variance tradeoff. List experiments may less bias, higher variance. Direct questions may biased, less variance.","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"declaration-17","chapter":"17 Experimental designs for descriptive inference","heading":"17.2.1 Declaration","text":"Model: model includes subjects’ true support Donald Trump (\\(Y_star\\)) whether “shy” (\\(S\\)). two variables combine determine subjects respond asked directly Trump support.\npotential outcomes model combines three types information determine subjects respond list experiment: responses three nonsensitive control items (\\(X\\)), true support Trump (\\(Y_star\\)), whether assigned see treatment control list (\\(Z\\)). Notice definition potential outcomes embeds liars design effects assumptions required list experiment design.\nalso global parameter reflects expectations proportion Trump supporters shy. ’s set 20%.Model: model includes subjects’ true support Donald Trump (\\(Y_star\\)) whether “shy” (\\(S\\)). two variables combine determine subjects respond asked directly Trump support.potential outcomes model combines three types information determine subjects respond list experiment: responses three nonsensitive control items (\\(X\\)), true support Trump (\\(Y_star\\)), whether assigned see treatment control list (\\(Z\\)). Notice definition potential outcomes embeds liars design effects assumptions required list experiment design.also global parameter reflects expectations proportion Trump supporters shy. ’s set 20%.Inquiry: estimand proportion voters actually plan vote Trump.Data strategy: randomly assign 50% 100 subjects treatment remainder control. survey, ask subjects direct question (\\(Y_{\\rm direct}\\)) list experiment question (\\(Y_{\\rm list}\\)).Answer strategy: estimate proportion truthful Trump voters two ways. First, take mean answers direct question. Second, take difference means responses list experiment question.","code":"\nmodel <- \n  declare_population(\n    N = 100,\n    U = rnorm(N),\n    X = rbinom(N, size = 3, prob = 0.5),\n    Y_star = rbinom(N, size = 1, prob = 0.3),\n    S = case_when(Y_star == 0 ~ 0L,\n                  Y_star == 1 ~ rbinom(N, size = 1, prob = 0.2))\n  ) + \n  declare_potential_outcomes(Y_list ~ Y_star * Z + X) \ninquiry <- declare_estimand(proportion = mean(Y_star))\ndata_strategy <- \n  declare_measurement(Y_direct = Y_star - S) +\n  declare_assignment(prob = 0.5) + \n  declare_reveal(Y_list, Z)\nanswer_strategy <-\n  declare_estimator(Y_direct ~ 1,\n                    model = lm_robust,\n                    estimand = \"proportion\",\n                    label = \"direct\") +\n  declare_estimator(Y_list ~ Z, estimand = \"proportion\", label = \"list\")\ndesign <- model + inquiry + data_strategy + answer_strategy"},{"path":"experimental-designs-for-descriptive-inference.html","id":"dag-16","chapter":"17 Experimental designs for descriptive inference","heading":"17.2.2 DAG","text":"liars assumption list experiments evident DAG: Sensitivity bias \\(S\\) parent list experiment outcome \\(Y^L\\) assumption (liars). design effects assumption directly visible.","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"diagnosis-2","chapter":"17 Experimental designs for descriptive inference","heading":"17.2.3 Diagnosis","text":"plot shows sampling distribution direct list experiment estimators. sampling distribution direct question tight biased; list experiment (requisite assumptions hold) unbiased, higher variance. choice two estimators prevalence rate depends – bias variance – important particular setting. See Blair, Coppock, Moor (2020) extended discussion choice research design depends deeply purpose project.","code":"\nsimulations_list <- simulate_design(design, sims = sims)"},{"path":"experimental-designs-for-descriptive-inference.html","id":"assumption-violations","chapter":"17 Experimental designs for descriptive inference","heading":"17.2.4 Assumption violations","text":"Recent work list experiments emphasizes possibility violations liars design effects assumptions. can diagnose properties design plausible violations .First, consider violations design effects assumption, means control item count differs depending whether subject assigned treatment control. Typically, means inclusion sensitive item changes responses control items, judged relative terms respondent became suspicious researcher’s intentions due taboo asking sensitive question.declare modified design defines two different potential control item counts depending whether respondent treatment group (\\(X_{\\rm treat}\\)) control group (\\(X_{\\rm control}\\)). potential outcomes list outcome also change: \\(X_{\\rm treat}\\) revealed treatment \\(X_{\\rm control}\\) control.Second, violation liars implies respondents respond truthfully even provided privacy protection list experiment. Two common circumstances researchers worry ceiling effects floor effects. ceiling effects, respondents respond maximum number control items rather truthful response plus one, avoid identified holding sensitive trait. floor effects problem reverse, respondents hide holding sensitive trait responding one truthful count treatment group.Ceiling floor effects ways liars assumption might violated. Respondents, noting sensitive item among list, might always respond zero (highest number) regardless control item count hide response. declaration made kinds violations, also changing potential outcomes \\(Y_{\\rm list}\\).","code":"\nmodel_design_effects <- \n  declare_population(\n    N = 100,\n    U = rnorm(N),\n    X_control = rbinom(N, size = 3, prob = 0.5),\n    X_treat = rbinom(N, size = 3, prob = 0.25),\n    Y_star = rbinom(N, size = 1, prob = 0.3),\n    S = case_when(Y_star == 0 ~ 0L,\n                  Y_star == 1 ~ rbinom(N, size = 1, prob = 0.2))\n  ) + \n  declare_potential_outcomes(\n    Y_list ~ (Y_star + X_treat) * Z + X_control * (1 - Z)\n  )\n\ndesign_design_effects <- \n  model_design_effects + inquiry + data_strategy + answer_strategy\ndiagnose_design_effects <- diagnose_design(design_design_effects, sims = sims)\nmodel_liars <- \n  declare_population(\n    N = 100,\n    U = rnorm(N),\n    X = rbinom(N, size = 3, prob = 0.5),\n    Y_star = rbinom(N, size = 1, prob = 0.3),\n    S = case_when(Y_star == 0 ~ 0L,\n                  Y_star == 1 ~ rbinom(N, size = 1, prob = 0.2))\n  ) + \n  declare_potential_outcomes(\n    Y_list ~ if_else(X == 3 & Y_star == 1 & Z == 1, 3, Y_star * Z + X)\n  )\n\ndesign_liars <- model_liars + inquiry + data_strategy + answer_strategy\ndiagnose_liars <- diagnose_design(design_liars, sims = sims)"},{"path":"experimental-designs-for-descriptive-inference.html","id":"redesign-4","chapter":"17 Experimental designs for descriptive inference","heading":"17.2.5 Redesign","text":"Researchers control three important design parameters affect inferential power list experiments: sample size, number control items, selection control items, proportion sample assigned treatment.Sample size. bias-variance tradeoff choice list direct questioning can diagnosed examining root mean-squared error (measure efficiency design) across two varying parameters: sample size amount sensitivity. declare new design varying \\(N\\) varying \\(\\mathrm{proportion_shy}\\), proportion Trump voters withhold truthful response asked directly:Diagnosing design, see low levels sensitivity low sample sizes, direct question preferred RMSE grounds. though direct question biased proportion Trump voters presence sensitivity bias (positive \\({\\rm proportion_shy}\\)), much efficient list experiment. large sample size, begin prefer list experiment low bias. high levels sensitivity, prefer list RMSE grounds despite inefficiency, bias large. Beyond list experiment, diagnosis illustrates comparing two possible designs need understand bias variance designs order select best one setting. designs, proportion shy feature model data answer strategy affect bias.upper left, see sensitivity bias always prefer direct question due inefficiency list experiment. red line always blue. However, get 0.1, sample sizes prefer direct question list: 3000 subjects. However, 3000 subjects RMSE list experiment better direct question. get 25 percent Trump supporters misreporting, always prefer list experiment terms RMSE. words, high levels sensitivity bias always willing tolerate efficiency loss get unbiased estimate region.many control items. sample size, early choice list researchers must make many control items select. also face tradeoff: control items, privacy protection respondent; items variance less efficient estimator proportion holding sensitive item. can quantify amount privacy protection provided average width confidence interval posterior prediction sensitive item given observed count. efficiency can quantified RMSE.control items. choice set control items ask can important number. three aims selection: reduce bias ceiling floor effects, provide sufficient cover respondents liars assumption met, increase efficiency estimates. first goal can met reducing number people whose latent control count one \\(J-1\\), one one lowest highest numbers possible treated group. Respondents band feel pressured subtract (add) responses hide () hold sensitive item. One solution add item high prevalance item low prevalence. Though address problem one, violate problem two: items obviously high low prevalence nothing add privacy protection. ideal control item count one low variance around middle range count. achieve providing sufficient cover, items inversely correlated can added.Proportion treated. design parameter often carefully considered experiments general list experiments particular proportion treated random assignment procedure. Often, researchers retain default 50-50 allocation two-arm trials. However, shown Section XX, variance outcome differs across treatment control, optimal allocation rule. Instead, units assigned group variance higher. list experiment, variance outcome typically higher treatment group simple reason number control items higher. variability items. address , researchers can set proportion treated higher 50%. explore tradeoff redesigning study varying probabilities assignment 20% 80% examining RMSE.","code":"\nmodel_sample_size <- \n  declare_population(\n    N = N,\n    U = rnorm(N),\n    X = rbinom(N, size = 3, prob = 0.5),\n    Y_star = rbinom(N, size = 1, prob = 0.3),\n    S = case_when(Y_star == 0 ~ 0L,\n                  Y_star == 1 ~ rbinom(N, size = 1, prob = proportion_shy))\n  ) +\n  declare_potential_outcomes(Y_list ~ Y_star * Z + X) \n\ndesign <- model_sample_size + inquiry + data_strategy + answer_strategy\n\ndesigns <- redesign(design, proportion_shy = seq(from = 0, to = 0.5, by = 0.05), N = seq(from = 500, to = 5000, by = 500))\ndiagnosis_tradeoff <- diagnose_design(designs, sims = sims, bootstrap_sims = b_sims)\n# make a plot\ndiagnosis_tradeoff %>%\n  get_diagnosands %>%\n  filter(proportion_shy < 0.35) %>% \n  mutate(estimator_lbl = factor(estimator_label, levels = c(\"direct\", \"list\"), labels = c(\"Direct question\", \"List experiment\"))) %>% \n  ggplot(aes(N, rmse, group = estimator_lbl, color = estimator_lbl)) +\n  # bias line\n  geom_line() + \n  scale_color_discrete(\"Question type\") + \n  labs(y = \"RMSE\") + \n  facet_wrap(~ proportion_shy, ncol = 4) + \n  dd_theme() + \n  theme(legend.position = \"bottom\")\nmodel_control_item_count <-\n  declare_population(\n    N = 100,\n    U = rnorm(N),\n    X = rbinom(N, size = J, prob = 0.5),\n    Y_star = rbinom(N, size = 1, prob = 0.3),\n    S = case_when(Y_star == 0 ~ 0L,\n                  Y_star == 1 ~ rbinom(N, size = 1, prob = 0.2))\n  ) + \n  declare_potential_outcomes(Y_list ~ Y_star * Z + X) \n\ndesign <- model_control_item_count + inquiry + data_strategy + answer_strategy\n\ndesigns <- redesign(design, J = 2:5)\ndiagnosis_control_item_count <- diagnose_design(designs, sims = sims)\nmodel_control_item_correlation <-\n  declare_population(\n    N = 100,\n    U = rnorm(N),\n    X_1 = draw_binary(0.5, N), \n    X_2 = correlate(given = X_1, rho = rho, draw_binary, prob = 0.5),\n    X_3 = draw_binary(0.5, N),\n    X = X_1 + X_2 + X_3,\n    Y_star = rbinom(N, size = 1, prob = 0.3),\n    S = case_when(Y_star == 0 ~ 0L,\n                  Y_star == 1 ~ rbinom(N, size = 1, prob = 0.2))\n  ) + \n  declare_potential_outcomes(Y_list ~ Y_star * Z + X) \n\ndesign <- model_control_item_correlation + inquiry + data_strategy + answer_strategy\n\ndesigns <- redesign(design, rho = seq(from = 0, to = 1, by = 0.25))\ndiagnose_control_item_correlation <- diagnose_design(designs, sims = sims)\ndata_strategy_prop_treated <- \n  declare_measurement(Y_direct = Y_star - S) +\n  declare_assignment(prob = prop_treated) + \n  declare_reveal(Y_list, Z)\n\ndesign <- model + inquiry + data_strategy_prop_treated + answer_strategy\n\ndesigns <- redesign(design, prop_treated = seq(from = 0.2, to = 0.8, by = 0.1))\ndiagnose_prop_treated <- diagnose_design(designs, sims = sims)"},{"path":"experimental-designs-for-descriptive-inference.html","id":"related-designs","chapter":"17 Experimental designs for descriptive inference","heading":"17.2.6 Related designs","text":"randomized responsecross-wise techniqueblock total responseThe list experiment one several experimental designs answering descriptive inquiries sensitive topics (review see XX). can target inquiry: proportion subjects hold sensitive trait. randomized response technique another design, many variants. “forced response” randomized response design (see (???)), respondents asked roll dice, depending dice result either answer honestly “forced” answer either “yes” “.” six-sided dice, respondents might asked answer “yes” roll 6, “” roll “1”, answer question truthfully roll number, two five. probability rolling 1 6 known, can back probability answering sensitive item observed data. Declaring design necessitates changes M (potential outcomes function dice roll); D (random assignment dice roll ); (estimator function observed outcomes known probability forced response). declare one :","code":"\nlibrary(rr)\n\nmodel_rr <- \n  declare_population(\n    N = 100,\n    U = rnorm(N),\n    X = rbinom(N, size = 3, prob = 0.5),\n    Y_star = rbinom(N, size = 1, prob = 0.3),\n    S = case_when(Y_star == 0 ~ 0L,\n                  Y_star == 1 ~ rbinom(N, size = 1, prob = 0.2))\n  ) + \n  declare_potential_outcomes(\n    Y_rr ~ \n      case_when(\n        dice == 1 ~ 0L,\n        dice %in% 2:5 ~ Y_star,\n        dice == 6 ~ 1L\n      ),\n    conditions = 1:6, assignment_variable = \"dice\")\n\ndata_strategy_rr <- \n  declare_measurement(Y_direct = Y_star - S) +\n  declare_assignment(prob_each = rep(1/6, 6), conditions = 1:6,\n                     assignment_variable = \"dice\") + \n  declare_reveal(Y_rr, dice)\n\nanswer_strategy_rr <-\n  declare_estimator(Y_direct ~ 1,\n                    model = lm_robust,\n                    estimand = \"proportion\",\n                    label = \"direct\") +\n  declare_estimator(Y_rr ~ 1, handler = rr_forced_known, \n                    label = \"forced_known\", estimand = \"proportion\")\n\n\ndesign <- model_rr + inquiry + data_strategy_rr + answer_strategy_rr"},{"path":"experimental-designs-for-descriptive-inference.html","id":"further-readings","chapter":"17 Experimental designs for descriptive inference","heading":"17.2.7 Further readings","text":"Miller (1984)Imai (2011)Blair, Coppock, Moor (2020)","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"conjoint-experiments","chapter":"17 Experimental designs for descriptive inference","heading":"17.3 Conjoint experiments","text":"Conjoint survey experiments become hugely popular political science beyond studying multidimensional choice (Hainmueller, Hopkins, Yamamoto 2014). common “forced-choice” design variant, subjects presented choice task: pair profiles (candidates, immigrants, policies) asked make binary choice .’re designing conjoint, make (least) three choices:number attributesThe number levels within attributeThe number choice tasks subjects asked rate.right number attributes governed “masking/satisficing” tradeoff (Bansak et al. 2019). don’t include important attribute (like partisanship candidate choice experiment), ’re worried subjects partially infer partisanship attributes (like race gender). , partisanship “masked”, estimates effects race gender biased “omitted variable.” add many attributes order avoid masking, may induce “satisficing” among subjects, whereby take little bit information, enough make “good enough” choice among candidates.right number levels governed sample size. attribute three levels, ’s like ’re conducting three-arm trial, ’ll want enough subjects arm. levels, lower power.right number choice tasks depends survey budget. can always add pairs profiles cost opportunity cost asking different question survey may serve higher scientific purpose. ’re worried respondents get bored task, can always throw profile pairs come later survey. Bansak et al. (2019) suggest can ask many pairs without much loss data quality.","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"declaration-18","chapter":"17 Experimental designs for descriptive inference","heading":"17.3.1 Declaration","text":"","code":"\n# applies the function to each pair\nY_function <- function(data) {\n  data %>%\n    group_by(pair) %>%\n    mutate(Y = if_else(E == max(E), 1, 0)) %>%\n    ungroup\n}\ndesign <- \n  declare_population(\n    subject = add_level(N = 500),\n    pair = add_level(N = 4),\n    candidate = add_level(N = 2, U = runif(N))\n  ) +\n  declare_assignment(assignment_variable = \"A1\") +\n  declare_assignment(assignment_variable = \"A2\", \n                     conditions = c(\"young\", \"middle\", \"old\")) +\n  declare_assignment(assignment_variable = \"A3\")  +\n  declare_step(\n    E = \n      0.05 * A1 + \n      0.04 * (A2 == \"middle\") + \n      0.08 * (A2 == \"old\") + \n      0.02 * A3 + U,\n    handler = fabricate) +\n  declare_measurement(handler = Y_function) +\n  declare_estimator(Y ~ A1 + A2 + A3,\n                    model = lm_robust, term = TRUE)"},{"path":"experimental-designs-for-descriptive-inference.html","id":"dag-17","chapter":"17 Experimental designs for descriptive inference","heading":"17.3.2 DAG","text":"","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"example-12","chapter":"17 Experimental designs for descriptive inference","heading":"17.3.3 Example","text":"","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"further-reading-11","chapter":"17 Experimental designs for descriptive inference","heading":"17.3.4 Further reading","text":"Hainmueller, Hopkins, Yamamoto (2014)Bansak et al. (2019)","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"behavioral-games","chapter":"17 Experimental designs for descriptive inference","heading":"17.4 Behavioral games","text":"behavioral game informative demonstrate ?","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"declaration-19","chapter":"17 Experimental designs for descriptive inference","heading":"17.4.1 Declaration","text":"","code":"\ndesign <-\n  declare_population(\n    games = add_level(N = 100),\n    players = add_level(\n      N = 2,\n      prosociality = runif(N),\n      fairness = prosociality,\n      cutoff = pmax(prosociality - 0.25, 0)\n    )\n  ) +\n  declare_estimand(mean_fairness = mean(fairness),\n                   mean_cutoff = mean(cutoff)) +\n  declare_assignment(\n    blocks = games, \n    conditions = c(\"proposer\", \"responder\"),\n    assignment_variable = \"role\"\n  ) + \n  declare_step(\n    id_cols = games, \n    names_from = role, \n    values_from = c(prosociality, fairness, cutoff), \n    handler = pivot_wider\n  ) + \n  declare_measurement(\n    proposal = fairness_proposer * 0.5, \n    response = if_else(proposal >= cutoff_responder, 1, 0)\n  ) + \n  declare_estimator(proposal ~ 1,\n                    model = lm_robust,\n                    estimand = \"mean_fairness\",\n                    label = \"mean_fairness\") +\n  declare_estimator(response ~ 1,\n                    model = lm_robust,\n                    estimand = \"mean_cutoff\",\n                    label = \"mean_cutoff\")"},{"path":"experimental-designs-for-descriptive-inference.html","id":"dag-18","chapter":"17 Experimental designs for descriptive inference","heading":"17.4.2 DAG","text":"","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"example-13","chapter":"17 Experimental designs for descriptive inference","heading":"17.4.3 Example","text":"","code":""},{"path":"complex-designs-1.html","id":"complex-designs-1","chapter":"18 Complex designs","heading":"18 Complex designs","text":"designs presented thus far, aim generally learn level variable particular causal effect. cases, single set observations collected answer strategy applied directly data answer pre-defined question.many studies form. studies can also “complex” multiple ways. instance although assumed researchers start well defined estimands, studies focus first figuring question ask proceed ask answer . studies seek learn levels effects search explicitly model phenomenon, asking instance “causes \\(Y\\)?” “model, class, best accounts observed data”? studies complex inquiries. studies complex data answer strategies, instance, gathering together findings multiple sub-studies order arrive overall conclusion.address declaration diagnosis complex designs section.","code":""},{"path":"complex-designs-1.html","id":"discovery","chapter":"18 Complex designs","heading":"18.1 Discovery","text":"Imagine study researcher interested just figuring effects intervention also accounts variation effects. candidate estimands decision whether promote candidates study estimands depends data patterns found along way. kind problem akin common problem selecting predictors selecting controls.simplicity define design form just single candidate. design lets us assess risks benefits different answer strategies given endogenous choice estimand.","code":""},{"path":"complex-designs-1.html","id":"design-declaration","chapter":"18 Complex designs","heading":"18.1.1 Design Declaration","text":"Model: population consists two groups (men women, instance). conditional average treatment effect possibly larger one group another. group candidate variable forming heterogeneous effects estimand.Model: population consists two groups (men women, instance). conditional average treatment effect possibly larger one group another. group candidate variable forming heterogeneous effects estimand.Inquiry: main purpose experiment estimate overall Average Treatment Effect. , though, consider secondary, heterogeneous effects analysis many researchers conduct examining ATE, seek assess whether effects larger one group another. potentially becomes second question interest, depending discovery process.Inquiry: main purpose experiment estimate overall Average Treatment Effect. , though, consider secondary, heterogeneous effects analysis many researchers conduct examining ATE, seek assess whether effects larger one group another. potentially becomes second question interest, depending discovery process.Data strategy: allocate treatment using complete random assignment.Data strategy: allocate treatment using complete random assignment.Answer strategy: Using random half data (test set), test interaction treatment group membership. find significant interaction \\(p \\leq 0.05\\) level, declare interaction new estimand estimate size interaction test data set.Answer strategy: Using random half data (test set), test interaction treatment group membership. find significant interaction \\(p \\leq 0.05\\) level, declare interaction new estimand estimate size interaction test data set.also include, comparison, “naive” analysis reports interaction estimated using full data whenever interaction significant.","code":""},{"path":"complex-designs-1.html","id":"declaration-20","chapter":"18 Complex designs","heading":"18.1.2 Declaration","text":"declaration makes use two conditional estimators.now declare design:include diagnosands take account fact runs designs produce estimates new estimand.","code":"\n# An estimator that estimates on testing data only if \n# estimates on training data were significant\n\nnew_estimator <- function(data){\n    with(data, data.frame(\n      estimate = ifelse(train_p[1] <= .05, \n                        coef(lm(Y ~ Z*X, subset = !train))[4], \n                        NA),\n      p.value = ifelse(train_p[1] <= .05, \n                       coef(summary(lm(Y ~ Z * X, subset = !train)))[4,4],\n                       NA),\n      term = \"Z:X\",\n      stringsAsFactors = FALSE))}\n\n# An estimator that estimates on all data only if estimates are significant\n\ncomparison_estimator <- function(data){\n  with(data, data.frame(\n    estimate = ifelse(all_p[1] < .05, coef(lm(Y ~ Z*X))[4], NA),\n    p.value = ifelse(all_p[1] < .05, all_p[1], NA),\n    term = \"Z:X\",\n    stringsAsFactors = FALSE))}\n# The design\n\ndiscovery <- \n  \n  declare_population(\n    N = 200, \n    X = sample(1:N %% 2)==1,\n    het_effect = sample(c(0,.5),1,TRUE),\n    train = sample(1:N %% 2)==1,\n    u = rnorm(N)) +\n  \n  declare_potential_outcomes(Y ~ Z + het_effect * Z * X + u) + \n    declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +\n    declare_assignment() +\n    reveal_outcomes() +\n    \n  # Main analysis\n  declare_estimator(Y ~ Z, estimand = \"ATE\", label = \"Main\") +\n  \n  # Exploration\n  declare_step(\n      train_p = (lm_robust(Y ~ Z * X, subset = train) %>% tidy())[4,4],\n      all_p   = (lm_robust(Y ~ Z * X) %>% tidy())[4,4],\n      handler = fabricate) +\n    \n  declare_estimand(\n    het = mean(Y_Z_1[X] - Y_Z_0[X]) - mean(Y_Z_1[!X] - Y_Z_0[!X])) +\n  \n  # Handler estimates only if p low in training group  \n  declare_estimator(\n    handler = tidy_estimator(new_estimator), \n    estimand = \"het\",\n    label = \"Discovery\") +\n    \n  declare_estimator(\n    handler = tidy_estimator(comparison_estimator), \n    estimand = \"het\",\n    label = \"Comparison\")\ndiscovery <- set_diagnosands(discovery, declare_diagnosands(\n  bias = mean((estimate - estimand), na.rm = TRUE),\n  RMSE = sqrt(mean((estimate - estimand)^2, na.rm = TRUE)),\n  frequency = mean(!is.na(estimate)),\n  false_pos = mean(p.value[estimand == 0] < 0.05, na.rm = TRUE),\n  false_neg = 1 - mean(p.value[estimand != 0] < 0.05, na.rm = TRUE)))"},{"path":"complex-designs-1.html","id":"diagnosis-3","chapter":"18 Complex designs","heading":"18.1.3 Diagnosis","text":"\nTable 18.1: Complete random sampling design diagnosis\nsee principled discovery method, using training testing data, provides essentially unbiased estimates heterogeneous effect, estimated. comparison method tends provide biased estimates, conditioning statistical significance tends exaggerate effect sizes (Gelman Carlin (2014)).consequences two approaches false discovery rates stark. true effect 0, probability falsely rejecting null 0 (conditional reporting) 1 comparison method: definition, significant estimates kept. Similarly, since estimates generated procedure statistically significant, false negative rate (conditional reporting) 0: estimand declared analysis conducted estimates guaranteed significant. principled discovery method exhibits conventional rates falsely rejecting true null (0.05) though fails reject null quite often, due weak power.Strikingly see example protection bias principled discovery strategy declared necessarily translate improved inferences average (measured MSE), bias-variance tradeoff inherent approach. Less data used final test adopting principled discovery approach, average estimates much noisier unprincipled comparison.Moreover principled strategy somewhat less likely produce result since less likely result discovered subset data entire data set.Reanalysis design can used assess optimal division units training testing data might given different hypothesized effect sizes.","code":""},{"path":"complex-designs-1.html","id":"structural-estimation","chapter":"18 Complex designs","heading":"18.2 Structural estimation","text":"Structural estimation used situations researchers general model mind processes work goal fit parameters model. fitted model might estimate levels unobserved variables, treatment effects, quantities. might even extrapolate estimate counterfactual quantities, effects interventions implemented (Reiss Wolak 2007).illustrate using example model bargaining pairs players, drawing example use Wilke Humphreys (2020).Wilke Humphreys (2020) imagine bargaining game payments customer \\(\\) taxi driver given :\\[\\pi_i =  \\theta_i(z_iy + (1-z_i)(1-y)) + (1-\\theta_i)\\chi\\]\\(y = \\sum_{j = 2}^n(-1^{j})\\delta^{j-1}\\) equilibrium offer made first mover predicted Rubinstein (1982) alternating offers bargaining model \\(n\\) possible bargaining rounds given discount factor \\(\\delta\\). customer’s payoff depends whether goes first (\\(z_i = 1\\)) second (\\(z_i = 0\\)). Non-rational customers (\\(\\theta_i = 0\\)) engage bargaining successfully invoke norm, insisting giving driver share \\(\\chi\\) endowment, irrespective whether go first second. let \\(q\\) denote probability \\(\\theta = 1\\).allow disturbance assume measured payments draw Beta distribution expectation given expected payment variance parameterized \\(\\kappa\\).imagine \\(Z\\) randomly assigned access data payments, \\(\\pi\\). also assume know price \\(\\chi\\). goal however simply measure effect \\(Z\\) \\(\\pi\\) estimate model parameters, \\(q\\), \\(\\delta\\), \\(\\kappa\\), can used estimate effect counterfactual quantities (assume model true).","code":""},{"path":"complex-designs-1.html","id":"design","chapter":"18 Complex designs","heading":"18.2.1 Design","text":"Model. declaration assume data indeed produced process similar assumed estimation stage.Model. declaration assume data indeed produced process similar assumed estimation stage.Inquiries. inquiries include parameters k, d, q corresponding \\(\\kappa\\), \\(\\delta\\) \\(q\\) model (treat \\(\\chi\\) known). addition interested effect \\(Z\\) payments two period game game indefinite duration.Inquiries. inquiries include parameters k, d, q corresponding \\(\\kappa\\), \\(\\delta\\) \\(q\\) model (treat \\(\\chi\\) known). addition interested effect \\(Z\\) payments two period game game indefinite duration.Data. First mover position, \\(Z\\), randomly assigned. Payments measured error (model however distinguish measurement error decision making error). imagine access data games \\(n=2\\) games \\(n=\\infty\\) order compare performance estimators conditions.Data. First mover position, \\(Z\\), randomly assigned. Payments measured error (model however distinguish measurement error decision making error). imagine access data games \\(n=2\\) games \\(n=\\infty\\) order compare performance estimators conditions.Analysis implemented using maximum likelihood identify parameter values consistent data (collection parameter values produce observed data greatest likelihood). declaration model employed employed M. report analysis results differences means structural estimation generated using \\(n=2\\) data (DIM_two, Struc_two) using \\(n=\\infty\\) data (DIM_inf, Struc_inf)Analysis implemented using maximum likelihood identify parameter values consistent data (collection parameter values produce observed data greatest likelihood). declaration model employed employed M. report analysis results differences means structural estimation generated using \\(n=2\\) data (DIM_two, Struc_two) using \\(n=\\infty\\) data (DIM_inf, Struc_inf)complex part design specification estimator, shown next:design makes use estimator estimate parameter values well treatment effects. accompanied simpler difference means estimator treatment effects.attraction structural estimation , fitted model, one can generate estimates effects treatments implemented. case \nparameters describe equilibrium outcomes two round game sufficient describe outcomes infinitely repeated game. understand effects treatment one case understand . least model correct. design generate estimates effects unimplemented treatments.\nTable 18.2: Complete random sampling design diagnosis\nsee good job recovering parameter values also recover treatment effects. using two period data estimate ATE_two good estimated using structural design based approaches. case data \\(n=\\infty\\) games however estimate structural model less precise, though unbiased. contrast estimate using design based methods estimand. Finally, \\(\\delta\\), see, better estimated using data 2 period games; estimate \\(\\kappa\\) biased though bias small.","code":"\nstructural_estimator <- function(data, pi, y, chi = 3/4){\n    \n    # Define negative log likelihood as a function of k, d and q\n    LL  <- function(k, d, q) {\n      m <- with(data, y(Z, d))\n      R <- q * dbeta(data[pi][[1]], k * chi, k * (1- chi)) +\n        (1 - q) * dbeta(data[pi][[1]], k * m, k * (1 - m))\n      - sum(log(R))\n    }\n\n  # Estimation\n  M <- mle2(\n    LL,\n    method = \"L-BFGS-B\",\n    start = list(k = 2, d = 0.50,  q = 0.50),\n    lower = c(k = 1,    d = 0.01,  q = 0.01),\n    upper = c(k = 1000, d = 0.99,  q = 0.99)\n  )\n  \n  # Format output from estimation\n  out <- data.frame(coef(summary(M)), outcome = pi)\n  \n  names(out) <- c(\"estimate\", \"std.error\", \"statistic\", \"p.value\", \"outcome\")\n  \n  # Use estimates of q and delta to predict average treatment effects (ATEs)\n  # Predicted ATE for n=2\n  out[4, 1] <- (1 - out[\"q\", \"estimate\"]) * (2 * out[\"d\", \"estimate\"] - 1)\n  \n  # Predicted ATE for n=infinity\n  out[5, 1] <- (1 - out[\"q\", \"estimate\"]) * (2 * out[\"d\", \"estimate\"] /\n                                               (1 + out[\"d\", \"estimate\"]) - 1)\n  \n  out\n}\n# Declare the design ----------------------------------------------------------\n\n# Define parameter values:\nd = 0.8       # True delta (unknown)\nk = 6         # Parameter to governance variance (unknown)\nq = 0.5       # Share of behavioral types in the population (unknown)\nchi = 0.75    # Price paid by norm following (\"behavioral\" customers) (known)\n\n# Design declaration:\n\ndesign <- \n  \n  # Define the population: indicator for behavioral type (norm = 1)\n  declare_population(N = 500, norm = rbinom(N, 1, q)) +\n  \n  # Define mean potential outcomes for n = 2 \n  declare_potential_outcomes(\n    pi_two ~ norm*chi + (1-norm)*(Z*d + (1-Z)*(1-d))) +\n  \n  # Define mean potential outcomes for n = infinity\n  declare_potential_outcomes(\n    pi_inf ~ norm*chi + (1-norm)*(Z*d/(1+d) + (1-Z)*(1-d/(1+d)))) +\n  \n  # Define estimands (quantities we want to estimate)\n  declare_estimand(ATE_two = mean(pi_two_Z_1 - pi_two_Z_0), # ATE n = 2\n                   ATE_inf = mean(pi_inf_Z_1 - pi_inf_Z_0), # ATE n = infinity\n                   k = k,                                   # kappa\n                   d = d,                                   # delta\n                   q = q) +                                 # q\n\n  # Declare assignment process \n  declare_assignment() +\n  \n  # Declare revealed potential outcomes\n  reveal_outcomes(pi_two, Z) + reveal_outcomes(pi_inf, Z) +\n  \n  # Get draws from beta distribution given means for n = 2 and n = infinity\n  declare_measurement(\n    pi_two_obs = rbeta(N, pi_two*k, (1-pi_two)*k),      \n    pi_inf_obs = rbeta(N, pi_inf*k, (1-pi_inf)*k)) +\n  \n  # Declare estimators\n  # Difference-in-means for n = 2\n  declare_estimator(pi_two_obs ~ Z, estimand = \"ATE_two\", label = \"DIM_two\") +\n  \n  # Difference-in-means for n = infinity\n  declare_estimator(pi_inf_obs ~ Z, estimand = \"ATE_inf\", label = \"DIM_inf\") +\n  \n  # MLE for n = 2\n  declare_estimator(handler = tidy_estimator(structural_estimator), \n                    pi = \"pi_two_obs\", \n                    y = function(Z, d) Z * d + (1 - Z) * (1 - d), \n                    estimand = c(\"k\",\"d\", \"q\", \"ATE_two\", \"ATE_inf\"), \n                    label = \"Struc_two\") +\n  \n  # MLE for n = infinity\n  declare_estimator(handler = tidy_estimator(structural_estimator),\n                    pi = \"pi_inf_obs\", \n                    y = function(Z, d) Z*d/(1+d) +  (1-Z)*(1-d/(1+d)),\n                    estimand = c(\"k\",\"d\",\"q\",\"ATE_two\", \"ATE_inf\"), \n                    label = \"Struc_inf\") "},{"path":"complex-designs-1.html","id":"exercises-8","chapter":"18 Complex designs","heading":"18.2.2 Exercises","text":"Declare diagnose version design benchmark model different analysis model. define parameter estimands case?","code":""},{"path":"complex-designs-1.html","id":"multi-site-trials","chapter":"18 Complex designs","heading":"18.3 Multi-site trials","text":"starting point fixed budget considering two possible designs: (1) single large study one context (2) set five studies five different contexts intervention outcome measures.heterogeneous effects, can get good predictions sample even average effects differ substantially (better multiple sites sites population different proportions subject types correlated heterogeneous effects).Two notable features design:\n- must heterogeneous effects work (otherwise estimates get biased toward zero due overfitting variables).\n- information covariate population sample (used proportion people heterogeneous type).","code":"\nsingle_site_design <- \n  declare_population(\n    site = add_level(\n      N = 10, \n      feasible_site = sample(c(rep(1, 8), rep(0, 2)), N, replace = FALSE),\n      study_effect = seq(from = -0.1, to = 0.1, length.out = N) \n    ),\n    subjects = add_level(N = n_subjects_per_site, noise = rnorm(N))\n  ) + \n  declare_potential_outcomes(Y ~ Z * (0.1 + study_effect + 0.025 * feasible_site) + noise) +\n  declare_estimand(PATE = mean(Y_Z_1 - Y_Z_0)) + \n  declare_estimand(PATE_feasible = mean(Y_Z_1 - Y_Z_0), subset = feasible_site == TRUE) + \n  declare_sampling(clusters = site, strata = feasible_site, strata_n = c(0, 1)) + \n  declare_sampling(strata = site, n = n_subjects_per_site) + \n  declare_assignment(blocks = site, prob = 0.5) + \n  declare_estimator(Y ~ Z, model = lm_robust)\n\nmulti_site_design <- \n  declare_population(\n    site = add_level(\n      N = 10, \n      feasible_site = sample(c(rep(1, 8), rep(0, 2)), N, replace = FALSE),\n      study_effect = seq(from = -0.1, to = 0.1, length.out = N) \n    ),\n    subjects = add_level(N = n_subjects_per_site, noise = rnorm(N))\n  ) + \n  declare_potential_outcomes(Y ~ Z * (0.1 + study_effect + 0.025 * feasible_site) + noise) +\n  declare_estimand(PATE = mean(Y_Z_1 - Y_Z_0)) + \n  declare_estimand(PATE_feasible = mean(Y_Z_1 - Y_Z_0), subset = feasible_site == TRUE) + \n  declare_sampling(clusters = site, strata = feasible_site, strata_n = c(0, n_study_sites)) + \n  declare_sampling(strata = site, n = n_subjects_per_site) + \n  declare_assignment(blocks = site, prob = 0.5) + \n  declare_estimator(handler = label_estimator(random_effects_meta_analysis), label = \"random-effects\")\n\nsingle_site_large_design <- redesign(single_site_design, n_subjects_per_site = 2500)\n\nsmall_study_five_sites <- redesign(multi_site_design, n_study_sites = 5, n_subjects_per_site = 500)\nsimulations_small_large <- simulate_design(single_site_large_design, small_study_five_sites, sims = sims)\ndiagnosis_small_large <- diagnose_design(simulations_small_large, bootstrap_sims = b_sims)"},{"path":"complex-designs-1.html","id":"bayesian-estimation-can-improve-estimates-of-effects-for-sampled-sites","chapter":"18 Complex designs","heading":"18.3.1 Bayesian estimation can improve estimates of effects for sampled sites","text":": can improve site-level effect estimates analyzing simple Bayesian model shrinkage property, even Bayesian model wrong distribution effects population.","code":""},{"path":"complex-designs-1.html","id":"three-study-papers","chapter":"18 Complex designs","heading":"18.4 Three-study papers","text":"many research projects, seek evaluate multiple observable implications single theory. cases, single piece evidence constitute sufficient evidence validate theory whole. Rather, believe theory multiple pieces evidence support .\nConventions around constitutes convincing pattern evidence vary. researchers believe theory unless piece evidence support statistically significant.\n\n\nLess stringent approaches simply seek evidence “consistent” theory, observation effects signed predicted direction., declare \\(N\\)-study design, examine consequences two different approaches evaluating theory light multiple studies. show , multiple observable implications generated process, conditioning significance can lead one strongly astray. Generally speaking, looking sign effects probative much less prone false negatives. small numbers studies, however, risks false positives high.","code":""},{"path":"complex-designs-1.html","id":"design-declaration-1","chapter":"18 Complex designs","heading":"18.4.1 Design Declaration","text":"Model: declare \\(N\\) populations, governed \ndata-generating process: X exogenous standard normally-distributed,\nM standard-normally distributed correlated X rho, Y function main effect X well interaction X M, size sign direct interactive effects determined tau gamma, respectively.Model: declare \\(N\\) populations, governed \ndata-generating process: X exogenous standard normally-distributed,\nM standard-normally distributed correlated X rho, Y function main effect X well interaction X M, size sign direct interactive effects determined tau gamma, respectively.Inquiry: want know, global sense, theory “right.”\n, means effect X Y positive increasing M, X affects M.\ntau positive, theory correct. zero negative, theory incorrect.Inquiry: want know, global sense, theory “right.”\n, means effect X Y positive increasing M, X affects M.\ntau positive, theory correct. zero negative, theory incorrect.Data strategy: conduct collect independent datasets \\(N\\) datasets size n. example , conduct three studies, assuming observe X Y first, M X second, Y, M, X, third.Data strategy: conduct collect independent datasets \\(N\\) datasets size n. example , conduct three studies, assuming observe X Y first, M X second, Y, M, X, third.Answer strategy: Using linear regression, estimate bivariate correlation X Y study 1, bivariate correlation X M study 2, interaction X M Y study 3.Answer strategy: Using linear regression, estimate bivariate correlation X Y study 1, bivariate correlation X M study 2, interaction X M Y study 3.","code":"\nn1 <- 100\nn2 <- 100\nn3 <- 100\nrho <- .5\ngamma <- tau <- .2\n\ngenerate_study_sample <- function(n, rho, tau, gamma, data){\n  fabricate(N = n, X = rnorm(N), M = rnorm(N, X * rho, sqrt(1 - rho^2)), \n            U = rnorm(N), Y = tau * X + gamma * M * X + U)\n}\n\nthree_study_design <-\n  # Study 1 -- Bivariate correlation between X and Y\n  declare_population(\n    n = n1,\n    tau = tau,\n    gamma = gamma,\n    rho = rho,\n    handler = generate_study_sample\n  ) +\n  declare_estimator(Y ~ X,\n                    term = \"X\",\n                    model = lm_robust,\n                    label = \"Study 1\") +\n  # Study 2 -- Bivariate correlation between M and X\n  declare_population(\n    n = n2,\n    tau = tau,\n    gamma = gamma,\n    rho = rho,\n    handler = generate_study_sample\n  ) +\n  declare_estimator(M ~ X,\n                    term = \"X\",\n                    model = lm_robust,\n                    label = \"Study 2\") +\n  # Study 3 -- Interaction in X and M\n  declare_population(\n    n = n3,\n    tau = tau,\n    gamma = gamma,\n    rho = rho,\n    handler = generate_study_sample\n  ) +\n  declare_estimator(Y ~ X + M + X:M,\n                    term = \"X:M\",\n                    model = lm_robust,\n                    label = \"Study 3\") "},{"path":"complex-designs-1.html","id":"dag-19","chapter":"18 Complex designs","heading":"18.4.2 DAG","text":"","code":""},{"path":"complex-designs-1.html","id":"takeaways-1","chapter":"18 Complex designs","heading":"18.4.3 Takeaways","text":"Let us compare performance “significant” versus “signed”\napproaches theory confirmation theory “correct” (tau gamma positive),\nversus “incorrect” (parameters zero).\nfirst approach, theory deemed “supported” evidence effects\nsignificant. second, theory supported evidence signs effects positive.first three rows table, theory correct tau, gamma, rho positive; second three rows, incorrect main effect interaction zero. “power” column tells us proportion simulations effect significant \\(\\alpha = .05\\) level, “significant” column tells us proportion simulations studies significant effects, “positive” column tells us often study found positively signed result, “positive column” tells us proportion simulations studies positively signed result.Notice , studies independent, probability \nsignificant equal product power:\nPr(studies significant) = Pr(Study 1 significant) \\(\\times\\)…\\(\\times\\) Pr(Study \\(N\\) significant). Thus, believe theory studies conducted test yield significant results, studies powered conventionally accepted level 80%, erroneously reject theory probability \\(.8^N\\). three, conventionally well-powered, randomized studies shooting right quantity interest, almost half cases right, think\nwrong.Notice , studies independent, probability \nsignificant equal product power:\nPr(studies significant) = Pr(Study 1 significant) \\(\\times\\)…\\(\\times\\) Pr(Study \\(N\\) significant). Thus, believe theory studies conducted test yield significant results, studies powered conventionally accepted level 80%, erroneously reject theory probability \\(.8^N\\). three, conventionally well-powered, randomized studies shooting right quantity interest, almost half cases right, think\nwrong.Furthermore, notice detrimental addition small study can metric, even gets important mechanism. soon condition inference theory correct significance observable implications, low-powered test can sharply increase risk false rejection.Furthermore, notice detrimental addition small study can metric, even gets important mechanism. soon condition inference theory correct significance observable implications, low-powered test can sharply increase risk false rejection.can say risk false positives? power individual studies : stated error rate \\(\\alpha = 0.05\\), studies every slightly anti-conservative. However, “significant” desideratum creates rejection rate high.can say risk false positives? power individual studies : stated error rate \\(\\alpha = 0.05\\), studies every slightly anti-conservative. However, “significant” desideratum creates rejection rate high.problems, though , alleviated disregard significance just look signs. theory right, good chance effects estimate positive: surmise theory correct roughly 94% time actually .problems, though , alleviated disregard significance just look signs. theory right, good chance effects estimate positive: surmise theory correct roughly 94% time actually .theory correct sense true effects zero, random error means positive half time negative half. Consequently, probability erroneously accepting theory based sign effects true underlying effects zero equal \\(0.5^N\\). , means erroneously infer right relatively high rate 12% simulations.theory correct sense true effects zero, random error means positive half time negative half. Consequently, probability erroneously accepting theory based sign effects true underlying effects zero equal \\(0.5^N\\). , means erroneously infer right relatively high rate 12% simulations.design , rates rejected accepted theories seemed\ndepend number studies considered. graph , look\ntwenty-nine different \\(N\\)-study designs, seek confirm theory\nreplicating evidence \\(N\\) times. first design made two studies,\nindependently evaluating hypothesis \\(Y\\) positively correlated \\(X\\).\n, consider conditioning inference theory whether\nresults significant results positive affects error rates., see significance probative way look observable\ntheoretical implications. soon four studies,\nvirtually chance confirming even true theory metric.\nsee sort reverse multiple-comparisons problem: increasing number\ntests makes false rejections increasingly likely interested\njoint probability tests saying thing.\nUnless number studies small, whether produce significant results\nessentially yields information whether theory correct., see significance probative way look observable\ntheoretical implications. soon four studies,\nvirtually chance confirming even true theory metric.\nsee sort reverse multiple-comparisons problem: increasing number\ntests makes false rejections increasingly likely interested\njoint probability tests saying thing.\nUnless number studies small, whether produce significant results\nessentially yields information whether theory correct.contrast, looking signs can highly probative. \napplication, optimal number studies eight. point,\nvirtually chance erroneously inferring theory \ncorrect effects zero, theory correct \ngood chance (almost 75%) available evidence signed\naccordingly. number studies increases, probability\ndiscordant results, using unanimity signs judge whether\ntheory correct becomes increasingly unwise.contrast, looking signs can highly probative. \napplication, optimal number studies eight. point,\nvirtually chance erroneously inferring theory \ncorrect effects zero, theory correct \ngood chance (almost 75%) available evidence signed\naccordingly. number studies increases, probability\ndiscordant results, using unanimity signs judge whether\ntheory correct becomes increasingly unwise.","code":"\n# Simulate design\nsimulations <- simulate_design(three_study_design)\n\n# Simulate null design\nnull_three_study_design <- \n  redesign(three_study_design, tau = 0, gamma = 0, rho = 0)\nnull_simulations <- simulate_design(null_three_study_design)"},{"path":"part-iii-exercises.html","id":"part-iii-exercises","chapter":"19 Part III Exercises","heading":"19 Part III Exercises","text":"written.variance list experiment given expression, \\(\\mathbb{V}(Y_i(0)\\) variance control item count \\(\\mathrm{cov}(Y_i(0), D_i^*)\\) covariance control item count sensitive trait.\\[\n\\frac{1}{N-1} \\bigg\\{ \\pi^*(1-\\pi^*) + 4 \\mathbb{V}(Y_i(0))  + 4 \\mathrm{cov}(Y_i(0), D_i^*) \\bigg\\}\n\\]goal compare direct question list experiment designs respect RMSE diagnosand. Recall RMSE equals square root variance plus bias squared: \\(\\mathrm{RMSE} = \\sqrt{\\mathrm{Variance} + \\mathrm{Bias}^2}\\). Assume following design parameters: \\(\\delta = 0.10\\), \\(\\pi^* = 0.50\\), \\(\\mathbb{V}(Y_i(0) = 0.075\\), \\(\\mathrm{cov}(Y_i(0), D_i^*) = 0.025\\).RMSE direct question \\(N\\) = 100?RMSE list experiment \\(N\\) = 100?Make figure \\(N\\) horizontal axis RMSE vertical axis. Plot RMSE designs range sample sizes 100 2000. Hint: ’ll need write function design takes \\(N\\) input returns RMSE. can get started filling starter function: direct_rmse <- function(N){ # write_your_function_here}large sample size need list experiment preferred direct question RMSE grounds?Comment answer (d) change \\(\\delta\\) equal 0.2? implications choice list experiments direct questions?","code":""},{"path":"research-design-lifecycle.html","id":"research-design-lifecycle","chapter":"20 Research Design Lifecycle","heading":"20 Research Design Lifecycle","text":"Empirical results tell us believe conducting study. Whether believe results depends quality research design. highlighted role diagnosis justifying design broader framework describe implications many stages research design lifecycle, design development, implementation, writing publishing piece, beyond, integration acquired knowledge collective scientific understanding world. stage process, research design – specification M, , D, – shapes choices well others learn work.part book works discrete stages lifecycle. presented linear fashion, stages intertwined common connection MIDA. example, show many disputes among scholars proper interpretation come differing understandings part M, , D, . preanalysis plan sufficiently precise beliefs features design, disputes can specified precisely, better resolve . also explore reason disputes preanalysis plan.every research project explicitly feature stages. example, prospective research designs like experiments surveys often included pilot studies learn important unknown features M implementing full studies. Retrospective studies, like textual analyses speeches delivered parliament, might .","code":""},{"path":"before.html","id":"before","chapter":"21 Before","heading":"21 Before","text":"","code":""},{"path":"before.html","id":"idea","chapter":"21 Before","heading":"21.1 Idea","text":"two ways start:\n– start theory identify inquiries rule alternatives\n– start identified result built theoryBrainstorming sessions can useful take kernel idea identify set possible research designs building . candidate designs can diagnosed assessed feasibility. participants brainstorming session need know idea can effectively help identify possible designs? suggest using following “problem statement”, core task place kernel idea MIDA.27 need parts MIDA — ’s brainstorming session! systematizing know helpful.’s usually helpful just answer strategyif specify M, , D, usually hard help. common situation context topic interest. fine! may enough yield productive brainstorming session designs, participants know part interested . can imagine many theoretical ideas, research questions, research strategies interest , specify theoretical kernel, inquiry, data strategy can’t fill rest blanks .Problem statement","code":""},{"path":"model-2.html","id":"model-2","chapter":"22 Model","heading":"22 Model","text":"population units interest? many ? characteristics?theoretical model?\n- Draw graphical representation part model (see Chapter 5). variables matter? causally connected? direction? parts confident based past work observations parts want learn design?","code":""},{"path":"inquiry-2.html","id":"inquiry-2","chapter":"23 Inquiry","heading":"23 Inquiry","text":"research questions?\n- model (), express question terms variables model.\n- started model, inquiries implied model, provide empirical evidence support falsify theoretical model?question important? learned answer, part world understand better (words, subgraph model world know something new confident ?).","code":""},{"path":"data-strategy-2.html","id":"data-strategy-2","chapter":"24 Data strategy","heading":"24 Data strategy","text":"treatment assignment scheme mind? level randomize? many units can assigned condition?measurement scheme mind? variables measure? instrument?ideal experiment run, constraints? (exercise may help stimulate ideas experiments feasibly run observational designs might mimic ideal experiment.)","code":""},{"path":"answer-strategy-2.html","id":"answer-strategy-2","chapter":"25 Answer strategy","heading":"25 Answer strategy","text":"use data results proposed data strategy produce answers inquiry?","code":""},{"path":"answer-strategy-2.html","id":"p4planning","chapter":"25 Answer strategy","heading":"25.1 Planning","text":"many research communities, becoming standard practice publicly register pre-analysis plan (PAP) prior implementation data strategy. PAPs serve many functions, importantly, clarify design choices made data collection made afterward. Sometimes – perhaps every time! – conduct research study, aspects \\(M\\), \\(\\), \\(D\\), \\(\\) shift along way. concern shift ways invalidate apparent conclusions study. example, “\\(p\\)-hacking” shady practice trying many regression specifications \\(p\\)-value associated important test attains statistical significance. PAPs protect researchers communicating skeptics design decisions made: regression specification detailed PAP posted data collected, test result \\(p\\)-hack.PAPs sometimes misinterpreted binding commitment report pre-registered analyses nothing . view unrealistic unnecessarily rigid. think researchers report pre-registered analyses somewhere (see Section 26.4 “populated PAPs”), study write-ups inevitably deviate way PAP – ’s good thing. Researchers learn conducting research; learning can reflected finalized answer strategy. Even binding, main point publicly posting pre-analysis plan communicate stage research process choices made.hunch main consequence actually writing PAP improvement research design . Just like research design declaration forces us think details model, inquiry, data strategy, answer strategy, describing choices publicly-posted document surely causes deeper reflection design. way, main audience PAP study authors .belongs PAP? Recommendations set decisions specified PAP remain remarkably unclear inconsistent across research communities. PAP templates checklists proliferating, number items suggest ranges nine sixty. PAPs becoming longer detailed, American Economic Association (AEA) Evidence Governance Politics (EGAP) study registries reaching hundreds pages researchers seek ever comprehensive. registries emphasize registration hypotheses tested, others emphasize registration tests used. read many PAPs found hard assess whether detailed plans actually contain key analytically-relevant details.view , minimally, PAP include design declaration. good deal discussion goes PAP centers answer strategy \\(\\) – estimator use, covariates condition , subsets data include. course, also need know details \\(D\\) – units sampled, treatments assigned, outcomes measured. need details assess properties design also gauge whether principles analysis randomize followed. need know \\(\\) need know target inference.28 need enough \\(M\\) describe \\(\\) sufficient detail. short, design declaration belongs PAP, design declaration specifies analytically-relevant design decisions.addition design declaration, PAP include mock analyses conducted simulated data. design declaration done formally code, creating simulated data resemble eventual realized data quite straightforward. think researchers run answer strategy mock data, creating mock figures tables eventually made real data. experience, step really causes researchers think hard aspects design.Strictly speaking, preanalysis plans include design declaration, require design diagnosis. since design finally settled design implemented usually chosen result diagnosis, can informative describe, preanalysis plan, reasons particular design chosen. reason, PAP might include estimates diagnosands like power, RMSE, bias. researcher writes PAP power detect small effect large, study comes back null, eventual writeup can much credibly rule “low power” explanation null. Moreover, ex ante design diagnosis communicates assumptions thought design good one ran study. reasons often basis convince skeptics value design, writing results known increases faith put .","code":""},{"path":"answer-strategy-2.html","id":"example-14","chapter":"25 Answer strategy","heading":"25.1.1 Example","text":"section, provide example supplement PAP design declaration. follow actual PAP Bonilla Tillery (2020), posted Predicted registry : https://aspredicted.org/q56qq.pdf. goal study estimate causal effects alternative framings Black Lives Matter (BLM) movement support movement among Black Americans overall well among subsets Black community. study authors models research transparency: prominently link PAP published article, conduct non-preregistered analyses except requested review process, replication archive includes materials required confirm analyses, able reproduce exactly minimal effort. goal section show design declaration can supplement complement existing planning practices.","code":""},{"path":"answer-strategy-2.html","id":"model-3","chapter":"25 Answer strategy","heading":"25.1.1.1 Model","text":"authors write PAP:hypothesize : H1: Black Nationalist frames BLM movement increase perceived effectiveness BLM among African American test subjects. H2: Feminist frames BLM movement increase perceived effectiveness BLM among African American women, decrease perceived effectiveness male subjects. H3: LGBTQ Intersectional frames BLM movement effect (demobilizing effect) perceived effectiveness BLM African American subjects.hypotheses reflect model coalition politics emphasizes tensions induced overlapping identities. Framing BLM movement feminist pro-LGBTQ may increase support among Black women Black LGBTQ identifiers, increase may come expense support among Black men Blacks identify LGBTQ. Similarly, model predicts subjects stronger attachment Black identity larger response Black nationalist framing BLM weaker attachments.model also includes beliefs distributions gender, LGBTQ status, Black identity strength. Data strategy, Black identity measured standard linked fate measure. background characteristics may correlated support BLM include age, religiosity, income, education, familiarity movement, included model well.focus study causal effects nationalism, feminism, intersectional frames relative general description Black Lives Matter movement. Model beliefs treatment effect heterogeneity embedded declare_potential_outcomes call. effect nationalism treatment hypothesized stronger, greater subjects’ sense linked fate; effect feminism treatment negative men positive women; effect intersectionality treatment positive LGBTQ identifies, negative non-identifiers.","code":"\nrescale <- function(x) (x - min(x)) / (max(x) - min(x))\nlikert_cut <- function(x)  as.numeric(cut(x, breaks = c(-100, 0.1, 0.3, 0.6, 0.8, 100), labels = 1:5))\n\nmodel <- \n  declare_population(\n    N = 800,\n    female = rbinom(N, 1, prob = 0.51),\n    lgbtq = rbinom(N, 1, prob = 0.05),\n    linked_fate = sample(1:5, N, replace = TRUE, \n                         prob = c(0.05, 0.05, 0.15, 0.25, 0.5)),\n    age = sample(18:80, N, replace = TRUE),\n    religiosity = sample(1:6, N, replace = TRUE),\n    income = sample(1:12, N, replace = TRUE),\n    college = rbinom(N, 1, prob = 0.5),\n    blm_familiarity = sample(1:4, N, replace = TRUE),\n    U = runif(N),\n    blm_support_latent = rescale(\n      U + 0.1 * blm_familiarity + \n        0.45 * linked_fate + \n        0.001 * age + \n        0.25 * lgbtq + \n        0.01 * income + \n        0.1 * college + \n        -0.1 * religiosity)\n  ) + \n  declare_potential_outcomes(\n    blm_support_Z_general = \n      likert_cut(blm_support_latent),\n    blm_support_Z_nationalism = \n      likert_cut(blm_support_latent + 0.01 + \n                   0.01 * linked_fate + \n                   0.01 * blm_familiarity),\n    blm_support_Z_feminism = \n      likert_cut(blm_support_latent - 0.02 + \n                   0.07 * female + \n                   0.01 * blm_familiarity),\n    blm_support_Z_intersectional = \n      likert_cut(blm_support_latent  - 0.05 + \n                   0.15 * lgbtq + \n                   0.01 * blm_familiarity)\n  )"},{"path":"answer-strategy-2.html","id":"inquiry-3","chapter":"25 Answer strategy","heading":"25.1.1.2 Inquiry","text":"inquiries study naturally include average effects three treatments relative “general” framing, well differences average effects subgroups. describing planned analyses, authors write:also look differences responses indicating pre-treatment familiarity BLM (4-Extensive knowledge 1-Never heard BLM), gender (particularly Feminist treatment), linked fate (particularly Nationalist treatment), LGBT+ affiliation (particularly LGBT+ treatment), though necessarily expecting moderations strong effect samples may lack adequate representation.code , specify treatment effect changes corresponding covariate \\(X\\) \\(\\frac{\\mathrm{cov}(\\tau_i, X)}{\\mathbb{V}(X)}\\), identical difference--difference binary covariates (female lgbtq) slope best linear predictor effect changes range linked_fate, blm_familiarity treating quasi-continuous .","code":"\nslope <- function(y, x) cov(y, x) / var(x)\n\ninquiry <-  \n  declare_estimands(\n    # Average effects\n    ATE_nationalism = \n      mean(blm_support_Z_nationalism - blm_support_Z_general),\n    ATE_feminism = \n      mean(blm_support_Z_feminism - blm_support_Z_general),\n    ATE_intersectional = \n      mean(blm_support_Z_intersectional - blm_support_Z_general),\n    \n    # Overall heterogeneity w.r.t. blm_familiarity\n    DID_nationalism_familiarity = \n      slope(blm_support_Z_nationalism - blm_support_Z_general, \n            blm_familiarity),\n    DID_feminism_familiarity = \n      slope(blm_support_Z_feminism - blm_support_Z_general, \n            blm_familiarity),\n    DID_intersectional_familiarity = \n      slope(blm_support_Z_intersectional - blm_support_Z_general, \n            blm_familiarity),\n    \n    # Treatment-specific heterogeneity\n    DID_nationalism_linked_fate = \n      slope(blm_support_Z_nationalism - blm_support_Z_general, \n            linked_fate),\n    DID_feminism_gender = \n      slope(blm_support_Z_feminism - blm_support_Z_general,\n            female),\n    DID_intersectional_lgbtq = \n      slope(blm_support_Z_intersectional - blm_support_Z_general, \n            lgbtq)\n  )"},{"path":"answer-strategy-2.html","id":"data-strategy-3","chapter":"25 Answer strategy","heading":"25.1.1.3 Data strategy","text":"subjects study 800 Black Americans recruited survey firm Qualtrics using quota sampling procedure. elide sampling step declaration – 800 subjects described declare_population call. reason , common practice analysis survey experiments convenience samples, authors formally extrapolate data make generalizations population Black Americans. inquiries study sample average effects. authors used different sampling strategy, using random sampling random digit dialing example, defined population sampling random sampling procedure.subjects’ background characteristics measured, assigned one four treatment conditions. Since survey conducted Qualtrics, assume authors used built-randomization tools, typically use simple (Bernoulli) random assignment.","code":"\ndata_strategy <- \n  declare_assignment(\n    conditions = \n      c(\"general\", \"nationalism\", \"feminism\", \"intersectional\"), \n    simple = TRUE\n  ) + \n  reveal_outcomes(blm_support, Z) "},{"path":"answer-strategy-2.html","id":"answer-strategy-3","chapter":"25 Answer strategy","heading":"25.1.1.4 Answer strategy","text":"authors write:run OLS regression predicting support , effectiveness , trust BLM treatment condition. […] also look differences responses indicating pre-treatment familiarity BLM (4-Extensive knowledge 1-Never heard BLM), gender (particularly Feminist treatment), linked fate (particularly Nationalist treatment), LGBT+ affiliation (particularly LGBT+ treatment), though necessarily expecting moderations strong effect samples may lack adequate representation. plan conduct analyses without controls. check group balance, may also run OLS analyses demographic controls (age, linked fate, gender, sexual orientation, religiosity, income, education, ethnic multi-racial backgrounds), report differences OLS results.DeclareDesign, corresponds five estimators, two shooting ATEs three shooting differences--CATEs. use OLS five – majority code bookkeeping ensure match right regression coefficient appropriate estimand.","code":"\nanswer_strategy <-\n  declare_estimator(\n    blm_support ~ Z,\n    term = c(\"Znationalism\", \"Zfeminism\", \"Zintersectional\"),\n    model = lm_robust,\n    estimand = \n      c(\"ATE_nationalism\", \"ATE_feminism\", \"ATE_intersectional\"),\n    label = \"OLS\"\n  ) +\n  declare_estimator(\n    blm_support ~ Z + age + female + as.factor(linked_fate) + lgbtq,\n    term = c(\"Znationalism\", \"Zfeminism\", \"Zintersectional\"),\n    estimand = \n      c(\"ATE_nationalism\", \"ATE_feminism\", \"ATE_intersectional\"),\n    model = lm_robust,\n    label = \"OLS with controls\"\n  ) +\n    declare_estimator(\n    blm_support ~ Z*blm_familiarity,\n    term = c(\"Znationalism:blm_familiarity\", \n             \"Zfeminism:blm_familiarity\", \n             \"Zintersectional:blm_familiarity\"),\n    model = lm_robust,\n    estimand = c(\"DID_nationalism_familiarity\", \n                 \"DID_feminism_familiarity\", \n                 \"DID_intersectional_familiarity\"),\n    label = \"DID_familiarity\"\n  ) +\n  declare_estimator(\n    blm_support ~ Z * linked_fate,\n    term = \"Zfeminism:linked_fate\",\n    model = lm_robust,\n    estimand = \"DID_nationalism_linked_fate\",\n    label = \"DID_nationalism_linked_fate\"\n  ) +\n  declare_estimator(\n    blm_support ~ Z * female,\n    term = \"Zfeminism:female\",\n    model = lm_robust,\n    estimand = \"DID_feminism_gender\",\n    label = \"DID_feminism_gender\"\n  ) +\n  declare_estimator(\n    blm_support ~ Z * lgbtq,\n    term = \"Zintersectional:lgbtq\",\n    model = lm_robust,\n    estimand = \"DID_intersectional_lgbtq\",\n    label = \"DID_intersectional_lgbtq\"\n  )"},{"path":"answer-strategy-2.html","id":"mock-analysis","chapter":"25 Answer strategy","heading":"25.1.1.5 Mock analysis","text":"Putting together, can declare complete design draw mock data .\nTable 25.1: Mock analysis Bonilla Tillery design.\nTable ?? shows mock analysis average effects (estimated without covariate adjustment) well heterogeneous effects analyses respect quasicontinuous moderators.\nMock regression table Bonilla Tillery design.\n\nFigure 25.1: Mock coefficient plot Bonilla Tillery design.\n","code":"\ndesign <- model + inquiry + data_strategy + answer_strategy\nmock_data <- draw_data(design)"},{"path":"answer-strategy-2.html","id":"design-diagnosis","chapter":"25 Answer strategy","heading":"25.1.1.6 Design diagnosis","text":"Finally, design diagnosis necessary component preanalysis plan, can useful show readers particular design chosen others. diagnosis shows design produces unbiased estimates estimands, better powered inquires others (assumptions effect size, original authors’). well-powered average effects, power increases include covariate controls. design probably small heterogeneous effect analyses, point directly conceded authors’ original PAP.\nTable 25.2: Design diagnosis Bonilla Tillery design.\nreading.Casey, Glennerster, Miguel (2012): early entryOlken (2015): halfway skepticalGreen Lin (2016): Standard operating proceduresChristensen Miguel (2018): reviewCoffman Niederle (2015): skeptical take (prefer replications).Humphreys, Sierra, Windt (2013): nonbindingMiguel et al. (2014): distinguishes disclosure PAPOfosu Posner (2020): apparently paps hinder publication?Rennie (2004)Zarin Tse (2008)Nosek et al. (2015)Findley et al. (2016) results-blind review","code":""},{"path":"answer-strategy-2.html","id":"criticism","chapter":"25 Answer strategy","heading":"25.2 Criticism","text":"silent piece declare-diagnose-redesign framework: getting outside advice design diagnosis implementationthere silent piece declare-diagnose-redesign framework: getting outside advice design diagnosis implementationmost scientific criticism takes place ex post, whenmost scientific criticism takes place ex post, whenwhy ? can’t change afterwardwhy ? can’t change afterwardresearch design documentresearch design document","code":""},{"path":"answer-strategy-2.html","id":"ethics","chapter":"25 Answer strategy","heading":"25.3 Ethics","text":"Social scientists also increasingly focused meeting ethical standards go beyond requirements national laws. Ethical appendices required journals describing protections human subjects many funding agencies now require defenses ethics research study funds disbursed.Common ethical principles include respect persons, beneficence (researchers must welfare participants mind designing research), informed consent participants, minimizing harm participants others.\nEthical considerations extend well beyond elements captured design declaration. instance, declaration analytic relevant components design may tell little level care respect accorded subjects. Nevertheless, think useful identify design relevant features can informative ethical judgments.","code":""},{"path":"answer-strategy-2.html","id":"ethical-principles-as-diagnosands","chapter":"25 Answer strategy","heading":"25.3.1 Ethical principles as diagnosands","text":"Researchers can use declare-diagnose-redesign process help inform ethical judgments. particular, diagnostic-statistic defined relevant ethical criterion. example, design can scored based total cost participants, many participants harmed (.e., many retraumatized asked past experience violence), average level informed consent measured survey comprehension study goals, risks adverse events.Consider two examples.Costs. common concern measurement imposes cost subjects, wasting time. Subjects’ time valuable resource – often donate willingly scientific enterprise taking survey similar. Sometimes generosity repaid financial compensation time. Sometimes subjects unknowing participants research study obtaining informed consent distort behavior hinder ability researchers study .Risks just realizations. subtly, different realizations data data strategy may also differ ethical status. result, ethics study determined looking fact happened study — many participants many treated, many harmed, many raised complaints. Instead, ethical status project depends judgments potential harms potential participants: happen, happened.design diagnosed, diagnosands can constructed summarize level ethical encumbrance across possible realizations design. first way ethical diagnosands can used determine whether study design meets set ethical thresholds. probability harm minimal enough? average level informed consent sufficient? Given characteristics vary across designs across realizations design, writing concretely measure ethical characteristic threshold design ethical can help structure thinking. (diagnoses threshold determinations can also shared ethical writeups design.)Among ethical designs, researchers must select single design implement. Often, ethical threshold met, select among feasible designs based research design criteria statistical power bias. Instead, continue assess ethical considerations alongside quality research design. Among ethical designs, still often tradeoffs much time asked subjects risk harm. select designs weight considerations (perhaps highly!) power designs. , can simply continue include diagnosands related ethical criteria diagnoses redesign studies.difficult challenge process order weigh ethical criteria research design criteria power cost, must put able measure two common scale. must able think value society research weigh risks participants others. Similarly, must able weigh precise estimates question ethical considerations also change based number units proportion treated among design features. moving forward research must implicitly weigh considerations. IRB applications, often directly asked weigh costs subjects benefits research subjects well society whole.Scholars increasingly calling reporting ethical considerations study design beyond IRB approval. declaring expectations ethical outcomes experiment terms diagnosands time participants devote study probability harm individuals, declared research design can input ethical reporting. Readers can review considered ethical outcomes design judge mitigation efforts undertook relation expectations. scholars proposed including ethical assessments preanalysis plans. Declarations ethical diagnosands natural complement preregistered assessments.readings.Humphreys (2015)Teele (2021)Meyer (2015)Luft (2020)Baron Young (2020)Lyall (2020)","code":""},{"path":"answer-strategy-2.html","id":"approvals","chapter":"25 Answer strategy","heading":"25.4 Approvals","text":"IRB approval (purpose protect university liability) - Gary Melissa’s paperIRB approval institution othersIRB context working - country IRBs, country IRBresearch clearances","code":""},{"path":"answer-strategy-2.html","id":"partners","chapter":"25 Answer strategy","heading":"25.5 Partners","text":"Partnering third-party organizations research — cooperating intervene world measure outcomes — increasingly common social sciences. Researchers seek produce (publish) scientific knowledge; work political parties, government agencies, non-profit organizations, businesses learn worked independently. groups consent working researchers order learn achieve organizational goals. example, government may want learn expand access healthcare corporation may want learn improve ad targeting.best case scenario, goals researchers partner organizations aligned. scientific question answered (inquiry) practical question organization cares , gains cooperation clear. research team gains access financial logistical capacity organization act world partner organization gains access scientific expertise researchers. Finding good research partner almost always amounts finding organization common – least conflicting – goal. Understanding private goals partner researcher essential selecting research design amenable parties. Research design declaration diagnosis can help problem formalizing tradeoffs two sets goals.One common divergence partner researcher goals partner organizations often want learn, care primary mission. business settings, dynamic sometimes referred “learn versus earn” “exploration-exploitation” tradeoff. aid organization (donors) cares delivering program many people possible; learning whether program intended effects outcomes interest obviously also important, resources spent evaluation resources spent program delivery.Research design diagnosis can help navigate learning-tradeoff. One instance tradeoff proportion units receive treatment (e.g., medicine) represents rate “” also affects amount learning extreme units treated can (typically) learning effect treatment. tradeoff represented graph power study vs. proportion treated (top facet), utility partner (bottom facet) increasing proportion treated. researchers power cut-standard 80% threshold. partner also strict cut-: need treat least 2/3 sample order fulfill donor requirement.absence partners, researchers might simply ignore proportion treated axis select design highest power. partner organization, researcher might use graph conversation partner jointly select design highest power sufficiently high proportion treated meet partner’s needs. represented “zone agreement” gray: region, design least 80% power least 2/3 sample treated. Deciding within region involves trade-power (decreasing proportion treated ) utility partner (increasing proportion treated). diagnosis surfaces zone agreement clarifies choice designs zone.\nFigure 25.2: Navigating research partnerships.\nChoosing proportion treated one example integrating partner constraints research designs generate feasible designs. second common problem set units must treated, ethical political reasons (e.g., home district government partner must receive treatment), must treated. constraints discovered treatment assignment, lead noncompliance, may substantially complicate analysis experiment even prevent providing answer original inquiry. Considerable thought given avoiding type noncompliance. Alan S. Gerber Green (2012) recommend, randomizing treatment, exploring possible treatment assignments partner organization using exercise elicit set units must treated. King et al. (2007) describe “politically-robust” design, uses pair-matched block randomization unit dropped due political constraints pair dropped study.29Design diagnosis can help circumstances providing mechanism specify possible patterns noncompliance diagnosing alternative designs mitigate negative consequences. addition, can used communicate partners consequences noncompliance research help make better decisions together avoid noncompliance treatment randomized research progress.best advice working partners involve design declaration diagnosis process. can develop intuitions means, variances, covariances important variables measured? Ask partner best guesses, may far educated . experimental studies, solicit partner’s beliefs magnitude treatment effect outcome variable, subgroup subgroup possible. specific – ask think average control group average treatment group. Sorting beliefs quickly sharpens discussion key design details. Share design diagnoses mock analyses study launched order build consensus around goals study.Sometimes partnership simply work . Indeed partnerships never even initiated researcher organizational goals far apart. find setting partnership much violence research design, find way walk away project.","code":""},{"path":"answer-strategy-2.html","id":"funding","chapter":"25 Answer strategy","heading":"25.6 Funding","text":"design, tradeoffs diagnosands quality research well costs. Costs function data strategy (expensive others!) data realized time. Collecting original data expensive analyzing existing data, collecting new data may less expensive depending easy reach specific individuals interview . result, diagnosing research designs including cost diagnosands important, diagnosands may usefully include average cost maximum cost. Researchers may make different decisions cost: cases, researcher select “best” design terms research design quality subject budget constraint, others choose cheapest among similar quality designs order save money future research. Diagnosis can help identify set decide among .relax budget constraint, researchers apply funding. Funding applicants wish obtain large grant possible design, difficulty credibly communicating quality design given subjectivity exercise. Funders, flip side, wish get value money set proposals decide fund, difficulty assessing quality proposed research. MIDA design declaration provide tool speaking common language can easily verified sides design proposed value knowledge set assumptions can interrogated funders.Funding applications often aim communicate research design proposed; learning answers design useful, important, interesting scholars, public, policymakers, another audience; research design provides credible answers question; researcher capable executing design; value--money design answers provides.new section funding applications aid communicating questions declaring MIDA design presenting diagnosis design. addition common diagnosands bias efficiency, two special diagnosands may valuable: cost , related, value--money. Cost can included design variant function design features sample size, number treated units, duration survey interviews. cost may vary parameters may vary across possible designs , example, number treated units random number. Simulating design across possible realizations design, thus, provides distribution costs function choices researcher makes. Value money diagnosand function cost also amount learned design. RMSE might one value criterion, another average difference priors posteriors Bayesian answer strategy (direct measure learning).cases, funders request applicants provide multiple options multiple price points make clear design altered funded lower (higher) level. Redesigning design differing sample sizes communicate researcher conceptualizes options, also provide funder understanding tradeoffs amount learning cost design variants. Applicants use redesign justify high cost request ask additional funding.Ex ante power analyses, required increasing number funders, illustrate crux misaligned incentives applicants funders. power analysis can demonstrate almost design “sufficiently powered” changing expected effect sizes noise. clarifying assumptions power analysis code, researchers can easily defend choices. Funders can easily interrogate assumptions. Power analyses using standard power calculators online difficult--interrogate assumptions built accommodate specifics many common designs. result, many return incorrect estimates power designs (Blair et al. 2020).Funders request design declarations can compare funding applications common scales: root mean-squared-error, bias, power. course, also want weigh considerations like importance question fit funding program. moving design considerations onto common scale takes guesswork process reduces reliance researcher claims properties.","code":""},{"path":"during.html","id":"during","chapter":"26 During","heading":"26 During","text":"","code":""},{"path":"during.html","id":"p4piloting","chapter":"26 During","heading":"26.1 Piloting","text":"designs results past studies important guides selecting M, , D, . understanding nodes edges causal graph M, expected effect sizes, distribution outcomes, feasible randomization schemes, many features directly selected past research chosen based literature review distribution past studies. However, researchers face problem guided past research: research context inquiries often differ least subtle ways past study. Even replicating past study, collecting data different time period effects vary time aspects M may differ original study. deal , often run pilot studies. take many forms: focus groups learn features M learn ask survey questions; small-scale tests measurement tools verify data collection technology works; mini studies planned design smaller scale.Pilot studies constrained time money. constrained, run full study learn wrong design run corrected design main study. Since due constraints, run either smaller mini studies test subset elements planned design. places us bind: running design smaller less complete study imagine conducting, properties pilot design measure .MIDA provides framework thinking can learned pilot research design diagnosis. Just like full study, can define inquiries decisions make parameter estimates draw designing full study.Figure 26.1, display results diagnosis 50-unit pilot study conducting prepare larger main study. consider two strategies: (1) determining sample size power analysis main study, selecting minimum \\(N\\) study 80% powered detect pilot study’s effect size); (2) setting fixed \\(N\\) determined budget constraint, case 500, using standard deviation units treated control group pilot determine minimum detectable effect size 500-unit main study.left panel sampling distribution effect size estimates, .e., histogram effect estimates pilot. design, standard deviation outcome set one, effect estimates standard deviation units. true effect size set 0.2. can see sampling distribution huge range, nearly -0.5 nearly 0.75. first problem sampling distribution many estimates, fact nearly quarter , negative (wrong sign!). might lead us choose wrong sample size choose one-sided tests wrong direction. second high likelihood guessing effect size much higher really . obtain one estimates 0.75 even 0.5, choose \\(N\\) small detect true effect size 0.2. short, estimates effect size 50-person pilot study simply variable useful designing main study.However, good news: can learn lot power main study pilot study, just effect estimates. right panel Figure 26.1, estimate minimum detectable effect size 500-unit main study, relying estimated standard deviation control group estimated standard deviation treatment group calculate estimated standard error effect estimate main study. calculate minimum detectable effect size using approximation Gelman Hill (2006): 2.8 times estimated standard error (pg. 441). find estimates MDE full study much precise, tightly centered around 0.25. Since don’t know larger smaller true effect size, must make argument based past studies’ effect sizes justify whether minimum size sufficiently large whether increase sample size order detect even smaller effects. reason MDE precisely estimated standard deviation control group much less variable estimate true standard deviation control potential outcome effect size estimate true effect size.\nFigure 26.1: Learning pilot studies.\ndiagnosing pilot studies way, can learn decisions can made confidence pilot data shaped instead expectations past studies qualitative knowledge. Diagnosis can also help us decide large pilot study need order estimate quantities like MDE full study precision.Beyond estimating MDE studies, facts can often usefully learned pilot studies take form existence proofs. often wish study variation \\(D\\) (treatment) affects variation \\(Y\\) (outcome), absence past data two variables, may know even variation \\(Y\\) explain. experimental studies, can learn whether treatment can implemented, observational study, can learn whether variation treatment variable.Baseline measurement may often used instead pilot study learn empirical features. sample size fixed interested learning whether outcome measures vary across units covary, can measure baseline make adjustments posttreatment survey. still control imperfect measures baseline improve efficiency.","code":""},{"path":"during.html","id":"implementation","chapter":"26 During","heading":"26.2 Implementation","text":"design declaration road map implementing study. data strategy tells procedure use sample units; assign treatments; variables measure. answer strategy function translates realized data set answers inquiries statistics communicate confidence answers. specified data answer strategies sufficient detail code, can directly run functions declared sample units assign treatment analyze data.road map useful tool learn go things go right, also identify take wrong turn need make decisions get answer. sense, design declaration living document, updated reflect set decisions make along way twists turns research road. model world inquiry declared, unable collect variable, treat subset units treatment, reach units follow-surveys, can compare alternative ways handling deviation original plan terms originally designed experiment. changes data strategy \\(D\\). can assess options also guide decision-making whether continue study use money another better purpose. can also use comparison diagnosands alternative options tool communicate research partners change practices needed. can also use defend intermediate data strategy choices finished reviewers readers.make changes \\(D\\), changes \\(\\) may also required order follow principle analyzing sample, assign treatment, measure. switch individual randomization cluster randomization logistically possible individually assign units treatment, typically want adjust answer strategy account clustering calculation standard errors. keeping data strategy answer strategy date implement study, may also identify new data must collected new steps take order still able provide credible answers.also learn design go along, anything goes wrong natural progression research. example, may know many units per cluster per block, key details assigning treatments analyzing data experiments. learn details, change data answer strategies reflect new details — diagnose new design sure still agree original choices. Beyond data answer strategies, may also learn new nodes edges model course research. learn new confounders mediators, update model, also consider whether changes data answer strategy necessitated ensure can answer original inquiry.short, design declaration living document can keep updated use tool guide along research path, just document write beginning study revisit writing . advice apparent tension idea preanalysis plans, precommit analysis choices data collected. need . useful keep original design declaration preregister , also useful keep declaration updated make changes along way inevitably happen. better position make good choices things go awry, also communicate made changes design.Related readings.Failure (Karlan Appel (2018))","code":""},{"path":"during.html","id":"pivoting","chapter":"26 During","heading":"26.3 Pivoting","text":"– Alex’s example\n– NollywoodNollywood\n- 215 cellphone towers randomly sampled universe four states Southeastern Nigeria; restricted sampling procedure prevent samples proximate towers reduce risk geographic spillovers. randomization design five conditions: two--two factorial treat vs placebo film X treat vs placebo SMS plus pure control measurement cluster randomized tower level.\n- learned data collection began SMS treatment misallocated, treatment SMS sent conditions accidentally partner. one hundred sampled towers treated. 109 towers untreated.\n- three dimensions choice: continue abandon; resample randomize; retain reduce number treatments\n- constraints: treatment many places text message blasted every subscriber major phone company area; film produced region filmed region, switch areas film treatment. (whole measurement strategy region-specific , choice language pretesting.)\n- ultimate choice: step-wedge design SMS crossed two arm trial film.","code":"\ndesign <-\n  declare_population(N = 744,\n                     U = rnorm(N),\n                     potential_outcomes(Y ~ 0.1 * Z + U, conditions = list(\n                       Z = c(\n                         \"backups\",\n                         \"pure_control\",\n                         \"film_placebo_text_placebo\",\n                         \"film_placebo_text_treat\",\n                         \"film_treat_text_placebo\",\n                         \"film_treat_text_treat\"\n                       )\n                     ))) +\n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_sampling(n = 215) +\n  declare_assignment(Z = complete_ra(\n    N,\n    m_each = c(15, 40, 40, 40, 40, 40),\n    conditions = c(\n      \"backups\",\n      \"pure_control\",\n      \"film_placebo_text_placebo\",\n      \"film_placebo_text_treat\",\n      \"film_treat_text_placebo\",\n      \"film_treat_text_treat\"\n    )\n  ), handler = fabricate) +\n  declare_measurement(Y = fabricatr::reveal_outcomes(Y ~ Z)) +\n  declare_estimator(Y ~ Z, model = lm_robust)\n\ndraw_data(design)\ndesign <-\n  declare_population(\n    towers = add_level(\n      N = 744,\n      U_tower = rnorm(N)\n    ),\n    citizens = add_level(\n      N = 14,\n      U = rnorm(N),\n      potential_outcomes(Y ~ 0.1 * Z + U + U_tower)\n    )\n  ) + \n  declare_sampling(clusters = towers) + \n  declare_sampling(n = 109, order_by = U_tower, handler = slice_max) + \n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + \n  declare_assignment(Z_film = cluster_ra(N, clusters = towers, prob = 0.5), handler = fabricate) + \n  declare_assignment(Z_)\ndeclare_measurement(Y = reveal_outcomes(Y ~ Z)) + \n  declare_estimator(Y ~ Z, clusters = towers, model = lm_robust)\n\ndraw_data(design)"},{"path":"during.html","id":"p4populatedpap","chapter":"26 During","heading":"26.4 Populated Preanalysis Plan","text":"Inevitably, authors pre-analysis plans fail anticipate , eventually, data generated study analyzed. Many reasons discrepancy discussed previous section implementation, reasons intervene well. common reason PAPs promise many analyses – process writing paper, analyses dropped, others combined, still others added writing revision process. next section, ’ll describe reconcile analyses--planned analyses--implemented, present section analysis plan immediately getting data back.echo proposals made Banerjee et al. (2020) Alrababa’h et al. (2020) researchers produce short reports fulfill promises made PAPs. Banerjee et al. (2020) emphasize writing PAPs difficult usually time constrained, natural final paper reflect thinking full set empirical approaches. “populated PAP” serves simply communicate results promised analyses. Alrababa’h et al. (2020) cite tendency researchers abandon publication studies return null results. order address resulting publication bias, recommend “null results reports” share results pre-registered analyses.recommend authors include mock analyses PAPs using mock data. major benefit quite specific details answer strategy. benefit comes time produce populated PAP, since realized data can quite straightforwardly swapped mock data. Given time invested producing mock analyses PAP, writing populated PAP takes much effort needed clean data, need done case.","code":""},{"path":"during.html","id":"example-15","chapter":"26 During","heading":"26.4.1 Example","text":"Section 25.1, declared design Bonilla Tillery (2020) following preanalysis plan. , declared answer strategy code. populated PAP, can run answer strategy code, swap simulated data real data collected study. present first regression table Table ?? coefficient plot Figure 26.2.\nStatistical models\n\nFigure 26.2: Coefficient plot Bonilla Tillery design based study’s realized data.\n","code":""},{"path":"during.html","id":"p4reconciliation","chapter":"26 During","heading":"26.5 Reconciliation","text":"Inevitably, research design implemented differ way research design planned. Treatments implemented conceived, people found interview, sometimes learn baseline measures informs measure later. understanding research design changed conception implementation crucial understanding learned design.Suppose original design described three-arm trial: one control two treatments, design implemented drops subjects assigned second treatment. Sometimes entirely appropriate reasonable design modification: perhaps turns due implementation failure, second treatment simply delivered. times, modification less benign – perhaps estimate effect second treatment achieve statistical significance, author simply omits analysis.reason, explicitly reconciling design planned design implemented first step writing paper. publicly-posted preanalysis plan can make reconciliation process especially credible – know sure planned design preanalysis plan describes pre-implementation. However, preanalysis plan prerequisite engaging reconciliation. scientific enterprise built large measure trust: ready believe researchers say, design thought implement due unanticipated developments, design ended implementing.cases, reconciliation lead additional learning beyond can inferred final design . units refuse included study sample units refused measurement, learn important features units. Understanding sample exclusions, noncompliance, attrition may inform future research design planning choices, contribute substantively understanding social setting. policy implemented way study likely also able work units refused participate, future research examine convince policy’s benefits.belongs reconciliation? minimum, need full description planned design, full description implemented design, list differences. can made explicit declaration designs computer code, comparing two design objects line--line.DeclareDesign take first steps comparing designs :","code":"\ndesign1 <- declare_population(N = 100, U = rnorm(N)) +\n  declare_potential_outcomes(Y ~ Z + U) +\n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_sampling(n = 75) +\n  declare_assignment(m = 50) +\n  reveal_outcomes(Y, Z) +\n  declare_estimator(Y ~ Z, estimand = \"ATE\")\n\ndesign2 <- declare_population(N = 200, U = rnorm(N)) +\n  declare_potential_outcomes(Y ~ 0.5*Z + U) +\n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_sampling(n = 100) +\n  declare_assignment(m = 25) +\n  reveal_outcomes(Y, Z) +\n  declare_estimator(Y ~ Z, model = lm_robust, estimand = \"ATE\")\n\ncompare_designs(design1, design2)\ncompare_design_code(design1, design2)\ncompare_design_summaries(design1, design2)\ncompare_design_data(design1, design2)\ncompare_design_estimates(design1, design2)\ncompare_design_estimands(design1, design2)"},{"path":"during.html","id":"example-16","chapter":"26 During","heading":"26.5.1 Example","text":"Section 25.1, described preanalysis plan registered Bonilla Tillery (2020). reconcile set conditional average treatment effect analyses planned PAP, analyses reported paper, reported appendix request reviewers Table 26.1. column two, see authors planned four CATE estimates: effects familiarity Black Lives Matter; gender; LGBT status; linked fate. two reported paper, others may excluded space reasons. two excluded analyses especially informative; excluded basis statistical significance. Another way handle uninteresting results present populated PAP posted Web site paper’s appendix.appendix, authors report set analyses requested reviewers. see perfect example transparently presenting set planned analyses highlighting analyses added afterward added. write:asked consider pertinent moderations beyond gender LGBTQ+ status. contained four following sections.Table 26.1:  Reconciliation Bonilla Tillery preanalysis plan.","code":""},{"path":"during.html","id":"writing","chapter":"26 During","heading":"26.6 Writing","text":"writing empirical paper, authors must convince reviewers readers question important research design selected provides useful answers question. MIDA comes . Elements MIDA appear every section paper, every element MIDA described somewhere paper.common model social science empirical papers five sections: introduction, theory hypotheses, research design, results, discussion. outline elements MIDA can usefully fit section , providing example paper highlighting element described.introduction section capitulation aspect MIDA brief. reader brought quickly speed whole research design, well expectations actual findings.next section, theory hypotheses, contains information causal model world (\\(M\\)) research question world (\\(\\)) well guesses \\(^M\\) (.e., hypotheses!). section lay features model necessary describe \\(\\).motivate hypotheses, theory section outline prior beliefs \\(^W\\), true answer inquiry, based past literature. meta-analysis systematic review past evidence provide systematic summary past answers \\(^W\\), informal literature review offered. priors used along study results construct posterior beliefs reported discussion section, .e., know conducting study. summarizing literature, important consider research designs past studies (see Synthesis section). Meta-analyses often formally account quality research designs past studies weighting inverse precision (upweighting informative studies -weighting uninformative ones). Literature reviews may informally. Moreover, summary past literature research design, try prevent common research design issues selection dependent variable ensuring discuss literature present views consonant hypotheses.Though research questions nearly ubiquitously included theory section, details model often left . Expected effect sizes, effects vary subgroups, expected proportions subgroups, variables expected correlated, amount variability outcome features model important effects reviewers readers judge quality research design. Without specifying portions \\(M\\), diagnosis conducted. Moreover, describe Reanalysis, define \\(M\\) paper, can clarify terms debate alternative analysis strategies. \\(M\\) undefined, author reanalysis must infer model make , may agree.research design methods section description \\(D\\) \\(\\) description diagnosis design. section, defend choice \\(D\\) \\(\\) \\(M\\) \\(\\). Papers commonly one \\(D\\) associated \\(\\) answering related questions; \\(D\\) \\(\\) described section.results section describes \\(^D\\). Increasingly, results presented visually well text, order effectively communicate results reader. Visualizing modeled results well raw data can improve results communicated simultaneously help readers connect research design, data, results.discussion section update understanding model form posteriors \\(^W\\) given prior expectations results \\(^D\\). may learn study new variables new edges matter model — new outcomes affected treatment. also often learn functional form causal relationships two variables, example new moderators precision size moderation. discussion section also chance point new MIDAs implemented learn \\(M\\). might relate new nodes discovered, parts model yet learn enough .Figure , illustrate elements MIDA incorporated Mousa (2020). study reports outcome randomized experiment Iraqi Christians assigned either -Christian soccer team team play alongside Muslims. experiment tested whether mixed team affected intergroup attitudes behaviors, among teammates back home games . highlight color areas discussing model \\(M\\) yellow, inquiry \\(\\) green, data strategy \\(D\\) blue, answer strategy \\(\\) pink.Paper MIDA elements highlighted (Mousa 2020)model inquiry largely appear abstract introductory portion paper, though aspects model discussed later . Much first three pages devoted data strategy, answer strategy appears briefly. division makes sense: paper, action experimental design whereas answer strategy follows straightforwardly using principles analyze randomize. paper mostly describes \\(M\\) \\(D\\), small portion devoted \\(\\) \\(\\). Finally, notable data strategy interspersed aspects model. reason author justifying choices randomization measurement using features model. highlights deep connection two discussed Part II.Papers often report results multiple data strategies others answers multiple related inquiries. Typically, single \\(M\\) described theory section, describes nodes edges used \\(\\)’s fit together. data strategy described, inquiries answer strategy targeting. estimate linked one inquiries. cases, reason multiple inquiries studied single paper aim paper falsify model world. case, model described diagnosis design include assessment good overall design, data answer strategies generate answers inquiry, falsifying model. Psychology three-study papers often take form: study 1 asks correlation X Y, study 2 X randomized, study 3 analysis mechanisms lead X Y examined.","code":""},{"path":"after.html","id":"after","chapter":"27 After","heading":"27 After","text":"","code":""},{"path":"after.html","id":"publication","chapter":"27 After","heading":"27.1 Publication","text":"Declaring MIDA research design can help editors reviewers assess quality research design. reviewers request changes study, declaration proposed changes diagnosis can used compare two possible designs .","code":""},{"path":"after.html","id":"reviewing-papers","chapter":"27 After","heading":"27.1.1 Reviewing papers","text":"Reviewers editors must decide whether devote scarce space editing bandwidth publishing paper. Criteria may include topic fit journal, importance question, much learned research. problem publication filter — publishing studies statistically-significant splashy results — long recognized cause “false” findings making way literature bias literature simply missing null findings. One remedy problem results-blind review studies: selecting studies without knowing nature results.Results-blind review can take place multiple ways. Journals instituted alternative submission paths “registered reports,” authors lay analyze data often include mock tables figures based simulated data. reports can constructed data collected. closely related preanalysis plans, fact often PAPs consist registered report.results-blind review require institutional setup: reviewers can review papers without regard results . Reviewers can print papers hide results simply aim write comments basis importance question quality research design.cases, needed authors clear statement research design report interpret results. Results-blind review need ignore features research design data displayed whether authors overclaiming based results. Authors can write plans visualize claims make based different patterns findings. Journals institute results-blind review can ensure reviewers able judge quality research design requesting details \\(M\\), \\(\\), \\(D\\), \\(\\) authors.","code":""},{"path":"after.html","id":"responding-to-reviewers","chapter":"27 After","heading":"27.1.2 Responding to reviewers","text":"research design study set stone final version posted journal Web site published print. , journal editors reviewers may ask changes answer strategies even data strategies inquiries , view, improve paper.reviewer may make three kinds requests: alternative analysis strategy replace augment original; additional data collection; addition new inquiry, based realized data new data collection; change model form adding dropping node edge, related changes parts design; change inquiry reviewer feels can better answered data answer strategy author selected.can understand making changes requested reviewers alters design. change improves design, adopting suggestions easy. changes irrelevant design – like reporting reviewer’s preferred descriptive statistics – case, following advice also easy. trouble comes reviewers propose changes actively undermine design? , diagnosing reviewer’s alternative design can effective way demonstrate proposed changes harm research design.","code":""},{"path":"after.html","id":"archiving","chapter":"27 After","heading":"27.2 Archiving","text":"One biggest successes push greater research transparency changing norms surrounding sharing data analysis code studies published. become de rigeur many journals post materials publicly-available repositories like OSF Dataverse. development undoubtedly good thing. older manuscripts, sometimes data analyses described “available upon request” course requests sometimes ignored. Furthermore, century now, study authors longer us even wanted respond requests. Public repositories much better chance preserving study information future.belongs replication archive? First, data \\(d\\) . Sometimes raw data, sometimes “cleaned” data actually called analysis scripts. ethically possible, think preferable post much raw data possible, example removing information like IP address geographic location used identify subject. usually consider data processing scripts clean prepare data analysis part data strategy \\(D\\) sense complete measurement procedures laid \\(D\\). Cleaning scripts might also considered part answer strategy sense apply interpretation data provided world. output cleaning scripts – cleaned data – included replication archive well.Replication archives also include \\(\\), set functions applied \\(d\\) produce \\(^D\\). vitally important actual analysis code archived natural-language descriptions \\(\\) typically given papers imprecise. small example, many articles describe answer strategies “ordinary least squares” fully describe set covariates used flavor standard errors estimated. differences can substantively affect quality research design. actual analysis code makes \\(\\) explicit.typical replication archives include \\(d\\) \\(\\), think future replication archives also include design declaration fully describes \\(M\\), \\(\\), \\(D\\), \\(\\) – , archive designs, just data analysis code. done code words. addition, diagnosis included, demonstrating properties understood author also indicating diagnosands author considered judging quality design.Figure shows file structure example replication. view replication archives shares much common TIER protocol, can found : https://www.projecttier.org/. includes raw data platform-independent format (.csv) cleaned data language-specific format (.rds), data features like labels, attributes, factor levels preserved imported analysis scripts. analysis scripts labeled outputs create, figures tables. master script included runs cleaning analysis scripts correct order. documents folder includes paper, supplemental appendix, pre-analysis plan, populated analysis plan, codebooks describe data. README file explains part replication archive. also suggest authors include script includes design declaration diagnosis.File structure archiving","code":""},{"path":"after.html","id":"reanalysis","chapter":"27 After","heading":"27.3 Reanalysis","text":"reanalysis existing study follow-study \\(d\\), original realized data, fixed changes \\(\\) sometimes \\(M\\) \\(\\) proposed. Given \\(d\\) fixed, data strategy \\(D\\). results given new MIDA, may differ original study’s results, reported.can learn reanalyses least five ways’r flagit()’(FIVE?). can confirm errors analysis strategy. Many reanalyses correct simple mathematical errors, typos data transcription, failures analyze following data strategy faithfully. reanalyses show whether results depend corrections.can reassess known \\(\\), using new information world learned original study published. , may learn new confounders alternative causal channels undermine credibility original answer strategy. reanalyzed, demonstrating results () change improves understanding \\(^W\\).Many reanalyses show original findings “robust” alternative answer strategies. better conceptualized claims robustness alternative models: one model may imply one answer strategy different model, another confounder, implies another. models plausible, good answer strategy robust even help distinguish reanalysis uncover robustness alternative models lack thereof.Reanalyses may also aim answer new questions considered original study, realized data can provide useful answers. example, authors may analyze outcomes originally analyzed.Reanalyses research designs. Whether reanalysis good design, much can contribute knowledge original inquiry, depend possible realizations data. \\(d\\) fixed reanalysis, analysts often instead tempted judge reanalysis based whether overturns confirms results original study. successful reanalysis way thinking demonstrates, showing original results changed alternative \\(\\), results robust plausible models. way thinking can lead incorrect assessments reanalyses. need consider answers obtain original answer strategy \\(\\) reanalysis strategy \\(^{\\prime}\\) many possible realizations data. good reanalysis strategy reveals high probability set models world can make credible claims \\(\\). Whether results fixed \\(d\\) realized change \\(\\) \\(^{\\prime}\\) tells us little probability. one draw.diagnose reanalysis, need define two answer strategies — \\(\\) \\(^{\\prime}\\) — also new diagnostic-statistic. need decide summarize answers two answer strategies. one returns TRUE one FALSE, conclude inquiry? function define summarize two results depends inquiry goals reanalysis. diagnosis reanalysis assess properties summary two studies possible realizations data. goal reanalysis instead learn new question, simply construct new MIDA altogether, holding constant \\(D\\) original study, change already collected \\(d\\) using .","code":""},{"path":"after.html","id":"replication","chapter":"27 After","heading":"27.4 Replication","text":"study completed, may one day replicated. Replication differs reanalysis replication study involves specification new MIDA collection new data study inquiry. discussed previous, reanalysis may re-specify parts research design, always re-uses original data \\(d\\) way.-called “exact” replications hold key features , D, fixed, draw new dataset \\(d_{\\rm new}\\) \\(D()\\) apply \\(\\) new \\(d\\) order produce fresh answer \\(a_{\\rm new}^D\\). Replications said “succeed” \\(a_{\\rm old}^D\\) \\(a_{\\rm new}^D\\) similar “fail” . Dichotomizing replication attempts successes failures usually helpful, better simply characterize similar \\(a_{\\rm old}^D\\) \\(a_{\\rm new}^D\\) .course, exact replication impossible: least elements M changed first study replication. Specifying might changed, e.g., outcomes vary time, help judge differences observed \\(a_{\\rm old}^D\\) \\(a_{\\rm new}^D\\). Statistical noise also play role.Replication studies benefit enormously knowledge gains produced original studies. example, learn large amount \\(M\\) likely value \\(^M\\) original study. \\(M\\) replication study can incorporate new information. example, learn original study \\(^M\\) positive might small, replication study respond changing \\(D\\) order increase sample size. Design diagnosis can help learn change design replication study light original study.changes \\(D\\) \\(\\) can made produce informative answers \\(\\), exact replication may preferred. Holding treatment outcomes may required provide answer \\(\\), increasing sample size sampling individuals rather villages changes may preferable exact replication. Replication designs can take advantage new best practices research design.designing original studies, anticipate someday work replicated. improves ex ante incentives. extent want future replication studies arrive similar answers original study produce (.e., want \\(a_{\\rm new}^D\\) match \\(a_{\\rm old}^D\\) closely possible), want choose designs bring \\(a_{\\rm old}^D\\) close \\(^M\\) possible, presupposition faithful replicators also design studies way \\(a_{\\rm new}^D\\) also close \\(^M\\).Replication studies necessarily differ original studies – literally impossible reproduce exact conditions original study way ’s impossible step river twice. Another way putting statement \\(D_{\\rm new}\\) necessarily different \\(D_{\\rm old}\\). Theory (.e., beliefs \\(M\\)) tool use say \\(D_{\\rm old}\\) similar enough \\(D_{\\rm new}\\) constitute close enough replication study. concrete example, many survey experimental replications involve using exact experimental stimuli changing study sample, e.g., nationally representative sample convenience sample.-called “conceptual” replications alter \\(M\\) \\(D\\), keep \\(\\) \\(\\) similar possible. , conceptual replication tries ascertain whether relationship one context (\\((M_{\\rm old})\\)) also holds new context (\\((M_{\\rm new}\\)). trouble promise conceptual replications lies success designer holding \\(\\) constant. often, conceptual replication fails changing \\(M\\), much changes \\(\\) much changes “concept” replication.summary function interpret difference \\(a_{\\rm old}^D\\) \\(a_{\\rm new}^D\\). may take new one throw old MIDA poor first. may taking average. may precision-weighted average. Specifying function ex ante may useful, avoid choice summary depending results replication. summary function reflected discussion section replication paper.reading.Clemens (2017) distinctions replication reanalysis","code":""},{"path":"after.html","id":"resolving-disputes","chapter":"27 After","heading":"27.5 Resolving Disputes","text":"Disputes arise reanalyses replication studies conducted claims made past studies learn pair. realized data two studies, \\(d\\) \\(d^{\\prime}\\), well two designs \\(MIDA\\) \\(MIDA^{\\prime}\\), together inform learn two studies. Disputes arise whether changes \\(M^{\\prime}\\), \\(^{\\prime}\\), \\(D^{\\prime}\\), \\(^{\\prime}\\) new study mean \\(d^{\\prime}\\) can informative original \\(\\).\\(M\\) always changes. reanalysis replication conducted original study, definition learned least \\(^w\\) \\(^d\\) first study. often learned much , distribution variables, existence new nodes edges, sometimes much related studies published . However, original author may agree new author updated \\(M\\). disputes substantive.offer five rules resolving disputes changes \\(\\), \\(D\\), \\(\\).Replacing alternative practices justified design simulation\n1. M always changes! (information \\(\\tau\\) \\(sd(\\tau)\\))\n2. Home ground dominance: Change D--\\(^\\prime\\) > M\n3. Robustness alternative models: Change D--\\(^\\prime\\) \\(\\geq\\) M \\(^\\prime\\) > M\\(^\\prime\\) E.g. change simple complete RA\n4. Model plausibility: \\(^\\prime\\) < M \\(^\\prime\\) > M\\(^\\prime\\), change \\(^\\prime\\) D--IFF M\\(^\\prime\\) plausible M E.g. switching balanced design believe variances equal across treatment groups\n5. Undefined inquiries. Change \\(^\\prime\\) undefined M defined M: can’t change \\(^\\prime\\), can’t change D D\\(^\\prime\\) means unidentifiable.","code":""},{"path":"after.html","id":"synthesis","chapter":"27 After","heading":"27.6 Synthesis","text":"One last, last, stage lifecycle research design eventual incorporation common scientific understanding world. Research findings specific – specific \\(^D\\)s need synthesized broader scientific understanding. research synthesis comprises new research design summarizes past research.Research synthesis takes two basic forms. first meta-analysis, series \\(^D\\)s analyzed together order better understand features distribution answers obtained literature. Traditional meta-analysis typically focuses average k answers: \\(a_1^D\\),\\(a_2^D\\),…\\(a_k^D\\). Studies can averaged together many ways better worse. Sometimes answers averaged together according precision – precision weighted average estimates many studies equivalent fixed-effects meta-analysis. Sometimes studies “averaged” counting many estimates positive significant, many negative significant, many null. typical averaging approach taken literature review. Regardless averaging approach, goal kind synthesis learn much possible particular \\(\\) drawing evidence many studies.second kind synthesis attempt bring together many \\(^D\\), targets different inquiry common model. kind synthesis takes place across entire research literature. Different scholars focus different nodes edges common model, synthesis needs incorporate diverse sources evidence.can best anticipate research findings synthesized? first kind synthesis – meta-analysis – must cognizant keeping commonly understood \\(\\) mind. want select inquiries novelty, commonly-understood importance. want many studies effects women versus men elected officials public goods want understand particular \\(\\) great detail specificity. specifics model \\(M\\) might differ study study, fact \\(\\)s similar enough synthesized allows specific kind knowledge accumulation.second kind synthesis – literature-wide progress full causal model – even greater care required. Specific studies make bespoke models \\(M\\) instead must understand specific \\(M\\) adopted study special case master \\(M\\) principle agreed wider research community. nonstop, neverending proliferation study-specific theories threat kind knowledge accumulation.\nDeclaring diagnosing properties meta design can informative planning individual study. first step every research synthesis process collecting past studies. Search strategies sampling strategies, can biased ways convenience samples individuals. Conducting Census past literature topic impossible: much research conducted published yet published. Selecting studies major journals alone may induce additional publication bias sample. Collecting working papers soliciting unpublished abandoned research topic strategies mitigate risks. choice answer strategy research synthesis typically driven assumptions model studies related contexts units within selected. model declaring research synthesis thus must include assumptions studies reach synthesizer, contexts units selected original studies. Diagnosis can help assess conditions analysis strategies provide unbiased, efficient estimates true effects either subset contexts studies’r flagit()’(CONFUSED “studies”) broader population.","code":""},{"path":"part-iv-exercises.html","id":"part-iv-exercises","chapter":"28 Part IV Exercises","heading":"28 Part IV Exercises","text":"written.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"Abell, Peter, Ofer Engel. 2019. “Subjective Causality Counterfactuals Social Sciences: Toward Ethnographic Causality?” Sociological Methods & Research, 0049124119852373.Alrababa’h, Ala’, Scott Williamson, Andrea Dillon, Jens Hainmueller, Dominik Hangartner, Michael Hotard, David Laitin, Duncan Lawrence, Jeremy Weinstein. 2020. “Learning Null Effects: Bottom-Approach.” SocArXiv. https://doi.org/10.31235/osf.io/5ebpy.Angrist, Joshua D., Jörn-Steffen Pischke. 2008. Mostly Harmless Econometrics: Empiricist’s Companion. Princeton: Princeton University Press.Aronow, Peter M., Donald P. Green, Donald K. K. Lee. 2014. “Sharp Bounds Variance Randomized Experiments.” Annals Statistics 42 (3): 850–71.Aronow, Peter M., Cyrus Samii. 2016. “Regression Produce Representative Estimates Causal Effects?” American Journal Political Science 60 (1): 250–67.Ashworth, Scott, Joshua D. Clinton, Adam Meirowitz, Kristopher W. Ramsay. 2008. “Design, Inference, Strategic Logic Suicide Terrorism.” American Political Science Review 102 (2): 269–73.Baird, Sarah, J. Aislinn Bohren, Craig McIntosh, Berk Ozler. 2018. “Optimal Design Experiments Presence Interference.” Review Economics & Statistics 5 (100): 844–60.Banerjee, Abhijit, Esther Duflo, Amy Finkelstein, Lawrence F Katz, Benjamin Olken, Anja Sautmann. 2020. “Praise Moderation: Suggestions Scope Use Pre-Analysis Plans Rcts Economics.” Working Paper 26993. Working Paper Series. National Bureau Economic Research. https://doi.org/10.3386/w26993.Bansak, Kirk, Jens Hainmueller, Daniel J. Hopkins, Teppei Yamamoto. 2019. “Beyond Breaking Point? Survey Satisficing Conjoint Experiments.” Political Science Research Methods, 1–19. https://doi.org/10.1017/psrm.2019.13.Baron, Hannah, Lauren Young. 2020. “Principles Practice: Methods Increasing Transparency Research Ethics Violent Contexts.”Baumgartner, Michael, Alrik Thiem. 2017. “Often Trusted Never (Properly) Tested: Evaluating Qualitative Comparative Analysis.” Sociological Methods & Research.Bennett, Andrew. 2015. “Appendix.” Process Tracing: Metaphor Analytic Tool, edited Andrew Bennett Jeffrey Checkel. New York: Cambridge University Press.Bennett, Andrew, Jeffrey T Checkel. 2015. Process Tracing. New York: Cambridge University Press.Björkman, Martina, Jakob Svensson. 2009. “Power People: Evidence Randomized Field Experiment Community-Based Monitoring Project Uganda.” Quarterly Journal Economics 124 (2): 735–69.Blair, Graeme, Jasper Cooper, Alexander Coppock, Macartan Humphreys. 2020. “Declaring Diagnosing Research Designs.” American Political Science Review.Blair, Graeme, Alexander Coppock, Margaret Moor. 2020. “Worry Sensitivity Bias: Social Reference Theory Evidence 30 Years List Experiments.” American Political Science Review 114 (4): 1297–1315.Bonilla, Tabitha, Alvin B. Tillery. 2020. “Identity Frames Boost Support Mobilization #Blacklivesmatter Movement? Experimental Test.” American Political Science Review 114 (4): 947–62. https://doi.org/10.1017/S0003055420000544.Brady, Henry E., David Collier. 2010. Rethinking Social Inquiry: Diverse Tools, Shared Standards. Lanham, Maryland: Rowman & Littlefield Publishers.Casey, Katherine, Rachel Glennerster, Edward Miguel. 2012. “Reshaping Institutions: Evidence Aid Impacts Using Pre-Analysis Plan.” Quarterly Journal Economics 127 (4): 1755–1812.Christensen, Garret, Edward Miguel. 2018. “Transparency, Reproducibility, Credibility Economics Research.” Journal Economic Literature 56 (3): 920–80. https://doi.org/10.1257/jel.20171350.Clemens, Michael . 2017. “Meaning Failed Replications: Review Proposal.” Journal Economic Surveys 31 (1): 326–42.Coffman, Lucas C., Muriel Niederle. 2015. “Pre-Analysis Plans Limited Upside, Especially Replications Feasible.” Journal Economic Perspectives 29 (3): 81–97.Collier, David. 2011. “Understanding Process Tracing.” PS: Political Science & Politics 44 (4): 823–30.Collier, David, Henry E. Brady, Jason Seawright. 2004. “Sources Leverage Causal Inference: Toward Alternative View Methodology.” Rethinking Social Inquiry: Diverse Tools, Shared Standards, edited David Collier Henry E. Brady. Lanham, Maryland: Rowman; Littlefield.Coppock, Alexander. 2017. “Shy Trump Supporters Bias 2016 Polls? Evidence Nationally-Representative List Experiment.” Statistics, Politics Policy 8 (1): 29–40.Cronbach, Lee J., Karen. Shapiro. 1982. Designing Evaluations Educational Social Programs. San Francisco: Jossey-Bass.Dawid, . Philip. 2000. “Causal Inference Without Counterfactuals.” Journal American Statistical Association 95 (450): 407–24.Deaton, Angus S. 2010. “Instruments, Randomization, Learning Development.” Journal Economic Literature 48 (2): 424–55.Dunning, Thad. 2012. Natural Experiments Social Sciences: Design-Based Approach. Cambridge: Cambridge University Press.Fairfield, Tasha. 2013. “Going Money : Strategies Taxing Economic Elites Unequal Democracies.” World Development 47: 42–57.Fairfield, Tasha, Andrew E. Charman. 2017. “Explicit Bayesian Analysis Process Tracing: Guidelines, Opportunities, Caveats.” Political Analysis 25 (3): 363–80.Fearon, James D, David D Laitin. 2008. “Integrating Qualitative Quantitative Methods.” Oxford Handbook Political Science.Fenno, Richard F. 1978. Home Style: House Members Districts. Pearson College Division.Findley, Michael G., Nathan M. Jensen, Edmund J. Malesky, Thomas B. Pepinsky. 2016. “Can Results-Free Review Reduce Publication Bias? Results Implications Pilot Study.” Comparative Political Studies 49 (13): 1667–1703.Frangakis, Constantine E, Donald B Rubin. 2002. “Principal Stratification Causal Inference.” Biometrics 58 (1): 21–29.Geddes, Barbara. 2003. Paradigms Sand Castles: Theory Building Research Design Comparative Politics. Ann Arbor, Michigan: University Michigan Press.Gelman, Andrew, John Carlin. 2014. “Beyond Power Calculations Assessing Type S (Sign) Type M (Magnitude) Errors.” Perspectives Psychological Science 9 (6): 641–51.Gelman, Andrew, Jennifer Hill. 2006. Data Analysis Using Regression Multilevel/Hierarchical Models. Cambridge: Cambridge University Press.Gelman, Andrew, Guido Imbens. 2017. “High-Order Polynomials Used Regression Discontinuity Designs.” Journal Business & Economic Statistics, . Forthcoming.Gerber, Alan S., Donald P. Green. 2012. Field Experiments: Design, Analysis, Interpretation. New York: W.W. Norton.Gerber, Alan S, Donald P Green. 2012. Field Experiments: Design, Analysis, Interpretation. WW Norton.Gerring, John, Lee Cojocaru. 2016. “Selecting Cases Intensive Analysis: Diversity Goals Methods.” Sociological Methods & Research 45 (3): 392–423.Goertz, Gary, James Mahoney. 2012. Tale Two Cultures: Qualitative Quantitative Research Social Sciences. Princeton: Princeton University Press.Green, Donald P., Winston Lin. 2016. “Standard Operating Procedures: Safety Net Pre-Analysis Plans.” PS: Political Science & Politics 49 (3): 495–99.Gulzar, Saad, Muhammad Yasir Khan. 2020. “Motivating Political Candidacy Performance: Experimental Evidence Pakistan.”Hainmueller, Jens, Daniel J. Hopkins, Teppei Yamamoto. 2014. “Causal Inference Conjoint Analysis: Understanding Multidimensional Choices via Stated Preference Experiments.” Political Analysis 22 (1): 1–30.Halpern, Joseph Y. 2000. “Axiomatizing Causal Reasoning.” Journal Artificial Intelligence Research 12: 317–37.Herron, Michael C., Kevin M. Quinn. 2016. “Careful Look Modern Case Selection Methods.” Sociological Methods & Research 45 (3): 458–92.Huber, John. 2013. “Theory Getting Lost ‘Identification Revolution’?”Humphreys, Macartan. 2015. “Reflections Ethics Social Experimentation.” Journal Globalization Development 6 (1): 87–112.Humphreys, Macartan, Alan M. Jacobs. 2015. “Mixing Methods: Bayesian Approach.” American Political Science Review 109 (4): 653–73.Humphreys, Macartan, Raul de la Sierra, Peter van der Windt. 2013. “Fishing, Commitment, Communication: Proposal Comprehensive Nonbinding Research Registration.” Political Analysis 21 (1): 1–20.Imai, Kosuke. 2011. “Multivariate Regression Analysis Item Count Technique.” Journal American Statistical Association 106 (494): 407–16.Imai, Kosuke, Gary King, Clayton Nall. 2009. “Essential Role Pair Matching Cluster-Randomized Experiments, Application Mexican Universal Health Insurance Evaluation.” Statistical Science 24 (1): 29–53.Imai, Kosuke, Gary King, Elizabeth . Stuart. 2008. “Misunderstandings Experimentalists Observationalists Causal Inference.” Journal Royal Statistical Society: Series (Statistics Society) 171 (2): 481–502.Imbens, Guido W. 2010. “Better Late Nothing: Comments Deaton (2009) Heckman Urzua (2009).” Journal Economic Literature 48 (2): 399–423.Imbens, Guido W., Donald B. Rubin. 2015. Causal Inference Statistics, Social, Biomedical Sciences. Cambridge: Cambridge University Press.Ingram, Matthew C, Imke Harbers. 2020. “Spatial Tools Case Selection: Using Lisa Statistics Design Mixed-Methods Research.” Political Science Research Methods 8 (4): 747–63.Jamison, Julian C. 2019. “Entry Randomized Assignment Social Sciences.” Journal Causal Inference 7 (1): 1–16.Karlan, Dean, Jacob Appel. 2018. Failing Field: Can Learn Field Research Goes Wrong. Princeton University Press.King, Gary, Emmanuela Gakidou, Nirmala Ravishankar, Ryan T Moore, Jason Lakin, Manett Vargas, Martha Marı́Téllez-Rojo, Juan Eugenio Hernández Ávila, Mauricio Hernández Ávila, Héctor Hernández Llamas. 2007. “‘Politically Robust’ Experimental Design Public Policy Evaluation, Application Mexican Universal Health Insurance Program.” Journal Policy Analysis Management 26 (3): 479–506.Kling, Jeffrey R, Jeffrey B Liebman, Lawrence F Katz. 2007. “Experimental Analysis Neighborhood Effects.” Econometrica 75 (1): 83–119.Lieberman, Evan S. 2005. “Nested Analysis Mixed-Method Strategy Comparative Research.” American Political Science Review 99 (3): 435–52.Lohr, Sharon. 2010. Sampling: Design Analysis. Boston: Brooks Cole.Luft, Aliza. 2020. “Repair Broken World? Conflict(ing) Archives Holocaust.” Qualitative Sociology 43 (3): 317–43.Lyall, Jason. 2020. “Preregister Ethical Redlines.”Mahoney, James. 2012. “Logic Process Tracing Tests Social Sciences.” Sociological Methods Research 41 (4): 570–97.Mahoney, James, Gary Goertz. 2004. “Possibility Principle: Choosing Negative Cases Comparative Research.” American Political Science Review 98 (4): 653–69.Martin, Lisa. 1992. Coercive Cooperation: Explaining Multilateral Economic Sanctions. Princeton: Princeton University Press.Meyer, Michelle N. 2015. “Two Cheers Corporate Experimentation: /B Illusion Virtues Data-Driven Innovation.” Colorado Technology Law Journal 13: 273.Middleton, Joel . 2008. “Bias Regression Estimator Experiments Using Clustered Random Assignment.” Statistics & Probability Letters 78 (16): 2654–9.Miguel, Edward, Colin Camerer, Katherine Casey, Joshua Cohen, Kevin M Esterling, Alan Gerber, Rachel Glennerster, et al. 2014. “Promoting Transparency Social Science Research.” Science 343 (6166): 30.Mill, John Stuart. 1884. System Logic, Ratiocinative Inductive: Connected View Principles Evidence Methods Scientific Investigation. Harper.Miller, Judith Droitcour. 1984. “New Survey Technique Studying Deviant Behavior.” PhD thesis, George Washington University.Montalvo, Jose G. 2011. “Voting Bombings: Natural Experiment Effect Terrorist Attacks Democratic Elections.” Review Economics Statistics 93 (4): 1146–54.Morris, Tim P., Ian R. White, Michael J. Crowther. 2019. “Using Simulation Studies Evaluate Statistical Methods.” Statistics Medicine.Mousa, Salma. 2020. “Building Social Cohesion Christians Muslims Soccer Post-Isis Iraq.” Science 369 (6505): 866–70.Nosek, Brian ., George Alter, George C. Banks, Denny Borsboom, Sara D. Bowman, Steven J. Breckler, Stuart Buck, et al. 2015. “Promoting Open Research Culture: Author Guidelines Journals Help Promote Transparency, Openness, Reproducibility.” Science 348 (6242): 1422.Ofosu, George K., Daniel N. Posner. 2020. “Pre-Analysis Plans Hamper Publication?” AEA Papers Proceedings 110 (May): 70–74.Olken, Benjamin . 2015. “Promises Perils Pre-Analysis Plans.” Journal Economic Perspectives 29 (3): 61–80.Pearl, Judea. 1999. “Probabilities Causation: Three Counterfactual Interpretations Identification.” Synthese 121 (1-2): 93–149.———. 2009. Causality: Models, Reasoning Inference. Second Edition. Cambridge: Cambridge University Press.Pearl, Judea, Dana Mackenzie. 2018. Book : New Science Cause Effect. Basic Books.Ragin, Charles. 1987. Comparative Method. Moving Beyond Qualitative Quantitative Strategies. Berkeley: University California Press.Reiss, Peter C, Frank Wolak. 2007. “Structural Econometric Modeling: Rationales Examples Industrial Organization.” Handbook Econometrics 6: 4277–4415.Rennie, Drummond. 2004. “Trial registration.” JAMA: Journal American Medical Association 292 (11): 1359–62.Rohlfing, Ingo. 2018. “Power False Negatives Qualitative Comparative Analysis: Foundations, Simulation Estimation Empirical Studies.” Political Analysis 26 (1): 72–89.Rosenbaum, Paul R. 2002. Observational Studies. New York: Springer.Rubin, Donald B. 1984. “Bayesianly Justifiable Relevant Frequency Calculations Applied Statistician.” Annals Statistics 12 (4): 1151–72.Rubinstein, Ariel. 1982. “Perfect Equilibrium Bargaining Model.” Econometrica: Journal Econometric Society, 97–109.Seawright, Jason, John Gerring. 2008a. “Case Selection Techniques Case Study Research: Menu Qualitative Quantitative Options.” Political Research Quarterly 61 (2): 294–308.———. 2008b. “Case Selection Techniques Case Study Research: Menu Qualitative Quantitative Options.” Political Research Quarterly 61 (2): 294–308.Sekhon, Jasjeet S. 2004. “Quality Meets Quantity: Case Studies, Conditional Probability, Counterfactuals.” Perspectives Politics 2 (2): 281–93.Shadish, William, Thomas D. Cook, Donald Thomas Campbell. 2002. Experimental Quasi-Experimental Designs Generalized Causal Inference. Boston: Houghton Mifflin.Sinclair, Betsy, Margaret McConnell, Donald P. Green. 2012. “Detecting Spillover Effects: Design Analysis Multilevel Experiments.” American Journal Political Science 56 (4): 1055–69.Skocpol, Theda. 1979. States Social Revolutions: Comparative Analysis France, Russia China. Cambridge University Press.Swank, Duane. 2002. Global Capital, Political Institutions, Policy Change Developed Welfare States. New York: Cambridge University Press.Teele, Dawn. 2021. “Virtual Consent: Bronze Standard Experimental Ethics.” Cambridge Handbook Experimental Political Science, edited Donald P. Green James N. Druckman. Cambridge University Press.Van Evera, Stephen. 1997. Guide Methods Students Political Science. Ithaca: Cornell University Press.Weingast, Barry R. 1998. “Political Stability Civil War: Institutions, Commitment, American Democracy.” Analytic Narratives, edited Robert H. Bates, Avner Greif, Margaret Levi, Jean-Laurent Rosenthal, Barry R. Weingast, 148–93. Princeton University Press.White, Ariel R., Noah L. Nathan, Julie K. Faller. 2015. “Need Vote? Bureaucratic Discretion Discrimination Local Election Officials.” American Political Science Review 109 (1): 129–42.Wilke, Anna, Macartan Humphreys. 2020. “Field Experiments, Theory, External Validity.” SAGE Handbook Research Methods Political Science; International Relations.Yamamoto, Teppei. 2012. “Understanding Past: Statistical Analysis Causal Attribution.” American Journal Political Science 56 (1): 237–56.Zarin, Deborah ., Tony Tse. 2008. “Moving Towards Transparency Clinical Trials.” Science 319 (5868): 1340–2.Zhang, Junni L., Donald B. Rubin. 2003. “Estimation Causal Effects via Principal Stratification Outcomes Truncated ‘Death’.” Journal Educational Behavioral Statistics 28 (4): 353–68.","code":""}]
