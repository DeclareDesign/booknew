[{"path":"index.html","id":"research-design-declaration-diagnose-redesign","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"Research Design: Declaration, Diagnose, Redesign","text":"","code":""},{"path":"index.html","id":"graeme-blair-jasper-cooper-alexander-coppock-and-macartan-humphreys","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"Graeme Blair, Jasper Cooper, Alexander Coppock, and Macartan Humphreys","text":"","code":""},{"path":"index.html","id":"draft-manuscript-under-advance-contract-princeton-university-press","chapter":"Research Design: Declaration, Diagnose, Redesign","heading":"Draft manuscript under advance contract, Princeton University Press","text":"","code":""},{"path":"preamble.html","id":"preamble","chapter":"1 Preamble","heading":"1 Preamble","text":"book, introduce way thinking research designs social sciences can make designs transparent robust. hope approach also make designing research studies easier: making easier produce good designs, also easier share designs build designs others developed.core idea start think design object can interrogated. object four main characteristics. understand four also interrelate. design encodes beliefs world, describes questions, lays go answering questions, terms data use use . key idea features can provided code —done right—information provided enough estimate quality design simulation.think way thinking research design pays dividends multiple points research lifecycle: Choosing question, synthesizing previous answers, conducting ethics review, piloting parts empirical strategy, crafting preanalysis plan, implementing study, summarizing results, writing paper, publishing result, archiving materials, engaging critical scholarship.","code":""},{"path":"preamble.html","id":"how-to-read-this-book","chapter":"1 Preamble","heading":"1.1 How to read this book","text":"multiple audiences mind writing book. First, ’re thinking college seniors produce course research paper. students need framework thinking ways part research process fit together. ’re also thinking graduate students seminar courses main purpose course read papers discuss well theory matches empirics. MIDA framework introduced Part way structure tasks accommodates many different empirical approaches: qualitative quantitative, descriptive causal, observational experimental. 30 minutes person try get understand book , give Part .Part II involved. provide mathematical foundations MIDA framework. walk though component research design detail: Models, Inquiries, Data strategies, Answer Strategies. describe finer points research design diagnosis carry “redesign.” imagine Part II assigned early graduate course research design social sciences.Part III, apply general framework specific research designs. result library common research designs. Many empirical research designs included library, . set entries covers large portion see current empirical practice, meant exhaustive. thinking two kinds audiences design library. first researcher wants know specific research design operates watch . turn ``regression discontinuity’’ entry learn ten important things know design. 10 important things everything know design. , refer end entry set --date methodological treatments topic. entry help simulate design’s properties explore design tradeoffs implementing . second reader mind person studying entry learn specifics regression discontinuity design necessarily, instead learn become research designer. regression discontinuity entry instance research design general lessons research design can drawn. Readers want research designers read design library entirety.last section book describes detail framework can help step research process. sections book readable anyone read Part . entry preanalysis plans can assigned experiments course guidance students filing first PAP. entry research ethics shared among coauthors start project. entry writing research paper assigned college seniors trying finish essay time.","code":""},{"path":"preamble.html","id":"how-to-work-this-book","chapter":"1 Preamble","heading":"1.2 How to work this book","text":"many times throughout book, describe research designs just words, computer code.\n\nwant work though code exercises, fantastic. path requires investment R, tidyverse DeclareDesign sets software packages. Chapter 3 helps get started. think way rewarding, understand learning curve. , course, tackle declaration, diagnosis, redesign processes using bespoke simulations computer language like ,1 easier DeclareDesign.want nothing code, can skip code exercises just focus text. written book understanding code required order understand research design concepts.\nfree, online version book many extra ways engage material. can download section code can play interactive simulations. bells whistles necessary understanding, learn different ways.","code":""},{"path":"preamble.html","id":"what-this-book-will-not-do","chapter":"1 Preamble","heading":"1.3 What this book will not do","text":"book research design, statistics textbook. derive estimators, provide guarantees general optimality designs, present mathematical proofs. provide answer practical questions might design.offer, hope, language express research designs. can help learn language can describe design . can declare design language, can diagnose , improve redesign.","code":""},{"path":"improving-research-designs.html","id":"improving-research-designs","chapter":"2 Improving research designs","heading":"2 Improving research designs","text":"book offers language describing research designs algorithm assessing properties. Together, language algorithm help researchers address two main problems. First, select high quality research designs can relied upon generate credible answers research questions. Without way measure quality design, ’s difficult choose strong ones weak ones. Second, researchers need communicate designs others. Without way describe design detail, ’s difficult explain scholars high quality right design question.language research design helps problems. design “declared” simple enough language computer can understand , can implement “diagnosis” algorithm assessing properties design click button. process makes easy compare among alternative designs pick strong one. Remarkably, basic structure can used whether researchers interested causal questions descriptive questions, whether focused theory testing inductive learning, whether use quantitative, qualitative, mixed methods. language use talk computer can used talk others. Reviewers, advisors, students, funders, journalists, public – yes computer – need know four basic things understand design.","code":""},{"path":"improving-research-designs.html","id":"the-four-components-of-research-design","chapter":"2 Improving research designs","heading":"2.1 The four components of research design","text":"Empirical research designs share common model, inquiry, data strategy, answer strategy. four together, refer MIDA, represent suppositions world works choices make researcher intervene learn world.model theory system study. includes causal beliefs causes . includes beliefs important variables distributed, things correlated, sequences events. course know true model—research. ’s point. point stipulating model can assess analysis perform model fact correct. practice, models describe sets models, hope world (real model!) sufficiently “like” models set.model defines set units, people neighborhoods social groups, wish study. Often, set units large population units afford take measurements , can nevertheless define make inferences sampling studying subset units. model includes set baseline characteristics describe unit probability distributions characteristic (.e., heights normally distributed, skew comes stunting infants). Finally, model includes set endogenous outcome variables may functions exogenous (pretreatment) characteristics effects interventions. endogenous outcome variable function defines variables affect values takes . outcome depends intervention, potential outcome, can define value outcome take unit received treatment outcome unit take receive treatment. Typically, endogenous outcome variables random variables, either function exogenous baseline variable defined probability distributions whether unit assigned treatment control randomly assigned part data strategy.Defining model can feel like odd exercise. Since researchers presumably want learn model, declaring advance may seem beg question. Yet declaring model often unavoidable declaring research designs. practice, already familiar researcher calculated power design, requires specification effect sizes. seeming arbitrariness declared model can mitigated assessing sensitivity diagnosis alternative models strategies (see Section 10). , researchers can inform substantive models existing data, baseline surveys. Just power calculators focus attention minimum detectable effects, design declaration offers tool demonstrate design properties change depending researcher assumptions.model research question. research question – ’re calling inquiry – feature model want measure. theories rich, single model, may many possible inquiries researcher seek learn . model may complex dance 10 interrelated variables, inquiry something like average causal effect single variable another, descriptive distribution third variable, prediction value single variable take time future.Inquiries may causal, average treatment effect, descriptive, proportion units hold certain characteristic. inquiry function exogenous characteristics units, endogenous outcome variables, . may defined units population defined model, average treatment effect units, may defined subset units, conditional average treatment effect women. defined distribution variables model, can define probability distribution inquiries, function variables.Together, model data form theoretical half research design. second half empirical, two components mirror theoretical half.data strategy full set procedures use gather information world. includes three basic sets procedures: sampling, assignment, measurement. Sampling refers fact empirical strategy comprehensive – units sampled study units aren’t. Even research designs (like census, example) sampling strategy don’t sample respondents different years different countries. Assignment procedures describe researchers generate variation world. ask subjects one question, subjects different question, ’ve generated variation basis assignment procedure. think assignment procedures often randomized, randomized experiment, many kinds research designs engage assignment procedures randomized. Measurement procedures ways researchers reduce complex multidimensional social world relatively parsimonious set data. data need “quantitative data” sense numbers values pre-defined scale – qualitative data data . Measurement vexing necessary reduction reality choice representations. Measurement always carries possibility measurement error, reduction hard.answer strategy summarize data data strategy produces. Just like inquiry summarizes part model, answer strategy summarizes data. Complex, multidimensional datasets don’t just speak – need summarized explained. can think answer strategy function takes data returns answers. research designs, literal function like lm_robust estimates ordinary least squares regression robust standard errors. research designs, function embodied researcher themself read case study documents summarize meaning.answer strategy choice estimator. includes full set procedures receiving dataset providing answer words, tables, graphs. includes data cleaning, data transformation, estimation, plotting, interpretation. choice OLS must defined, focus attention coefficient estimate Z variable assess uncertainty using confidence interval construct coefficient plot certain way visualize inference. answer strategy also includes -procedures researchers implicitly explicitly take depending initial results features data. stepwise regression procedure, answer strategy final regression results iterative model selection, procedure – answer depend features change depending sampling variability. Just like values inquiry, values estimates result answer strategy probability distribution, result variables defined model (probability distributions) data strategy (sampling treatment assignment defined probability distributions).theoretical empirical halves research design go hand--hand description research design complete unless halves communicated. ask, “’s research design?” respond “’s regression discontinuity design,” ’ve maybe learned answer strategy might , don’t yet enough information decide whether ’s strong design learn model inquiry whether data answer strategies indeed well suited .Declaring design entails separating parts idea belong \\(M\\), \\(\\), \\(D\\), \\(\\). declaration process can challenge mapping ideas excitement project MIDA always straightforward. promise rewarding task. can express research design terms four components newly able think properties.","code":""},{"path":"improving-research-designs.html","id":"example","chapter":"2 Improving research designs","heading":"2.1.1 Example","text":"illustrate application MIDA framework study motivations political office-seekers Pakistan. Gulzar Khan (n.d.) conducted experiment test whether prosocial personal benefits important motivations running office. randomly-assigned villages receive different encouragements run measured affected rate running office, types people chose run, congruence policy positions general population.model describes citizens villages conduct study, village live . describes individual characteristics potential outcomes response possible treatment assignments. inquiry average treatment effect receiving receiving encouragement run office. data strategy (1) randomly sampling 50 citizens per village; (2) random assignment villages either encouragement run office (treatment) encouragement (control); (3) measurement via posttreatment survey. answer strategy calculating difference--means, standard errors accounting clustering treatment assignment village.","code":""},{"path":"improving-research-designs.html","id":"declaring-a-design-in-code","chapter":"2 Improving research designs","heading":"2.2 Declaring a design in code","text":"can implement MIDA framework software package. Indeed, design declared writing mathematical notation diagnosed using analytical formula. discuss Section 10 analytical diagnoses challenging majority designs social sciences: account specific features research designs varying numbers units per cluster interaction choices made model, inquiry, data strategy, answer strategy.Social scientists use number tools conducting statistical analysis: Stata, R, Python, Julia, SPSS, SAS, Mathematica, . time writing, Stata R commonly used platforms academic work, though may change future. wrote companion software book, DeclareDesign, R statistical environment availability tools implementing research designs free, open-source, high-quality.illustrate declaring design code, declare simplified version Gulzar Khan (n.d.) study – declare complete design Section 5.1.model, describe hierarchical structure 100 villages\n100 villages, home somewhere 20 100 citizens. citizens harbors two potential outcomes, Y_Z_0 Y_Z_1. Y_Z_0 citizen’s latent likehood standing election untreated; Y_Z_1 latent probability treated. simplified model includes constant treatment effect 0.025 units latent scale (2.5 percentage points) units.inquiry average treatment effect population, defined average difference potential outcomes:data strategy consists 4 steps:assignment: randomly assign one third neutral condition, one third personal appeal, remaining third social appeal.reveal outcomes: Subjects reveal outcomes (.e., probability running office) according randomly-assigned condition.measurement: measure binary outcome Yobs unobserved, latent variable Y: units latent outcome 0.95 Yobs 1 0 otherwise.answer strategy consists difference--means Y depending Z, standard errors clustered village level order respect clustering assignment data stategy. link estimator ATE explicitly link answer strategy inquiry.concatenate four aspects declare complete design:design declare book, also present graphical representation design. Figure 2.1, visualize simplified form Gulzar-Khan design.\nFigure 2.1: Simplified DAG Gulzar Khan study\ndeclared design, can learn number ways:can “diagnose” simulate properties via diagnose_design(design)can simulate data explore possible estimation strategies analyzing real data draw_data(design)can obtain simulated estimates via draw_estimates(design), data comes ,can apply planned design data via get_estimates(design, data = study_data).introduce software detail next section.designed rest book can read even use R, translate code language choice. Web site, translation parts declaration diagnosis process Stata, Python, Excel. addition, link “Design wizard” lets declare diagnose variations standard designs via web interface.","code":"\nmodel <-\n  declare_population(\n    villages = add_level(N = 192),\n    citizens = add_level(N = 48, U = runif(N))\n  ) +\n  declare_potential_outcomes(Y_Z_neutral = U,\n                             Y_Z_personal = Y_Z_neutral + 0.02,\n                             Y_Z_social = Y_Z_neutral + 0.03)\ninquiry <- declare_estimand(ATE_personal = mean(Y_Z_personal - Y_Z_neutral),\n                            ATE_social = mean(Y_Z_social - Y_Z_neutral))\ndata_strategy <-\n  declare_assignment(clusters = villages, conditions = c(\"neutral\", \"personal\", \"social\")) + \n  declare_reveal(outcome_variables = Y, assignment_variables = Z) +\n  declare_measurement(Yobs = if_else(Y > 0.97, 1, 0))\nanswer_strategy <- \n  declare_estimator(Y ~ Z, term = c(\"Zpersonal\", \"Zsocial\"), clusters = villages, \n                    model = lm_robust,\n                    estimand = c(\"ATE_personal\", \"ATE_social\"))\ndesign <- model + inquiry + data_strategy + answer_strategy"},{"path":"improving-research-designs.html","id":"assessing-research-design-quality-design-diagnosis","chapter":"2 Improving research designs","heading":"2.3 Assessing research design quality: design diagnosis","text":"’ve declared design, can diagnose . Design diagnosis process simulating research design order understand range possible ways study turn . diagnosis stage define design properties desirable research setting. let computers simulations us imagining design choices influence sampling distributions — put lightly — cognitively demanding.Diagnosis opportunity write make study success. long time, researchers classified studies successful based statistical significance. Accordingly, statistical power (probability statistically significant result) front--mind diagnosand researchers set designing studies. learn pathologies relying statistical significance, learn diagnosands beyond power just , important. example, “credibility revolution” throughout social science trained laser-like focus bias diagnosand. Studies coming new criticism lacking “strong identification,” usually implies data answer strategies lead biased answers depending incorrect model . Randomized experimentation promises unbiased answers inquiries, least data answer strategies implemented well.Design diagnosis relies two concepts, functions research designs. quantities researcher third party calculate respect design.first diagnostic statistic, summary statistic generated “run” design—, results given possible realization variables, given model data strategy. example statistic \\(e\\) refers “difference estimated actual average treatment effect” . \\(e\\) statistics depends model (since ATE depends model’s assumptions potential outcomes). statistic \\(s = \\mathbb{1}(p \\leq 0.05)\\), interpreted “result considered statistically significant 5% level,” presupposes answer strategy reports \\(p\\)-value. Diagnostic statistics governed probability distributions arise model data generation, given model, may stochastic.Second, diagnosand summary distribution diagnostic statistic. example, bias average value \\(e\\) statistic power average value \\(s\\) statistic. diagnosands include things like root-mean-squared-error (RMSE), Type , Type II, Type M, Type S error rates.One especially important diagnosand “success rate,” average value “success” diagnostic statistic. researcher, get decide make study success. matters research scenario? statistical significance? , optimize design respect power. matters research setting answer correct sign ? diagnose frequently answer strategy yields answer sign inquiry. Diagnosis opportunity articulate make study success simulate often obtain success. Often, success multidimensional aggregation multiple diagnosands.diagnose design, first define set diagnosands (see Section 10), statistical properties design. case, select bias (difference estimate estimand, PATE); root mean-squared error; statistical power design.diagnose design, involves simulating design , calculate diagnosands based simulations data.\nTable 2.1: Diagnosis simplified Gulzar-Khan design.\n","code":"\n# Select diagnosands\ndiagnosands <- declare_diagnosands(\n  bias = mean(estimate - estimand),\n  rmse = sqrt(mean((estimate - estimand)^2)),\n  power = mean(p.value <= 0.05),\n  keep_defaults = FALSE\n)\n# Diagnose the design\ndiagnosis <- diagnose_design(design, diagnosands = diagnosands, sims = sims)"},{"path":"improving-research-designs.html","id":"redesign","chapter":"2 Improving research designs","heading":"2.4 Redesign","text":"subtitle book “Declaration, Diagnosis, Redesign” emphasize three important steps conceptualization research design. far, ’ve outlined first two points: declaration diagnosis. design declared, learned diagnose respect important diagnosands, last step redesign.Redesign entails playing design parameters understand implications important diagnosands. can mean variety things. Many diagnosands (power, RMSE) depend size study. can redesign study, varying “sample size” feature data strategy determine big needs achieve target diagnosand: 90% power, say, RMSE 0.02. also vary aspect answer strategy, say, covariates used adjust regression model. Sometimes changes data answer strategies interact: use better covariates increase precision estimates answer strategy, collect information part data strategy. redesign question now becomes, better collect pre-treatment information subjects money better spent increasing total number subjects? Finally, redesign sometimes means changing model. , sometimes want understand whether design yields right inferences even underlying data generating processes shift beneath feet. summary, redesign entails enumerating set possible designs given resource theoretical constraints picking best one.DeclareDesign, redesign() function replaces key inputs design form new design.","code":""},{"path":"improving-research-designs.html","id":"putting-designs-to-use","chapter":"2 Improving research designs","heading":"2.5 Putting designs to use","text":"book, asking scholars add new step workflow. want researchers formally declare diagnose designs order learn improve . Much work declaring diagnosing designs already part social scientists conduct research: grant proposals, IRB protocols, preanalysis plans, dissertation prospectuses contain design information justifications design appropriate question. lack common language describe designs properties, however, seriously hampers utility practices assessing improving design quality. inclusion declaration-diagnosis-redesign step research process can help address basic difficulty.outline three phases scientific process MIDA declaration-diagnosis-redesign framework can assist study authors, readers, research funders.Making design choices. move towards increasing credibility research social sciences places premium considering alternative data strategies analysis strategies early stages research projects, reduces researcher discretion, importantly can improve quality final research design. nothing new idea determining features sampling estimation strategies ex ante, practice many designs finalized late research process, data collected. Frontloading design decisions difficult existing tools rudimentary often misleading, clear current practice features design must considered ex ante.provide framework identifying features affect assessment design’s properties, declaring designs diagnosing inferential quality, frontloading design decisions. Declaring design’s features code enables direct exploration alternative data analysis strategies using simulated data; evaluating alternative strategies diagnosis; exploring robustness chosen strategy alternative models. Researchers can undertake step study implementation data collection.Communicating design choices. Bias published results can arise many reasons. example, researchers may deliberately inadvertently select analysis strategies produce statistically significant results. Proposed solutions reduce kind bias focus various types preregistration analysis strategies researchers (Rennie 2004; Zarin Tse 2008; Casey, Glennerster, Miguel 2012; Nosek et al. 2015; Green Lin 2016). Study registries now operating numerous areas social science, including hosted American Economic Association, Evidence Governance Politics, Center Open Science. Bias may also arise reviewers basing publication recommendations statistical significance. Results-blind review processes introduced journals address form bias (e.g., Findley et al. 2016).However, effectiveness design registries results-blind review reducing scope either form publication bias depends clarity elements must included describe design. practice, registries rely checklists preanalysis plans exhibit great variation, ranging lists written hypotheses --results journal articles. view, solution problem lie ever--specific questionnaires, rather new way characterizing designs whose analytic features can diagnosed simulation.actions taken researchers described data strategy answer strategy; two features design clearly relevant elements preregistration document. order know design choices made ex ante arrived ex post, researchers need communicate data answer strategies unambiguously. However, assessing whether data answer strategies good requires specifying model inquiry. Design declaration can clarify researchers third parties aspects study need specified order meet standards effective preregistration.Declaration design code also enables final infrequently practiced step registration process, researcher ``reports reconciles’’ final planned analysis. Identifying whether features design diverge ex ante ex post declarations highlights deviations preanalysis plan. magnitude deviations determines whether results considered exploratory confirmatory. present, exercise requires review dozens pages text, differences (similarities) immediately clear even close readers. Reconciliation designs declared code can conducted automatically, comparing changes code (e.g., move use stratified sampling function simple random sampling) comparing key variables design sample sizes.Challenging Design Choices. independent replication results studies publication essential component shift toward credible science. Replication — whether verification, reanalysis original data, reproduction using fresh studies — provides incentives researchers clear transparent analysis strategies, can build confidence findings.2In addition rendering design transparent, design declaration can allow different approach re-analysis critique published research. standard practice replicators engaging reanalysis propose range alternative strategies assess robustness data-dependent estimates different analyses. problem approach , divergent results found, third parties clear grounds decide results believe. issue compounded fact , changing analysis strategy, replicators risk departing estimand original study, possibly providing different answers different questions. worst case scenario, can difficult determine learned original study replication.coherent strategy facilitated design simulations use design declaration conduct “design replication.” design replication, scholar restates essential design characteristics learn study revealed, just original author reports revealed. procedure helps us understand conditions results study can believed. emphasizing abstract properties design, design replication provides grounds support alternative analyses basis original authors’ intentions basis degree divergence results. Conversely, provides authors grounds question claims made critics.","code":""},{"path":"improving-research-designs.html","id":"avoiding-declaration-and-diagnosis-pitfalls","chapter":"2 Improving research designs","heading":"2.6 Avoiding declaration and diagnosis pitfalls","text":"Declaring design just like writing recipe. can cook without writing recipe, , can think whole process start finish, can critique process, can modify . said, designing high quality research difficult comes many pitfalls, subset addressed MIDA framework. Others fail help entirely , cases, may even exacerbate . outline four concerns.first worry evaluative weight get placed essentially meaningless diagnoses. Given design declaration includes declarations conjectures world possible choose inputs design passes diagnostic test set . instance, simulation-based claim unbiasedness incorporates features design still good respect precise conditions simulation (contrast, analytic results, available, may extend general classes designs). Still worse, simulation parameters might selected properties. power analysis, instance, may useless implausible parameters chosen raise power artificially. framework may encourage honest declarations, nothing enforces . ever, garbage-, garbage-.Second, see risk research may get evaluated basis narrow, perhaps inappropriate set diagnosands. Statistical power often invoked key design feature – even well-powered studies biased away targets interest little theoretical use. appropriateness diagnosand depends purposes study. framework guide researchers critics appropriate set diagnosands evaluate design. advantage approach however choice diagnosands gets highlighted new diagnosands can generated response substantive concerns.Third, emphasis statistical properties design can obscure substantive importance question answered qualitative features design. similar concern raised regarding ``identification revolution’’ focus identification risks crowding attention importance questions addressed Huber (2013). framework can help researchers determine whether particular design answers question well (), also nudges make sure questions defined clearly independently answer strategies. , however, help researchers choose good questions.Finally, see risk variation suitability design declaration different research strategies may taken evidence relative superiority different types research strategies. believe range strategies can declared diagnosed wider one might first think possible, strong designs might declarable, either ex ante ex post. advantage framework, hope, can help clarify strategy can completely declared. design declared, nondeclarability framework provides, cases urge caution drawing conclusions design quality.specification model, inquiry, data strategy, answer strategy captures analysis-relevant features design, describe substantive elements, theories derived interventions implemented. Yet many aspects design explicitly labeled features enter framework analytically relevant. example, logistical details data collection duration time treatment administered endline data collection enter model longer time data collection affects subject recall treatment. However, information design declaration typically insufficient assess substantive elements, important separate part assessing quality research study.","code":""},{"path":"improving-research-designs.html","id":"further-reading","chapter":"2 Improving research designs","heading":"2.7 Further Reading","text":"Brady Collier (2010)","code":""},{"path":"primer.html","id":"primer","chapter":"3 Software primer","heading":"3 Software primer","text":"section, introduce DeclareDesign R describe use implement step design-diagnose-redesign process.","code":""},{"path":"primer.html","id":"installing-r","chapter":"3 Software primer","heading":"3.1 Installing R","text":"can download statistical computing environment R free CRAN. also recommend free program RStudio, provides friendly interface R.3Once got RStudio installed, open install DeclareDesign related packages. include three packages enable specific steps research process (fabricatr simulating social science data; randomizr, random sampling random assignment; estimatr design-based estimators). can also install DesignLibrary, gets standard designs --running one line. install , can type:also recommend install get know tidyverse suite packages data analysis, use throughout book:introductions R tidyverse especially recommend free resource R Data Science.","code":"\ninstall.packages(c(\"DeclareDesign\", \"fabricatr\", \"randomizr\", \"estimatr\", \"DesignLibrary\"))\ninstall.packages(\"tidyverse\")"},{"path":"primer.html","id":"building-a-step-of-a-research-design","chapter":"3 Software primer","heading":"3.2 Building a step of a research design","text":"research design concatenation steps best way learn build design learn make step. start making—declaring—step implements random assignment.Almost steps take dataset input return dataset output. imagine input data describes set voters Los Angeles. research project planning involves randomly assigning voters receive (receive) knock door canvasser. data look like :\nTable 3.1: Example data\n100 voters dataset.want function takes dataset, implements random assignment, adds dataset, returns new dataset containing random assignment.write function can also use one declare_* functions DeclareDesign designed write functions. one functions kind function factory: takes set parameters research design like number units random assignment probability inputs, returns function output.generating assignment functions can use declare_assignment like :big idea understand object created, simple_random_assignment_step, particular assignment, function creates assignment. particular tidy function summarizes design assignment. can run function data:\nTable 3.2: Data output following implementation assignment step.\noutput expanded dataset new column indicating treatment assignment (Z). bonus data also includes implied assignment propensity unit condition (Z_cond). important thing understand implementing step means taking dataset sending new dataset .parts step declaration may seem little bit odd. First, tell R anything number units dataset. Second, give data. step declaration creates functions meant flexible, function work size dataset. told declare_assignment want assign treatment probability 0.6 (implicitly control probability 1-0.6 = 0.4), regardless large dataset . send declaration data , although assignment function data, assignment function function data. Put differently, assignment function takes data argument, function create assignment function (declare_assignment()) .implement research design conducted , can use exact functions generated design phase. way diagnose design use function many times. one reasons declare assignment step — learn properties design code can actually use randomly assign treatment.Every step research design MIDA can written using one declare_* functions. next section, walk step declare using DeclareDesign.","code":"\nsimple_random_assignment_step <- declare_assignment(prob = 0.6)\nsimple_random_assignment_step(voter_file) "},{"path":"primer.html","id":"options-and-defaults","chapter":"3 Software primer","heading":"3.2.1 Options and defaults","text":"declare_* functions many options. general specify default values usually provided (though familiar default values ). instance might noticed ran assignment step , new variable created called Z. declare_assignment argument assignment_variable defaults Z. can change course whatever want.subtly, declare_* functions also default “handlers” default arguments. handlers generally quite well developed sets functions implement tasks needed declare_ function. instance declare_assignment defaults conduct_ra randomizr package handler passes additional arguments give conduct_ra, , token, assumes default default values handler. example prob = 0.6 argument. look documentation, prob argument declare_assignment argument conduct_ra, default value .5. left bit gotten function assigned treatment probability .5.","code":""},{"path":"primer.html","id":"your-own-handlers","chapter":"3 Software primer","heading":"3.2.2 Your own handlers","text":"built functions provide DeclareDesign package quite flexible handle many major designs framework built never constrained provide. point rather using default handlers (conduct_ra), can write function implements procedures. discipline framework imposes write procedure function takes data send data back.example turn functions design steps.\nTable 3.3: Data generated using custom function\n, course, great difference example custom_assignment my_assignment_step since my_assignment_step just function applies custom_assignment. Even still, worth declaring step formally design step using declare_assignment since lets DeclareDesign know step fits whole design, interpret , call .","code":"\ncustom_assignment <- function(data)\n  mutate(data, X = rbinom(n = nrow(data), 1, prob = 0.5))\n\nmy_assignment_step <- declare_assignment(handler = custom_assignment)\n\nmy_assignment_step(voter_file)  "},{"path":"primer.html","id":"research-design-steps","chapter":"3 Software primer","heading":"3.3 Research design steps","text":"section, walk declare step research design using DeclareDesign. next section, build steps research design, describe interrogate design.","code":""},{"path":"primer.html","id":"model","chapter":"3 Software primer","heading":"3.3.1 Model","text":"model defines structure world, size background characteristics well interventions world determine outcomes. DeclareDesign, split model two main design steps: population potential outcomes steps. ","code":""},{"path":"primer.html","id":"population","chapter":"3 Software primer","heading":"3.3.1.1 Population","text":"population defines number units population, multilevel structure data, background characteristics. can define population several ways.cases, may start design data population. happens, simulate . can simply declare data population:\nTable 3.4: Draw fixed population\ncomplete data population, simulate . Relying data simulation functions fabricatr package, declare_population asks size variables population. instance want function generates dataset 100 units random variable u write:run population function, get different 100-unit dataset time:\nTable 3.5: Three draws population function.\nfabricatr package can simulate many different typs data, including various types categorical variables different types data structures, panel multilevel strucures.can read fabricatr website get started simulating data structure.exmaple simple two-level data structure, imagine setting 100 households random number individuals within household. two level structure declared :Remember, every step research design process, can short-circuit default way things bring code. useful complex design, already written code design want use directly. works setting handler:","code":"\ndeclare_population(data = voter_file)\ndeclare_population(N = 100, u = rnorm(N))\ndeclare_population(\n  households = add_level(N = 100, individuals_per_hh = sample(1:10, N, replace = TRUE)),\n  individuals = add_level(N = individuals_per_hh, age = sample(1:100, N, replace = TRUE))\n)\ncomplex_population_function <- function(data, N_units) {\n  data.frame(u = rnorm(N_units))\n}\n\ndeclare_population(handler = complex_population_function, N_units = 100)"},{"path":"primer.html","id":"potential-outcomes","chapter":"3 Software primer","heading":"3.3.1.2 Potential outcomes","text":"Defining potential outcomes easy single expression per potential outcome. may function background characteristics, potential outcomes, R functions.4\nTable 3.6: Adding potential outcomes population.\nalso simpler interface define potential outcomes function treatment assignment variable. names potential outcomes constructed outcome name (Y lefthand side formula) assignment_variables argument (Z).Either way creating potential outcomes works; one may easier harder code given research design setting.","code":"\ndeclare_potential_outcomes(\n  Y_Z_0 = u, \n  Y_Z_1 = Y_Z_0 + 0.25)\ndes <- declare_population(N = 100, u = rnorm(N)) +\n  declare_potential_outcomes(Y_Z_0 = u, Y_Z_1 = Y_Z_0 + 0.25)\n\ndraw_data(des)\ndeclare_potential_outcomes(Y ~ u + 0.25 * Z, assignment_variables = Z)"},{"path":"primer.html","id":"inquiry","chapter":"3 Software primer","heading":"3.3.2 Inquiry","text":"define inquiry, declare estimand, function background characteristics population, potential outcomes, . define average treatment effect experiment simple design follows:Notice defined PATE (population average treatment effect), said nothing special related population. fact, looks like just defined average treatment effect. define estimand design going determine whether refers population, sample, form estimand. see moment.","code":"\ndeclare_estimand(PATE = mean(Y_Z_1 - Y_Z_0))"},{"path":"primer.html","id":"data-strategy","chapter":"3 Software primer","heading":"3.3.3 Data strategy","text":"data strategy constitutes one steps representing interventions researcher makes world sampling assignment measurement. Typically, may include sampling assignment.","code":""},{"path":"primer.html","id":"sampling","chapter":"3 Software primer","heading":"3.3.3.1 Sampling","text":"sampling step relies randomizr package conduct random sampling. See Section 8.1 overview many kinds sampling possible. define simple 50-unit sample population follows:draw data simple design point, smaller: 100 units population data frame 50 units representing sample. data frame, inclusion probability, probability included sample. randomizr includes default. case, every unit population equal 0.5 probability inclusion.\nTable 3.7: Sampled data.\nSampling also non-random, accomplished using handler.","code":"\ndeclare_sampling(n = 50)"},{"path":"primer.html","id":"assignment","chapter":"3 Software primer","heading":"3.3.3.2 Assignment","text":"Assignment also relies, default, randomizr package random assignment. , define assignment 50% probability assignment treatment 50% control.Assignment results data frame additional indicator Z assignment well probability assignment. , assignment probabilities constant, designs described Section 8.2 crucial information analysis stage.\nTable 3.8: Sampled data assignment indicator.\n","code":"\ndeclare_assignment(prob = 0.5)"},{"path":"primer.html","id":"other-data-strategies","chapter":"3 Software primer","heading":"3.3.3.3 Other data strategies","text":"Random sampling random assignment kinds data strategies. Others may include merging fixed administrative data sources, collapsing data across months days, operations. can include steps design , using declare_step. , must define handler, using custom function declare_population. handlers may prove useful dplyr verbs mutate summarize, fabricate function fabricatr package.add variable using fabricate:district-month data may want analyze district level, collapsing across months:5","code":"\ndeclare_step(handler = fabricate, add_variable = rnorm(N))\ncollapse_data <- function(data, collapse_by) {\n  data %>% group_by({{ collapse_by }}) %>% summarize_all(mean, na.rm = TRUE)\n}\n\ndeclare_step(handler = collapse_data, collapse_by = district)"},{"path":"primer.html","id":"answer-strategy","chapter":"3 Software primer","heading":"3.3.4 Answer strategy","text":"model data strategy steps, simulated dataset two key inputs answer strategy: assignment variable outcome. answer strategies, pretreatment characteristics model might also relevant. data look like :\nTable 3.9: Data revealed outcomes.\nestimator difference--means estimator, compares outcomes group assigned treatment assigned control. estimatr package makes easy calculates design-based standard error p-value confidence interval :\nTable 3.10: Difference--means estimate simulated data.\nNow, order declare estimator, can send name model declare_estimator. R many models work declare_estimator, including lm, glm, ictreg package list package, etc. design-based estimators estimatr can used.declaration, also define estimand targeting difference--means estimator.6 Typically, estimand targeting, sometimes may consider targeting one assessing good estimator estimates . example, may want know good job instrumental variables job targeting complier average causal effect, also close gets average average treatment effect.","code":"\ndifference_in_means(Y ~ Z, data = simple_design_data)\ndeclare_estimator(Y ~ Z, model = difference_in_means, estimand = \"PATE\")"},{"path":"primer.html","id":"building-a-design-from-design-steps","chapter":"3 Software primer","heading":"3.4 Building a design from design steps","text":"last section, defined set individual research steps. draw one version together :construct research design object can operate — diagnose , redesign , draw data , etc. — add together + operator. + creates design object.book, ’ll use compact way writing design, define +:","code":"\npopulation <- declare_population(N = 100, u = rnorm(N)) \npotential_outcomes <- declare_potential_outcomes(Y_Z_0 = u, Y_Z_1 = Y_Z_0 + 0.25) \nestimand <- declare_estimand(PATE = mean(Y_Z_1 - Y_Z_0)) \nsampling <- declare_sampling(n = 50) \nassignment <- declare_assignment(prob = 0.5) \nreveal <- declare_reveal(outcome_variables = Y, assignment_variables = Z) \nestimator <- declare_estimator(Y ~ Z, model = difference_in_means, estimand = \"PATE\")\nsimple_design <- \n  population + potential_outcomes + estimand + sampling + assignment + reveal + estimator\nsimple_design <- \n  declare_population(N = 100, u = rnorm(N)) +\n  declare_potential_outcomes(Y_Z_0 = u, Y_Z_1 = Y_Z_0 + 0.25) +\n  declare_estimand(PATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_sampling(n = 50) +\n  declare_assignment(prob = 0.5) +\n  declare_reveal(outcome_variables = Y, assignment_variables = Z) +\n  declare_estimator(Y ~ Z, model = difference_in_means, estimand = \"PATE\")"},{"path":"primer.html","id":"order-matters","chapter":"3 Software primer","heading":"3.4.1 Order matters","text":"defining design, order steps included design via + operator matters. Think order design causal order steps take place.order encodes several important aspects design:\n- First, fact estimand follows potential outcomes comes sampling assignment means population estimand, population average treatment effect. calculated data created far.\n- estimator comes assignment reveal outcomes steps. didn’t, difference--means work, wouldn’t access treatment variable realized outcomes.","code":"\npopulation + potential_outcomes + estimand + sampling + assignment + reveal + estimator"},{"path":"primer.html","id":"simulating-a-research-design","chapter":"3 Software primer","heading":"3.5 Simulating a research design","text":"Diagnosing research design — learning properties — requires first simulating running design . need simulate data generating process, calculate estimands, calculate estimates result.simple design defined object, can easily learn kind data generates, values estimand estimates, features simple funtions DeclareDesign. chain together functions similar way dplyr ggplot pipeline.draw simulated data based design, use draw_data:\nTable 3.11: Simulated data draw.\ndraw_data runs “data steps” design, model (population potential outcomes) data strategy (typically sampling assignment).simulate estimands single run design, use draw_estimands. runs two operations : draws data, calculates estimands point defined design. example, design estimand comes just potential outcomes. design, draw_estimands run first two steps calculate estimands estimand function declared:\nTable 3.12: Estimands calculated simulated data.\nSimilarly, can draw estimates single run draw_estimates simulates data appropriate moment calculates estimates.\nTable 3.13: Estimates calculated simulated data.\ndiagnose design, want data frame includes estimates estimands many runs design. , want run design, draw estimates estimands, stack results. exactly simulate_design :\nTable 3.14: Simulations data frame.\n","code":"\ndraw_data(simple_design)\ndraw_estimands(simple_design)\ndraw_estimates(simple_design)\nsimulate_design(simple_design, sims = 500)"},{"path":"primer.html","id":"diagnosing-a-research-design","chapter":"3 Software primer","heading":"3.6 Diagnosing a research design","text":"simulations data frame created allows us diagnose design (calculate summary statistics simulations) directly. can now calculate bias, root mean-squared error, power estimator-estimand pair. DeclareDesign, two steps. First, declare diagnosands. functions simulations data. precoded several standard diagnosands (see Section 10).Next, take simulations data diagnosands, diagnose. runs single operation, calculate diagnosands simulations data.\nTable 3.15: Design diagnosis.\ncan also single step. send diagnose_design design object, first run simulations , calculate diagnosands simulations data frame results.","code":"\nstudy_diagnosands <- declare_diagnosands(\n  select = c(bias, rmse, power), \n  mse = mean((estimate - estimand)^2))\ndiagnose_design(simulations_df, diagnosands = study_diagnosands)\ndiagnose_design(simple_design, diagnosands = study_diagnosands)"},{"path":"primer.html","id":"comparing-designs","chapter":"3 Software primer","heading":"3.7 Comparing designs","text":"diagnosis phase, often want compare properties two designs see prefer basis diagnosand values. two ways compare. First, can compare designs — kinds estimates estimands produce, steps design. can compare diagnoses.compare diagnoses, run diagnosis one calculate difference diagnosand two designs conduct statistical test null effect difference.","code":"\ncompare_designs(simple_design, redesigned_simple_design)\ncompare_diagnoses(simple_design, redesigned_simple_design)"},{"path":"primer.html","id":"comparing-many-variants-of-a-design","chapter":"3 Software primer","heading":"3.7.1 Comparing many variants of a design","text":"diagnosis phase, often want compare properties two designs see prefer basis diagnosand values. two ways compare. First, can compare designs — kinds estimates estimands produce, steps design. can compare diagnoses. can using redesign:alternative way write function makes designs based set design inputs. offers researcher flexibility setting design variants. call functions designers. ’s simple designer based running example:create single design, based original parameters 100-unit sample size treatment effect 0.25, can run:Now simulate multiple designs, can use DeclareDesign function expand_design. examine simple design several possible sample sizes, might want conduct minimum power analysis. hold effect size constant.simulation diagnosis tools can take set expanded designs (R list) simulate , creating column called design_label keep apart. example:","code":"\nredesign(simple_design, N = c(100, 200, 300, 400, 500))\nsimple_designer <- function(sample_size, effect_size) {\n  declare_population(N = sample_size, u = rnorm(N)) +\n    declare_potential_outcomes(Y_Z_0 = u, Y_Z_1 = Y_Z_0 + effect_size) +\n    declare_estimand(PATE = mean(Y_Z_1 - Y_Z_0)) +\n    declare_sampling(n = 50) +\n    declare_assignment(prob = 0.5) +\n    declare_reveal(outcome_variables = Y, assignment_variables = Z) +\n    declare_estimator(Y ~ Z, model = difference_in_means, estimand = \"PATE\")\n}\nsimple_design <- simple_designer(sample_size = 100, effect_size = 0.25)\nsimple_designs <- expand_design(simple_designer, sample_size = c(100, 500, 1000), effect_size = 0.25)\ndiagnose_design(simple_designs)"},{"path":"primer.html","id":"library-of-designs","chapter":"3 Software primer","heading":"3.7.2 Library of designs","text":"DesignLibrary package, created set common designs designers, can get started quickly also easily set range design variants comparison.","code":"\nlibrary(DesignLibrary)\n\nb_c_design <- block_cluster_two_arm_designer(N = 1000, N_blocks = 10)\n\ndiagnose_design(b_c_design)"},{"path":"part-i-exercises.html","id":"part-i-exercises","chapter":"4 Part I Exercises","heading":"4 Part I Exercises","text":"","code":""},{"path":"declaration.html","id":"declaration","chapter":"5 Declaration","heading":"5 Declaration","text":"Chapter 2, gave high-level overview framework describing research designs terms models, inquiries, data strategies, answer strategies, process diagnosing properties, general purpose approach improving better fit research tasks. Now chapter, place approach firmal formal footing. , employ elements Pearl’s (2009) approach causal modeling (directed acyclic graphs, DAGs short), provides syntax mapping design inputs design outputs. also use potential outcomes framework presented, example, Imbens Rubin (2015), many social scientists use clarify inferential targets.goal formalization formalization’s sake. Describing research design DAG helps us see fundamental symmetries across theoretical (M ) empirical (D ) halves research design. recurring theme book research designs tend stronger relationship M mirrored relationship D ; aim chapter make somewhat mystical claim concrete.define research design \\(\\Delta\\), four elements \\(<M,,D,>\\). Describing research design entails “declaring” four elements.\\(M\\) reference model, set reference models, world works. Following Pearl’s definition probabilistic causal model, model \\(M\\) contains three core elements. first specification variables \\(X\\) research conducted. includes endogenous exogenous variables (\\(V\\) \\(U\\) respectively) ranges variables. formal literature sometimes called signature model (Halpern 2000). second element (\\(F\\)) specification endogenous variable depends variables. can considered functional relations , Imbens Rubin (2015), potential outcomes. third final element probability distribution exogenous variables, written \\(P(U)\\). Since model probabalistic, can think \\(m\\) element \\(M\\).inquiry \\(\\) summary variables \\(X\\), perhaps given interventions variables. inquiry might average value outcome \\(Y\\): \\(\\mathbb{E}[Y] = \\int{y\\times Pr(Y=y)}\\), average value outcome conditional value treatment \\(Z\\): \\(\\mathbb{E}[Y|Z=1] = \\int{y\\times Pr(Y=y|Z=1)}\\). Using Pearl’s notation can distinguish descriptive inquiries causal inquiries. Causal inquiries summarise distributions arise interventions, indicated \\(()\\) operator, e.g., \\(\\Pr(Y | (Z = 1))\\). Descriptive inquiries summarise distributions arise without intervention, \\(\\Pr(Y | Z =1)\\).let \\(^m\\) denote answer \\(\\) model. Conditional model, \\(^m\\) value estimand, quantity researcher wants learn . connection \\(^m\\) model can seen following relationship: \\(^m = (m)\\).saying goes, models wrong may perhaps useful. denote true causal model world \\(W\\) realized world \\(w\\) draw true causal model. true answer, , \\(^w = (w)\\). answer model \\(^m\\) may close far true value \\(^w\\), say wrong. model \\(M\\) far \\(W\\), course \\(^m\\) need correct. note \\(^w\\) might undefined, since inquiries can stated terms theoretical models. theoretical model wrong enough, inquiry might nonsensical applied real world.data strategy, \\(D\\), generates data \\(d\\). Data \\(d\\) arises, model \\(M\\) probability \\(P_M(d|D)\\). data strategy includes sampling strategies assignment strategies, denote \\(P_S\\) \\(P_Z\\) respectively. Measurement techniques also part data strategies can thought selection observable variables carry information unobservable variables. data strategy operates \\(w\\) produce observed data: \\(D(w) = d\\).\\(M\\) reflects beliefs true underlying causal processes work \\(D\\) procedure results collection creation data. light, phrase “data generating process” imprecise. Scholars usually use phrase “true DGP” refer \\(M\\), even though \\(M\\) doesn’t create data, \\(D\\) applied real world.answer strategy, \\(\\) generates answer \\(^d\\) using data \\(d\\). encode relationship \\((d) = ^d\\).full set causal relationships \\(M\\), \\(\\), \\(D\\), \\(\\) respect \\(m\\), \\(^m\\), \\(d\\), \\(^d\\), \\(w\\), \\(^w\\) can seen DAG schematic representation research design.\nFigure 5.1: MIDA DAG\nFigure 5.1 illustrates research design correspondence \\((m) = ^m\\) \\((d) = ^d\\). theoretical half research design produces answer inquiry theory. emprical half research design produces empirical estimate answer inquiry.Neither answer necessarily close truth \\(^w\\), course. , shown figure, truth directly accessible either us theory empirics. gamble empirical research however theoretical models close enough truth: truth like set models imagine model set. indeed truth can thought one possible worlds model set simpler set relations theory empirics, shown Figure 5.2.\nFigure 5.2: MIDA DAG: pretend world included set models stipulate.\nTable 5.1:  Elements research design.discussion see striking analogy \\(M\\), \\(\\) relationship \\(D\\), \\(\\) relationship. answer aim gotten applying \\(\\) draw \\(M\\). answer access gotten applying \\(\\) draw \\(D\\). hope, usually, two answers . cases suggests \\(\\) “like” \\(\\): instance interested mean population access random sample, data available us \\(D\\) like ideal data observe \\(M\\) directly. indeed taking mean data likely good strategy. principle hold general matter. instance data sampled non uniform probabilities, simple mean observed data likely good strategy calculate average population. Rather, focus extent two sets answers line .","code":""},{"path":"declaration.html","id":"p2gulzarkhanrevisited","chapter":"5 Declaration","heading":"5.1 Example","text":"section, declare Gulzar Khan (n.d.) example finer detail. aim capture analytically-relevant features design. Chapter 2, declared words code simplified version design, order communicate key features. make move throughout book: present simplified versions canonical designs text, also provide worked examples details specific studies. Moving back forth two , hope, enable learn general principles declare designs always involve fine details research implementation.first describe design words declare code:model study describes set units: citizens living two Pakistani districts study took place , Haripur Abbottabad. endogenous outcomes interest whether citizen filed papers run office; whether citizen elected; Euclidean distance citizen’s preferences average citizen preferences. outcomes function causal model three treatments, emphasize either social personal benefits holding public office encourage anyone run office . model include expected treatment effect magnitude treatment suppositions correlated outcomes within villages study.inquiry effect encouragement run office focused prosocial motivations officeholding (rather encouragements emphasized personal benefits) rate filing papers run office people living villages randomly assigned receive.data strategy entailed three steps: (1) randomly sampling 192 villages among villages Haripur Abbottabad districts, using random walk procedure select 48 citizens village participate experiment; (2) randomly-assigning village equal probability one three conditions (neutral, social benefits, personal benefits); (3) collecting administrative data filed papers run matching back pretreatment survey data 9,216 citizens. Sampling, treatment assignment, measurement three common data strategy steps experiment; experiments, instead, include sampling step instead assign treatments within convenience sample.Gulzar Khan (n.d.) two-step answer strategy, fitting linear model predicting whether citizen ran office (outcome) indicators social benefits personal benefits treatments, calculating difference two coefficients estimate whether social benefits less effective personal benefits. answer strategy includes presenting table estimated difference, standard error clustered village account village-level random assignment, p-value calculated using permutation inference.\nFigure 5.3: DAG Gulzar-Khan design.\n","code":"\ngulzar_khan_design <- \n  \n  declare_population(\n    # study is conducted in two districts in Pakistan, with 311 and 359 villages in them\n    districts = add_level(N = 2, name = c(\"Haripur\", \"Abbottabad\"), N_villages = c(311, 359)),\n    \n    # villages nested within districts\n    villages = add_level(N = N_villages),\n    \n    # avg. 6500 citizens per village\n    citizens = add_level(N = 6500)\n  ) +\n  \n  # main outcome is whether a citizen filed papers to run for office\n  # we define potential outcomes in response to being assigned to a social, personal, or neutral appeal to run\n  declare_potential_outcomes(\n    filed_papers ~ rbinom(N, 1, prob = 0.05 + 0.05 * (Z_appeal == \"social\") + 0.01 * (Z_appeal == \"personal\")),\n    assignment_variable = Z_appeal, conditions = c(\"neutral\", \"social\", \"personal\")\n  ) + \n  \n  # inquiry is the difference in rates of filing papers between the social and personal appeal conditions\n  declare_estimand(ATE = mean(filed_papers_Z_appeal_social - filed_papers_Z_appeal_personal)) + \n  \n  # sample 192 villages\n  declare_sampling(clusters = villages, n = 192, sampling_variable = \"S_villages\") + \n  \n  # sample 48 citizens in each village via random walk\n  declare_sampling(strata = villages, n = 48, sampling_variable = \"S_citizens\") + \n  \n  # assign villages to three arms with equal probabilities for three types of appeals to run for office\n  declare_assignment(\n    m_each = c(48, 72, 72),\n    clusters = villages,\n    conditions = c(\"neutral\", \"social\", \"personal\"),\n    assignment_variable = Z_appeal\n  ) + \n  \n  # recode treatment assignment for analysis into indicators for the two conditions of interest\n  declare_step(\n    Z_social_village = if_else(Z_appeal == \"social\", 1, 0),\n    Z_personal_village = if_else(Z_appeal == \"personal\", 1, 0),\n    handler = mutate\n  ) +\n  \n  # 1. run a linear regression with condition indicators\n  # 2. calculate the difference in effects between people in villages assigned to social appeals compared\n  #    to those assigned to personal appeals\n  # 3. calculate robust standard errors clustered on village\n  declare_estimator(\n    filed_papers ~ Z_social_village + Z_personal_village, \n    linear_hypothesis = \"Z_social_village - Z_personal_village = 0\",\n    term = \"Z_social_village - Z_personal_village = 0\",\n    clusters = villages,\n    model = lh_robust\n  )"},{"path":"declaration.html","id":"further-reading-1","chapter":"5 Declaration","heading":"5.2 Further reading","text":"Imbens Rubin (2015) potential outcomesHalpern (2000) causal models","code":""},{"path":"specifying-the-model.html","id":"specifying-the-model","chapter":"6 Specifying the model","heading":"6 Specifying the model","text":"research design – whether research question fundamentally causal descriptive – implicitly relies models. Indeed models can enter multiple stages, example guiding research question analysis stage. focus role models providing background understanding world works, performance design can assessed. call “reference models” (distinguished analytic models later). role provide stipulation world works—instance — variables important interrelate—allowing us ask questions : reference model true, answer question able figure answer?\\(M\\) MIDA refers reference models. describe often think \\(M\\) set models, typical element \\(m\\) though cases \\(M\\) may just single model.Critically, whether design good bad depends reference model. data analysis strategy might fare well one model world poorly another. Thus get point can assess design need make reference model—many cases, family reference models—explicit. chapter go difficult task.’ll make use two different formal languages describing causal models: DAGs potential outcomes.potential outcomes formalization emphasizes counterfactual notion causality. \\(Y_i(Z = 0)\\) outcome unit \\(\\) occur causal variable equal zero \\(Y_i(Z = 1)\\) outcome occur \\(Z\\) set one. difference defines effect treatment outcome unit \\(\\). Since one potential outcome can ever revealed, least one two potential outcomes necessarily counterfactual. Usually, potential outcomes notation \\(Y_i(Z)\\) reports outcomes depend one feature – \\(Z\\), ignoring determinants outcomes. say don’t matter, , just focus. sense contained subscript \\(\\) since units carry relevant features \\(Z\\). can, generalize settings want focus one cause, case use expressions form \\(Y(0,0)\\) \\(Y(0,1)\\).graphical model formulation approach makes use DAGs “directed acyclic graphs” characterize causal relations. node graph variable edges connect represent possible causal effects. arrow “parent” node “child” node indicates value parent sometimes determines outcome child; formally: parent’s value argument functional equation determining child’s outcome. Though consistent counterfactual notion causality, DAGs emphasize mechanistic accounts: exposure variable changes, outcome variable changes result—though possibly different ways different units. DAGs nonparametric. means encode beliefs full causal model. don’t show variables related, just related.Defining reference model requires defining set variables (meaning ranges) stipulating relate . potential outcomes framework captured differences potential outcomes different conditions. graphical modeling literature define set nonparametric structural causal relationships, indicating variables depend variables. variables hand, can visualize model using directed acyclic graph (DAG).Despite may inferred sometimes heated disagreements scholars prefer one formalization , DAGs potential outcomes compatible systems thinking causality. use language causal graphs use language potential outcomes. choose use languages useful expressing different facets research design. use DAGs describe web causal interrelations concise way (writing potential outcomes every relationship model tedious). use potential outcomes want zoom particular causal relationships make fine distinctions inquiries apply different sets units (’s difficult describe effect heterogeneity graphical models).illustrate ideas using simple DAG describe model abstract research design collect information \\(N\\) units. assign treatment random \\(Z\\), collect posttreatment outcome \\(Y\\). know determinants outcome beyond \\(Z\\), don’t need write beliefs main inquiry average treatment effect \\(Z\\) \\(Y\\). ’ll say determinants causally related \\(Y\\), \\(Z\\), since \\(Z\\) randomly assigned us.\nnonparametric structural equation determining \\(Y\\) can written like :\\[\\begin{align*}\nY &= f_Y(Z, U)\n\\end{align*}\\]parametric structural equation can written like :\\[\\begin{align*}\nY &= Z + U\n\\end{align*}\\]Equivalently, potential outcomes (\\(Y\\)) might written :\\[Y_i(0) = u_i,  Y_i(1) = 1 + u_i, \\\\{1,2,\\dots, n\\}.\\]DAG encodes model graphical form:\nFigure 6.1: Simple DAG. U unobserved.\n","code":""},{"path":"specifying-the-model.html","id":"what-is-the-population","chapter":"6 Specifying the model","heading":"6.1 What is the population?","text":"first choice make declaring \\(M\\) set units wish make inferences. largely determined inquiry (\\(\\)). might usefully distinguish three types sets units—refer “population.”Finite population. inquiry population Americans Brazilians, reference model describe characteristics Americans Brazilians. might analyze data sample , seek nevertheless make inferences (finite) population.Finite population. inquiry population Americans Brazilians, reference model describe characteristics Americans Brazilians. might analyze data sample , seek nevertheless make inferences (finite) population.Finite sample. Moving , inquiry might sample finite population. interested sample average treatment effect, need describe units sample. practical purposes sample population. Subtly however, interested sample know sample population inquiry might concern typical sample population way need estimand.Finite sample. Moving , inquiry might sample finite population. interested sample average treatment effect, need describe units sample. practical purposes sample population. Subtly however, interested sample know sample population inquiry might concern typical sample population way need estimand.Superpopulation. Moving might imagine units draws infinite population. interested “happens shake fizzy drink?”: set instances infinite characterize superpopulation want describe distribution possible cases finite set cases.Superpopulation. Moving might imagine units draws infinite population. interested “happens shake fizzy drink?”: set instances infinite characterize superpopulation want describe distribution possible cases finite set cases.population mind affects reference populations modeled also draws population interpreted.illustrate consider simple model believe \\(Y\\) depends upon \\(Z\\) \\(Z\\) randomly assigned, probability .5. Say believe potential outcomes, treatment effects, different person. model, like many books, might declared like :model can used generate data, like :\nTable 6.1: Data simple model\nNote first now declaration stochastic component. X randomized individual level features U treatment effects tau. Note also “data” contains potential outcomes—, draw includes description world work different conditions simply description possibly observable data. words reports data generating process, just data.populations draws given stochastic component? type population just modeled? modeled one many populations?fact three distinct ways interpret model specification.Large Population Interpretation M: large population—perhaps superpopulation. run model get see new sample use sample learn large population. interpretation \\(M\\) singleton—specified one model.Large Population Interpretation M: large population—perhaps superpopulation. run model get see new sample use sample learn large population. interpretation \\(M\\) singleton—specified one model.Priors Interpretation M: third interpretation stochastic component represents uncertainty set possible worlds. might , instance, design performs well possible worlds poorly others, case nature distribution important assessing expectation design perform well. interpretation \\(M\\) singleton.Priors Interpretation M: third interpretation stochastic component represents uncertainty set possible worlds. might , instance, design performs well possible worlds poorly others, case nature distribution important assessing expectation design perform well. interpretation \\(M\\) singleton.Multiple Worlds Interpretation M: second interpretation data draw (non stochastic) model distribution functions provide handy way generating multiple plausible models. want learn design performance possible worlds. interpretation M collection models may interested performance \\(m\\) \\(M\\).Multiple Worlds Interpretation M: second interpretation data draw (non stochastic) model distribution functions provide handy way generating multiple plausible models. want learn design performance possible worlds. interpretation M collection models may interested performance \\(m\\) \\(M\\).interpretation matters diagnosis (see Section 10).","code":"\nM <-\n  declare_population(N = 100, U = rnorm(N), tau = 1+rnorm(N), Z = rbinom(N, 1, .5)) +\n  declare_potential_outcomes(Y ~ tau * Z + U)\ndraw_data(M) %>% \n  head %>% kable(caption = \"Data from a simple model\")"},{"path":"specifying-the-model.html","id":"what-variables-to-include","chapter":"6 Specifying the model","heading":"6.2 What variables to include?","text":"","code":""},{"path":"specifying-the-model.html","id":"types-of-variables","chapter":"6 Specifying the model","heading":"6.2.1 Types of variables","text":"key components model reference models examine “variables” (“nodes”) ways stand relation . Informally variable quantity can take different values: day week, winner election, height mountain. variables formally similar can play distinguished roles model.Often variables labeled according role play causal model later discussions often refer variables way using following terms:Outcome variables: variable whose level responses want understand, generally referred \\(Y\\), Figure 6.2. Variously described “dependent variables,” “endogenous variables,” “left hand side variables,” “response variables.”Explanatory variables: variables affect outcome variables, often referred \\(X\\)s though often \\(Z\\), \\(D\\), \\(W\\). often use \\(D\\) refer main causal variable interest particular study.Conditioning moderating variables. Variables might alter effect \\(D\\) \\(Y\\). See example \\(X2\\) Figure 6.2. Note figure simply indicates \\(X2\\) cause \\(Y\\) graph indicate interaction \\(X1\\) \\(X2\\). One account general matter two variables cause outcome surprising interact way.Mediators. Variables “along path” explanatory variables outcomes. \\(M\\) example mediator figure. Mediators often studied assess “” \\(D\\) causes \\(Y\\).Confounders. Variables introduce noncausal correlation \\(D\\) \\(Y\\). figure \\(X1\\) unobserved confounder causes \\(X\\) \\(Y\\) introduce correlation even \\(D\\) cause \\(Y\\).Instruments. instrumental variable variable can induce exogenous change explanatory variable help us figure relationship \\(D\\) \\(Y\\), used instrumentally—sake. give much detailed treatment variables Section 16.3. \\(Z\\) often reserved instruments; see Figure 6.2.Colliders. Colliders variables caused two variables. Colliders can important conditioning collider introduces statistical noncausal relationship causes collider. figure 6.2 \\(K\\) collider can create correlation \\(D\\) \\(Y\\) (via \\(U\\)) conditioned upon.Note labels reflect researcher’s interest much position model. Another researcher examining graph might, instance, label \\(M\\) explanatory variable \\(K\\) outcome interest.\nFigure 6.2: DAG explantory variable interest (D), outcome interest (Y), mediator (M), confounder (X1), moderator (X2), instrument (Z), collider (K).\n","code":""},{"path":"specifying-the-model.html","id":"what-variables-are-needed-for-declaration-and-diagnosis","chapter":"6 Specifying the model","heading":"6.2.2 What variables are needed for declaration and diagnosis?","text":"considering variables include model researchers often torn two conflicting goals. First, want learn world works words, fill large causal model. infinite number nodes edges model — people vote save spend money find romantic partners, interrelated. Second, generally seek simple explanations possible. Indeed accounts point research generate simplified representations world. analogy map often used: useful map usually less detail object mapping.used considerations vying considering variables “interest” include model. instance, causes examine explaining outcome. highlight MIDA framework first consideration – realism–likely carries weight settings. reason model, understood , analysis benchmark analysis assessed. excellent “analytic model” might ignore unimportant details problem, reference model might specify details order establish indeed can ignored without harming inferences.design declaration framework variables need specify \\(M\\) general comprised () need latter three elements research design: inquiry \\(\\), data strategy \\(D\\), answer strategy \\(\\) (b) need diagnosis.Consider first set first:\\(\\): order reason whether data collect able provide answer inquiry, need define variables used construct inquiry. descriptive research, mean variables summarize. causal research, mean potential outcomes different states world, treatment control. example, studying effects voter mobilization campaign vote choice three candidates running primary election, define vote choice variable values takes two circumstances: presence voter mobilization campaign (treatment) without (control).\\(D\\): data strategy — sampling, treatment assignment, measurement — defines many variables need specify model. Sampling procedures often involve stratification (e.g., sampling equal proportions men women), clustering (e.g., sampling individuals household participate research), . model, need define variables used stratify cluster. Similarly, treatment assignment can involve assigning treatments within blocks cluster assignment units assigned status. variables used construct blocks form clusters defined model. Finally, variables measured also defined model. measure latent variables imperfectly, example sensitive questions true characteristic exists respondent always admit , define latent trait measured responses.\\(\\): Finally, answer strategies rely collected data provide answer inquiry, variable collected data defined model. (variables also defined measurement strategy.) Beyond outcomes treatment variables, may need variables define clusters used clustered standard errors, construct weights poststratification estimates match population characteristics, visualize data., enough diagnosis? might . engage diagnosis may want ask design fares world worked particular way. instance, gender subjects might primary interest us enter inquiry, data strategy, analysis, might worry gender confounds inferences—example affects treatment uptake outcomes. case may include gender reference model order assess extent analysis sensitive .","code":""},{"path":"specifying-the-model.html","id":"specifying-structural-relations","chapter":"6 Specifying the model","heading":"6.3 Specifying structural relations","text":"DAGs convey beliefs whether two variables causally related, encode beliefs related. criticism DAGs — just don’t encode causal beliefs system. assess many properties research design need go . need specify probability distribution exogenous variables functional forms endogenous variables (relate parent variables). include incorporating beliefs effect sizes, also correlations variables, intra-class correlations (ICCs), interactions. say: order declare diagnose designs, ’ll need make leap nonparametric models parametric structural causal models. move without costs – specific choice make nonparametric model opportunity wrong world!slightly complex example helps demonstrate can use model declaration facilitate representation class possibly stochastic models. Suppose according model, effect \\(Z\\) larger units \\(X = 1\\). can encode belief design declaration . declare_population function, write \\(U\\) normally distributed mean 0 standard deviation 1; \\(X\\) follows Bernoulli distribution. declare_potential_outcomes function, describe average effect treatment depends \\(X\\).design describes complex world treatment effects \\(Z\\) depend particular way background variable \\(X\\). incorporate different beliefs causal model changing tau_X0 tau_X1 parameters. However, regardless value interaction (either zero number), DAG looks ability design answer questions interest vary. highlights point DAGs enough characterize model—design declaration requires functional equations also: either specific equations families equations. Thus although might feel uncomfortable specifying particular values tau_X0 tau_X1 beholden particular values. can easily vary see design performs range models, even expand declaration specify distribution values tau_X0 tau_X1.","code":"\ntau_X0 <- 0.5\ntau_X1 <- 1\n\nM <-\n  declare_population(N = 100, U = rnorm(N), X = rbinom(N, 1, .5)) +\n  declare_potential_outcomes(Y ~ (X==0)*Z*tau_X0 + (X==1)*Z*tau_X1+U)"},{"path":"specifying-the-model.html","id":"substantive-justifications-for-choices-of-nodes-and-equations","chapter":"6 Specifying the model","heading":"6.4 Substantive justifications for choices of nodes and equations","text":"far described formal considerations described substantive considerations including particular variables stipulating particular relations .justification choice reference model depend purpose design. Broadly distinguish reality tracking models, discursive models, sufficient models.","code":""},{"path":"specifying-the-model.html","id":"reality-tracking-models","chapter":"6 Specifying the model","heading":"6.4.1 Reality tracking models","text":"Reality tracking reference models seek approximate truth well possible.content models typically comes two places: reading past literature qualitative research. Past theoretical work can guide set nodes relevant connected edges. Past empirical work can provide insight set edges exist (). However, past research thin topic, substitute insights gained qualitative data collection: focus groups interviews key informants know aspects model hidden researcher; archival investigations understand draw understand causal process actors longer alive, gain insights contained administrative records; immersive participant observation see eyes social actors behave. Fenno (1982) calls “soaking poking.” mode inquiry, discovery, separate qualitative research designs provide answer inquiry deductively. examine throughout book (particular examples library entry XX YY). Instead, qualitative insights , Lieberman (2005) labels “model-building” case studies, aim answer question rather yield new theoretical model. Quantitative research often seen distinct qualitative research, model building phase qualitative.Identifying nodes edges important constructing \\(\\), \\(D\\), \\(\\) (thus including \\(M\\)) typically come two sources: reading past literature conducting new qualitative research. Past theoretical work can guide choice nodes relevant connected edges. Past empirical work can provide insight set edges exist (). However, past research thin topic, substitute insights gained qualitative data collection: focus groups interviews key informants know aspects model hidden researcher; archival investigations understand draw understand causal process actors longer alive, gain insights contained administrative records; immersive participant observation see eyes social actors behave. Fenno (1982) calls “soaking poking.” mode inquiry, discovery, separate qualitative research designs provide answer inquiry deductively. examine throughout book (particular examples library entry XX YY). Instead, qualitative insights , Lieberman (2005) labels “model-building” case studies, aim answer question rather yield new theoretical model. Quantitative research often seen distinct qualitative research, model building phase qualitative.next step — selecting statistical distributions parameters describe exogenous variables functional forms endogenous variables — often uncomfortable. know magnitude effect intervention research correlation two outcomes goal research. ’s conducting study! However, fully dark cases can make educated guesses parameters like effect sizes, intraclass correlations, correlations variables.can conduct meta-analyses past, relevant studies topic identify range plausible effect sizes, intraclass correlations, correlations variables, model parameters. Conduct meta-analysis might simple collecting three papers measured similar outcomes past calculating average intraclass correlation range across three. sophisticated still straightforward analysis calculate precision-weighted average effect sizes use baseline effect size model, also calculate predictive interval random effects meta-analysis characterize expected range effect sizes across differing contexts.key question conduct meta-analysis select studies “relevant.” four dimensions might want compare past studies current setting: similarity type units, treatments, outcomes, contexts (Cronbach Shapiro 1982). Except case pure replication studies, typically studying (possibly new) treatment new setting, new participants, new outcomes, perfect overlap. However, variation effects across contexts dimensions help structure range guesses specified model.past studies especially close , may want define probability distributions approximate causal model world, use data past study directly stand . , instead declaring variables distributions potential outcomes, can resample past data obtain simulated alternative possible worlds.past studies sufficiently similar dimension, can collect new data pilot studies. discuss risks relying data small pilot studies planning new research Chapter 20.5. short, rely effect size estimates small pilot studies, use parameters range outcome data standard errors estimates less noisy declaring new design.cases, can define many variables early data collection study, e.g., baseline survey. conduct baseline survey, can use set individuals selected study baseline characteristics define many variables model. can define expectations effect sizes features endogenous variables, rely correlations exogenous variables baseline data. danger fix characteristics model using baseline data, consider data revealed sampling procedure yielded different set study participants conducted data collection month later.","code":""},{"path":"specifying-the-model.html","id":"discursive-models","chapter":"6 Specifying the model","heading":"6.4.2 Discursive models","text":"purposes reference model might developed track reality reflect assumptions scholarly debate. instance purpose might question whether given conclusion valid assumptions maintained scholarly community. Indeed possible reference model used specifically researcher thinks inaccurate, allowing show even wrong background assumptions world, analysis produce good answers.particularly important class discursive models might called “agnostic models”: models make weaker assumptions world researcher good faith holds. directed acyclic graph, every arrow indicates possible relation cause outcome. big assumptions models however seen arrows absence arrows: every missing arrow represents claim outcome affected possible cause. Analysis strategies often depend upon assumptions. Even arrows included, functional relations might presuppose particular features important inference. instance researcher using “instrumental variables” analysis generally assume \\(Z\\) causes \\(Y\\) though \\(D\\) paths. assumption absent arrows. analysis might also assume \\(Z\\) never affects \\(D\\) negatively. assumption functional forms. agnostic reference model might loosen assumptions, allow possibility violations exclusion restrictions violations monotonicity assumptions use see analysis fares conditions researcher believes true, skeptic might willing entertain.","code":""},{"path":"specifying-the-model.html","id":"sufficient-models","chapter":"6 Specifying the model","heading":"6.4.3 Sufficient models","text":"purposes validity inferences, lessons learned diagnosis depend \\(w\\) lying \\(M\\), relevant question whether kinds inferences one might draw given stipulated reference models also hold reasonable well true data generating process. instance aim assess whether analysis strategy generates unbiased estimate treatment effect may go pains make sure model treatment assignment carefully modeling size treatment effect correctly may important. idea learn model study sufficient inferences broader class model within true data generating process might lie.","code":""},{"path":"specifying-the-model.html","id":"getting-it-right","chapter":"6 Specifying the model","heading":"6.5 Getting it right?","text":"","code":""},{"path":"specifying-the-model.html","id":"robustness-to-multiple-models","chapter":"6 Specifying the model","heading":"6.5.1 Robustness to multiple models","text":"noted uncomfortable part declaring \\(M\\) choosing point estimates parameters effect size, mean variance normal distribution describes unknown heterogeneity. also emphasized general need specify single point! Indeed, uncertainty model usually lead us range even empirical distribution past studies parameter.suggest three strategies choosing ranges: logical bounds parameter, choosing range possible effect sizes based largest change bottom scale top scale; empirical distribution past studies, either full range parameter predictive interval random effects meta-analysis; best case-worst case bounds, based substantive interpretation results light past results, example ranging effect size zero largest plausible effect size. design performs well terms power bias one three ranges parameter might labeled “robust multiple models.”separate goal assessing performance research design different models implied alternative theories. good design provide probative evidence model correct regardless model aligns true causal model world.important example assessing performance research design “null model” true effect size zero. good research design report high probability insufficient evidence reject null effect. research design, alternative model large effect size, high probability return evidence rejecting null hypothesis zero effect. example makes clear order understand whether research design strong, need understand performs just multiple models, models implied alternative theoretical understandings world.Two alternative theories \\(X\\) causes \\(Y\\), mediator \\(M_1\\) mediator \\(M_2\\) (), imply two different structural causal models. inquiry variable mediates relationship, need understand research design performs providing evidence correct possibilities.","code":""},{"path":"specifying-the-model.html","id":"fundamental-uncertainty","chapter":"6 Specifying the model","heading":"6.5.2 Fundamental uncertainty","text":"Even best reference models sure simplifications true data generating process. true causal structure world \\(W\\) generates draws world \\(w\\). inquiry \\(\\) might even defined \\(W\\), \\((w)\\) might \\(NA\\). Applying data strategy \\(D\\) \\(w\\) might produce unexpected results. \\(D(w)\\) need anything like \\(D(m)\\). disjuncture large part point research first place. know \\(W\\) – require omniscience. learned parts \\(W\\) put \\(M\\) – ’s science. research produces unexpected results, ’s indication something MIDA whack opportunity learning. next research project amend MIDA order bring \\(M\\) closer \\(W\\).","code":""},{"path":"defining-the-inquiry.html","id":"defining-the-inquiry","chapter":"7 Defining the inquiry","heading":"7 Defining the inquiry","text":"inquiry summary reference model. Suppose reference model \\(D\\) possibly affects \\(Y\\). Using framework provided Pearl Mackenzie (2018), one inquiry might descriptive, associational: average level \\(Y\\) \\(D=1\\)? second might effects interventions: average treatment effect \\(D\\) \\(Y\\)? third counterfactuals: share units \\(Y\\) different \\(D\\) different? theory involves variables, many questions open , instance regarding effect one variable passes , modified , another.inquiry research question. Simple complex, causal descriptive, inquiry can thought summary data generating processes. Like models, inquiries—questions ask—theoretical objects. easy confuse inquiries output answer strategies. theory posits existence Average Treatment Effect, might use answer strategy like difference--means estimate , estimate fundamentally distinct inquiry. Estimates empirical, inquiries theoretical.general, inquiry summary function \\(\\) operates instance model \\(m \\M\\).7 summarize model inquiry, obtain “answer model.” formalized \\((m) = ^m\\). can think difference \\(\\) \\(^m\\) difference question answer. \\(\\) question ask model \\(^m\\) answer. Alternatively, can think \\(\\) “estimand” (estimated) \\(^m\\) value estimand.book talk inquiries, usually referring single-number summaries models. common estimands descriptive, means, conditional means, correlations, partial correlations, quantiles, truth statements variable model. Others causal, average difference one variable second variable set two different values. can think single-number inquiry atom research question.inquiries “atomic” way, inquiries complex single-number summary. example, best linear predictor \\(Y\\) given \\(X\\) two-number summary: pair numbers (slope intercept) minimizes total squared distance line value \\(Y\\). Note causal estimand still well defined. need stop two-number summaries though. imagine best quadratic predictor \\(Y\\) given \\(X\\) (three-number summary), . See Figure 7.1. inquiry full conditional expectation function \\(Y\\) given \\(X\\), matter wiggly, nonlinear, nuanced shape function – principle 1,000 number summary model, much .\nFigure 7.1: Inquiries based different numbers parameters\ncourse need number : answer question might “blue” “normal distribution.” might set. instance qualitative comparative analysis (QCA) Boolean variables (“crisp-set QCA”) common inquiry : minimal set conditions sufficient produce \\(Y\\) (Ragin 1987). instance, \\(Y\\) happens time causal factor \\(\\) present \\(\\) absent \\(B\\) present, \\(^M\\) set: \\(\\{, \\neg \\& B\\}\\).set questions explanatory model. instance, researcher might articulate handful important questions model come certain way model rejected. complex inquires made series atomic inquiries – ’re interested sub-inquiries insofar help us understand real inquiry – model world good one .","code":""},{"path":"defining-the-inquiry.html","id":"three-families-of-inquiries","chapter":"7 Defining the inquiry","heading":"7.1 Three families of inquiries","text":"Pearl Mackenzie (2018) usefully describe “ladder causation” can used categorize classes inquiries. bottom rung ladder focuses descriptive inquiries world , , . second rung focuses questions effects interventions: happens \\(Y\\) something \\(X\\)? third rung counterfactuals: \\(Y\\) different \\(X\\) different. last two rungs involve causal inquiries, question world , , future variables set different levels.Descriptive causal inference common difficulty inferring unseen things observed data. fundamental problem causal inference well known. unit simultaneously treated untreated can every observe one potential outcome particular unit can never measure causal effects – infer . Similar problems confront descriptive inference. concepts want measure latent constructs. Perfect measurement generally possible, measurements latent constructs generally include measurement error.\nspecifically, descriptive inference conclusion features latent variable \\(Y^*\\) set units basis observations measured variable \\(Y\\), measurements possibly taken different set units. feature seek describe—inquiry—summary \\(Y^*\\) mean perhaps covariance second latent variable \\(X^*\\). descriptive research, draw inferences features nodes latent causal model \\(M\\). measurement can imperfect two reasons: get observe quantity interest directly units study, units study subset units interest. challenges gives rise focus descriptive inference rather measurement alone.causal inference conclusion responses variables changes variables. Even exceptional job measuring \\(X^*\\) \\(Y^*\\) \\(X\\) \\(Y\\), still trouble learning effects causal effects quite literally unobservable, inferred.","code":""},{"path":"defining-the-inquiry.html","id":"descriptive-inquiries","chapter":"7 Defining the inquiry","heading":"7.1.1 Descriptive inquiries","text":"Descriptive inquiries usually latent variables since mostly care true values variables models. Measured variables often distinct latent variables giving rise problem descriptive inference. Normally define inquiries terms true latent variables rather terms measured counterparts; making distinction explicit design object especially important risks measurement error loom large.Table 7.1 enumerates common descriptive estimands. estimands common need counterfactual quantities order define. Note especially covariance (similarly, correlation) \\(X\\) \\(Y\\) enters descriptive estimand, line best fit \\(Y\\) given \\(X\\).Table 7.1:  Descriptive inquiries","code":""},{"path":"defining-the-inquiry.html","id":"inquiries-about-causal-effects","chapter":"7 Defining the inquiry","heading":"7.1.2 Inquiries about causal effects","text":"Inquiries causal effects involve comparison least two possible worlds. example, inquiry might causal effect \\(X\\) \\(Y\\) single unit. order infer causal effect, need know value \\(Y\\) two worlds: one world \\(X\\) set 1 one \\(X\\) set 0.Table 7.2 enumerates common causal estimands. estimands vary population refer —instance statements samples (SATEs) populations (PATEs)? can also depend upon possibly unobservable characteristics populations—value covariate (CATEs)–way respond causes (CACE targets effect treatment specifically compliers —take treatment encouraged ). Finally may summaries one potential outcome: instance interaction effect defined individual level effect one treatment effect another treatment.Table 7.2:  Causal inquiriesGenerations students told excise words connote causality empirical writing. “Affects” becomes “associated ” “impacts” becomes “moves .” careful causal language course important (’s really true correlation imply causation!). change language usually accompanied change inquiry. Many times faced drawing causal inferences less ideal data – deficiencies data strategy lead us far away inferential targets. inquiry causal inquiry, move “causes” “correlated ” might good description actual data analysis, doesn’t move us closer providing answer inquiry.","code":""},{"path":"defining-the-inquiry.html","id":"causalattribution","chapter":"7 Defining the inquiry","heading":"7.1.3 Causal attribution inquiries","text":"Another kind data-dependent inquiry distinct notion causal effect causal attribution. causal effect inquiry focuses change outcome induced change causal variable (unit-level across units) irrespective values outcome takes. contrast, causal attribution inquiries focus probabilities condition realized outcomes, , “probability absence outcome hypothetical absence treatment (\\(Y_i(0) = 0\\)) given actual presence (\\(X_i = Y_i = 1\\))” (Yamamoto 2012, 240–41). Goertz Mahoney (2012) refer causal attribution inquiries cause--effects questions start outcome (effect) seek validate hypothesis cause.dependence inquiries actual outcomes makes harder (though impossible!) answer tools quantitative science, though often central interest scientific policy agendas occupied large number qualitative studies. Questions like “‘economic crisis necessary democratization Southern Cone Latin America?’ ‘high levels foreign investment combination soft authoritarianism export-oriented policies sufficient economic miracles South Korea Taiwan?’” examples inquiries (Goertz Mahoney 2012). Though bear resemblance related causal effects inquiries focus observed subsets (average treatment effect treated, ATT)8 important confuse two kinds inquiries, often happens.increasingly common explicitly formalize causal effect inquiries, less common formalize causal attribution inquiries. , however, can important providing specificity required diagnose design computer. Pearl (1999) provides formal definitions inquiries using language causal necessity sufficiency, depicted table . put inquiries context democratic peace hypothesis, example, given country dyad-year, \\(Y_i = 1\\) \\(X_i = 1\\) represent “Peace” “democracies” \\(Y_i = 0\\) \\(X_i = 0\\) represent “War” “democracies.” \\(\\Pr(Y_i(X_i = 0) = 0 \\mid X_i = Y_i = 1)\\) asks, among peaceful, fully democratic dyads, proportion wars democracies—, proportion dyad-years democracy necessary cause peace? Similarly, \\(\\Pr(Y_i(X_i = 1)=1 \\mid X_i = Y_i = 0)\\) asks, among dyads war least one non-democracy given year, proportion experienced peace countries democracies—words, proportion cases democracy sufficient cause peace? Yamamoto (2012) extends account focus causal attribution inquiries focus important subsets, compliers.Table 7.3:  Causal attribution inquiriesLike designs, causal attribution inquiries can declared, simulated, diagnosed computer. Something consider, however, model may produce datasets effect occur, questions defined units occurred undefined. One way avoid construct model event occurs least one unit probability one.","code":""},{"path":"defining-the-inquiry.html","id":"estimands-declared","chapter":"7 Defining the inquiry","heading":"7.2 Estimands declared","text":"Table 7.4 shows “draws” three estimands. declare simple reference model inquiry—rather set inquiries. estimand draws implemented drawing particular model \\(m\\) \\(M\\) applying \\(\\) draw.\nTable 7.4: One model three estimands.\nfirst estimand descriptive estimand: average outcome \\(Y\\) \\(Z = 1\\).\nNote declaration estimand make use counterfactual quantities.second estimand causal effect. inquiry asked model time inquiry takes average difference, across individual, two potential outcomes. Unlike descriptive estimand, estimand uses information \\(Y\\)’s potential outcomes use information distribution \\(X\\).third inquiry asks attribution question, case continuous data. case ask share units positive values \\(Y\\) \\(Z=1\\) negative values \\(Y\\) \\(Z=0\\). estimand requires information potential outcomes actual outcomes (\\(Z\\)).illustration “run” M+provides different value estimand (, precisely, true design first third estimand). variation understood? noted last chapter might think superpopulation distribution estimands interest understanding superpopulation distribution might think model, \\(M\\), representing set possible worlds interested performance . approach take matter turn diagnosis.","code":"\nM <-\n  declare_population(N = 100, U = rnorm(N), Z = rbinom(N, 1, .5)) +\n  declare_potential_outcomes(Y ~ 0.5 * Z + U) +\n  declare_reveal()\n\nI <- declare_estimand(\n  YZ1 = mean(Y[Z==1]),\n  ATE = mean(Y_Z_1 - Y_Z_0),\n  POC = mean((Y_Z_0<0)[Z==1 & Y_Z_1>0])\n  )\n\ndraw_estimands(M+I)"},{"path":"defining-the-inquiry.html","id":"common-complexities-in-defining-estimands","chapter":"7 Defining the inquiry","heading":"7.3 Common complexities in defining estimands","text":"","code":""},{"path":"defining-the-inquiry.html","id":"data-dependent-inquiries","chapter":"7 Defining the inquiry","heading":"7.3.1 Data-dependent inquiries","text":"inquiries introduced thus far depend variables model, features data answer strategies (attribution estimands exception). However common inquiries depend realizations research design.first type depend realizations data \\(d\\): inquiries units within sample depend units enter sample; inquiries treated units depend treated. sample average treatment effect common inquiry used experimental researchers wish worry effects generalize population identifying causal effect within units front . true sample average treatment effect \\(^w\\) every possible sample draw. fixed values selected inquiry define \\(\\) sample average effect depends sample actually selected sampling procedure. true condition inquiries set treated units—case estimand defined conditional received treatment.curiosity one might wonder assess whether given sampling scheme assignment procedure good, ex ante, cases question posed conditional sampling assignment. Odder still one might wonder conduct statistics effect estimates sample treatment assignment fixed, since, conditional upon features, hard see variation sampling distribution estimates might come . reasonable response concerns many cases one can pose ex ante question form: say committed family designs allocate 50 units treatment unit equally likely assigned: effect receive treatment, whoever end ? case “run” study produces different treatment group quality design assessed respect distribution possible treatment groups; similarly errors calculated distribution. Moreover, given constraints admissible assignment schemes one can assess whether one scheme fares better another different criteria.Table 7.5:  Data-dependent inquiriesA second design-dependent inquiry class depends \\(d\\) \\((d) = ^d\\), answer given data. \\(^d\\)-dependent inquiry, know inquiry seeing results study. collect data ten variables calculate correlation ten separate outcome variable, report magnitude correlation seventh variable - one statistically distinguishable zero. inquiry “seven variables significant” problem, long include multiple comparisons correction adjust probability finding one significant null effects due random chance. started question correlation variable seven outcome, also problem. However, procedure think inquiry correlation variable seven outcome, problem: accounting multi-step procedure answer strategy able provide good answers inquiry. short, although can dangers guided realization answer selecting inquiry, problem procedure-based answer strategies—just must honest original inquiry. problem comes descriptive inference looking multiple correlations example, also many places searching heterogeneity treatment effects experiments nested research designs iterate levels data (Lieberman 2005).","code":""},{"path":"defining-the-inquiry.html","id":"non-decomposable-inquiries","chapter":"7 Defining the inquiry","heading":"7.3.2 Non-decomposable inquiries","text":"inquiries looked group level summaries individual level inquiries. decomposable sense can think average effect large group average set average effects smaller groups. inquiries form however. —best linear fit—complex summaries possible data. Others still notional. Consider example estimand Regression Discontinuity Design (RDD) shoots . RDD model (See section @(RDD)) imagine units \\(Y_i(1)\\), \\(Y_i(0)\\) \\(\\) also value “running variable”, \\(x_i\\) units receive treatment \\(x_i>0\\). case “effect point discontinuity” might written:\\[E_{|x_i = 0}(Y_i(Z=1) - Y_i(Z=0))\\]Curiously however may units form \\(x_i = 0\\) easily think estimand summary individual potential outcomes.One way resolve think \\(X\\) randomized, least locally, case can define estimand :\\[E_{}(Y_i(Z=1, X=0) - Y_i(Z=0, X=0))\\]Another way imagine complex summary construct continuous CEF case \\(Z=1\\) \\(Z=0\\) evaluate difference \\(X=0\\). Though complex summary potential outcomes though nature estimand (estimate) depends CEF constructed.","code":""},{"path":"defining-the-inquiry.html","id":"model-dependent-inquiries","chapter":"7 Defining the inquiry","heading":"7.3.3 Model dependent inquiries","text":"similar conceptualization can used settings model parameters estimands. instance might access data decisions subjects lab seek estimate “coefficient risk aversion.” reference model might include coefficient—indeed might willing believe number really exists subjects. Yet might still ask: access possible choices, coefficient best summarize choices agent. answer provides estimand seek estimate finite data.","code":""},{"path":"defining-the-inquiry.html","id":"complex-counterfactual-inquiries","chapter":"7 Defining the inquiry","heading":"7.3.4 Complex counterfactual inquiries","text":"counterfactual queries described far involve imagining change one variable assessing effects another, yet much complex causal queries imaginable including quantities events possible even refernce model. example “controlled direct effect.” imagine instance \\(X\\) causes \\(Y\\) directly indirectly. Say particular \\(X\\) causes \\(M\\) conditional \\(M=0\\) \\(X\\) causes \\(Y\\) directly. controlled direct effect \\(X\\) \\(Y\\) :\\[CDE = Y(X=1, M=1) - Y(X=0, M=1)\\]well defined quantity potential outcomes DAG formulation. can require mental gymnastics situations , say \\(M=1\\) never arise indeed \\(X\\) 0. instance: Democrat win presidential elections affect success Democrats mid term elections, conditional presidency remaining Republican hands.","code":""},{"path":"defining-the-inquiry.html","id":"inquiries-with-continuous-causal-variables","chapter":"7 Defining the inquiry","heading":"7.3.5 Inquiries with continuous causal variables","text":"Inquiries focus small number potential outcomes usually quite easy write : binary treatment model spillovers, two potential outcomes average treatment effect defined average difference across units. treatment like income, might millions potential outcomes, one corresponding different dollar amount? truly continuous treatments, might even infinite number potential outcomes.One approach think estimand data-dependent marginal effect. example \\(E[Y_i(X_i) - Y_i(X_i-1)]\\) captures expected average difference observed data, people income \\(X_i\\), unobservable outcome one dollar less. Another approach think continuous-treatment estimands parameter regression omniscient run. example might define \\(\\alpha\\) \\(\\beta\\) solutions :\n\\[\\min_{(\\alpha,\\beta)}\\sum_i\\int \\left(Y_i(x) - \\alpha - \\beta x\\right)^2f(x)dx\\]\n\\(Y_i(x)\\) (unknown) potential outcome unit \\(\\) condition \\(x\\). Estimand \\(\\beta\\) can thought coefficient one get \\(x\\) one able regress possible potential outcomes possible conditions units (given density interest \\(f(x)\\)). Often, straightforward approach can define inquiry finite number potential outcomes drawn range treatment variable.\n","code":""},{"path":"defining-the-inquiry.html","id":"undefined-and-unanswerable-inquiries","chapter":"7 Defining the inquiry","heading":"7.3.6 Undefined and unanswerable inquiries","text":"Declaring design terms \\(MIDA\\) may lead two awkward conclusions: inquiry \\(\\) returns \\((m) = ^m = \\mathrm{NA}\\), .e., answer inquiry undefined; answer may (currently) unanswerable, , feasible \\(D\\) \\(\\) yield answer \\(\\). cases, one option change inquiry. often selected \\(\\) importance, may want try find answer. case undefined inquiries, option select new one. case unanswerable inquiries, can work identify novel \\(D\\) \\(\\) change set feasible designs provide answer. cases, however, fact inquiry unanswerable may due unchangeable limitations research fundamental problems causal inference descriptive inference.","code":""},{"path":"defining-the-inquiry.html","id":"example-1","chapter":"7 Defining the inquiry","heading":"7.3.7 Example","text":"Björkman Svensson (2009) reports results cluster-randomized trial effects community-based monitoring health clinics Uganda improve children’s health. main inquiry average treatment effect child mortality rate. study showed program success: control group, child mortality rate 144 per 1000 live births, compared 97 treatment group, 33% reduction child mortality.study also considered second inquiry: average treatment effect weight--age children 18 months. inquiry harder think , precisely know community-based monitoring saved lives many children. see problem, consider Table 7.6, shows four types infants distinguished basis potential outcomes. Type (“Adverse”) alive control, dies treatment. Type B (“Beneficial”) just reverse: child dies untreated, survives treated. Type C (“Chronic”) die either condition Type D (“Destined”) live either condition. first three types, child dies one condition, , . Since weight--age exists child survives, treatment effect weight , B, C types undefined.main trouble average treatment effect averages four types ATE also undefined. Since inquiry undefined, ’ll need select different one.might want switch inquiry average effect among D types . However inquiry different problem – ’s unanswerable. Even though inquiry undefined, ’s unanswerable won’t able learn D types . treatment group, can’t tell Bs Ds control group, can’t tell Ds. reason can’t tell types apart using realized data fundamental problem causal inference.Table 7.6:  Latent typesWhat can done situation? might try find covariate \\(X\\) correlated D type condition analysis effect weight covariate. Note among subgroup, effect mortality equal zero. Finding covariate hard, though perhaps impossible.Alternative, might define inquiry average treatment average weight living children within cluster, inquiry requires careful interpretation. treatment saves lives children whose health marginal particular, effect average weight easily negative, even though treatment improves health. specifically interpret answer inquiry refer effect treatment health. said, might nevertheless useful number know position administering healthcare resources – ’d want send additional help treatment areas lives unhealthy children (otherwise perished) saved.","code":""},{"path":"defining-the-inquiry.html","id":"how-should-you-select-inquiries","chapter":"7 Defining the inquiry","heading":"7.4 How should you select inquiries?","text":"’s hard know start picking research question. want pick one interesting right one facilitate real-world decision. want pick research questions can learn answer someday, lot effort. Unfeasible research questions abandoned soon possible, course ’s hard . trouble ’s hard know research questions feasible start looking , ’s really hard quit research projects learn unfeasible sunk cost fallacy. Among feasible research questions, want select ones likely obtain informative answers, terms moving priors . last criteria often help us select among related, feasible inquiries DAG know can learn less research.Sometimes, people give advice students follow “theory-first” route picking research question. Read literature, find unsolved puzzle, start choosing among methodological approaches might answer problem. Others eschew theory-first approach: “earth going happen land upon unsolved – yet somehow solvable – puzzle just reading!?” advice-givers emphasize method-first route. Master technical data-gathering analysis procedures first, set find opportunities apply . theory-first people say: “know interesting theoretical question smacked face!?”Iteration two typically necessary. order select research questions, empiricists concerned entire research design. develop empirical strategies provide answers inquiries. learn lot select data answer strategies ways map inquiries models. empiricists learn models inquiries (theory) well data strategies answer strategies (empirics).first criteria subjective importance question. object importance may scientist, considering value building theoretical understanding world; policymaker, deciding collect allocate resources government; private firm, making decisions invest resources maximize profit; another individual organization. scientific enterprise designed around idea importance eye scientist objective quantity. two reasons. First, scientific practical importance discovery may understood decades later, pieces causal model put together world faces new problems. Moreover, “importance” differs different segments society, scientists must able study questions judged important groups power order discover new ways solve problems faced left-groups.Among important questions, researchers can select \\(\\) think likely able learn . start prior distribution \\(^w\\), true answer inquiry, past research, good research design substantially update prior distribution, either moving mean distribution reducing uncertainty .final criteria among important, probative research designs must feasible \\(M\\), \\(D\\), \\(\\) answer \\(\\). means picking good question \\(\\) just involve theory. study \\(M\\) understand \\(\\)s worth knowing. also study \\(D\\) \\(\\) order learn demonstrate \\(\\). central goal research methodologists expand feasible set \\(D\\) \\(\\) can provide informative answers important questions.Clearly research designs vary important, feasible, probative , criteria provide immediate answer select design. Instead, researcher must choose weighting three, norms research community may guide weightings. disciplines, providing minimally probative answer highly important question may preferable highly probative answer less important question.\n\n\n\n\n\n\n’s best advice get started picking research question: Write \\(M\\) \\(\\) causal model think important get started thinking selecting strong \\(D\\)s \\(\\)s. goal learn map \\((M)\\) \\((D)\\). return theory find new important inquiries, write new model inquiry data strategy answer strategy. process make progress \\(M\\), , bring \\(M\\) closer \\(W\\), thereby making \\(M\\) truer. iteration, consider informative answers can provide: much update know world?People looking forest mushrooms often don’t see mushroom long period time. , acclimate. get “eyes ,” successful finds seem around every bend. analogy, theory building process going walk forest methods training learning spot mushrooms – need get eyes answerable research questions worth answering.","code":""},{"path":"defining-the-inquiry.html","id":"further-reading-2","chapter":"7 Defining the inquiry","heading":"7.5 Further reading","text":"Goertz Mahoney (2012) differences across inquiries qualitative quantitative research.Dawid (2000) cause--effects questions.Yamamoto (2012) causal attribution.Zhang Rubin (2003) “truncation--death”","code":""},{"path":"crafting-a-data-strategy.html","id":"crafting-a-data-strategy","chapter":"8 Crafting a data strategy","heading":"8 Crafting a data strategy","text":"order collect information world, researchers must deploy data strategy. Depending design, data strategy include decisions following: sampling, assignment, measurement. Sampling procedure selecting units measured; assignment procedure allocating treatments sampled units; measurement procedure turning information sampled units data.Sampling choices confront fundamental problem generalization: want make inferences units sampled. reason, need pay special attention procedure units selected sample. might use random sampling procedure order generate design-based justification generalizing samples specific population units drawn. Nonrandom sampling procedures also possible: convenience sampling, respondent-driven sampling, snowball sampling data strategies include explicitly randomized component. Nonrandomized designs usually require model-based inference order generalize sample population.Assignment choices confront fundamental problem causal inference: want make inferences conditions units assigned. reason, experimental design focused assignment treatments. many treatment conditions ? use simple coin flip decide receives treatment, use complicated strategy like blocking? Experimenters course also concerned sampling measurement procedures, random assignment treatments make randomized experiments distinctive among research designs.Measurement choices confront fundamental problem descriptive inference: want make inferences latent values basis measured values. tools use measure critical part data strategy. many social scientific studies, main way collect information surveys. huge methodological literature survey administration developed help guide questionnaire development. Bad survey questions yield distorted noisy responses. biased question systematically misses true latent target designed measure, case question low validity. question high variance (hypothetically) obtain different answers time asked, case question low reliability. concerns validity reliability disappear move survey environment. example, information shows administrative database result many human decisions, possibility increasing decreasing distance measurement latent measurement target.three problems fundamental sense surmounted researchers humble face . Strong research design can help, can never sure sample generalizes, know happened counterfactual state world, true latent value outcome (even exists). Researchers choose good sampling, assignment, measurement techniques , combined applied world, produce analysis-ready information. discuss answer strategies – set analysis choices data collected – next chapter. data answer strategies course intimately interconnected. analyze data depends deeply collected collect data depends just deeply plan analyze . moment, thinking many choices might make part data strategy, applied research design setting, considered concert answer strategy.data strategy \\(D\\) set procedures result dataset \\(d\\). important keep two concepts straight. apply data strategy \\(D\\) world \\(w\\), produces dataset \\(d\\). say \\(d\\) “” result \\(D\\), since apply data strategy world, obtain data obtain. crafting data strategy, think many datasets data strategy produced. datasets might really excellent. example, good datasets, achieve good covariate balance across treatment control groups. might draw sample whose distribution observable characteristics looks really similar population. datasets might worse: vagaries randomization, particular realizations random assignment random sampling might less balanced. settle data strategies might produce weak datasets – control procedures choose. want choose data strategy \\(D\\) likely result high-quality dataset \\(d\\).illustrate data strategy three elements: sampling, assignment, measurement. Sampling procedure selecting units measured; assignment procedure allocating treatments sampled units; measurement procedure turning information sampled units data. empirical studies data strategy every data strategy involves sampling, assignment, measurement. Even studies researchers measure full universe cases involve sampling, since future units included. Even studies researchers apply treatments involve assignment, since forces end assigning units conditions. Even studies researchers take new measurements involve measurement, since choice measurements made others use must nevertheless made.figure, rule threats inference come implementation: failure treat (noncompliance), failure inclusion sample (attrition), causal relationships random sampling assignment well measurement latent outcome (excludability). last section chapter, discuss threats realized can mitigate risks present design.","code":""},{"path":"crafting-a-data-strategy.html","id":"p2sampling","chapter":"8 Crafting a data strategy","heading":"8.1 Sampling strategies","text":"Sampling process units selected population studied. sampling procedures involve randomization others .\n\nWhether sampling procedure randomized large implications answer strategy. Randomized designs support “design-based inference,” refers idea rely known features sampling process producing population-level estimates – much next chapter Answer strategies. randomization breaks (e.g., design encounters attrition) nonrandomized designs used, fall back model-based inference generalize sample population. Model-based inference relies researcher beliefs nature uncontrolled sampling process order make inferences population. possible, design-based inference advantage letting base inferences known rather assumed features data selection process. said, randomly sampled individuals fail respond seek make inferences new populations, must apply model-based inference even data result random process.ever content study sample full population? infinite populations choice. finite populations first best explanation cost: ’s expensive time-consuming conduct full census population. Even well-funded research projects face problem, since money effort spent answering one question also spent answering second question. tend sample rather measure every unit population face opportunity costs well. second reason reason sample diminishing marginal returns additional data collection. Increasing number sampled units 1,000 2,000 greatly increase precision estimates. Moving 100,000 101,000 improve things , scale improvement much smaller.","code":""},{"path":"crafting-a-data-strategy.html","id":"randomized-sampling-designs","chapter":"8 Crafting a data strategy","heading":"8.1.1 Randomized sampling designs","text":"Owing natural appeal design-based inference, start randomized designs proceeding nonrandomized designs. Randomized sampling designs typically begin list units population, choose subset sample using random process. random processes can simple (every unit equal probability inclusion) complex (first select regions random, villages random within selected regions, households within selected villages, individuals within selected households).Table ?? collects kinds random sampling together offers example functions randomizr package can use conduct kinds sampling. basic form simple random sample, also called “coin flip” Bernoulli random sampling. simple random assignment, units population probability \\(p\\) included sample. sometimes called coin flip random sampling though unit, flip weighted coin probability \\(p\\) landing heads-. quite straightforward, drawback simple random sampling can’t sure number sampled units advance. average, ’ll sample \\(N*p\\) units, sometimes slightly units sampled sometimes fewer.Table 8.1:  Kinds random samplingComplete random sampling addresses problem. complete random sampling, exactly \\(n\\) \\(N\\) units sampled. unit still inclusion probability \\(p = n/N\\), contrast simple random sampling, guaranteed final sample size \\(n\\).9. Complete random sampling represents improvement simple random sampling rules samples fewer \\(N*p\\) units sampled. One circumstance might nevertheless go simple random sampling size population known advance sampling choices made “fly.”Complete random sampling solves problem fixing total number sampled units, doesn’t address problem total number units particular characteristics fixed. Imagine population \\(N_{y}\\) young people \\(N_{o}\\) old people. sample exactly \\(n\\) population \\(N_{y} + N_{o}\\), number sampled young people (\\(n_y\\)) sampled old people (\\(n_{o}\\)) bounce around sample sample. can solve problem conducting complete random sampling within group units. procedure goes name stratified random sampling, since sampling conducted separately within strata units.10 example, strata formed dichotomoous grouping people “young” “old” categories, general, sampling strata can formed information units sampled. Stratification offers least three major benefits. First, defend sampling surprisingly units stratum “bad luck.” Second (discussed chapter answer strategies) stratification tends produce lower variance estimates inquiries. Finally, stratification allows researchers “oversample” subgroups particular interest.Stratified sampling confused cluster sampling. Stratified sampling means fixed number units particular group drawn sample. Cluster sampling means units particular group brought sample together. example, cluster sample households, interview individuals living sampled household. Clustering introduces dependence sampling procedure – one member household sampled, members also always sampled. Relative complete random sample size, cluster samples tend produce higher variance estimates. Just individual sampling designs, cluster sampling comes simple, complete, stratified varieties exactly parallel logics motivations.Lastly, turn multi-stage random sampling, conduct random sampling multiple levels heirarchically-structured population. example, might first sample regions, villages within regions, households within villages, individuals within households. sampling steps might stratified clustered depending researcher goals. purpose multi-stage approach typically balance logistical difficulties visiting many geographic areas relative ease collecting additional data arrived.Figure 8.1 gives graphical interpretation kinds random sampling. , imagine population 64 units two levels heirarchy. concreteness, can imagine units indiviudals nested within 16 households four people 16 households nested within four villages four people . Starting top left, simple random samping individual level. inclusion probability set 0.5, average, sample 32 people, particular draw, actually sampled 29. Complete random sampling (top center), fixes problem, exactly 32 people sampled – 32 unevenly spread across four villages. addressed stratified sampling – sample exactly 8 people random village 16 total people.Moving middle row figure, three approaches clustered random sampling. simple sampling cluster, cluster probability \\(p\\) inclusion sample, average sample eight clusters. time, sampled seven, problem can fixed complete random sampling (center facet), uneven distribution across villages. Stratified cluster sampling ensures exactly two households village sampled.bottom row figure illustrates approaches multistage sampling. bottom left panel, conduct simple random sample individuals sampled cluster. bottom center, draw complete random sample indiviudals sampled household. bottom left, stratify individual level characteristic – always draw one individual row household. “Row” refer age household members. doubly-stratified multistage random sampling procedure ensures sample two households village within households, one older member one younger member.\nFigure 8.1: Nine kinds random sampling\n","code":""},{"path":"crafting-a-data-strategy.html","id":"nonrandomized-sampling-designs","chapter":"8 Crafting a data strategy","heading":"8.1.2 Nonrandomized sampling designs","text":"nonrandomized sampling procedures defined don’t – don’t use randomization – hugely varied set procedures described way. ’ll consider just common ones, since idiosyncracies approach hard systemetize.Convenience sampling refers practice gathering units population least expensive way available . Convenience sampling good choice generalizing explicit population main goal design, example sample average treatment effect theoretically-important inquiry. many decades, social science undergraduates abundant data source available academics many important theoretical claims established basis experiments conducted samples. recent years, however, online convenience samples like Mechanical Turk, Prolific, Lucid mostly supplanted undergraduates convenience sample choice. (mostly nonexperimental) circumstances, however, convenience sampling likely lead badly biased estimates. example, cable news shows often conduct viewer polls taken seriously. polls might promote viewer loyalty (might worth cable executives’ perspective) provide credible evidence population large thinks believes.Many types qualitative quantitative research involve convenience sampling. Archival research often involves convenience sample documents certain topic exist archive. question documents differ different archive, documents available archives differ ever make archive importantly shapes can learn (Aliza Luft paper cite). decline telephone survey response rates (cite), researchers can longer rely random digit dialing obtain representative sample people many countries, instead must rely convenience samples internet panels agree phone numbers list. Sometimes, reweighting techniques answer strategy can, cases, help recover estimates population whole sampling credible model unknown sampling process can agreed upon.Next, consider purposive sampling. Purposive catch-term rule-based sampling strategies involve random draws also purely based convenience cost. common example quota sampling. Sampling purely based convenience often means end many units one type another type. Quota sampling addresses problem continuing search subjects target counts (quotas) find subject found. Loosely speaking, quota sampling convenience sampling stratified random sampling complete random sampling: fixes problem enough (many) subjects particular types sampled employing specific quotas. Importantly, however, guarantee sampled units within type representative type : quota samples within-stratum convenience sampes.second common form purposive sampling respondent-driven sampling, used sample hard--reach populations HIV-positive needle users. RDS methods often begin convenience sample systematically obtain contacts units share characteristic order build large sample.three nonrandom sampling procedures – convenience, quota, respondent-driven – illustrated Figure ~ref(fig:nonrandomsamplingquilt). Imagining village easier reach, obtain convenience sample contacting everyone can reach village moving village B. process doesn’t yield good coverage across villages can turn quota sampling. quota sampling scheme, talk five people easiest reach four villages. Finally, conduct respondent-driven sample, select one seed unit village, person recruits four closest friends (may may reside village).\nFigure 8.2: Three forms non-random sampling.\n","code":""},{"path":"crafting-a-data-strategy.html","id":"sampling-designs-for-qualitative-research","chapter":"8 Crafting a data strategy","heading":"8.1.3 Sampling designs for qualitative research","text":"Case selection another term sampling strategy. case study research, whether qualitative quantitative, way select (typically small) set cases great importance, considerable attention paid developing case selection methods.John Stuart Mill (1884) elaborated methods induction inspired two popular case selection methods based whether outcomes “different” “agree.” method difference involves selecting cases divergent outcomes otherwise look similar. one characteristic covaries outcome, becomes candidate cause. example, Skocpol (1979) compares historical periods France, Russia, United Kingdom, Germany look similar many regards. first two, however, social revolutions, second two . presence agrarian institutions provided degree political autonomy peasants France Russia absence UK Germany becomes possible clue understanding underlying causal structure social revolutions. discuss debate around answer strategy section answer strategies. contrast, method agreement involves selecting cases share outcome diverge host characteristics. characteristics common cases become candidates causal attribution.11 strategies, selection cases guided inquiry answer strategy.qualitative case selection strategies borrow directly toolkit quantitative, large-\\(N\\) analysis. datasets large number cases available, Lieberman (2005) proposes using predicted values regression model—often referred “regression line”—initial quantitative analysis order select cases -depth analysis.12 , inquiry answer strategy guide case selection. inquiry focused uncovering causal relationship sought quantitative analysis, Lieberman (2005) suggests selecting cases relatively well-predicted maximize variation causal variable. points Martin (1992) Swank (2002) examples designs employing strategy. However, Lieberman (2005) advocates different case selection strategy goal expand upon theory initially tested quantitative analysis. instance, recommends choosing cases lying far regression line, well-predicted may therefore lead insights alternative mechanisms left initial regression.Seawright Gerring (2008a) use regression line analogy describe seven different sampling strategies tailored suit different inquiries, depicted Figure 8.3.13 horizontal axis represents characteristic cases, \\(X\\), degree labor union strength. vertical axis represents outcome, \\(Y\\), degree welfare state generosity. line represents predicted cross-case relationship, dark grey squares represent possible cases population, blue squares represent sample chosen strategy.\nFigure 8.3: Case selection Strategies\nTypical cases typify cross-case relationship can chosen order explore validate mediating mechanisms. researcher’s model implies union membership increases welfare spending democracies effects negotiations government, example, researcher might look evidence processes cases well-predicted theory. Diverse cases maximize variation \\(X\\) \\(Y\\), extreme cases located maximal distance cases just one dimension—example, researcher chooses two cases highest degree union strength. diverse extreme cases might lie regression line, deviant cases defined distance . Influential cases whose exclusion noticeably change imaginary regression line (.e., highest leverage regression). Thus, diverse extreme case selection can useful exploratory work, like updating theoretical model finding new ways pose inquiry, deviant influential case selection can useful understanding scope conditions theory. Confusingly, “similar systems” design employs Mill’s method difference order select cases similar \\(X\\) different \\(Y\\), “different systems” design employs method agreement select cases agree \\(Y\\) different characteristics.unlikely every strategy available provide equally good answers every research design, one method always fare better others. Declaring simulating design simplifying assumptions model, inquiry, answer strategy clarifies “cases choose affect answers get” (Geddes 2003).example, Michael C. Herron Quinn (2016) used Monte Carlo simulations study well seven strategies depicted perform model stipulates cases four causal types,14 inquiry average treatment effect population, answer strategy involves, perhaps optimistically, perfectly observing selected cases’ causal type. simplifying assumptions, uncover clear hierarchy set prescriptions: extreme deviant case selection fare much worse methods terms three diagnosands considered (root mean square error, variance, bias mean posterior distribution). contrast, influential case selection outperforms strategies, followed closely diverse simple random sampling. authors acknowledge, however, hierarchy might look different inquiry aimed different, exploratory quantity (discovering number causal types exist). Humphreys Jacobs (2015) provide simulations incorporate process tracing inferential procedure highlight importance “probative value” case selection. point rarely case selection strategy fits problems equally well—best strategy one optimizes particular diagnosand given stipulations inquiry, model, answer strategy. can justify stipulations importance diagnosand, defending choice sampling strategy straightforward.point speaks two greatest controversies qualitative case selection: whether randomize whether select using information dependent variable.Since qualitative case analysis labor-intensive, sample sizes typically small (1-30 case range). Scholars like Seawright Gerring (2008a) point random sampling therefore produces high variance can even lead abberant samples variation outcomes explanatory variables, share idiosyncratic features locations. , advocate purposive selection. Fearon Laitin (2008) point , however, convenience samples purposive selection can lead severe bias, especially goal make population-level inference. advocate use stratified random sampling. Yet, question whether randomly sample resolved deciding claims correct, since correct different diagnosands: nonrandom sampling might minimize variance maximizing bias, random sampling might increase variance minimizing bias. even diagnosand-specific claims conditional specific model, inquiry, answer strategy authors mind.reasoning also applies question whether incorporate information dependent variable sampling decisions. Geddes (2003) famously critiqued Skocpol (1979) making inference role foreign threat social revolution based selection cases experienced revolutions.15 essence, Geddes criticizes resultant lack variation outcome, makes difficult assess whether revolutions failed occur, example, countries face foreign threats.16 However, critique assumes Skocpol’s answer strategy based Mill’s method difference17 inquiry focuses something like average effect foreign threat revolution—.e., effects--cause question. design library, declare process-tracing design inquiry cause--effects question: , average effect \\(X\\) \\(Y\\), , given \\(Y\\) happened, likelihood happened \\(X\\)? inquiries, selecting dependent variable essential (’s hard study \\(Y\\) happened didn’t happen!), advantageous: cuts space competing causal hypotheses half, drastically simplifies answer strategy. Note prescription necessarily hold quantitative answer strategies---@yamamoto2012understanding shown “negative” cases necessary estimate cause--effects questions, even necessary define inquiry.Thus, many data strategy choices, whether makes sense sample using information outcomes depend , M, , mention diagnosands, \\(\\phi\\), judge design performance.\n","code":""},{"path":"crafting-a-data-strategy.html","id":"choosing-among-sampling-designs","chapter":"8 Crafting a data strategy","heading":"8.1.4 Choosing among sampling designs","text":"short, choice sampling strategy depends features model inquiry, different sampling strategies can compared terms power RMSE design diagnosis. model defines population units interested making inferences , target population sampling strategy match much possible. model also points us important subgroups (defined nodes endogenous variables) may wish stratify , depending variability within subgroups. Whether select convenience, random, purposive sampling depends budget logistical constraints well efficiency (power RMSE) design. little bias convenience sampling, often want select cost reasons; obtain convenience sample right composition, may choose purposive method ensures . choice simple stratified sampling comes inquiry diagnosis RMSE: inquiry involves comparison subgroups, often select stratified sampling. either, diagnosis alternative designs terms power RMSE guide selection.Sampling, like data strategies, intervention world researchers. result, may independent causal effects outcomes. see next section, deep difference sampling treatment assignment, case randomly sampling two potential outcomes. treatment assignment, assigning units treatment group control group, obtaining sample treated potential outcome second random sample control potential outcome. random sampling, assigning units sampled sampled. obtain one sample sampled potential outcome. However, non-sampled potential outcome exists conceptually, may differ sampled potential outcome. act including units sample may change outcomes, example selected participate medical trial believe going receive better care placebo effect changes health condition. research design conduct experiment sampled units, also unobtrusively measure outcomes nonsampled units used estimate effect inclusion sample. design provide random sample nonsampled potential outcome.’re often interested extrapolating sample population sample drawn , new populations. design-based inference solutions making inferences population directly sampled , model-based inference required extrapolate new populations population another time period.","code":""},{"path":"crafting-a-data-strategy.html","id":"example-wang2015forecasting","chapter":"8 Crafting a data strategy","heading":"8.1.5 Example: Wang et al. (2015)","text":"Xbox forecasting elections","code":""},{"path":"crafting-a-data-strategy.html","id":"p2assignment","chapter":"8 Crafting a data strategy","heading":"8.2 Treatment assignment","text":"many studies, researchers intervene world set level causal variable interest. procedures used assign units treatment tightly analogous procedures explored previous section sampling. Like sampling, assignment procedures fall two classes, randomized nonrandomized. Among radnomized procedures can distinguish host two arm trial designs, multiarm designs, set designs advanced designs.","code":""},{"path":"crafting-a-data-strategy.html","id":"two-arm-trials","chapter":"8 Crafting a data strategy","heading":"8.2.1 Two arm trials","text":"analogy sampling assignment runs deep. sampling designs discussed previous section directly equivalent assignment designs. Simple random sampling analogous Bernoulli random assignement, stratified random sampling analogous blocked random assignment . Many design tradeoffs hold well: just like cluster sampling generates higher variance estimates individual sampling, clustered assignment generates higher variance estimates individual assignment. usually think randomized assignment designs , nonrandomized designs research applies treatments also occur. example, researchers sometimes treat convenience sample, search different convenience sample serve control group. Within-subject designs subjects measured, treated, measured second example nonrandomized application treatment.analogy sampling assignment runs deep sense, assignment sampling. Instead sampling units study, sample alternative possible worlds. treatment group represents sample alternative world units treated control group represents sample alternative world units untreated.18 can reencounter fundamental problem causal inference lens – unit sampled one possible world, can’t sampled possible world. Table @(tab:assignmenttypes) collects together common forms random assignment.Table 8.2:  Kinds random assignmentFigure 8.4 visualizes nine kinds random assignment, arranged according whether assignment procedure simple, complete, blocked according whether assignment procedure carried individual, cluster, saturation level. top left facet, simple (Bernoulli) random assignment, units 50% probability treatment, total number treated units bounce around assignment assignment. top center, problem fixed: complete random assignment, exactly \\(m\\) \\(N\\) units assigned treatment \\(N - m\\) assigned control. complete random assignment fixes number units treated exactly \\(m\\), number units treated within particular group units (defined pre-treatment covariate) bounce around. block random assignment, conduct complete random assignment within block separately, directly control number treated within block. Moving simple complete random assignment tends decrease sampling variablity bit, ruling highly unbalanced allocations. Moving complete blocked can help , long blocking variable correlated outcome. Blocking rules assignments many units particular subgroup treated. build intuition correlation blocking variable outcome important, consider forming blocks random; none assignments complete random assignment ruled , sampling distributions two assignment procedures equivalent.second row Figure 8.4 shows clustered designs units within cluster receive treatment assignment. Clustered designs common household-level, school-level, village-level designs, impratical infeasible conduct indiviudal level assignment. units within cluster alike units different clusters (cases), clustering increases sampling variability relative individual level assignment. Just like individual-level designs, moving simple complete complete blocked tends result lower sampling variability.final row Figure 8.4 shows series designs analogous multi-stage sampling designs shown Figure 8.1 – purpose subtly different spirit. Multi-stage sampling designs employed reduce costs – first clusters sampled units within cluster sampled. saturation randomization design (sometimes called “partial population design” cite Berk et al) uses similar procedure contain learn spillover effects. clusters chosen treatment, units within clusters treated. Units untreated treated clusters can compared units untreated untreated clusters order suss intra-cluster spillover effects (Sinclair, McConnell, Green). figure shows saturation designs comes simple, complete, blocked varieties.\nFigure 8.4: Nine kinds random assignment. first row individuals sampling units, second row clusters sampled, third clusters sampled individuals within clusters sampled. first column units sampled independently, second units sampled hit target, third units sampled hit targets within strata.\n","code":""},{"path":"crafting-a-data-strategy.html","id":"multiarm-and-factorial-trials","chapter":"8 Crafting a data strategy","heading":"8.2.2 Multiarm and factorial trials","text":"Thus far considered assignment strategies allocate subjects either treatment control. designs considered far generalize quite nicely multiarm trials. Trials three, four, many arms can course simple, complete, blocked, clustered, feature variable saturation. Figure ?? shows blocked versions three-arm trial, factorial trial, four-arm trial.three-arm trial left, subjects can assiged control condition one two treatments. design enables three comparisons: comparison treatment control condition, also comparison two treatment conditions . four-arm trial right, subjects can assigned control condition one three treatments. design supports six comparisons: treatments control, three pairwise comparisons across treatments.two--two factorial design center panel shares similarities three-arm four-arm trials. Like three arm, considers two treatments T1 T2, also includes fourth condition treatments applied. Factorial designs can analyzed like four-arm trial, structure design also enables subtle analyses. particular, factorial structures allows researchers investigate whether effects one treatment depend level treatment.\nFigure 8.5: Multi-arm random assignment.\n","code":""},{"path":"crafting-a-data-strategy.html","id":"complex-designs","chapter":"8 Crafting a data strategy","heading":"8.2.3 Complex designs","text":"Patient preference trialsStepped-wedge assignmentMulti arm bandits\nFigure 8.6: Step-wedge random assignment.\n","code":""},{"path":"crafting-a-data-strategy.html","id":"non-randomized-assignment","chapter":"8 Crafting a data strategy","heading":"8.2.4 Non randomized assignment","text":"Alphabetical assignmentRDDBayesian optimal assignment","code":""},{"path":"crafting-a-data-strategy.html","id":"example-sociology-natural-experiment-from-government-or-official-cutoff","chapter":"8 Crafting a data strategy","heading":"8.2.5 Example: sociology natural experiment from government or official cutoff","text":"","code":""},{"path":"crafting-a-data-strategy.html","id":"measurement","chapter":"8 Crafting a data strategy","heading":"8.3 Measurement","text":"Measurement part data strategy variables collected population units enable sampling, variables collected sample treatment assignment including used treatment assignment, outcomes collected treatment assignment. variables used answer strategy collected measurement, aside treatment assignment variable assignment sample inclusion probabilities.fundamental problem description can never measure latent variables interested , \\(Y^*\\), fear, support political candidate, economic well-. Instead, use measurement technology imperfectly observe , represent function \\(Q\\) yield observed outcome \\(Y^{\\mathrm obs}\\): \\(Q(Y^*) = Y^{\\mathrm obs}\\). measurement strategy set functions \\(Q\\) variable measure.two basic ways assess quality function \\(Q\\): bias, difference observed latent outcome, \\(Y^{\\mathrm obs} - Y^*\\), given special label measurement validity; measurement reliability, variance across multiple outcomes given individual, \\(\\mathbb{V}(Y_1^{\\mathrm obs}, Y_2^{\\mathrm obs}, Y_3^{\\mathrm obs})\\).Researchers select several characteristics \\(Q\\): collects measures; mode measurement; often measures taken; many different observed measures \\(Y^*\\) collected summarized single measure; information provided participants measured (). design characteristics may affect validity, reliability, .Data may collected researcher , participant, third party. forms qualitative research participant-observation interview-based research, researcher may primary data collector. survey research, interviewer typically hired agent researcher, many cases multiple interviewers hired. interviewers may ask questions differently, leading less reliable (variable) answers cases validity problems ask questions way leads biased measures \\(Y^*\\). Participants often asked collect data , either self-administered surveys, journaling, taking measurements using thermometers scales. primary concern self-reports validity: respondents report measurements truthfully. parallel concern raised participants collect data made aware fact measured others. Finally, data may collected agents government organizations, yielding -called administrative data. difference administrative data forms data identity data collector.variety measurement strategies data collectors obtain data. Humans can code data observation five senses sight, hearing, touch, smell, taste, asking humans self-reports surveys. Measurement instruments can also used record waves light (e.g., photos), sound (e.g., audio seismic recordings), electromagnetism (e.g., EKGs x-rays), combinations one (e.g., video); characteristics atmosphere (e.g., temperature pressure), water (e.g., salinity pollution), soil (.e., mercury pollution); human animal health (e.g., blood tests). Considerable recent progress made taking advantage measurement modes due increasing computing power machine learning techniques can code streams raw data photos, videos, sources translate usable data. translation raw data coded data can used analysis part \\(Q\\) measurement strategy.data collected, often, can also affect validity reliability. inquiry guide data collected relation events election holiday period time treatment delivered research participants. inquiry defines whether effect interest month treatment case long-term effects year . However, data need collected single time period. model encodes beliefs autocorrelation (correlation time) data varies time, can help guide whether collect multiple measurements just one whether measure data baseline well endline. data expected highly variable (low autocorrelation), taking multiple measurements averaging may provide efficiency gains. exhibit high autocorrelation, multiple measures waste resources — approximately measure returned time. However, high autocorrelation large gains collecting baseline measure treatment experiment, controlling baseline outcomes major change response treatment. Thus, estimates treatment effects much efficient (T cite). Beliefs autocorrelation can guide decisions tradeoff including participants measures smaller set participants.Parallel considerations apply choice measure latent \\(Y_i^*\\) multiple measurement tools time period. possible, taking multiple, different measures averaging typically yield efficiency improvements. (cases, may tradeoff budget constraint terms survey length number subjects.) multiple measures can combined construct single measure \\(Y_i^*\\). tools produce answers highly correlated, taking multiple measures unlikely worth cost correlation low worth taking multiple measurements averaging improve efficiency.Beyond , two important features data collection strategy often. experiments, data often collected treatment delivered, sometimes baseline enable block randomization -comparisons. difference--difference studies, data always collected , buttress claims parallel trends treated control often multiple periods treatment. process tracing studies, data typically collected many periods change independent variable change dependent variable demonstrate connection two. often data collected treatment (change independent variable) influences reliability: measuring highly variable outcomes multiple times averaging can increase measurement reliability. flip side variables fast change, taking measurements baseline compare endline measurements likely increase reliability. outcomes change slowly, however, baseline measurements likely substantially improve efficiency estimates treatment effects (discussion, see cite T).Perhaps common pitfall measurement sensitivity bias affects measures self-reported participants presence social pressure risk sanction authorities prefer hide wish detect sensitive characteristics population (BCM2020). Sensitivity bias represents excludability violation: latent outcome \\(Y_i^*\\) depends measurement tool used. generally, excludability violations measurement imply act measurement affects latent outcome, often known Hawthorne effect. Typically, want avoid excludability violations, trying measure latent quantity exists independent measured.Selecting among measurement modes, data collectors, time periods, frequency, number measurements reduces tradeoffs validity reliability. want largest set valid, measures combine measurement \\(Y_i^*\\) meet budget logistical constraints. However, measurement techniques vary validity reliability, often must decide weighting two concerns given research goals. cases, validity important even measures unreliable, others may want weight approximately equally.Learning measurement tools valid reliable ultimately guesswork, though can informed guesswork. measure true \\(Y_i^*\\), truly “validate” measurement technique. Often studies present validation studies comparing proposed measure “ground truth,” measured administrative data second technique reduce measurement error. However, neither measurement known exactly \\(Y_i^*\\), ultimately studies comparisons multiple techniques advantages disadvantages. make useless, rather used understand measurements differ average variability. may also useful informing combine multiple measures.inattentiondifferential measurement\nparallel measurement across treatment conditions\nmeasurement bias subtracted T-C comparison – really measuring (identification saved yes, identify)","code":""},{"path":"crafting-a-data-strategy.html","id":"example-weaver2019too","chapter":"8 Crafting a data strategy","heading":"8.3.1 Example: Weaver, Prowse, and Piston (2019)","text":"","code":""},{"path":"crafting-a-data-strategy.html","id":"threats-to-implementation","chapter":"8 Crafting a data strategy","heading":"8.4 Threats to implementation","text":"\nFigure 8.7: DAG exclusion restrictions.\nData strategies plans sample units, assign treatments, measure outcomes. studies always go according plan. section, explore threats inferences emerge implementing data strategies. Anticipating planning threats presents two benefits: can assess properties designs actually run, obtaining realistic guesses power bias study given problems like attrition noncompliance; can incorporate procedures mitigate risk threats part designs., adapt figure first present chapter’s introduction introduce threats come noncompliance (failure treat), attrition (failure included sample provide measures), excludability violations (causal effects random sampling, random assignment, measurement latent outcome).","code":""},{"path":"crafting-a-data-strategy.html","id":"noncompliance","chapter":"8 Crafting a data strategy","heading":"8.4.1 Noncompliance","text":"first type threat implementation noncompliance: random assignment \\(Z\\) imperfectly manipulates treatment variable \\(D\\). absence noncompliance, \\(D_i = Z_i\\). One-sided noncompliance setting treated units fail treated (receive control condition instead). Two-sided noncompliance units fail treated, addition units assigned control group receive treatment.randomized experiments, noncompliance may happen due administrative error, miscommunications researcher partners, shortages materials staff, transportation problems, participants refusing treatment, inability researchers find participant treat, among problems.Noncompliance also affects observational designs causal inference nature non-random administrative process affects treatment threshold cut-. instrumental variables designs, noncompliance core: instrument \\(Z\\) causally affects treatment \\(D\\), factors also affect \\(D\\). regression discontinuity design, threshold determines treatment status certain value scale unit treated . noncompliance can also occur, leading fuzzy regression discontinuity designs, threshold imperfect units receive treatment units .two-sided noncompliance, four types participants, determined potential outcomes \\(D\\) function condition unit randomly assigned two. two-arm trial, two treatment potential outcomes: whether receive treatment assigned control (\\(D_i(Z_i = 0)\\)) whether receive treatment assigned treatment (\\(D_i(Z_i = 1)\\)). four types enumerated labeled Table 8.3 .Table 8.3:  Compliance typesNever-takers never take treatment, matter treatment assigned. Compliers take exactly treatment assigned. Sefiers exactly opposite. assigned treatment, refuse , assigned control, take treatment. Like name suggests, always-takers take treatment regardless treatment condition assigned . settings can rule types basis design information. Canvassing experiments, example, feature always takers way “take treatment” unless assigned treatment, never-takers subjects might home canvassers come knocking. experimentsa said experience one-sided noncompliance.presence noncompliance, change inquiry inevitable. average treatment effect estimated comparing assigned treatment assigned control, assignment differs whether treatment received. average difference assigned two conditions can relabeled intent--treat effect. Comparing received treatment also option, without additional herculean assumptions, unobserved heterogeneity now jointly affects \\(D\\) \\(Z\\). randomized experiment broken: treatment group longer comparable control group expectation. Instead, complier average treatment may obtained using instrumental variables estimation, implies switching local inquiry among complier types. effect may differ average treatment effect compliers differ systematically types. Estimating complier average treatment effect requires addition assumptions top randomized experiments, including ignorability treatment assignment , case two-sided noncompliance, monotonicity assumption rules defiers.case randomized experiments, spending budget time carefully design treatment delivery protocols avoid noncompliance help avoid minimize threat noncompliance.parallel set decisions faces designer observational study noncompliance treatments. Instrumental variables designs imply noncompliance inquiry complier average treatment effect (cases, intent--treat effect reduced form effect also interest). Researchers adopt regression discontinuity designs also focus local effect among units near threshold, case fuzzy regression discontinuity design noncompliance must switch complier local average treatment effect.Compliance need binary: assigned treatment, may receive partial treatment none . However, typically difficult measure partial compliance, difficult still account possibility analysis (separate effects partial compliance full compliance without treatments attempt manipulate treatment less strongly).multi-arm trials continuous rather binary instruments, noncompliance becomes complex problem define address data strategy answer strategy. must define complier types according (potentially-infinite) possible treatment conditions. multiarm trials, complier types first treatment may second treatment; words, units comply different rates different treatments. Apparent differences complier average treatment effects intent--treat effects, result, may reflect differences treatment effects different rates compliance.","code":""},{"path":"crafting-a-data-strategy.html","id":"attrition","chapter":"8 Crafting a data strategy","heading":"8.4.2 Attrition","text":"Attrition special form noncompliance sampling measurement, measures units sampled. two types missing data may result: single measure missing, commonly known item nonresponse; measures missing participant, known survey nonresponse. Though terms coined survey researchers, problems identical collected surveys passive measurement instrumentation. Item nonresponse may result many causes, often respondents wish answer specific questions sensitive intrusive. Survey nonresponse questions answered may instead due lack interest measurement presented, inability contact subject, inability data collector build rapport gain trust participant.Attrition occurs least one unit respond least one measurement item. Whether attrition problem depends whether participant response \\(R\\) causally affected variables sampling completely random. Though missingness completely random rare, possible, often due administrative computer error. attrition completely random, effect variable \\(R\\), loss sample size distortion estimates.descriptive inquiries, attrition completely random, types units respond different way respond. Model-based inference required adjust estimates match characteristics original sample, upweighting types respondents less likely respond downweighting likely respond. However, guess wrong set variables reweight model \\(M\\), miss target.causal inquiries, also need know whether random assignment \\(Z\\) affects \\(R\\), well \\(U\\). \\(Z\\) affects \\(R\\) (lower/higher rate attrition one treatment group ), \\(U\\) affect \\(R\\), just loss sample size distortion estimates. simply (less) precise estimate potential outcomes one group . However, \\(Z\\) \\(U\\) affect \\(R\\), random sample either treated potential outcome control potential outcome treatment control group. Therefore, resort model-based inference groups, reweighting group based assumptions (\\(M\\)) \\(U\\) \\(Z\\) affect \\(R\\). Alternatively, can construct bounds estimates causal effect, incorporate uncertainty due non-random attrition.case descriptive causal inquiries, also design-based way estimate effects presence uncertainty rely assumptions \\(M\\): double sampling. missing outcomes, can take second sample respond conduct additional measurement try obtain units’ outcomes. spend resources making sure can obtain measures randomly-sampled units, can use random sample non-responders combination original sample construct full sample. still loss sample size, take subsample second round, attempt interview nonresponders. can also use double sampling causal inquiries, conducting random sample nonresponders treatment group stand missing observations second control group.","code":""},{"path":"crafting-a-data-strategy.html","id":"excludability","chapter":"8 Crafting a data strategy","heading":"8.4.3 Excludability","text":"final type threat implementation research excludability violations. Excludability means define potential outcomes, can exclude variable definition potential outcome. define treated potential outcome latent outcome \\(Y_i^*(D_i = 0)\\), invoke four important excludability assumptions: effect sampling \\(S\\), treatment assignment \\(Z\\) (except treatment \\(D\\)!), measurement \\(Q\\) latent outcome. invoke assumptions, must define treated potential outcome \\(Y_i^*(S_i, Z_i, D_i = 0, Q_i)\\).problem simply one definitions: order define inquiry average treatment effect average possible values \\(S_i\\), \\(Z_i\\), \\(Q_i\\). Many potential outcomes, unsampled units (\\(S_i = 0\\)), impossible obtain known complex potential outcomes. average impossible--observe outcomes, thus identify even infinite data quantity like average treatment effect absence excludability assumption.Excludability assumptions required identification causal descriptive claims data, order interpret effects way want . can often identify average treatment effect separate (exclude!) effects treatment assignment measurement.three excludability assumptions \\(S_i\\), \\(Z_i\\), \\(Q_i\\) representd gray dotted lines Figure 8.7, strong assumptions often met practice.first excludability assumption causal effect sampling \\(Y_i^*\\). assumption violated fact included sample changes attitudes. asked focus group, even assigned treatment asked survey questions, reflect political beliefs change mind, sampling excludability assumption may violated.excludability \\(Z\\) \\(Y^*\\) subtle. studying causal effects, certainly believe treatment assignment effect outcomes. treatment assignment excludability assumption says effect works received treatment \\(D\\). words, independent effect assigned apart actually treated. many ways assumption might violated. observational studies using instrumental variables, excludability \\(Z\\) \\(Y^*\\) assumption alternative channels instrument affects outcomes except endogenous variable. studies effect economic growth civil conflict using rainfall instrument economic growth, must invoke excludability assumption rainfall independent effect civil conflict besides economic growth.Equally worrying excludability measurement assumption, \\(Q\\) affect \\(Y^*\\). Hawthorne effects, fact measured changes outcomes, another word violation excludability assumption. outcomes depend whether subjects know measured , exclude effect measurement effects.final excludability assumption represent DAG \\(Z\\) must effect \\(Q\\). whether measure outcomes depend whether unit assigned treatment. excludability assumptional commonly referred requirement measurement parallel across treatment conditions. measure outcomes using survey treatment group using remote measurement device control, separate (exclude!) effect measurement effect treatment.Researchers avoid excludability violations possible report . cases, best can produce unbiased estimates effect exclude multiple causes. Future research can used disentangle credibly invoke excludability assumption.","code":""},{"path":"crafting-a-data-strategy.html","id":"interference","chapter":"8 Crafting a data strategy","heading":"8.4.4 Interference","text":"four endogenous outcomes DAG research design : \\(R\\), whether participant responds data collection; \\(D\\), whether respondent receives treatment; \\(Y^*\\), latent outcome; \\(Y\\), observed outcome. Setting aside attrition noncompliance moment, \\(R\\) function sampling; \\(D\\) treatment assignment; \\(Y^*\\) \\(D\\); \\(Y\\) measurement strategy \\(Q\\). Interference problem endogneous variables depend whether sampled, assigned treatment, measured, whether units sampled, assigned treatment, measured. assume example \\(Y_i(Z_i) = Y_i(\\mathbf{Z})\\). words, \\(Y_i\\) outcome unit \\(\\), function treatment assignment status \\(Z_i\\) units (\\(\\mathbf{Z}\\). noncompliance assignment, assume instead \\(Y_i(Z_i, D_i) = Y_i(\\mathbf{Z}, \\mathbf{D})\\) \\(D_i(Z_i) = D_i(\\mathbf{Z})\\), need know assignment received treatment status individual \\(\\) know outcome need know whether unit assigned treatment know received treatment.interference assumption strong. Interference common, perhaps even ubiquitous, often referred spillovers. Interference may occur participants tell treatment status, also may occur subtle ways. treatments displace outcomes crime, reduce crime treated locales crime simply moved localities. overall crime rate may change. Displacement form interference. Resource allocation decisions affected treatment also often suffer interference. citizens treated information report potholes , roads agency may move resources untreated areas treated areas, increasing number potholes control areas.Interference can also induced sampling. potential outcomes depend whether units included sample, sampling interference, .e. \\(Y_i(S_i = 1) \\neq Y_i(\\mathbf{S})\\). subjects asked questions induce consider relative status respect people, people sample may affect comparison group potential outcomes.Interference can affect measurement, just affects sampling treatment assignment. Measurement interference occurs \\(Y_i^*\\) depends whether units measured. participants experiment discuss measurement technique researchers asking participants, may lead violation. Psychology researchers academic campuses worry possibility encourage subjects discuss lab potential participants. worried interference treatment assignment also measurement, often implicit measurement techniques can defeated (measure something \\(Y_i^*\\)) concept known session. researchers worry measuring sensitive questions sensitive topic revealed participants either earlier participants local leaders discover purpose.presence interference, unable obtain random sample potential outcome \\(Y_i(Z_i = 1)\\). reason , even simplified setting, units longer either receive treatment , may directly treated, indirectly treated near directly treated, far others truly receive treatment. Thus, group assigned control made units receive indirect treatment treatment. receive purely determined randomization scheme: whether indirectly treated depends close distance social connections units. obtain random sample one potential outcomes, produce unbiased estimates quantities like average treatment effect even average effect depend . may, however, able change inquiry, switching set units can credibly assume interference.cases, interference individuals within household may highly likely (read family’s mail), interference households may less likely. others, interference may likely across households neighborhood neighborhoods. case, may able random sample \\(Y_j(Z_j = 0)\\), potential outcome neighborhood \\(j\\), randomly assigning households within neighborhoods receive treatment condition (.e., cluster random assignment). represents change inquiry – potential outcomes individuals potential outcomes neighborhood – presence interference change may required. Similar solutions available presence sampling measurement interference.Another way change inquiry sample units inclusion study separated physical social distance interference unlikely. might restrict sample include people separated least one mile least two social connections (friends friends--friends ruled ). , sample individual potential outcomes, research design complicated heterogeneous sampling probabilities. high density areas, low sampling inclusion probabilities low density areas high probabilities, easier find samples people separated distance areas low density. fixed problem interference, still units indirectly treated, constructed sampling procedure prevent sampling units design. sample units either randomly assigned treatment control separating units none sample indirectly treated.Interference can also occur time. Potential outcomes can depend whether unit treated present time period, also past. Time interference occurs contemporaneous potential outcome depends sampling, treatment, measurement past. Design can help avoid sampling partially treated units introducing washout periods researcher assumes subjects forget (longer affected ) past treatments research activities.","code":""},{"path":"crafting-a-data-strategy.html","id":"further-reading-3","chapter":"8 Crafting a data strategy","heading":"8.5 Further reading","text":"Lohr (2010) sampling.Shadish, Cook, Campbell (2002), Gerber Green (2012) experimental designSeawright Gerring (2008b) Lieberman (2005) case selectionCollier (2011), Fairfield (2013), Humphreys Jacobs (2015) process tracing","code":""},{"path":"choosing-an-answer-strategy.html","id":"choosing-an-answer-strategy","chapter":"9 Choosing an answer strategy","heading":"9 Choosing an answer strategy","text":"answer strategy plan information gathered world order generate answer inquiry. Qualitative quantitative methods courses overflow advice answer strategies choose. conditions use Ordinary Least Squares, use logit? machine learning algorithm appropriate choice comparative case study informative? answer strategy worth pursuing fundamental limitations data strategy?perspective answer strategies informed three elements research design: model, inquiry, data strategy. Sometimes answer strategy advice offered basis realized data \\(d\\) , without particular attention paid important features question answered manner data collected generated. example, bit received methodological wisdom holds whenever dependent variable binary, binary choice statistical model like logit probit must used. advice based knowledge \\(d\\) contains binary outcome; whether appropriate depends features \\(\\) features \\(D\\). example, \\(\\) average treatment effect \\(D\\) includes randomization treatment, answer strategies beyond binary choice models may serve just well better. student asks, “estimator use?” response always, “depends.” depend ? model, inquiry, data strategy.Choosing “design-aware” answer strategies sounds straightforward enough, precise way approach applied real empirical setting course differ case case. highest-level advice want pick answer strategy \\(\\) answer provides \\((d) = ^d\\) close answer model world, \\((m) = ^m\\). implication strive parallelism across \\(\\) \\(\\). idea sometimes described “plug-principle.” function \\(\\) close function \\(\\), can “plug ” \\(d\\) \\(m\\). Following plug-principle leads straightforward estimation procedures. inquiry population mean, can use sample mean estimator estimate , long data strategy produces data \\(d\\) sufficiently representative \\(m\\). extent possible, want choose data strategies enable plug-estimator answer strategies.level, answer strategies rely unverifiable hopefully plausible assumptions. descriptive inference, assume sample represents population well. causal inference, assume treated untreated units similar respects beyond treatment status. kinds inference, assume measurements close latent construsts wish measure. Answer strategies model-based consequential assumptions part \\(M\\). Answer strategies design-based extent can pull assumptions \\(M\\) assure design, .e., choosing \\(D\\) way can confident assumption true. Observational causal inference often relies assumption “selection observables,” claim within groups units observed characteristics, treatments -randomly assigned. Observational causal inference model-based sense selection observables assumption grounded researchers’ theoretical model world, course might wrong. Experimental causal inference design-base sense can sure treatments indeed randomly assigned random assignment part data strategy. observational experimental settings need make assumption, one case, rely theoretical model , rely data strategy. possible, prefer assuring assumptions design asserting basis theoretical model.Answer strategies come number varieties. familiar point-estimators produce estimates parameters. Ordinary least squares, difference--means, logit, random forests, long list others point-estimator answer strategy class. second class comprised tests. Tests return binary decision (True False). Null hypothesis significance tests common form test quantitative research. Qualitative researchers also employ tests; go names like “hoop test” “straw---wind” test. Bayesian answer strategies return full posterior distributions. contrast point-estimators answer strategies interval-estimators. class estimator choose – particular estimator within class select – depend features model, inquiry, data strategy. sections follow walk important ways design features influence answer strategy, though means offer full accounting. range estimation approaches vast book place tally strengths weaknesses. Instead, goal provide framework thinking go choosing one answer strategy another.","code":""},{"path":"choosing-an-answer-strategy.html","id":"following-the-model","chapter":"9 Choosing an answer strategy","heading":"9.1 Following the model","text":"described chapter 6, can (non-parametrically) express model world directed acyclic graph, DAG. DAGs express parts model, precisely nonparametric. don’t encode variables cause , just whether . Even , writing theoretical model parsimonious form nonparametric structual causal model can guide answer strategies enormously powerful way. Given DAG, can learn whether answer strategy sufficient estimating causal effect. , can learn variables answer strategy must condition , must left alone.want estimate particular causal relationship, average causal effect D Y, can read DAG whether relationship identified . common (useful) criterion identification “backdoor criterion.” exists unblocked “backdoor path” D Y, relationship identified.19 backdoor path causal path begins arrow D ends arrow Y. Backdoor paths can blocked two ways: conditioning analysis variable along path conditioning “collider” along path.Pearl (p. 61) defines backdoor criterion:Given ordered pair variables D, Y DAG G, set variables X, satifies backdoor criterion relative D, Y node X descendant D, X blocks every path D Y contains arrow D. (pg. 61 Pearl primer)definition hand, can inspect DAGs find “adjustment sets.” adjustment set set variables may conditioned upon answer strategy. “Conditioned upon” sufficiently vague phrase include conditioning procedures controlling variable regression setting stratifying analysis according variables.general, \\(X\\) adjustment set satisfies backdoor criterion, can estimate conditional probability distributions \\(Y\\) level \\(D\\) using expression.\\[\\Pr(Y = y \\mid (D=d)) = \\sum_x \\Pr(Y = y \\mid D = d, X = x) \\Pr(X = x)\\]can write expression using potential outcomes notation:\\[\\Pr(Y_i(D_i = 1) = y) = \\sum_x \\Pr(Y_i(D_i = 1) = y \\mid X_i = x) \\Pr(X_i = x)\\]Figure 9.1 shows three DAGs three variables D, X, Y, well additional unmeasured variable \\(U\\) (consider \\(U\\) later sections). three cases, inquiry average treatment effect D Y, three cases, X, D, Y correlated. first case, \\(X\\) confounds causal relationship D Y, say simply compared units different levels D, estimates causal effect prone bias. However, conduct analysis separately within levels X (say, condition X), combine separate analyses, overall estimate unbiased. first DAG setting analysts mind controlling observables order estimate causal effects.dangerous possibility, however, represented second DAG. causal graph, X doesn’t confound relationship D Y – instead, downstream consequence variables. analyst mistakenly conditions X, noncausal confounding path opens D Y, biasing estimates average effect D Y. contrast first second graphs illustration general principle theoretical models guide analytic choices. first case, estimates unbiased control \\(X\\), second case control X.third case describes setting D direct effect Y indirect effect travels X – case, X mediator. condition X, estimate effect D Y biased (X descendant D). Like collider case, effect identified control X, identified condition. intuition problems associated controlling mediators: controlling X “controls away” portion effect.example illustrates small way model world guides answer strategy. three cases, principle dataset D, X, Y. followed common regression advice, control X three cases correlated D Y – approach works X confounder. X collider mediator, control strategy induce bias. Without changes design, empirical tests can distinguish three DAGs, real way, theoretical assumptions Model must relied upon correctly choose control strategy.\nFigure 9.1: Three roles variable X.\nconsider three possible paths estimating effect treatment \\(D\\) \\(Y\\). First, consider conditioning \\(X\\) without making additional assumptions DAG; second, consider invoking ignorability assumption, often known selection--observables; third, consider randomizing treatment measuring \\(X\\) treatment. Finally, consider role estimation narrowing set possible DAGs describe world.","code":""},{"path":"choosing-an-answer-strategy.html","id":"identification","chapter":"9 Choosing an answer strategy","heading":"9.1.1 Identification","text":"can learn answer inquiry can identify causal effect, meaning infinite data estimate without bias causal effect. Identification can obtained backdoor criterion met, frontdoor criterion met, causal relationships \\(D\\) \\(Y\\). conditions violated several circumstances, including observed confounder \\(X\\) confounds relationship \\(D\\) \\(Y\\) left unadjusted unobserved confounder \\(U\\).reason causal identification, turn back DAGs. start simple DAG four variables: treatment \\(D\\), outcome \\(Y\\), observed variable \\(X\\) (may may confound relationship \\(D\\) \\(Y\\)), unobserved variable \\(U\\). six edges variables, three possible relations: variable may cause (e.g., \\(D \\rightarrow Y\\)), caused (e.g., \\(D \\leftarrow Y\\)), causally related every variable. three possible relationships six edges, \\(3^6 = 729\\) conceptually possible DAGs. rule DAGs variables cause \\(U\\), defined \\(U\\) unobserved confounder. leaves 216 possible combinations. know one! goal researchers narrow possible DAGs., collect data three observable variables, Table ??. faced choice answer strategy. presence possible confounding \\(X\\) /\\(U\\), several options research design: can estimate causal effect \\(D\\) \\(Y\\) controlling observed confounder \\(X\\) (), invoking additional conditional independence assumptions, randomizing treatment. case, aim rule DAGs causal effect identified.\nTable 9.1: Simulated data DAG varaibles \\(X\\), \\(D\\), \\(Y\\), \\(U\\).\n","code":""},{"path":"choosing-an-answer-strategy.html","id":"consequences-of-conditioning","chapter":"9 Choosing an answer strategy","heading":"9.1.1.1 Consequences of conditioning","text":"first strategy can control observed variable \\(X\\) . Whether controlling \\(X\\) enables, prevents, affect causal identification effect \\(D\\) \\(Y\\) depends beliefs DAG true DAG.illustrate possible beliefs edges visualizing 216 DAGs \\(U\\) unobserved confounder (see Figure 9.2). large groups three squares nine group DAGs based \\(U\\) affects variables model: top left grouping 27 DAGs represents \\(U\\) affects \\(D\\), \\(X\\) \\(Y\\). Within grouping, three squares nine squares. group nine squares represents set DAGs single relationship \\(D\\) \\(Y\\). top left group nine DAGs \\(D\\rightarrow Y\\).can rule several 216 acyclicity: gray squares graphs cyclical. (operate view cycling possible world, imply variables simultaneously cause .)color rest squares terms whether controlling \\(X\\) identify causal effect \\(D\\) \\(Y\\).cases, effect \\(D\\) \\(Y\\) identified regardless whether control \\(X\\) (white squares). example, left DAG, \\(X\\) affects \\(D\\) \\(U\\) affects \\(Y\\) neither affect . causal effect identified path \\(D\\) \\(Y\\) except direct effect \\(D\\rightarrow Y\\). words, neither \\(X\\) \\(U\\) confound relationship, potential outcomes \\(Y\\) independent \\(D\\).cases, effect \\(D\\) \\(Y\\) identified control \\(X\\) (blue squares). situations \\(X\\) confounds relationship additional confounding unmeasured confounders. cases potential outcomes \\(Y\\) independent \\(D\\) conditional \\(X\\).However, conditioning \\(X\\) risky, one blue DAGs might purple DAGs effect \\(D\\) \\(Y\\) identified condition \\(X\\) (purple squares). DAGs \\(X\\) collider, opening backdoor path \\(D\\) \\(Y\\) aside direct effect \\(X\\) conditioned . knew one DAGs, control \\(X\\).also many situations — majority fact, pink squares — relationship \\(D\\) \\(Y\\) never identified, regardless whether control \\(X\\). result confounding unobservable confounders \\(U\\). upper left quadrant contains cases \\(U\\) affects variables, middle left \\(U\\) affects \\(D\\) \\(Y\\). cases, identification require minimum ability control unmeasured confounders. situations causal identification fails causal order \\(D\\) \\(Y\\) reversed, .e. \\(Y\\) causes \\(D\\). Without additional assumptions control order variables collected data strategy, rule possibility, first fourth columns subgraphs labeled \\(D\\leftarrow Y\\).Without additional assumptions manipulation data strategy, know among 200 acyclic DAGs represented plot. result, danger either effect \\(D\\) \\(Y\\) unidentified regardless , making wrong choice control control.\nFigure 9.2: Consequences conditioning variable X 216 possible DAGs.\n","code":""},{"path":"choosing-an-answer-strategy.html","id":"adding-model-based-assumptions","chapter":"9 Choosing an answer strategy","heading":"9.1.1.2 Adding model-based assumptions","text":"can address problem unmeasured confounding invoking conditional independence assumptions. ``selection--observables’’ answer strategy invokes assumption \\(D\\) statistically independent potential outcomes \\(Y\\) given X, .e. adjusting \\(X\\). words, controlling \\(X\\) blocks backdoor paths \\(D\\) \\(Y\\). assumption also known conditional independence ignorability assumption.Figure 9.3, display 216 possible causal graphs , ruling gray cyclical. Among 200 remain, color way controlling \\(X\\) add fourth color: lavender DAGs rule based conditional independence assumption. ruled , upper left quadrants, unobserved confounding \\(U\\) addressed adjusting \\(X\\).Analysts invoke conditional independence assumption assure many fewer circumstances identification possible either controlling \\(X\\) controlling \\(X\\). However, strong assumption possible test directly. Instead, analyst must justify assumption based circumstantial qualitative quantitative evidence. task rule evidence either relationship \\(U\\) \\(D\\) relationship \\(U\\) \\(Y\\) . first case, evidence might take form background knowledge values \\(D\\) determined. treatment might assigned using cut-rule, case cut-assigned treatment (e.g., admitted college) . case, relationship unobserved variables \\(U\\) treatment \\(D\\), relationship \\(X\\) (score) \\(D\\). Controlling \\(X\\) enable causal identification even \\(Y\\) affected \\(U\\). However, assumption conditional independence \\(U\\) \\(D\\) given \\(X\\) strong assumption: analyst must sure unobserved variable \\(U\\) directly affects \\(D\\), legacy applicants may “pushed” threshold cut-close enough .selection--observables, still many DAGs raise problems us. still many DAGs \\(D\\leftarrow Y\\), \\(D\\) caused \\(Y\\) instead way around. rule assumption, never identify causal effect \\(D\\) \\(Y\\) regardless whether control \\(X\\). DAGs blue (effect identified condition \\(X\\) otherwise) purple (opposite, achieve identification condition \\(X\\)) still remain. blue involve \\(X\\) confounding relationship \\(D\\) \\(Y\\) purple \\(X\\) collider conditioning opens backdoor path \\(U\\). words, conditional independence \\(D\\) \\(U\\) given \\(X\\) insufficient identify effect, without ruling scenarios.\nFigure 9.3: Consequences conditional ignorability assumption 216 possible DAGs.\n","code":""},{"path":"choosing-an-answer-strategy.html","id":"design-based-identification","chapter":"9 Choosing an answer strategy","heading":"9.1.1.3 Design-based identification","text":"unable rule confounding assumption adjustment, can randomly assign \\(D\\) sever connections unobserved variables \\(U\\) \\(D\\) design. Ruling confounders assumption adjustment requires model correct, often known model-based inference, whereas ruling confounders design labeled design-based inference.20 addition, can measure \\(X\\) treatment rule situations \\(D\\) causes \\(X\\), can lead collider bias opening backdoor paths \\(D\\) \\(Y\\)., dramatically reduce set possible DAGs, set causal order \\(X\\) \\(D\\), dramatically expand number settings effect \\(D\\) \\(Y\\) identified due restriction causal order randomization \\(D\\) guarantees ignorability \\(U\\).Figure 9.4, swap colors now indicate DAGs ruled measuring \\(X\\) treatment (salmon squares), ruled random assignment (blue squares). remaining white squares, effect \\(D\\) \\(Y\\) causally identified, regardless whether adjust \\(X\\) . remove conditionality inference depending whether control. good, ultimately even presence strong ignorability assumptions outlined last section many possible DAGs controlling failing control lead bias. Now, inferences depend guessing correct DAG.\nFigure 9.4: Consequences randomizing Z measuring X treatment 216 possible DAGs.\nshort, controlling timing measurement \\(X\\) randomizing \\(D\\) move assumptions conditional independence \\(M\\) assumed (possibly incorrect!) model world data strategy, control can guarantee design.","code":""},{"path":"choosing-an-answer-strategy.html","id":"estimation","chapter":"9 Choosing an answer strategy","heading":"9.1.2 Estimation","text":"identification causal effect \\(D\\) \\(Y\\) either assumption design enables us undertake two tasks: estimate average treatment effect, estimate sign effect, estimate whether affect .first task, estimating magnitude effect \\(D\\) \\(Y\\), can accomplished using model-based inference selection--observables design randomized experiment. cases, apply plug-principle, replacing true unknown average potential outcomes treatment (control) sample analogues, average outcomes treatment (control) group. data randomized experiments, \\(X\\) ignorable, can either adjust (may reduce variability estimates) without bias. selection--observables, rule many DAGs assumption include \\(X\\) collider \\(X\\) opens backdoor path \\(D\\) \\(Y\\) order safely select answer strategy.estimating sign effect, can calculate sign effect magnitude, conduct statistical test null hypothesis zero effect distinguish among zero, positive, negative effects.third task, determining whether effect , similarly involves statistical test null hypothesis zero effect. fail reject null, posterior belief effect, reject two-sided test leave believing effect. zero average effect null hypothesis test can help us take final step distinguishing among 216 possible DAGs representing relationships \\(D\\) \\(Y\\) confounders \\(X\\) \\(U\\). Figure XX, display 16 DAGs identified random assignment pretreatment measurement \\(X\\). need use data two-sided null hypothesis test distinguish top row (effect \\(D\\rightarrow Y\\)) bottom row (effect). design got us way , need use data narrow one two rows eight DAGs. Since inquiry causal relationship \\(D\\) \\(Y\\), may concerned distinguishing among eight. , need develop alternative research design order learn causal effects \\(X\\).","code":""},{"path":"choosing-an-answer-strategy.html","id":"robustness","chapter":"9 Choosing an answer strategy","heading":"9.1.3 Robustness","text":"Robustness checks modified answer strategies aim demonstrate “robustness” “sensitivity” answer strategy choices. Often primary analysis quantity interest effect treatment \\(D\\) outcome \\(Y\\) presented appendix set modified analyses presented. author discusses whether variation magnitude statistical significance estimates across specifications.Robustness checks implicitly response fundamental uncertainty identified last several sections true DAG. know! result, robustness check motivated particular DAG. complete set robustness checks, can make claims “robust” estimates effect \\(D\\) \\(Y\\) alternative DAGs. robustness analyses aim falsify DAG, case controlling \\(X\\). three variables \\(X_1\\), \\(X_2\\), \\(X_3\\) either confound relationship \\(D\\) \\(Y\\) play role causal system, may want present robustness checks including excluding combination three variables. , rule variable collider mediator assumption. analyses can help distinguish among DAGs give us evidence effect \\(D\\) \\(Y\\) across possible DAGs involve confounding combination \\(X_1\\), \\(X_2\\), \\(X_3\\). However, important highlight assumptions make selecting DAGs ruling relationships. order controlling \\(X_2\\) sensible strategy, must invoke assumption \\(X_2\\) collider. possible , clear can learn adjusting \\(X_2\\). find inclusion wipes effect \\(D\\) \\(Y\\) may causal relationship \\(X_2\\) collider.illustrate simple analysis correlation two variables y1 y2, true positive correlation. y2 also function observed covariate x measurement error. main analysis bivariate regression predicting y2 y1. compare answer strategy one run analysis, also run robustness check controlling x. analyst unsure true DGP wish demonstrate reviewer’s results dependent functional form choose.Using MIDA way thinking designs, discuss diagnosis section another notion “robustness” design. typical way think robustness checks multiple secondary analyses conditional observed data build confidence analysis fixed data. However, motivation robustness checks uncertainty true data generating process. declaring design terms MIDA, can think robustness single estimator multiple possible true data generating processes. estimator robust sense one unbiased low uncertainty regardless , say, true functional form y1 y2. determine whether estimator robust, can redefine set designs different functional forms assess rate correct decisions robustness checks strategy different model.","code":""},{"path":"choosing-an-answer-strategy.html","id":"following-the-inquiry","chapter":"9 Choosing an answer strategy","heading":"9.2 Following the inquiry","text":"Answering descriptive inquiries involves studying existence nodes values take . might study whether behavior exists world, measure frequency. might also study frequency behavior varies time across people. answer strategy, following plug-principle, often involve average value node sample estimator average value population.contrast, target causal inquiry answer strategy, interested existence (non-existence) edge two nodes. can label two \\(D\\) (treatment) \\(Y\\) (outcome).","code":""},{"path":"choosing-an-answer-strategy.html","id":"plug-in-principle","chapter":"9 Choosing an answer strategy","heading":"9.2.1 Plug-in principle","text":"plug-principle refers idea good estiamtes population quantities \\((m) = ^m\\) can often generated choosing \\(\\) similar \\(\\) “plugging-” \\(d\\) \\(m\\). Suppose inquiry average treatment effect among \\(N\\) units population.\\((m) = \\frac{1}{N}\\sum_1^N[Y_i(1) - Y_i(0)] = \\frac{1}{N}\\sum_1^NY_i(1) - \\frac{1}{N}\\sum_1^NY_i(0) = ATE\\)can develop plug-estimator average treatment effect replacing population means (\\(\\frac{1}{N}\\sum_1^NY_i(1)\\) \\(\\frac{1}{N}\\sum_1^NY_i(0)\\)) sample analogues:\\((d) = \\frac{1}{m}\\sum_1^m{Y_i} - \\frac{1}{N - m}\\sum_{m+1}^N{Y_i}\\),units 1 though \\(m\\) reveal treated potential outcomes remainder reveal untreated potential outcomes. difference--means estimator course depends strength analogy population mean sample mean, turn depends features Data strategy. many () forms random assignment, sample mean estimators treated untreated outcomes unbiased estimators. discussed Section 9.3, specifics random assignment strategy probabilities assignment vary unit must taken account.formally, Aronow Miller describe plug-estimator :..d. random variables \\(X_1, X_2, \\ldots, X_n\\) common CDF \\(F\\), plug-estimator \\(\\theta = T(F)\\) : \\(\\widehat\\theta = T(\\widehat F)\\).","code":""},{"path":"choosing-an-answer-strategy.html","id":"point-estimation","chapter":"9 Choosing an answer strategy","heading":"9.2.2 Point estimation","text":"Point estimation possibly common class answer strategy quantitative social science. Point estimators things like difference--means, ordinary least squares regression, instrumental variables regression, random forests, LASSO, ridge regression, BART… list goes . data analysis tools taught many graduate methods courses. ’s incredible even though population estimators proliferates passing year, number kinds inquiries useful estimating stays relatively flat. point estimatation, mainly interested estimating averages, differences, variances, conditional expectation functions increasing dimensionality. course many kinds estimands (like ratios quantiles), main point , variation scholars conduct point estimation answer strategy, inquiry.Bias variance tradeoffoverfittingadd modeling assumptions might decrease variance lot cost little bias.interacts testing: win RMSE, higher false positive rate.(point people Hastie Tibshirani)","code":""},{"path":"choosing-an-answer-strategy.html","id":"estimating-the-variance","chapter":"9 Choosing an answer strategy","heading":"9.2.3 Estimating the variance","text":"Variance estimation often difficult point estimation.can often bootstrap, plugs (re-) sampling sample sampling population.","code":""},{"path":"choosing-an-answer-strategy.html","id":"tests","chapter":"9 Choosing an answer strategy","heading":"9.2.4 Tests","text":"Tests elemental kind answer strategy. Tests yield binary yes/answers inquiry. qualitative traditions, hoop tests, straw---wind tests, smoking-gun tests, doubly-decisive tests common. tests procedures making analysis decisions structured way. frequentist statistics, significance tests used make decision whether reject fail reject particular null hypothesis.","code":""},{"path":"choosing-an-answer-strategy.html","id":"partial-identification","chapter":"9 Choosing an answer strategy","heading":"9.2.5 Partial identification","text":"confoundingEV boundstrimming bounds","code":""},{"path":"choosing-an-answer-strategy.html","id":"p2followdatastrategy","chapter":"9 Choosing an answer strategy","heading":"9.3 Following the data strategy","text":"However, order benefit two controlled decisions data strategy, must follow dictum due R.. Fisher “analyze randomize.” answer strategy follow data strategy. four components: make comparisons across randomly-assigned conditions; analyze data level random assignment; make comparisons groups within random assignment conducted (e.g., strata); adjust differences probabilities random assignment. parallel set rules govern answer strategies descriptive inference: draw random samples describing population; analyze data level random sampling (primary sampling unit); adjust differences probabilities sampling.Making comparisons across randomly-assigned conditions within groups random assignment conducted directly follow comparison causal identification assignment vs. design. make comparisons rely differences \\(D\\) \\(X\\), example difference treatment effects two subgroups, susceptible confounding \\(U\\), \\(X\\) randomly-assigned. Similarly, analysis data block-randomized experiments can broken failing account blocking, blocks randomly assigned fact typically constructed outcomes units block similar within blocks different across blocks. differential probabilities assignment across blocks pool data ignoring blocking structure, unweighted comparisons may contrasting groups differ systematically. descriptive inferences, must always adjust different sampling inclusion probabilities.random sampling, sampling probability quantifies person’s likelihood sample. taking inverse probability, can generate weights allow us make really skewed sample representative target population. basic insight can extended random sampling many contexts. applies random treatment assignment, use inverse probability weighting make unbiased inferences presence spillovers heterogeneous assignment probabilities. even extends use poststratification nonrandom sampling inverse propensity weighting unbiased causal inference presence confounding. reason, understanding connection sampling probabilities inverse probability weights one important concepts descriptive causal inference. However, weights come mean can often seem confusing. fact, basic intuition simple random sampling analogy extends directly contexts.sample, select subset units stand unselected units. ’s mean say sample “representative”—units literally represent units (). can even quantify exact number units population unit sample represents. sample \\(n = 10\\) people group \\(N = 1000\\) people, make inference larger group smaller group, 10 people sample standing \\(N/n = 100\\) people population. sample people, person just represents one person, : \\(N/n = 1\\). Let’s call \\(w\\) number people population person sample represents: \\(w = N / n\\). let’s call \\(p\\) probability sample: \\(p = n / N\\). Notice \\(p\\) \\(w\\) reciprocal one another. Since number 1 divided fraction gives reciprocal, can use inverse \\(p\\) find \\(w\\): \\(\\frac{1}{p} = \\frac{1}{1} \\times \\frac{N}{n} = w\\). Now know inverse probability weight : count number people population given unit sample represents.Now suppose two samples, size \\(n_1 = n_2 = 10\\). first sample drawn population 10 people, \\(X = 1\\). second sample drawn population 20 people, \\(X = 0\\). want know true mean \\(X\\) across populations: \\(\\bar{X} = \\frac{10 + 0}{10 + 20} = \\frac{1}{3}\\). pool data take average \\(X\\), get wrong answer: \\(\\hat{\\bar{X}} = \\frac{1}{2}\\). get wrong answer people first group overrepresented: \\(1/2\\) sample group 1, \\(1/3\\) combined population group 1. terms sampling probabilities weights, every person first sample represents one person: \\(\\frac{1}{p_1} = \\frac{1}{1} \\frac{N_1}{n_1} = \\frac{1}{1} \\frac{1}{1} = 1 = w_1\\), whereas every person second sample stands two people, \\(\\frac{1}{p_2} = \\frac{1}{1} \\frac{20}{10} = 2 = w_2\\). weighting sample estimate average number people person pooled sample represents, recover right answer:\\[\\hat{\\bar{X}}_{IPW} = \\frac{\\sum X_i w_i}{\\sum w_i} = \\frac{n_1 \\times 1 \\times w_1 + n_2 \\times 0 \\times w_2}{n_1 \\times w_1 + n_2 \\times w_2} = \\frac{10 \\times 1\\times 1 + 10 \\times 0 \\times 2}{10 \\times 1 + 10 \\times 2} = \\frac{1}{3} = \\bar{X}.\\]Notice example equivalent stratified random sampling strategy, units group 1 sampled \\(p_1 = 1\\) units group 2 sampled probability \\(p_2 = 1/2\\). time use stratified random sampling probabilities vary among strata, weight estimates observation’s contribution equivalent number units represents, corrects - underrepresentation introduced sampling procedure.way thinking provides framework poststratification nonrandom samples. Suppose researcher comes across dataset 20 people. don’t know anything sampling strategy—, ’s non-random sample. However, know 10 people group 1 20 people group 2 population, able construct estimates sampling probabilities using \\(\\hat{p_1} = n_1/N_1\\) \\(\\hat{p_2} = n_2/N_2\\). , can construct weights using \\(1/p_1\\) \\(1/p_2\\) reweight data representative. ’s basic intuition behind post-stratification: use known population counts reverse engineer probability weights hypothetical stratified random sampling strategy given . See entry X design library .analogy extends even consider random treatment assignment procedures also sampling procedures. Rather sampling entire units, however, treatment assignments randomly sample different potential outcomes. assign 5/15 people treatment rest control, example, five treated potential outcomes need “represent” \\(1/(5/15) = 3\\) people’s treated potential outcomes ten people assigned control represent \\(1/(10/15) = 1.5\\) people’s control potential outcomes. tried pool different experiments get average treatment effect across people experiments, need take account fact different experiments might underrepresent treatment control potential outcomes, weight accordingly. issue arises block randomized designs treatment assignment probabilities vary block (see entry X design library), designs spillovers spatial networks induce differential assignment probabilities spillovers (see entry X design library), stepped wedge designs earlier waves overrepresent control potential outcomes later waves overrepresent treatment potential outcomes (see entry X design library). Finally, way poststratification uses covariates construct sample weights corresponding hypothetical random sampling procedure, matching selection observable designs use covariates reverse engineer probabilities sampling treatment control potential outcomes order reweight data account systematic selection certain kinds units treatment (confounding).","code":""},{"path":"choosing-an-answer-strategy.html","id":"formalizing-qualitative-modes-of-inference","chapter":"9 Choosing an answer strategy","heading":"9.4 Formalizing qualitative modes of inference","text":"researchers don’t implement qualitative answer strategies writing executing code computer. Instead, use logic common sense arrive answer, weighing different sources evidence might include arguments made researchers, transcripts impressions interviews participant observation, various kinds archival records. Since much inferential work resides mind researcher, formalizing qualitative modes inference design diagnosis presents formidable task formalizing quantitative methods already available code.Yet possible often simpler might think represent qualitative modes inference using code. task also becoming easier due increasing use formal notation software represent qualitative inference describe . course, representation answer strategy code involve unrealistic simplifications. unrealistically simplistic representations researcher’s process still shed light stake often bewildering set design choices make.","code":""},{"path":"choosing-an-answer-strategy.html","id":"boolean-formalizations","chapter":"9 Choosing an answer strategy","heading":"9.4.1 Boolean formalizations","text":"One simplest ways formalize qualitative strategy constructing “-” statements, computers well. example, Blair et al. (2020), simulate researcher employing nested mixed methods analysis (Lieberman 2005) understand average causal effect. Formalizing procedure even simplest syntax revealed many different choices facing simulated researcher: share qualitative cases need validate large-N theory, example, latter determined valid? much effort allocate put building versus testing theories? Moreover, revealed choices mattered lot likelihood getting good answer.","code":""},{"path":"choosing-an-answer-strategy.html","id":"bayesian-formalizations","chapter":"9 Choosing an answer strategy","heading":"9.4.2 Bayesian formalizations","text":"Recent developments qualitative methods sought take Bayes’ rule “metaphor analytic tool” (Bennett 2015). approach characterizes qualitative inference one prior beliefs world can specified numerically updated basis evidence observed. minimum, writing answer strategy computer requires specifying beliefs, expressed probabilities, likelihood seeing certain kinds evidence different hypotheses. provide simple example strategy design library. Michael C Herron Quinn (2016) provide one approach formalizing qualitative answer strategy focuses understanding average treatment effect.\nHumphreys Jacobs (2015) provide approach can used formalize answer strategies targeting causal effect causal attribution inquiries, Fairfield Charman (2017) formalize Bayesian approach approaches causal attribution problem attaching posterior probability competing alternative hypotheses.\n(???) suggest use “Supra-Bayesian” methods aggregate multiple participant-provided narratives ethnographic studies targeting causal attribution estimands.","code":""},{"path":"choosing-an-answer-strategy.html","id":"set-theoretic-formalizations","chapter":"9 Choosing an answer strategy","heading":"9.4.3 Set-theoretic formalizations","text":"Set-theoretic approaches formalize qualitative reasoning employ deterministic concepts causal necessity sufficiency. Variants qualitative comparative analysis (QCA), typically seeks understand minimal set combinations factors sufficient produce outcome, implemented computers, therefore straightforward declare diagnose. QCA subject numerous simulation studies: (ADD ). provide example Blair et al. (2020).","code":""},{"path":"choosing-an-answer-strategy.html","id":"games-as-formal-narratives","chapter":"9 Choosing an answer strategy","heading":"9.4.4 Games as formal narratives","text":"counterfactual accounts causality (see Dawid 2000 alternative), causal inferences often rely speculations counterfactual state affairs. average outcome control group experiment, example, represents counterfactual average outcome treatment group assigned treatment. comparative historical analyses, conjectures often focus case level. Speaking “balance rule” introduced United States senate Missouri Compromise 1820, provided veto power coalitions Northern Southern states policy, Weingast (1998) (p.176) speculates \n> veto engineered prevent Northern assaults slavery, assaults actually observed, might succeeded. Southern veto senate made assaults slavery relatively rare.reaches conclusion analysis formal model extensive form.","code":""},{"path":"choosing-an-answer-strategy.html","id":"answer-strategies-as-procedures","chapter":"9 Choosing an answer strategy","heading":"9.5 Answer strategies as procedures","text":"Consider randomized experiment seeks estimate causal effect treatment. answer strategy just “Logistic Regression Covariate adjustment”. includes every step process takes raw data, cleans recodes , considers 5 alternative estimators (DIM, OLS covariate adjustment, fancy thing colleague suggested couldn’t get converge), finally settling logit.Multiple estimates. Answer strategies can account many statistical tests conducting. Often, generating answer single inquiry, may construct multiple estimates provide different types answers varying quality. present results many null hypothesis tests, rate falsely rejecting least one tests even true goes , due multiple comparisons problem. plan adjust problem, adjustments part answer strategy, typically adjust p-values report decisions readers make . may three survey items imperfectly estimate latent quantity. presenting results, present three estimates three regressions, adjust three estimates using procedure family-wise error rate correction, average three items together index present one estimate one regression. three methods select change properties answer strategy.Analysis procedures. final estimator goes paper neither beginning end answer strategy. Procedures, , explore data determine final set estimates part answer strategy. Procedures summarizing multiple estimates one example many.Commonly, final estimator selected depended exploratory procedure multiple models assessed, example comparing model fit statistics. answer strategy research design fit final model — multiple step -procedure. procedures may part prespecified analysis plan may informal, may sometimes possible declare full design data obtained. (may find different analysis procedure data dependent preferable, diagnose design fact.) reason declare procedure rather final estimator diagnosis design may differ. procedure may powerful, example assessed multiple sets covariate controls selecting specification lowest standard error estimate. procedure may also exhibit poor coverage, accounting multiple bites apple.also sometimes find model planned run analyze data estimated. cases, iterative estimation procedure first model run, changes specification made, second third model presented result. full set steps — decision tree, depending estimable — answer strategy can evaluate whether good one realized data possible realizations decision tree decisions different.fact, examples analysis procedures types research, quantitative qualitative. Many strategies causal inference observational data involve estimation strategy set falsification placebo tests. answer provided research designs depends crucial way results tests: tests fail, design provides definitive answer. qualitative research, process tracing involves set steps, results depend information gathered earlier steps. Many mixed methods strategies also multi-step procedures. Nested designs involve running quantitative analysis selecting cases basis predictions regression. designs assessed considering single step procedure isolation.\nthings go according plan. compare answer strategies, can imagine estimators possible things go well well things go wrong, missing data outliers variables. good answer strategy, might single estimator, procedure ---, can handle states world. Procedures addressing deviations expected analyses part answer strategy. Even absence preanalysis plan, often way expect analyze data things go well. — data missing, noncompliance intervention, study suspended example — answers change. procedures determine answer study provides (cases ), part answer strategy. Standard operating procedures (lin green) documents systematize procedures advance.demonstrate fact properties procedures differ properties design final estimator simple example. compare two possible estimation specifications, without covariates, procedure run models report model paper lower p-value. models exactly , properties procedure differ properties either two possible models. particular, procedure higher power either two models, exhibits poor coverage, means bias measure uncertainty.","code":""},{"path":"choosing-an-answer-strategy.html","id":"interpretation","chapter":"9 Choosing an answer strategy","heading":"9.6 Interpretation","text":"answer strategies described thus far also yield single answer, whether point estimate, set bounds, p-value. Yet take tiny proportion writeup study’s results. Much ink taken tables report , figures visualize , results discussion sections describe . Importantly, report, visualize, describe results change depending realization data. choices often depend results simple ways, scale outcomes changing, complex ways, whether result even reported paper statistically significant.report, visualize, describe results part answer strategy. answer provide readers depends components, numbers pop regression. Considering aspects answer strategy change depending data strategy turns ","code":""},{"path":"choosing-an-answer-strategy.html","id":"further-reading-4","chapter":"9 Choosing an answer strategy","heading":"9.7 Further reading","text":"Gelman Hill (2006) [stories] multilevel modeling.Aronow Samii (2016) generalizability regression estimators observational settings.Van Evera (1997) hoop tests.","code":""},{"path":"p2diagnosis.html","id":"p2diagnosis","chapter":"10 Diagnosis","heading":"10 Diagnosis","text":"Research design diagnosis process evaluating properties research design. Since “property research design” cumbersome phrase, made word “diagnosand” refer properties research design like diagnose. Many diagnosands familiar. Power probability obtaining statistically significant result. Bias average deviation estimates true value estimand. diagnosands exotic, like Type-S error rate, probability estimate incorrect sign, conditional statistically significant (Gelman Carlin 2014).Research designs strong empirical answer \\(^d\\) generated design close true answer \\(^w\\). Since can never know \\(^w\\), assess whether distribution \\(^d\\) possible realizations research design close distribution \\(^m\\), answer model. can assess close distributions ? distribution \\(^m\\) easy simulate. Given theoretical model, can easily ask computer generate distribution estimand. often , distribution \\(^M\\) degenerate – fixed population, example, theoretical models posit estimands like Average Treatment Effect just one value.distribution \\(^d\\) trickier estimate. actual research design implemented , don’t get see distribution actual answers eventuated application D world. solve problem, make small – extremely consequential – substitution \\(m\\) \\(w\\) DAG research design. Swapping \\(m\\) \\(w\\), can ask computer simulate distribution \\(^d\\) conditional model. model wrong, simulated distribution \\(^d\\) wrong – ever, garbage , garbage . Figure YY shows DAG use simulate research designs. simulate designs, \\(d\\) affected \\(m\\) (realization theoretical model), rather \\(w\\). makes sense, since computer simulations can entirely untethered reality.Design diagnosis process simulating \\((m) = ^m\\) \\((d) = ^d\\) many draws \\(M()\\) \\(D(m)\\), comparing . specific comparisons make called “diagnostic statistics.”“diagnostic statistic” function \\(g\\) \\(^d\\), answer given data, \\(^m\\), answer given model, answers. function might really simple, like identity function, \\(g(^d)=^d\\), difference two answers: \\(g(^m, ^d) = ^d - ^m\\). \\(^m\\) \\(^d\\) random variables, function random variables also random variable, diagnostic statisitic also random variable. diagnosand summary random variable.\n\\[\n\\phi = f(g(^m, ^d))\n\\]\\(f()\\) statistical functional summarizes random variable. example, expectation function \\(\\mathbb{E}[X]\\) summarizes random variable \\(X\\) expectation, mean, variance function summarizes expectation squared deviation random variable mean. ’ll use Greek letter \\(\\phi\\) describe idea diagnosand general.Let’s back moment work concrete examples common diagnosands (see section 10.3 exhaustive list). Consider diagnosand “bias.” Bias average difference estimand estimate. model two potential outcomes, treated potential outcome untreated potential outcome, inquiry might average treatment effect (difference two potential outcomes averaged units population sample, abbreviated ATE). single realization \\(m\\) model \\(M\\), value ATE particular number, call \\(^m\\). data strategy simply collect data come treated versus don’t (.e., use random assignment), answer strategy difference--means, answer \\(^d\\) systematically different \\(^m\\). diagnostic statistic error \\(^d - ^m\\); error random variable draw \\(m\\) \\(M\\) slightly different. expectation random variable \\(\\mathbb{E}[^d - ^m]\\), value bias diagnosand.Answer strategies commonly rely measures uncertainty like \\(p\\)-values, standard errors, confidence intervals order make decisions interpret \\(^d\\). measure estimated data make decision \\(^d\\) can used diagnostic statistic summarized diagnosand. Like bias, statistical power expectation, time diagnostic statistic \\(\\mathbb{1}(p \\leq 0.05)\\), indicator function equals 1 \\(p\\)-value greater 0.05 0 otherwise. Power describes frequently (beliefs model) research design return statistically significant result. standard error provides another diagnostic statistic whose expectation provides expected standard error diagnosand, example. can especially informative comparison another diagnosand, \\(\\sqrt{\\mathbb{V}(^d)}\\), actual standard deviation estimates generated model.diagnosands can calculated analytically. can straightforward calculate \\(\\sqrt{\\mathbb{V}(^d)}\\) two-arm experiment willing make lot simplifying assumptions.21 diagnosands even moderately complex designs, however, require Monte Carlo computer simulation. main purpose DeclareDesign software package making simulation step easier.practice, important diagnose design multiple possible \\(M\\)’s, given fundamental uncertainty world \\(w\\). know precise distributions exogenous variables exact functional forms potential outcomes (e.g., know true effect size). Diagnosis, therefore, typically involve simulating properties fixed set inquiries, data strategies, answer strategies multiple likely models. \\(D\\) \\(\\) given \\(\\) provide good diagnosand values multiple \\(M\\)’s can said robust multiple models.\nFigure 10.1: MIDA DAG\n","code":""},{"path":"p2diagnosis.html","id":"estimating-diagnosands-analytically","chapter":"10 Diagnosis","heading":"10.1 Estimating diagnosands analytically","text":"Diagnosis can done analytic, pencil--paper methods. Indeed, research design textbooks often contain many formulas calculating power variety designs. example, Gerber Green include following power formula:write:“illustrate power analysis, consider completely randomized experiment \\(N>2\\) \\(N\\) units selected binary treatment. researcher must now make assumptions distributions outcomes treatment control units. example, researcher assumes control group normally distributed outcome mean \\(\\mu_c\\), treatment group normally distributed outcome mean \\(\\mu_t\\), group’s outcomes standard deviation \\(\\sigma\\). researcher must also choose \\(\\alpha\\), desired level statistical significance (typically 0.05).\nscenario, exists simple asymptotic approximation power experiment (assuming significance test two-tailed):\n\\[\n\\beta = \\Phi \\bigg(\\frac{|\\mu_t - \\mu_c| \\sqrt{N}}{2\\sigma} - \\Phi^{-1} (1 - \\frac{\\alpha}{2}) \\bigg)\n\\]\n\\(\\beta\\) statistical power experiment, \\(\\Phi(\\cdot)\\) normal cumulative distribution function (CDF), \\(\\Phi^{-1}(\\cdot)\\) inverse normal CDF.”power formula makes detailed assumptions \\(M\\), \\(D\\), \\(\\)? \\(M\\), assumes potential outcomes normally distributed group specific means common variance. \\(D\\), assumes particular randomization strategy (simple random assignment). \\(\\), assumes particular hypothesis testing approach (equal variance \\(t\\)-test \\(N - 2\\) degrees freedom). set assumptions may “close enough” many research settings, can difficult understand specific impacts different beliefs \\(M\\), \\(D\\) \\(\\) value diagnosand. instead normally distributed, potential outcomes measured 1 - 5 Likert scales? randomization procedure includes blocking? include covariates treatment effect estimation approach? Formulas large sources design variation derived (clustering), certainly every design variant.quickly, hope analytic design diagnosis fades. analytic formulas abstractions – abstract away design details sometimes design details important. problem confined “power” diagnosand. randomized experiments, claims bias diagnosand quite general. Many randomized designs unbiased ATE, . Designs encounter noncompliance, attrition, forms spillover may unbiased ATE. Even without complications, cluster randomized trials heteogeneous cluster sizes unbiased (joel, imai).Diagnosands depend design details, conduct study matters properties. means design diagnosis must design-aware. Since designs heteogenous can vary many dimensions, computer simulation feasible way diagnose anything beyond simplest ideal-type designs.","code":""},{"path":"p2diagnosis.html","id":"estimating-diagnosands-via-simulation","chapter":"10 Diagnosis","heading":"10.2 Estimating diagnosands via simulation","text":"Research design diagnosis usually occurs two-step, simulation-based procedure. First simulate research designs cover, collecting “diagnostic statistics” run simulation. Second, summarise distribution diganostic statistics order estimate diagnosands.estimate diagnosands summarizing distribution diagnostic statistics – course raises question: diagnostic statistic? take draw model (\\(m\\)) calculate value inquiry \\((m) = ^m\\). take one draw data strategy (\\(D(m) = d\\)), calculates value answer strategy \\((d) = ^d\\). diagnostic statistic function \\(^m\\) \\(^d\\).simple diagnostic statistic “error,” difference estimate estimand: \\(error = ^d - ^m\\). bias diagnosand expectation error statistic \\(\\mathbb{E}[error]\\) possible ways study come .Usually, consider many diagnostic statistics time. ’s design declaration two-arm trial balanced (50/50) design. 100 subjects responses treatment drawn normal distribution mean 0.1 sd 0.1.One draw simulation returns following:One draw simulation returns following:Figure ?? shows information might obtain single run simulation. filled point estimate \\(^d\\). open triangle estimand \\(^m\\). bell-shaped curve normal-approximation based estimate sampling distribution. standard deviation estimated distribution estimated standard error, expresses uncertainty. confidence interval around estimate another expression uncertainty: ’re sure \\(^d\\) , things going according plan, confidence intervals constructed way bracket \\(^d\\) 95% time.\nFigure 10.2: Visualization one draw design diagnosis.\nsingle draw, can’t yet estimate diagnosands, can estimate diagnostic statistics. estimate higher estimand draw, error 0.10 - 0.08 = 0.02. Likewise, squared error (0.10 - 0.08)^2 = 0.0004. \\(p\\)-value 0.04, just barely lower threshold 0.05, “statistical significance” diagnostic statistic equal TRUE. confidence interval stretches 0.003 0.156, value estimand (0.10) bounds, “covers” diagnostic statistic equal TRUE well.Learning distribution diagnostic statistics main barrier design diagnosis. simply write distribution diagnostic statistics, straightforward matter summarize order calculate diagnosands. distributions diagnositic statistics depend complex information four parts research design: M, , D, . example, error statistic depends \\(^d\\) \\(^m\\), details matter greatly.calculate distributions diagnostic statistics, simulate designs just , many many times . bias diagnosand average error many runs simulation. statistical power diagnosand fraction runs estimate significant. coverage diagnosand fraction runs confidence interval covers estimand.figure visualizes just 10 runs simulation (obtained simulate_design(design)). can see run, \\(^m\\) little different. might seem counterintuitive – isn’t estimand supposed fixed number? estimands fixed, others stochastic, depending specifics model. Notice design declaration, drew potential outcomes distribution rather fixed numbers. choice incorportates modeling uncertainty. treatment effects unit close 0.1, ’re sure close particulat unit. can also see draws produce statistical significant estimates (shaded areas small confidence intervals don’t overlap zero), . get sense true standard error seeing point estimates bounce around. get feel difference estimates standard error true standard error. Design diagnosis process learning ways study might come , just one way .\nFigure 10.3: Visualization ten draws design diagnosis.\nline code one. simulate design 1000 times calculate diagnositic satistics, summarise terms bias, true standard error (standard deviation sampling distribution), RMSE, power, coverage.","code":"\ndiagnosis <- diagnose_design(design, sims = 1000, \n                             diagnosands = declare_diagnosands(\n                               select = c(\"bias\", \"sd_estimate\", \"rmse\", \"power\", \"coverage\")))\ndiagnosis## \n## Research design diagnosis based on 100 simulations. Diagnosand estimates with bootstrapped standard errors in parentheses (100 replicates).\n## \n##  Design Label Estimand Label Estimator Label Term N Sims   Bias SD Estimate\n##        design            ATE       estimator    Z    100   0.00        0.04\n##                                                          (0.00)      (0.00)\n##    RMSE  Power Coverage\n##    0.04   0.62     0.96\n##  (0.00) (0.05)   (0.02)"},{"path":"p2diagnosis.html","id":"interpreting-diagnosands","chapter":"10 Diagnosis","heading":"10.2.1 Interpreting diagnosands","text":"Say decide simulate design 1000 times see ’s biased. seconds, computer spits bias .02. Unsure whether concerned, simulate design another 1000 times. Now computer says bias .01. Since simulation randomly generated, expect get estimates bias vary. Given random variation simulations large estimate estimand , maybe even quite likely get .01 .02 bias even true bias zero. maybe design biased.frequentist statistics, separate signal noise. estimate (, bias diagnosand) varies depending simulation turns . want quantify variation order make decision whether estimated bias large enough significant. simulation standard error (, Monte Carlo standard error) estimate variation created simulation procedure. estimated variation high relative estimated diagnosand, need increase number simulations.\nMorris, White, Crowther (2019) provide range helpful formulas estimate simulation standard errors diagnosands. Define \\(^d_i\\) point estimate \\(\\)’th simulation, \\(n_{sim}\\) total number simulations, average point estimates across simulations \\(\\bar{^d} = \\frac{1}{n_{sim}}\\sum ^d_i\\). , estimand constant \\(^d_i\\) independently normally distributed, simulation standard error bias can calculated \\(\\sqrt{1/(n_{sim}(n_{sim} - 1)) \\sum^{n_{sim}}_{= 1} (^d_i - \\bar{^d})^2}\\). can code:course, estimate variance relies assumptions won’t work soon estimand constant estimates normally distributed. Rather using analytic formulas, recommend using approach called nonparametric bootstrapping estimate simulation standard errors. Nonparametric bootstrapping can used estimate standard error diagnosand whose diagnostic statistic independently identically distributed, don’t limit classic diagnosands formula. Nonparametric bootstrapping quite simple : randomly resample \\(n_{sims}\\) diagnostic statistics replacement large number times re-estimate diagnosand resampled collection diagnostic statistics. standard deviation resulting distribution diagnosand estimates gives estimate simulation standard error. DeclareDesign nonparametric bootstrapping default whenever diagnose_design() function called. numbers reported parentheses diagnosand estimates table . ’s simulation standard error bias calculated bootstrap resampling 100 times:can see, nonparametric bootstrap gets us close correct answer given analytic formula, without making assumptions shape distribution diagnostic statistics.","code":"\nsimulated_estimates <- with(diagnosis$simulations_df, estimate)\nmean_estimate <- mean(simulated_estimates)\nn_sim <- length(simulated_estimates)\nSE_bias <- sqrt(1/(n_sim * (n_sim - 1)) * (sum((simulated_estimates - mean_estimate)^2)))\nSE_bias## [1] 0.004152528\ndiagnosis$diagnosands_df %>% pull(\"se(bias)\")## [1] 0.003838199"},{"path":"p2diagnosis.html","id":"how-many-simulations-to-run","chapter":"10 Diagnosis","heading":"10.2.2 How many simulations to run","text":"Unlike increasing sample size real world, increasing precision diagnosand estimates simple waiting little longer computer runs higher number simulations. However, running 100 simulations may take matter seconds, running 10,000,000 simulations may take days. know ’ve done enough?rule thumb, multiplying simulation standard errors two head order make decisions whether simulations, since gets close upper lower bounds 95% confidence interval (properly calculated muliplying standard error 1.96). , change randomization strategy power increases .78 .82 simulation standard error .015, increase number simulations: upper bound first estimate \\((.78 + 0.015*1.96 = .81)\\) overlaps lower bound second \\((.82 - 0.015*1.96)\\), apparent improvement power may artefact simulation.","code":""},{"path":"p2diagnosis.html","id":"diagnosands","chapter":"10 Diagnosis","heading":"10.3 How to choose among diagnosands","text":"diagnostic statistic summary function \\(^m\\) \\(^d\\), diagnosand summary diagnostic-statistics. result, great many choose . Table 10.1, introduce set diagnostic statistics (far complete!), Table 10.2 set diagnosands including commonly less commonly considered.Table 10.1:  Diagnostic statistics.Table 10.2:  Diagnosands.diagnosands relevant every study. example, descriptive study whose goal estimate fraction people France left-handed, statistical power irrelevant. hypothesis test null hypothesis 0 percent people France left-handed preposterous. know sure fraction zero, just don’t know precise value. much important diagnosand study RMSE (root-mean-squared-error), measure well-estimated estimand incorporates bias variance.Often, need look several diagnosands order understand might going wrong. design exhibits “undercoverage” (e.g., coverage less \\(1 - \\alpha\\)), might standard errors small (-estimation variance sampling distribution) point estimate biased, combination two. really perverse instances, might biased point estimate , thanks overly-wide confidence intervals, just happens get covered 95% time. , assessing coverage ’s important look point estimate bias, also check average estimated standard error lines true standard error. Alternatively, look “bias-eliminated coverage,” assesses coverage purely terms confidence interval width, ignoring bias.Many research design decisions involve trading bias variance. trade-settings, may need accept higher variance order decrease bias. Likewise, may need accept bit bias order achieve lower variance. tradeoff captured mean-squared error – average squared distance \\(^d\\) \\(^m\\). course ideally like low mean-squared error possible. like achieve low variance low bias simultaneously.illustrate, consider following three designs represented three targets. inquiry bullseye target. data answer strategies combine generate process arrows shot towards target. left, bad archer: even though estimates unbiased sense hit bullseye “average”, arrows target. middle, Katniss Everdeen (heroine Hunger Games novels good bow) data answer strategies: target low variance. right, archer consistent (low variance) biased. mean squared error highest left lowest middle.archery metaphor common research design textbooks effectively conveys difference variance bias, elide important point. really matters target archer shooting . Figure ?? shows bizarre double-target representing two inquiries. empirical strategy unbiased precise left inquiry, clearly biased right inquiry. describing properties \\(^d\\), clear \\(^\\) associated .RMSE exactly equal weighting variance bias. Yet many weightings two diagnosands possible, different researchers vary weightings. weights may also depend research question researcher studying, career stage, strength priors, size effects, features.evaluating research design diagnosis, need know researcher’s weighting relevant diagnosands. can think utility unction. utility function includes important study big questions, shift beliefs research field, overturn established findings, obtain unbiased answers, get sign right. utility function evaluated given design yield utility can compared across designs (process redesign, described detail next section).often consider diagnosand power . diagnosand probability getting statistically significant result, course depends many things design including, crucially, unknown magnitude parameter estimated. can think statistical power probability success, success defined getting significant results. conventional power target 80% power. One imagine redefining statistical power “null risk,” probability obtaining null result. terms, conventional power target 20% null risk, one five chance “failure.” odds aren’t great, recommend designing studies lower null risk. considering power alone also misleading: researcher wants design study 80% powered returns highly biased estimates 2-3x true estimate. Another way saying researchers always carry power bias. much care feature determines weight power bias utility function.Diagnosands need hypothesis testing even statistical analysis data . often tradeoff much learn research design cost terms money time. financial time budgets provide hard constraints designs, also margin many researchers wish select cheaper (shorter) designs order carry studies finish degree sooner. Time cost also diagnostic statistics! may wish explore maximum cost study maximum amount time take.Ethical considerations also often enter process assessing research designs, implicitly. can explicitly incorporate utility function valuing minimizing harm maximimizing degree informed consent requested subjects. collecting, researchers often believe face tradeoff informing subjects subject data collection (ethical consideration, requirement IRB) one hand bias comes Hawthorne demand effects. can incorporate considerations research design diagnosis specifying diagnostic statistics related amount disclosure purposeses research number subjects harmed research.","code":""},{"path":"p2diagnosis.html","id":"diagnosing-with-respect-to-variations-in-m","chapter":"10 Diagnosis","heading":"10.4 Diagnosing with respect to variations in M","text":"always uncertain M – certain M (real dispute ), need conduct new empirical research . Research design diagnosis can account uncertainty evaluating performace design alternative models. unsure exact value intra-cluster correlation outcomes encounter, simulate variance estimator range plausible ICC values. unsure true average treatment effect, simulate power study range plausible effect sizes. Uncertainty model inputs like means, variances, covariances data eventually collect major reason simulate range plausible values.","code":""},{"path":"p2diagnosis.html","id":"estimating-the-minimum-detectable-effect-size","chapter":"10 Diagnosis","heading":"10.4.1 Estimating the minimum detectable effect size","text":"","code":""},{"path":"p2diagnosis.html","id":"adjudicating-between-competing-models","chapter":"10 Diagnosis","heading":"10.4.2 Adjudicating between competing models","text":"can apply principle competing models. Imagine believe \\(M_1\\) true scholarly rival believes \\(M_2\\). spirit scientific progress, design study together. design () demonstrate \\(M_1\\) true true (B) demonstrate \\(M_2\\) true true. order come agreement properties design, need simulate design models.","code":""},{"path":"p2diagnosis.html","id":"further-reading-5","chapter":"10 Diagnosis","heading":"10.5 Further reading","text":"Gelman Carlin (2014) Type M Type S errorsMichael C. Herron Quinn (2016) case selection / sampling biasBaumgartner Thiem (2017) Rohlfing (2018) diagnosands qualitative researchRubin (1984) diagnosands Bayesian research–>","code":""},{"path":"redesign-1.html","id":"redesign-1","chapter":"11 Redesign","heading":"11 Redesign","text":"Diagnosis process learning value diagnosand research utility — weighted summary diagnosands — single design, often multiple specifications M assess robustness alternative models. Redesign process changing parts D order learn diagnosand values researcher utility change. redesign process complete research selects best (one best) D among feasible set, measured researcher utility.example, can compare distribution errors changes use different data strategy \\(D'\\): \\(P_M(^d - ^m|D')\\) different answer strategy \\('\\): \\(P_M(^{d'} - ^m|D)\\). can also hold data answer strategies fixed consider distribution errors alternative model \\(M'\\): \\(P_M(^{d} - ^{m'}|D)\\).Researchers typically wish find optimal design question subject financial logistical constraints. Even simple designs infinite variations, search space must limited. Typically, researchers want find optimal values small set easily-controlled design parameters. example, researcher might want find sample size \\(N\\) number treated units \\(N_t\\) minimize design’s bias subject fixed budget experiment data collection unit costs $25 treating one unit costs $5. solve optimization problem:\\[\\begin{equation*}\n\\begin{aligned}\n& \\underset{N, N_t}{\\text{argmin}}\n& & E_M(^{d} - ^{m}|D_{N, N_t}) \\\\\n& \\text{s.t.}\n& & 25 * N + 5 * N_t \\leq 5000\n\\end{aligned}\n\\end{equation*}\\]However, optimizing design single diagnosand may lead poor design choices. redesign optimizing power diagnosand, likely find design highly powered highly biased — targeting wrong estimand. can solve switching root mean-squared error diagnosand, weighted combination bias efficiency design. generally, choice optimal design requires researcher select weighting diagnosands. must balance bias, efficiency, risk imprecise null results, risk getting sign effect wrong, diagnosands given research goals.full evaluation design — declaration, diagnosis, redesign — depends assessment one diagnosands, comparing diagnoses achieved alternative designs.may encountered figures implicitly redesign. Power curves example redesign process. power curve sample size horizontal axis statistical power vertical axis. power curve, can learn big study needs order achieve desired level statistical power. “minimum detectable effect” figure similar. sample size horizontal axis , plots smallest effect size 80% power can achieved. plots useful learning something like, “given budget, can sample 400 units. 400 units, MDE 0.5 standard deviation effect. theory says effect smaller , something closer 0.1 SDs. apply funding study something else.”highest level advice redesign , beginning processes least, change one design parameter time. Vary data strategy, holding model, inquiry, answer strategy constant. Change answer strategy, holding aspects design constant.","code":""},{"path":"redesign-1.html","id":"redesign-mountain","chapter":"11 Redesign","heading":"11.1 Redesign Mountain","text":"figure shows schematic redesign process. vary design parameter 1 design parameter 2 independently learn value diagnosand combination design parameters. redesign process can 2 dimensions course. suggest varying design parameters control first.\nFigure 11.1: Redesign mountain.\n","code":""},{"path":"part-ii-exercises.html","id":"part-ii-exercises","chapter":"12 Part II Exercises","heading":"12 Part II Exercises","text":"Imai, King, Stuart (2008)Imagine reviewer paper claims smoking causes smoking addiction. data offered support claim shows 100 subjects smoke addicted smoking 100 subject smoke addicted.data support conclusion smoking causes addiction smoking?Now imagine reviewer paper claims sailing causes sailing addiction. data offered support claim shows 100 subjects sail addicted sailing 100 subject sail addicted. data support conclusion sailing causes addiction sailing?answers () (b) differ? Sketch (either words DAG) alternative causal models smoking sailing produced two datasets.answers () (b) differ? Sketch (either words DAG) alternative causal models smoking sailing produced two datasets.Propose data answer strategy distinguish two causal models described (c).Propose data answer strategy distinguish two causal models described (c).Complier Average Causal Effect (CACE) defined average effect treatment subset subjects “comply” treatment assignment, .e., assigned treatment, take treatment assigned control, take treatment. Average Treatment Effect Treated (ATT) defined average effect treatment subjects treated. two estimands subtley different. Declare design draw_estimands demonstrate two estimands can different. Hint: ATT depends data strategy CACE .Complier Average Causal Effect (CACE) defined average effect treatment subset subjects “comply” treatment assignment, .e., assigned treatment, take treatment assigned control, take treatment. Average Treatment Effect Treated (ATT) defined average effect treatment subjects treated. two estimands subtley different. Declare design draw_estimands demonstrate two estimands can different. Hint: ATT depends data strategy CACE .Give abstract, can tell MIDAGive abstract, can tell MIDAGive passage one paper quote another paper, ’s MIDA. look paper say missed. quoted passage correctly characterize design?Give passage one paper quote another paper, ’s MIDA. look paper say missed. quoted passage correctly characterize design?","code":""},{"path":"research-design-library.html","id":"research-design-library","chapter":"13 Research Design Library","heading":"13 Research Design Library","text":"section book enumerates series common social science research designs. entry include description design terms MIDA also declaration design code. ’ll often diagnose designs range values design parameters order point especially interesting unusual features design.goal section provide comprehensive accounting empirical research designs. ’s also describe particular designs exhaustive detail, quite sure order designs useful practical purpose, need modified. entries design library recipes , follow instructions, come high-quality research.Instead, hope entries provide inspiration tailor particular class designs – blocked--clustered randomized trial catch--release design – research setting. basic structure design library entry useful, specifics plausible ranges outcomes, sample size constraints, etc, different particular setting.’ve split designs Inquiry Data strategy. Inquires can descriptive causal Data strategies can observational experimental. leads four categories research: Observational descriptive, Experimental descriptive, Observational Causal, Experimental causal. third dimension along studies can vary whether Answer strategy qualitative quantitative. include dimension typology, ’d end eight broad categories research design. don’t see qualitative-quantitative difference answer strategy fundamental differences inquiry data strategy, ’ll just include qualitative quantitative designs four categories. Besides, social scientists always appreciate good two--two:broadest terms, descriptive inquiries can described \\(f(\\mathbf{Y(Z = Realized)})\\), \\(f()\\) function \\(\\mathbf{Y(Z = Realized)}\\) vector realized outcomes. , descriptive designs seek summarize (using \\(f()\\)) world (represented \\(\\mathbf{Y(Z = Realized)}\\)). Descriptive designs can better worse answering inquiry. quality descriptive research designs depends extent measurement, sampling, estimation error.Causal inquiries can described \\(f(\\mathbf{Y(Z)})\\), \\(Z\\) realized vector treatments, instead vector take counterfactual values. standard causal inquiry Average Treatment Effect, \\(f()\\) function takes average difference two potential outcome vectors, \\(Y(Z = 1)\\) \\(Y(Z = 0)\\). many causal inquiries beyond ATE – thing common functions realized outcomes, potential outcomes. quality causal research designs depends everything descriptive design depends , also understanding quality mechanism assigns units treatment conditions.research designs suffer kind missing data problem. Rubin pointed missing data surveys come people didn’t survey people refused answer. causal inference problems, data missing potential outcomes revealed world. Descriptive studies, data missing true values things measured. Measurement error missing data problem !Observational research designs typified researchers impact units study. simply record outcomes happened world happened even study occur. Experimental research designs active – cause potential outcomes revealed others. way, researchers impact units study. reason, experimental studies tend raise ethical questions observational studies. Experimenters literally change potential outcomes become realized outcomes.Sometimes lines types research become blurry. Hawthorne effect name given idea measuring thing changes . Hawthorne effects, observational research designs also change potential outcomes revealed. , difference Y(Z = measured) Y(Z = unmeasured), act observation changes observed. Passive data collection methods sometimes preferred grounds.","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"observational-designs-for-descriptive-inference","chapter":"14 Observational designs for descriptive inference","heading":"14 Observational designs for descriptive inference","text":"section introduction","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"random-sampling","chapter":"14 Observational designs for descriptive inference","heading":"14.1 Random sampling","text":"","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"declaration-1","chapter":"14 Observational designs for descriptive inference","heading":"14.1.1 Declaration","text":"","code":"\nfixed_population <- declare_population(N = 500, Y = sample(1:7, N, replace = TRUE))()\n\ndesign <- \n  declare_population(data = fixed_population) + \n  declare_estimand(Y_bar = mean(Y)) + \n  declare_sampling(n = 100) + \n  declare_estimator(Y ~ 1, model = lm_robust, estimand = \"Y_bar\")"},{"path":"observational-designs-for-descriptive-inference.html","id":"dag","chapter":"14 Observational designs for descriptive inference","heading":"14.1.2 Dag","text":"","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"example-2","chapter":"14 Observational designs for descriptive inference","heading":"14.1.3 Example","text":"","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"simple-random-sampling","chapter":"14 Observational designs for descriptive inference","heading":"14.1.4 Simple random sampling","text":"Often interested features population, data entire population prohibitively expensive collect. Instead, researchers obtain data small fraction population use measurements taken sample draw inferences population.Imagine seek estimate average political ideology residents small town Portola, California, left-right scale varies 1 (liberal) 7 (conservative). draw simple random sample residents equal chance inclusion study. ’s straightforward design formally declaring make easy assess properties.","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"design-declaration","chapter":"14 Observational designs for descriptive inference","heading":"14.1.4.1 Design Declaration","text":"Model:\nEven basic designs, researchers bring bear background model world. described Chapter 1, three elements model signature, probability distributions variables, functional equations among variables. signature specification variable interest, \\(Y\\), well defined domain (seven possible values 1 7). code declaration , assume uniform distribution 7 values. choice speculation population distribution \\(Y\\); features design diagnosis depend choice distribution. functional equations seem absent one variable model. consider elaboration model includes three variables: true outcome, \\(Y\\); decision measure outcome, \\(M\\); measured outcome, \\(Y^M\\). ignore complication now assumption \\(Y = Y^M\\), .e., \\(Y\\) measured perfectly. Finally, model also includes information size population. Portola, California, population approximately 2100 people 2010, \\(N = 2100\\).Model:Even basic designs, researchers bring bear background model world. described Chapter 1, three elements model signature, probability distributions variables, functional equations among variables. signature specification variable interest, \\(Y\\), well defined domain (seven possible values 1 7). code declaration , assume uniform distribution 7 values. choice speculation population distribution \\(Y\\); features design diagnosis depend choice distribution. functional equations seem absent one variable model. consider elaboration model includes three variables: true outcome, \\(Y\\); decision measure outcome, \\(M\\); measured outcome, \\(Y^M\\). ignore complication now assumption \\(Y = Y^M\\), .e., \\(Y\\) measured perfectly. Finally, model also includes information size population. Portola, California, population approximately 2100 people 2010, \\(N = 2100\\).Inquiry:\ninquiry population mean \\(Y\\): \\(\\frac{1}{N} \\sum_1^N Y_i = \\bar{Y}\\).Inquiry:inquiry population mean \\(Y\\): \\(\\frac{1}{N} \\sum_1^N Y_i = \\bar{Y}\\).Data strategy:\nsimple random sampling, draw random sample without replacement size \\(n\\), every member population equal probability inclusion sample, \\(\\frac{n}{N}\\). \\(N\\) large relative \\(n\\), units drawn approximately independently. design measure \\(Y\\) \\(n=100\\) units sample; \\(N-n\\) units measured.Data strategy:simple random sampling, draw random sample without replacement size \\(n\\), every member population equal probability inclusion sample, \\(\\frac{n}{N}\\). \\(N\\) large relative \\(n\\), units drawn approximately independently. design measure \\(Y\\) \\(n=100\\) units sample; \\(N-n\\) units measured.Answer strategy:\nestimate population mean sample mean estimator: \\(\\widehat{\\overline{Y}} = \\frac{1}{n} \\sum_1^n Y_i\\). Even though inquiry implies answer single number, answer strategy typically also provides statistics help us assess uncertainty around single number. construct 95% confidence interval around estimate, calculate standard error sample mean, approximate sampling distribution sample mean estimator using formula includes finite population correction. particular, approximate estimated sampling distribution \\(t\\) distribution \\(n - 1\\) degrees freedom. code answer strategy, spell step turn.Answer strategy:estimate population mean sample mean estimator: \\(\\widehat{\\overline{Y}} = \\frac{1}{n} \\sum_1^n Y_i\\). Even though inquiry implies answer single number, answer strategy typically also provides statistics help us assess uncertainty around single number. construct 95% confidence interval around estimate, calculate standard error sample mean, approximate sampling distribution sample mean estimator using formula includes finite population correction. particular, approximate estimated sampling distribution \\(t\\) distribution \\(n - 1\\) degrees freedom. code answer strategy, spell step turn.","code":"\nfixed_population <- declare_population(N = 500, Y = sample(1:7, N, replace = TRUE))()\n\nrandom_sampling_design <- \n  declare_population(data = fixed_population) + \n  declare_estimand(Ybar = mean(Y)) + \n  declare_sampling(n = 100) + \n  declare_estimator(Y ~ 1, model = lm_robust, estimand = \"Ybar\")\nplot(random_sampling_design)"},{"path":"observational-designs-for-descriptive-inference.html","id":"takeaways","chapter":"14 Observational designs for descriptive inference","heading":"14.1.4.2 Takeaways","text":"design declared can run diagnosis plot results Monte Carlo simulations design:diagnosis indicates simple random sampling, sample mean estimator population mean unbiased. graph left shows sampling distribution estimator: ’s centered directly true value inquiry. Confidence intervals also sampling distribution – change depending idiosyncrasies sample happen draw. figure right shows 95% time confidence intervals cover true value estimand, . sample size grows, sampling distribution estimator gets tighter, coverage confidence intervals stays 95% – just properties want answer strategy.Things work well seems. exercises suggest small modifications design point conditions things might break .","code":"\ndiagnosis <- diagnose_design(\n  design, sims = sims, bootstrap_sims = b_sims, diagnosands = diagnosands) "},{"path":"observational-designs-for-descriptive-inference.html","id":"stratified-and-clustered-random-sampling","chapter":"14 Observational designs for descriptive inference","heading":"14.1.5 Stratified and clustered random sampling","text":"Researchers often randomly sample individual level may, among reasons, costly logistically impractical. Instead, may choose randomly sample households, political precincts, group individuals order draw inferences population. strategy may cheaper simpler may also introduce risks less precise estimates.Say interested average party ideology entire state California. Using cluster sampling, randomly sample counties within state, within selected county, randomly sample individuals survey.Assuming enough variation outcome interest, random assignment equal-sized clusters yields unbiased imprecise estimates. sampling clusters, select groups individuals may share common attributes. Unlike simple random sampling, need take account intra-cluster correlation estimation standard error.22 higher degree within-cluster similarity, variance observe cluster-level averages imprecise estimates.23 address considering cluster-robust standard errors answer strategy .","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"design-declaration-1","chapter":"14 Observational designs for descriptive inference","heading":"14.1.5.1 Design Declaration","text":"Model:specify variable interest \\(Y\\) (political ideology, say) discrete variable ranging 1 (liberal) 7 (conservative). define functional model since interested population mean \\(Y\\). model also includes information number sampled clusters number individuals per cluster.Inquiry:estimand population mean political identification \\(Y\\). employed random sampling, can expect value sample mean (\\(\\widehat{\\overline{y}}\\)) approximate true population parameter (\\(\\widehat{\\overline{Y}}\\)).Data strategy:Sampling follows two-stage strategy. first draw random sample 30 counties California, county select 20 individuals random. guarantees county probability included sample resident within county probability sample. design estimate \\(Y\\) n = 600 respondents.Answer strategy:estimate population mean sample mean estimator: \\(\\widehat{\\overline{Y}} = \\frac{1}{n} \\sum_1^n Y_i\\), estimate standard errors assumption independent heteroskedastic errors well cluster-robust standard errors take account correlation errors within clusters. demonstrate imprecision estimated \\(\\widehat{\\overline{Y}}\\) cluster standard errors presence intracluster correlation coefficient (ICC) 0.402.","code":"\nN_blocks <- 1\nN_clusters_in_block <- 1000\nN_i_in_cluster <- 50\nn_clusters_in_block <- 30\nn_i_in_cluster <- 20\nicc <- 0.402\n\n# M: Model\nfixed_pop <-\n  declare_population(\n    block = add_level(N = N_blocks),\n    cluster = add_level(N = N_clusters_in_block),\n    subject = add_level(N = N_i_in_cluster,\n                        latent = draw_normal_icc(mean = 0, N = N, clusters = cluster, ICC = icc),\n                        Y = draw_ordered(x = latent, breaks = qnorm(seq(0, 1, length.out = 8)))\n    )\n  )()\n\ncluster_sampling_design <- declare_population(data = fixed_pop) +\n  \n  # I: Inquiry\n  declare_estimand(Ybar = mean(Y)) +\n  \n  # D: Data Strategy\n  declare_sampling(strata = block, \n                   clusters = cluster, n = n_clusters_in_block, \n                   sampling_variable = \"Cluster_Sampling_Prob\") +\n  \n  declare_sampling(strata = cluster,   n = n_i_in_cluster, \n                   sampling_variable = \"Within_Cluster_Sampling_Prob\") +\n  \n  # A: Answer Strategy\n  declare_estimator(Y ~ 1,\n                    model = lm_robust,\n                    clusters = cluster,\n                    estimand = \"Ybar\",\n                    label = \"Clustered Standard Errors\")"},{"path":"observational-designs-for-descriptive-inference.html","id":"takeaways-1","chapter":"14 Observational designs for descriptive inference","heading":"14.1.5.2 Takeaways","text":"appreciate role clustering better also plot simulated values estimand standard errors clustered clustered standard errors. first add additional estimator design take account clusters.figure may give us impression estimate clustered standard errors less precise, fact, correctly accounts uncertainty surrounding estimates. blue lines graph demonstrate estimates simulations contain estimand. table graphs show, share simulations total number simulations, also known coverage, (correctly) close 95% estimations clustered standard errors 54% estimations without clustered standard errors. expected, mean estimate bias specifications.","code":"\ndiagnosis <- diagnose_design(cluster_sampling_design, sims = sims)\nnew_design <- cluster_sampling_design + declare_estimator(Y ~ 1,\n                                                          model = lm_robust,\n                                                          estimand = \"Ybar\",\n                                                          label = \"Naive Standard Errors\")\ndiagnosis <- diagnose_design(new_design, sims = sims)"},{"path":"observational-designs-for-descriptive-inference.html","id":"exercises","chapter":"14 Observational designs for descriptive inference","heading":"14.1.5.3 Exercises","text":"Modify declaration change distribution \\(Y\\) uniform something else: perhaps imagine extreme ideologies prevalent moderate ones. sample mean estimator still unbiased? Interpret answer.Change sampling procedure favor units higher values ideology. sample mean estimator still unbiased? Interpret answer.Modify estimation function use formula standard error: \\(\\widehat{se} \\equiv \\frac{\\widehat\\sigma}{\\sqrt{n}}\\). equation differs one used declaration (ignores total population size \\(N\\)). Check coverage new design incorrect \\(N=n\\). Assess large \\(N\\) difference procedures matter.","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"poststratification","chapter":"14 Observational designs for descriptive inference","heading":"14.2 Poststratification","text":"Poststratification tool learning population sample whose units drawn unequal (usually unknown) probabilities. essence, know features population can measure sample, can reweight sample look like population—least along dimensions can measure. simplest form, poststratification just involves counting dividing. Say want poststratify sample , terms gender age, looks like people twenties United States population 2010. gives twenty groups—one age gender—whose size need count 2010 Decennial census sample. , simply weight person sample size group population divided size group sample. illustrate declaration , turns weights equivalent inverse sampling probabilities one obtain one construct sample using groups strata. ’s “stratification” poststratification comes —’s “post” weights constructed sampling already taken place. online example, illustrate Bayesian multi-level regression poststratification (MRP) can help deal fact , number group-defining features increases, sample may contain information groups defined combination features.","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"declaration-2","chapter":"14 Observational designs for descriptive inference","heading":"14.2.1 Declaration","text":"Model: fixed, finite population comprised two groups defined binary variable, \\(X\\). 100 people \\(X = 0\\) 50 \\(X = 1\\). outcome, \\(Y\\), takes one three values, 0, 1, 2, function \\(X\\).Model: fixed, finite population comprised two groups defined binary variable, \\(X\\). 100 people \\(X = 0\\) 50 \\(X = 1\\). outcome, \\(Y\\), takes one three values, 0, 1, 2, function \\(X\\).Inquiry: researcher wants know true average \\(Y\\) population.Inquiry: researcher wants know true average \\(Y\\) population.Data strategy: Ten individuals group included sample. Thus, whereas 2/3 population belong \\(X = 0\\) group, 1/2 sample . Conversely, 1/3 population belongs \\(X = 1\\) group, 1/2 sample .Data strategy: Ten individuals group included sample. Thus, whereas 2/3 population belong \\(X = 0\\) group, 1/2 sample . Conversely, 1/3 population belongs \\(X = 1\\) group, 1/2 sample .Answer strategy: answer strategy involves taking weighted average \\(Y\\) sample, unit’s weight equal size group population divided size group sample. Specifically, underrepresented group, \\(X = 0\\), get weight 100/10 = 10, overrepresented group, \\(X = 1\\), gets weight 50 / 10 = 5. poststratification groups perfectly line sample strata, , approach equivalent weighting inverse sampling probability: 1/(10/100) = 10 1/(10/50) = 5. cases, weights tell us many units population unit sample represents.Answer strategy: answer strategy involves taking weighted average \\(Y\\) sample, unit’s weight equal size group population divided size group sample. Specifically, underrepresented group, \\(X = 0\\), get weight 100/10 = 10, overrepresented group, \\(X = 1\\), gets weight 50 / 10 = 5. poststratification groups perfectly line sample strata, , approach equivalent weighting inverse sampling probability: 1/(10/100) = 10 1/(10/50) = 5. cases, weights tell us many units population unit sample represents.","code":"\nfixed_population <- declare_population(\n  group = add_level(N = 2, X = c(0,1), population_n = c(100,50)),\n  individual = add_level(N = population_n, Y = X + sample(0:1, N, replace = TRUE)))()\n\ndesign <- \n  declare_population(data = fixed_population) + \n  declare_estimand(Ybar = mean(Y)) + \n  declare_sampling(strata_n = c(10,10), strata = X) + \n  declare_step(handler = group_by, groups = X) +\n  declare_step(handler = mutate, \n               sample_n = n(), \n               weight = population_n / sample_n) +\n  declare_estimator(Y ~ 1, term = \"(Intercept)\", model = lm_robust, weights = weight, estimand = \"Ybar\")"},{"path":"observational-designs-for-descriptive-inference.html","id":"dag-1","chapter":"14 Observational designs for descriptive inference","heading":"14.2.2 Dag","text":"","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"exercises-1","chapter":"14 Observational designs for descriptive inference","heading":"14.2.3 Exercises","text":"Add unweighted estimator design diagnose . Explain unweighted estimator biased, well direction bias.Remove X definition Y. Explain bias now equivalent two approaches.Change strata_n 4 people sampled first group 25 sampled second group.\nCalculate poststratification weight group hand (hint: can check answer using draw_data(design)$weight).\nCalculate sample inclusion probability group hand (hint: can check answer using draw_data(design)$S_inclusion_prob).\nCalculate inverse sampling probability group. equal poststratification weights.\nCalculate poststratification weight group hand (hint: can check answer using draw_data(design)$weight).Calculate sample inclusion probability group hand (hint: can check answer using draw_data(design)$S_inclusion_prob).Calculate inverse sampling probability group. equal poststratification weights.","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"example-3","chapter":"14 Observational designs for descriptive inference","heading":"14.2.4 Example","text":"","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"example-4","chapter":"14 Observational designs for descriptive inference","heading":"14.2.5 Example","text":"can use global bib file via rmarkdown cites like : Imai, King, Stuart (2008)chunk set echo = TRUE eval = do_diagnosisRight simulations, want save simulations rds.Now simulating, saving, loading done, can use simulations whatever want.","code":"\n# US population\ndelaware_population_df <- fabricate(\n  data = delaware_senate_districts_df,\n  individuals = add_level(\n    N = population_size,\n    race_white = rbinom(N, 1, prob = prop_white),\n    race_black = rbinom(N, 1, prob = prop_black),\n    race_asian = rbinom(N, 1, prob = prop_black),\n    race_hispanic_other = rbinom(N, 1, prob = prop_hispanic_other),\n    pid_republican = rbinom(N, 1, prob = prop_republican),\n    pid_democrat = rbinom(N, 1, prob = prop_democrat)\n  )\n) %>% \n  select(-starts_with(\"prop_\"), -population_size)\n\n# population weights for MRP\nmrp_weights <- delaware_population_df %>%\n  group_by(district, race_white, race_black, race_asian, race_hispanic_other, pid_republican, pid_democrat) %>% \n  summarize(n_cell = n()) %>% \n  group_by(district) %>% \n  mutate(proportion_cell = n_cell/sum(n_cell)) %>% \n  select(-n_cell) %>% \n  ungroup \n\ndelaware_population_df <- mrp_weights %>% \n  select(district, proportion_cell) %>% \n  right_join(delaware_population_df)\n\n# Lax and Philips APSR 2009\n# Policies are coded dichotomously, 1 for the progay policy and 0 otherwise: Adoption (9 states allow second-parent adoption in all jurisdictions)\n\ndesign <-\n  declare_population(\n    data = delaware_population_df, \n    \n    districts = modify_level(district_effect = rnorm(N)),\n    \n    individuals = modify_level(\n      noise = rnorm(N, mean = district_effect),\n      policy_support = rbinom(N, 1, prob = pnorm(\n        0.25 + 0.2 * race_white - 0.1 * race_black - 0.2 * race_hispanic_other - \n          0.1 * pid_democrat + 0.15 * pid_republican + noise))\n    )\n  ) + \n  \n  declare_estimand(handler = function(data) {\n    data %>%\n      group_by(district) %>%\n      summarize(estimand = mean(policy_support)) %>%\n      ungroup %>% \n      mutate(estimand_label = \"mean_policy_support\")\n  }) +\n  \n  declare_sampling(n = 500) +\n  \n  declare_estimator(handler = tidy_estimator(function(data) {\n    data %>%\n      group_by(district) %>%\n      summarize(estimate = mean(policy_support))\n  }), label = \"strata_means\", estimand = \"mean_policy_support\") + \n\n  # this estimator owes code to https://timmastny.rbind.io/blog/multilevel-mrp-tidybayes-brms-stan/\n  declare_estimator(handler = tidy_estimator(function(data) {\n\n    model_fit <- glmer(\n      formula = policy_support ~ race_white + race_black + race_asian + race_hispanic_other +\n        pid_democrat + pid_republican + (1 | district),\n      data = data, family = binomial(link = \"logit\"))\n\n    data %>%\n      mutate(\n        support_predicted =\n          prediction(model_fit, data = ., allow.new.levels = TRUE, type = \"response\"),\n        support_predicted_weighted = support_predicted * proportion_cell\n      ) %>%\n      group_by(district) %>%\n      summarize(estimate = sum(support_predicted_weighted))\n\n  }), label = \"mrp_mle\", estimand = \"mean_policy_support\")\n\ndat <- draw_data(design)\n\ndraw_estimates(design)\n\nsims <- simulate_design(design, sims = 3)\n\ndiag <- diagnose_design(design, sims = 100, diagnosands = declare_diagnosands(select = bias), add_grouping_variables = \"state\")\nsimulations_pilot <- simulate_design(design, sims = sims)\nkable(head(simulations_pilot))"},{"path":"observational-designs-for-descriptive-inference.html","id":"inference-about-unobserved-variables","chapter":"14 Observational designs for descriptive inference","heading":"14.3 Inference about unobserved variables","text":"","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"declaration-3","chapter":"14 Observational designs for descriptive inference","heading":"14.3.1 Declaration","text":"","code":"\ndesign <-\n  declare_population(N = 10, Y_star = rnorm(N), true_rank = rank(Y_star)) +\n  declare_measurement(Y_1 = 0.1 * Y_star + rnorm(N, sd = 0.25),\n                      Y_2 = Y_star + rnorm(N, sd = 0.25),\n                      Y_3 = 1 + 0.5 * Y_star + rnorm(N, sd = 0.25),\n                      Y_idx = (Y_1 + Y_2 + Y_3) / 3, \n                      ranking = rank(Y_idx)) \n\n# simulate_design(design) %>% \n#   summarize(ranking_correct = mean(true_rank == ranking))"},{"path":"observational-designs-for-descriptive-inference.html","id":"dag-2","chapter":"14 Observational designs for descriptive inference","heading":"14.3.2 Dag","text":"","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"example-5","chapter":"14 Observational designs for descriptive inference","heading":"14.3.3 Example","text":"","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"structural-estimation","chapter":"14 Observational designs for descriptive inference","heading":"14.4 Structural estimation","text":"","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"declaration-4","chapter":"14 Observational designs for descriptive inference","heading":"14.4.1 Declaration","text":"","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"dag-3","chapter":"14 Observational designs for descriptive inference","heading":"14.4.2 Dag","text":"","code":""},{"path":"observational-designs-for-descriptive-inference.html","id":"example-6","chapter":"14 Observational designs for descriptive inference","heading":"14.4.3 Example","text":"","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"experimental-designs-for-descriptive-inference","chapter":"15 Experimental designs for descriptive inference","heading":"15 Experimental designs for descriptive inference","text":"ever need experiment descriptive inference?Suppose want understand causal model \\(M\\) violin. particular, descriptive inquiry \\(\\) pitch highest string, E string. want know E string tune. Call latent pitch string \\(Y^*\\). matter hard listen string, can’t hear \\(Y^*\\) – latent. part data strategy \\(D\\), measure pitch \\(P\\) plucking : \\(Y^* -> Y <- P\\). descriptive research causal model \\(M\\), DAG violin includes four string nodes cause pitch nodes; ’d like know descripitive fact pitch nodes (frequency virbaring .)also causal inquiry: untreated potential outcome pitch unplucked string, defined frequency vibrartion – strings never perfectly still, can call untreated potential outcome \\(Y_i(0) = 0hz\\). treated potential outcome vibrations string plucked \\(Y_i(1) = 650hz\\). causal effect plucking string \\(Y_i(1) - Y_i(0) = 650 - 0 = 650\\).sense causal inference descriptive inference . Whether framed desriptive inquiry casual inquiry, arrive answer 650 hertz. Violinists reading know means E string flat need tuned \\(659.3hz\\) (using equal temperment).","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"audit-experiments","chapter":"15 Experimental designs for descriptive inference","heading":"15.1 Audit experiments","text":"","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"declaration-5","chapter":"15 Experimental designs for descriptive inference","heading":"15.1.1 Declaration","text":"","code":"\ndesign <- \n  declare_population(N = 100,\n                     U = rnorm(N)) +\n  declare_potential_outcomes(R ~ if_else(Z + U > 0.5, 1, 0), conditions = list(Z = c(0, 1))) +\n  declare_potential_outcomes(Q ~ if_else(R == 1, Z + U, NA_real_), conditions = list(Z = c(0, 1), R = c(0, 1))) +\n  declare_estimand(ATE_R = mean(R_Z_1 - R_Z_0)) + \n  declare_estimand(CATE_ar = mean(Q_Z_1_R_1 - Q_Z_0_R_1), subset = (R_Z_1 == 1 & R_Z_0 == 0)) + \n  declare_assignment(prob = 0.5) +\n  declare_reveal(R, Z) +\n  declare_reveal(Q, c(Z, R)) +\n  declare_estimator(R ~ Z, estimand = \"ATE_R\", label = \"ATE_R\") +\n  declare_estimator(Q ~ Z, subset = (R == 1), estimand = \"CATE_ar\", label = \"CATE_ar\")"},{"path":"experimental-designs-for-descriptive-inference.html","id":"dag-4","chapter":"15 Experimental designs for descriptive inference","heading":"15.1.2 Dag","text":"","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"example-7","chapter":"15 Experimental designs for descriptive inference","heading":"15.1.3 Example","text":"basic requirement good research design question seeks answer fact answer, least plausible models world. framework, means inquiry \\(\\) must associated answer \\(^M\\), refers answer model. Interestingly, sometimes might conscious questions ask answers. Fortunately, ask computer answer question, complains.question answer? Answerless questions can arise inquiries depend variables exist undefined units. words, mismatch model inquiry, ’re asking question something doesn’t exist.Consider audit experiment (see Audit Experiment Design) seeks assess effects email Latino name (versus White name) whether well election officials respond requests information. example, use positive negative tone. questions seem reasonable enough. problem, however, officials don’t send responses, tone undefined. subtly, official send email sent different treatment condition, tone undefined one potential outcomes.","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"design-declaration-2","chapter":"15 Experimental designs for descriptive inference","heading":"15.1.4 Design Declaration","text":"Model:\nmodel two outcome variables, \\(R_i\\) \\(Y_i\\). \\(R_i\\) stands “response” equal 1 response sent, 0 otherwise. \\(Y_i\\) tone response normally distributed defined. \\(Z_i\\) treatment equals 1 email sent using Latino name 0 otherwise. table shows potential outcomes four possible types subjects, depending potential outcomes \\(R_i\\). types always respond regardless treatment D types never respond, regardless treatment. B types respond treated, whereas C types respond treated. table also includes columns potential outcomes \\(Y_i\\), showing potential outcome subjects express depending type. key thing note B, C, D types, effect treatment \\(Y_i\\) undefined messages never sent tone. last (important) feature model outcomes \\(Y_i\\) possibly correlated subject type. Even though \\(\\mathbb{E}[Y_i(1) | \\text{Type} = ]\\) \\(\\mathbb{E}[Y_i(1) | \\text{Type} = B]\\) exist, ’s reason expect .\ndesign assume distribution types 40% , 5% B, 10% C, 45% D.Model:model two outcome variables, \\(R_i\\) \\(Y_i\\). \\(R_i\\) stands “response” equal 1 response sent, 0 otherwise. \\(Y_i\\) tone response normally distributed defined. \\(Z_i\\) treatment equals 1 email sent using Latino name 0 otherwise. table shows potential outcomes four possible types subjects, depending potential outcomes \\(R_i\\). types always respond regardless treatment D types never respond, regardless treatment. B types respond treated, whereas C types respond treated. table also includes columns potential outcomes \\(Y_i\\), showing potential outcome subjects express depending type. key thing note B, C, D types, effect treatment \\(Y_i\\) undefined messages never sent tone. last (important) feature model outcomes \\(Y_i\\) possibly correlated subject type. Even though \\(\\mathbb{E}[Y_i(1) | \\text{Type} = ]\\) \\(\\mathbb{E}[Y_i(1) | \\text{Type} = B]\\) exist, ’s reason expect .\ndesign assume distribution types 40% , 5% B, 10% C, 45% D.Causal TypesInquiry:\ntwo inquiries. first straightforward: \\(\\mathbb{E}[R_i(1) - R_i(0)]\\) Average Treatment Effect response. second inquiry undefined inquiry answer: \\(\\mathbb{E}[Y_i(1) - Y_i(0)]\\). also consider third inquiry, defined: \\(\\mathbb{E}[Y_i(1) - Y_i(0) | \\mathrm{Type} = ]\\), average effect treatment tone among \\(\\) types.Inquiry:two inquiries. first straightforward: \\(\\mathbb{E}[R_i(1) - R_i(0)]\\) Average Treatment Effect response. second inquiry undefined inquiry answer: \\(\\mathbb{E}[Y_i(1) - Y_i(0)]\\). also consider third inquiry, defined: \\(\\mathbb{E}[Y_i(1) - Y_i(0) | \\mathrm{Type} = ]\\), average effect treatment tone among \\(\\) types.Data strategy:\ndata strategy use complete random assignment assign 250 500 units treatment.Data strategy:data strategy use complete random assignment assign 250 500 units treatment.Answer strategy:\n’ll try answer three inquiries difference--means estimator, diagnosis reveal, strategy works well inquiries others.Answer strategy:’ll try answer three inquiries difference--means estimator, diagnosis reveal, strategy works well inquiries others.","code":"\n# Model -------------------------------------------------------------------\npopulation <- declare_population(\n  N = 500,\n  type = sample(c(\"A\", \"B\", \"C\", \"D\"), size = N, \n                replace = TRUE, prob = c(.40, .05, .10, .45)))\n\npotential_outcomes <- declare_potential_outcomes(\n  R_Z_0 = type %in% c(\"A\", \"C\"),\n  R_Z_1 = type %in% c(\"A\", \"B\"),\n  Y_Z_0 = ifelse(R_Z_0, rnorm(n = sum(R_Z_0), mean = .1*(type == \"A\") - 2*(type == \"C\")), NA),\n  Y_Z_1 = ifelse(R_Z_1, rnorm(n = sum(R_Z_1), mean = .2*(type == \"A\") + 2*(type == \"B\")), NA)\n)\n\n# Inquiry -----------------------------------------------------------------\nestimand_1 <- declare_estimand(ATE_R = mean(R_Z_1 - R_Z_0))\nestimand_2 <- declare_estimand(ATE_Y = mean(Y_Z_1 - Y_Z_0))\nestimand_3 <- declare_estimand(\n  ATE_Y_for_As = mean(Y_Z_1[type == \"A\"] - Y_Z_0[type == \"A\"]))\n\n# Data Strategy -----------------------------------------------------------\nassignment <- declare_assignment(m = 250)\n\n# Answer Strategy ---------------------------------------------------------\nestimator_1 <- declare_estimator(R ~ Z, estimand = estimand_1, label = \"ATE_R\")\nestimator_2 <- declare_estimator(Y ~ Z, estimand = estimand_2, label = \"ATE_Y\")\nestimator_3 <- declare_estimator(Y ~ Z, estimand = estimand_3, label = \"ATE_YA\")\n\n# Design ------------------------------------------------------------------\ndesign <- \n  population + \n  potential_outcomes + \n  assignment + \n  estimand_1 + estimand_2 + estimand_3 + \n  declare_reveal(outcome_variables = c(\"R\", \"Y\")) + \n  estimator_1 + estimator_2 + estimator_3"},{"path":"experimental-designs-for-descriptive-inference.html","id":"takeaways-2","chapter":"15 Experimental designs for descriptive inference","heading":"15.1.5 Takeaways","text":"now diagnose design:learn three things design diagnosis. First, expected, experiment unbiased average treatment effect response.Next, see second inquiry, well diagnostics , undefined. diagnosis tells us definition potential outcomes produces definition problem estimand. Note diagnosands defined, including power, depend answer strategy estimand.Finally, third estimand – average effects \\(\\) types – defined estimates biased. reason tell data types \\(\\) types: conditioning correct subset. Indeed, unable condition correct subset. subject responds treatment group, don’t know \\(\\) \\(B\\) type; control group, can’t tell responder \\(\\) \\(C\\) type. difference--means estimator ATE \\(Y\\) among \\(\\)s whenever \\(\\)s different outcomes \\(B\\)s \\(C\\)s.cases, problem might resolved changing inquiry. Closely related estimands can often defined, perhaps redefining \\(Y\\) (e.g., emails never sent tone zero). redefinitions problem, one examine , require estimating effects unobserved subgroups difficult challenge.","code":"\ndiagnosis <- diagnose_design(design, sims = sims)"},{"path":"experimental-designs-for-descriptive-inference.html","id":"applications","chapter":"15 Experimental designs for descriptive inference","heading":"15.1.6 Applications","text":"kind problem surprisingly common. three distinct instances problem:\\(Y\\) decision vote Democrat (\\(Y=1\\)) Republican (\\(Y=0\\)), \\(R\\) decision turn vote \\(Z\\) campaign message. decision vote may depend treatment subjects vote \\(Y\\) undefined.\\(Y\\) weight infants, \\(R\\) whether child born \\(Z\\) maternal health intervention. Fertility may depend treatment weight unborn (possibly never conceived) babies defined.\\(Y\\) charity contributions made fundraising \\(R\\) whether anything contributed \\(Z\\) encouragement contribute. identity beneficiaries defined contributions.problem exhibit form post treatment bias (see section Post treatment bias) issue goes beyond picking right estimator. problem conceptual: effect treatment outcome just doesn’t exist subjects.","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"exercises-2","chapter":"15 Experimental designs for descriptive inference","heading":"15.1.7 Exercises","text":"amount bias third estimand depends distribution types correlation types potential outcomes Y. Modify declaration estimator effect Y unbiased, changing distribution types. Repeat exercise, changing correlation type potential outcomes \\(Y\\).amount bias third estimand depends distribution types correlation types potential outcomes Y. Modify declaration estimator effect Y unbiased, changing distribution types. Repeat exercise, changing correlation type potential outcomes \\(Y\\).Try approaching problem redefining inquiry, seeking assess effect treatment share responses positive tone.Try approaching problem redefining inquiry, seeking assess effect treatment share responses positive tone.","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"experiments-for-sensitive-questions","chapter":"15 Experimental designs for descriptive inference","heading":"15.2 Experiments for sensitive questions","text":"","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"declaration-6","chapter":"15 Experimental designs for descriptive inference","heading":"15.2.1 Declaration","text":"","code":"\ndesign <-\n  declare_population(\n    N = 100,\n    U = rnorm(N),\n    Y_star = rbinom(N, size = 1, prob = 0.3),\n    S = case_when(Y_star == 0 ~ 0L,\n                  Y_star == 1 ~ rbinom(N, size = 1, prob = 0.2)),\n    X = rbinom(N, size = 3, prob = 0.5)\n  ) +\n  declare_estimand(proportion = mean(Y_star)) +\n  declare_measurement(Y_direct = Y_star - S) +\n  declare_potential_outcomes(Y_list ~ Y_star * Z + X) +\n  declare_assignment(prob = 0.5) +\n  declare_estimator(Y_direct ~ 1,\n                    model = lm_robust,\n                    estimand = \"proportion\",\n                    label = \"direct\") +\n  declare_estimator(Y_list ~ Z, estimand = \"proportion\", label = \"list\")"},{"path":"experimental-designs-for-descriptive-inference.html","id":"dag-5","chapter":"15 Experimental designs for descriptive inference","heading":"15.2.2 Dag","text":"","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"example-8","chapter":"15 Experimental designs for descriptive inference","heading":"15.2.3 Example","text":"setup: descriptive estimand, proportion holding sensitive characteristic; two experimental designs recover , list experiments randomized responseif identification assumptions violated (focus ceiling/floor), estimates ATE still unbiased descriptive estimandcompare design ceiling/floor categories minimized Glynn (2013) design advice use negatively-correlated items high prevalence low prevalence itemboth designs exhibit bias-variance tradeoff (control variance RR)","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"list-experiments","chapter":"15 Experimental designs for descriptive inference","heading":"15.2.4 List experiments","text":"Sometimes, subjects might tell truth directly asked certain attitudes behaviors. Responses may affected sensitivity bias, tendency survey subjects dissemble fear negative repercussions reference group learns true response (Blair, Coppock, Moor 2018). cases, standard survey estimates based direct questions biased. One class solutions problem obscure individual responses, providing protection social legal pressures. obscure responses systematically experiment, can often still identify average quantities interest. One design list experiment (introduced Miller (1984)), asks respondents count number `yes’ responses series questions including sensitive item, rather yes answer sensitive item . List experiments give subjects cover aggregating answer sensitive item responses questions.2016 Presidential Election U.S., observers concerned pre-election estimates support Donald Trump might downward biased “Shy Trump Supporters” – survey respondents supported Trump hearts, embarrassed admit pollsters. assess possibility, Coppock (2017) obtained estimates Trump support free social desirability bias using list experiment. Subjects control treatment groups asked: “list [three/four] things people people . Please tell MANY . want know ones , just many. [three/four] things:”treatment group averaged 1.843 items control group averaged 1.548 items, difference--means estimate 0.296. show unbiased estimator average treatment effect asked respond treated list control list (invoking usual assumptions randomized experiments, including SUTVA). estimand proportion people support Donald Trump. difference--means unbiased estimator proportion respondents say “yes” sensitive item, invoke two additional assumptions: design effects “liars” (see Imai 2011). first highlights fact need good estimate average control item count control group (example 1.843). use net control item count responses treated group (left sensitive item proportion). respondents provide different control item count treated group control group, example evaluate items relatively inclusion sensitive item changes answers (see Flavin Keane 2011), design breaks . liars assumption says respondents provide truthful answers sensitive item within count. justification assumption plausible cover asked within count makes possible respondents answer truthfully.estimate , assumptions, free sensitivity bias, ’s also much higher variance. 95% confidence interval list experiment estimate nearly 14 percentage points wide, whereas 95% confidence interval (possibly biased!) direct question asked sample closer 4 percentage points.choice list experiments direct question therefore bias-variance tradeoff. List experiments may less bias, higher variance. Direct questions may biased, less variance.","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"declaration-7","chapter":"15 Experimental designs for descriptive inference","heading":"15.2.4.1 Declaration","text":"Model: model includes subjects’ true support Donald Trump whether “shy”. two variables combine determine subjects respond asked directly Trump support.\npotential outcomes model combines three types information determine subjects respond list experiment: responses three nonsensitive control items, true support Trump, whether assigned see treatment control list. Notice definition potential outcomes embeds liars design effects assumptions required list experiment design.\nalso global parameter reflects expectations proportion Trump supporters shy. ’s set 6%, large enough make difference polling, large implausible.Model: model includes subjects’ true support Donald Trump whether “shy”. two variables combine determine subjects respond asked directly Trump support.potential outcomes model combines three types information determine subjects respond list experiment: responses three nonsensitive control items, true support Trump, whether assigned see treatment control list. Notice definition potential outcomes embeds liars design effects assumptions required list experiment design.also global parameter reflects expectations proportion Trump supporters shy. ’s set 6%, large enough make difference polling, large implausible.Inquiry: estimand proportion voters actually plan vote Trump.Inquiry: estimand proportion voters actually plan vote Trump.Data strategy: First sample 500 respondents U.S. population random, randomly assign 250 500 treatment remainder control. survey, ask subjects direct question list experiment question.Data strategy: First sample 500 respondents U.S. population random, randomly assign 250 500 treatment remainder control. survey, ask subjects direct question list experiment question.Answer strategy: estimate proportion truthful Trump voters two ways. First, take mean answers direct question. Second, take difference means responses list experiment question.Answer strategy: estimate proportion truthful Trump voters two ways. First, take mean answers direct question. Second, take difference means responses list experiment question.plot shows sampling distribution direct list experiment estimators. sampling distribution direct question tight biased; list experiment (requisite assumptions hold) unbiased, higher variance. choice two estimators prevalence rate depends – bias variance – important particular setting. See Blair, Coppock, Moor (2018) extended discussion choice research design depends deeply purpose project.","code":"\n# Model -------------------------------------------------------------------\nproportion_shy <- .06\n\nlist_design <-\n  \n  # Model\n  declare_population(\n    N = 5000,\n    # true trump vote (unobservable)\n    truthful_trump_vote = draw_binary(.45, N),\n    \n    # shy voter (unobservable)\n    shy = draw_binary(proportion_shy, N),\n    \n    # direct question response (1 if Trump supporter and not shy, 0 otherwise)\n    Y_direct = if_else(truthful_trump_vote == 1 & shy == 0, 1, 0),\n    \n    # nonsensitive list experiment items\n    raise_minimum_wage = draw_binary(.8, N),\n    repeal_obamacare = draw_binary(.6, N),\n    ban_assault_weapons = draw_binary(.5, N)\n  ) +\n  \n  declare_potential_outcomes(\n    Y_list_Z_0 = raise_minimum_wage + repeal_obamacare + ban_assault_weapons,\n    Y_list_Z_1 = Y_list_Z_0 + truthful_trump_vote\n  ) +\n  \n  # Inquiry\n  declare_estimand(proportion_truthful_trump_vote = mean(truthful_trump_vote),\n                   ATE = mean(Y_list_Z_1 - Y_list_Z_0)) +\n  \n  # Data Strategy\n  declare_sampling(n = 500) +\n  declare_assignment(prob = .5) +\n  declare_reveal(Y_list) +\n  \n  # Answer Strategy\n  declare_estimator(\n    Y_direct ~ 1, model = lm_robust, term = \"(Intercept)\", estimand = \"proportion_truthful_trump_vote\", label = \"direct\") +\n  declare_estimator(\n    Y_list ~ Z, model = difference_in_means, estimand = c(\"proportion_truthful_trump_vote\", \"ATE\"), label = \"list\")\nsimulations_list <- simulate_design(list_design, sims = sims)"},{"path":"experimental-designs-for-descriptive-inference.html","id":"violations-of-identifying-assumptions","chapter":"15 Experimental designs for descriptive inference","heading":"15.2.4.2 Violations of identifying assumptions","text":"model, definition treated potential outcome, Y_list_Z_1 = Y_list_Z_0 + truthful_trump_vote, bakes design effects liars assumptions. first component control item count Y_list_Z_0, ensures respondent’s count control items groups. second true trump vote, assumes liars.learn experimental design assumptions hold? examine case “ceiling effects,” respondents whose control item count maximum (example, vote yes three control items) withhold true support Trump treatment group. thus redefine treated potential outcome function original count, respond 4 (control items plus Trump support) instead respond 3. “liars.”see list experiment still unbiased estimator average difference responses treatment list shorter control list (ATE). ceiling effects, longer unbiased estimator proportion truthful Trump vote. Indeed, unbiased direct question. divergence illustrates common feature experimental designs descriptive inference: average treatment effect can estimated without bias SUTVA randomization designs, additional assumptions required order add interpretation ATE descriptive quantity interest. burden researcher demonstrate credibility additional assumptions. experimental design alone sufficient justification.","code":"\nlist_design_ceiling <- replace_step(\n  list_design, step = 2, \n  new_step = declare_potential_outcomes(\n    Y_list_Z_0 = raise_minimum_wage + repeal_obamacare + ban_assault_weapons,\n    Y_list_Z_1_no_liars = Y_list_Z_0 + truthful_trump_vote,\n    Y_list_Z_1 = ifelse(Y_list_Z_1_no_liars == 4, 3, Y_list_Z_1_no_liars))\n)\ndiagnosis_list_ceiling <- diagnose_design(list_design_ceiling, sims = sims, bootstrap_sims = b_sims)"},{"path":"experimental-designs-for-descriptive-inference.html","id":"addressing-potential-assumption-violations-by-design","chapter":"15 Experimental designs for descriptive inference","heading":"15.2.4.3 Addressing potential assumption violations by design","text":"Researchers may bolster assumptions identify descriptive estimand changes data strategy answer strategy. Changes data strategy list experiment aim reduce risk design effects violations liars assumptions. example, risk ceiling effects, Glynn (2013) proposes selecting control items inversely correlated. three items, two items perfectly negatively correlated (.e., say “yes” one item say “” ), control item count always maximum three ceiling effects bite. illustrate design change replacing population declaration design ceiling effects. change population, really part data strategy involves choice measurement tool (control items researcher selects ask respondents).see design stil lunbiased ATE now unbiased proportion truthful Trump vote. longer ceiling effects, represented violation design effects assumption required interpret ATE proportion truthful Trump vote.Changes answer strategy proposed address design effects liars assumption. Blair Imai (2012) propose statistical test design effects assumption; pass, suggest analyzing list experiment data (.e., procedure makes answer strategy). Scholars also identified improvements answer strategy address violations liars: Blair Imai (2012) provides model adjusts ceiling floor effects Li (2019) provides bounds approach relaxes assumption.","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"randomized-response-technique","chapter":"15 Experimental designs for descriptive inference","heading":"15.2.5 Randomized response technique","text":"","code":"\nlibrary(rr)\n\nrr_forced_known <- function(data) {\n  fit  <- try(rrreg(Y_forced_known ~ 1, data = data, p = 2/3, p0 = 1/6, p1 = 1/6, design = \"forced-known\"))\n  pred <- try(as.data.frame(predict(fit, avg = TRUE, quasi.bayes = TRUE)))\n  if(class(fit) != \"try-error\" & class(pred) != \"try-error\") {\n    names(pred) <- c(\"estimate\", \"std.error\", \"conf.low\", \"conf.high\")\n    pred$p.value <- with(pred, 2 * pnorm(-abs(estimate / std.error)))\n  } else {\n    pred <- data.frame(estimate = NA, std.error = NA, conf.low = NA, conf.high = NA, p.value = NA, error = TRUE)\n  }\n  pred\n}\n\nrr_mirrored <- function(data) {\n  fit  <- try(rrreg(Y_mirrored ~ 1, data = data, p = 2/3, design = \"mirrored\"))\n  pred <- try(as.data.frame(predict(fit, avg = TRUE, quasi.bayes = TRUE)))\n  if(class(fit) != \"try-error\" & class(pred) != \"try-error\") {\n    names(pred) <- c(\"estimate\", \"std.error\", \"conf.low\", \"conf.high\")\n    pred$p.value <- with(pred, 2 * pnorm(-abs(estimate / std.error)))\n  } else {\n    pred <- data.frame(estimate = NA, std.error = NA, conf.low = NA, conf.high = NA, p.value = NA, error = TRUE)\n  }\n  pred\n}\n\nproportion_shy <- .06\n\nrr_design <-\n  declare_population(\n    N = 100, \n    \n    # true trump vote (unobservable)\n    truthful_trump_vote = draw_binary(.45, N),\n    \n    # shy voter (unobservable)\n    shy = draw_binary(proportion_shy, N),\n    \n    # Direct question response (1 if Trump supporter and not shy, 0 otherwise)\n    Y_direct = as.numeric(truthful_trump_vote == 1 & shy == 0)) +\n  \n  declare_estimand(sensitive_item_proportion = mean(truthful_trump_vote)) +\n  \n  declare_potential_outcomes(Y_forced_known ~ (dice == 1) * 0 + (dice %in% 2:5) * truthful_trump_vote + (dice == 6) * 1, conditions = 1:6, assignment_variable = \"dice\") +\n  declare_potential_outcomes(Y_mirrored ~ (coin == \"heads\") * truthful_trump_vote + (coin == \"tails\") * (1 - truthful_trump_vote), conditions = c(\"heads\", \"tails\"), assignment_variable = \"coin\") +\n  \n  declare_assignment(prob_each = rep(1/6, 6), conditions = 1:6, assignment_variable = \"dice\") +\n  declare_assignment(prob_each = c(2/3, 1/3), conditions = c(\"heads\", \"tails\"), assignment_variable = \"coin\") +\n  \n  declare_reveal(Y_forced_known, dice) +\n  declare_reveal(Y_mirrored, coin) +\n  \n  declare_estimator(handler = tidy_estimator(rr_forced_known), label = \"forced_known\", estimand = \"sensitive_item_proportion\") +\n  declare_estimator(handler = tidy_estimator(rr_mirrored), label = \"mirrored\", estimand = \"sensitive_item_proportion\") +\n  declare_estimator(Y_direct ~ 1, model = lm_robust, term = \"(Intercept)\", label = \"direct\", estimand = \"sensitive_item_proportion\")\n\nrr_design <- set_diagnosands(rr_design, diagnosands = declare_diagnosands(select = c(mean_estimate, bias, rmse, power)))\nrr_diagnosis <- diagnose_design(rr_design, sims = sims, bootstrap_sims = b_sims)\nkable(reshape_diagnosis(rr_diagnosis))"},{"path":"experimental-designs-for-descriptive-inference.html","id":"bias-variance-tradeoff","chapter":"15 Experimental designs for descriptive inference","heading":"15.2.5.1 Bias-variance tradeoff","text":"","code":"\nrr_designs <- redesign(rr_design, proportion_shy = seq(from = 0, to = 0.5, by = 0.05), N = seq(from = 500, to = 5000, by = 500))\nrr_tradeoff_diagnosis <- diagnose_design(rr_designs, sims = sims, bootstrap_sims = b_sims)"},{"path":"experimental-designs-for-descriptive-inference.html","id":"references","chapter":"15 Experimental designs for descriptive inference","heading":"15.2.6 References","text":"","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"conjoint-experiments","chapter":"15 Experimental designs for descriptive inference","heading":"15.3 Conjoint experiments","text":"Conjoint survey experiments become hugely popular political science beyond studying multidimensional choice. popular “forced-choice” design variant, subjects presented choice task: pair profiles (candidates, immigrants, policies) asked make binary choice .’re designing conjoint, make (least) three choices:number attributesThe number levels within attributeThe number choice tasks ask subjects rate.right number attributes governed “masking/satisficing” tradeoff. don’t include important attribute (like partisanship candidate choice experiment), ’re worried subjects partially infer partisanship attributes (like race gender). , partisanship “masked”, estimates effects race gender biased “omitted variable.” add many attributes order avoid masking, may induce “satisficing” among subjects, whereby take little bit information, enough make “good enough” choice among candidates.right number levels governed sample size. attribute three levels, ’s like ’re conducting three-arm trial, ’ll want enough subjects arm. levels, lower power.right number choice tasks depends survey budget. can always add pairs profiles cost opportunity cost asking different question survey may serve higher scientific purpose. ’re worried respondents get bored task, can always throw profile pairs come later survey. (???) suggest can ask many pairs without much loss data quality.","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"declaration-8","chapter":"15 Experimental designs for descriptive inference","heading":"15.3.1 Declaration","text":"","code":"\n# applies the function to each pair\nY_function <- function(data) {\n  data %>%\n    group_by(pair) %>%\n    mutate(Y = if_else(E == max(E), 1, 0)) %>%\n    ungroup\n}\ndesign <- \n  declare_population(\n    subject = add_level(N = 500),\n    pair = add_level(N = 4),\n    candidate = add_level(N = 2, U = runif(N))\n  ) +\n  declare_assignment(assignment_variable = \"A1\") +\n  declare_assignment(assignment_variable = \"A2\", \n                     conditions = c(\"young\", \"middle\", \"old\")) +\n  declare_assignment(assignment_variable = \"A3\")  +\n  declare_step(\n    E = \n      0.05 * A1 + \n      0.04 * (A2 == \"middle\") + \n      0.08 * (A2 == \"old\") + \n      0.02 * A3 + U,\n    handler = fabricate) +\n  declare_measurement(handler = Y_function) +\n  declare_estimator(Y ~ A1 + A2 + A3,\n                    model = lm_robust, term = TRUE)"},{"path":"experimental-designs-for-descriptive-inference.html","id":"dag-6","chapter":"15 Experimental designs for descriptive inference","heading":"15.3.2 Dag","text":"","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"example-9","chapter":"15 Experimental designs for descriptive inference","heading":"15.3.3 Example","text":"","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"behavioral-games","chapter":"15 Experimental designs for descriptive inference","heading":"15.4 Behavioral games","text":"","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"declaration-9","chapter":"15 Experimental designs for descriptive inference","heading":"15.4.1 Declaration","text":"","code":"\ndesign <-\n  declare_population(\n    games = add_level(N = 100),\n    players = add_level(\n      N = 2,\n      prosociality = runif(N),\n      fairness = prosociality,\n      cutoff = pmax(prosociality - 0.25, 0)\n    )\n  ) +\n  declare_estimand(mean_fairness = mean(fairness),\n                   mean_cutoff = mean(cutoff)) +\n  declare_assignment(blocks = games, conditions = c(\"proposer\", \"responder\"), assignment_variable = \"role\") + \n  declare_step(id_cols = games, names_from = role, values_from = c(prosociality, fairness, cutoff), handler = pivot_wider) + \n  declare_measurement(proposal = fairness_proposer * 0.5, \n                      response = if_else(proposal >= cutoff_responder, 1, 0)) + \n  declare_estimator(proposal ~ 1,\n                    model = lm_robust,\n                    estimand = \"mean_fairness\",\n                    label = \"mean_fairness\") +\n  declare_estimator(response ~ 1,\n                    model = lm_robust,\n                    estimand = \"mean_cutoff\",\n                    label = \"mean_cutoff\")"},{"path":"experimental-designs-for-descriptive-inference.html","id":"dag-7","chapter":"15 Experimental designs for descriptive inference","heading":"15.4.2 Dag","text":"","code":""},{"path":"experimental-designs-for-descriptive-inference.html","id":"example-10","chapter":"15 Experimental designs for descriptive inference","heading":"15.4.3 Example","text":"","code":""},{"path":"observational-designs-for-causal-inference.html","id":"observational-designs-for-causal-inference","chapter":"16 Observational designs for causal inference","heading":"16 Observational designs for causal inference","text":"section introductioncite Dunning (2012), Angrist Pischke (2008)","code":""},{"path":"observational-designs-for-causal-inference.html","id":"selection-on-observables","chapter":"16 Observational designs for causal inference","heading":"16.1 Selection on observables","text":"","code":""},{"path":"observational-designs-for-causal-inference.html","id":"declaration-10","chapter":"16 Observational designs for causal inference","heading":"16.1.1 Declaration","text":"","code":"\ndesign <-\n  declare_population(N = 100, \n                     X_1 = rnorm(N),\n                     X_2 = rnorm(N),\n                     Z = if_else(X_1 + X_2 > 0, 1, 0),\n                     U = rnorm(N)) +\n  declare_potential_outcomes(Y ~ Z + X_1 + X_2 + U) +\n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_reveal() +\n  declare_estimator(Y ~ X_1 + X_2, model = lm_robust, estimand = \"LATE\") "},{"path":"observational-designs-for-causal-inference.html","id":"dag-8","chapter":"16 Observational designs for causal inference","heading":"16.1.2 Dag","text":"","code":""},{"path":"observational-designs-for-causal-inference.html","id":"example-11","chapter":"16 Observational designs for causal inference","heading":"16.1.3 Example","text":"(matching regression etc.)\n","code":""},{"path":"observational-designs-for-causal-inference.html","id":"classic-confounding","chapter":"16 Observational designs for causal inference","heading":"16.1.4 Classic Confounding","text":"want know effect Z Y, ’s confounded XDIM biased, OLS unbiased happen get functional forms right enough.\nFigure 16.1: DAG one observed confounder\n","code":"\ndesign_1 <-\n  declare_population(N = 100, \n                     U_z = rnorm(N),\n                     U_x = rnorm(N),\n                     U_y = rnorm(N),\n                     X = U_x) +\n  declare_potential_outcomes(Y ~ 0.5*Z + X + U_y) +\n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_assignment(prob_unit = pnorm(U_z + U_x), simple = TRUE) +\n  declare_estimator(Y ~ Z, estimand = \"ATE\", label = \"DIM\") +\n  declare_estimator(Y ~ Z + X, model = lm, estimand = \"ATE\", label = \"OLS\")\ndx_1 <- diagnose_design(design_1, sims = sims, bootstrap_sims = b_sims)\ndx_1## \n## Research design diagnosis based on 100 simulations. Diagnosand estimates with bootstrapped standard errors in parentheses (20 replicates).\n## \n##  Design Label Estimand Label Estimator Label Term N Sims   Bias   RMSE  Power\n##      design_1            ATE             DIM    Z    100   0.94   0.96   1.00\n##                                                          (0.02) (0.02) (0.00)\n##      design_1            ATE             OLS    Z    100   0.01   0.23   0.55\n##                                                          (0.02) (0.01) (0.04)\n##  Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand\n##      0.06          1.44        0.24    0.27        0.00          0.50\n##    (0.02)        (0.02)      (0.02)  (0.00)      (0.00)        (0.00)\n##      0.95          0.51        0.23    0.23        0.00          0.50\n##    (0.02)        (0.02)      (0.01)  (0.00)      (0.00)        (0.00)"},{"path":"observational-designs-for-causal-inference.html","id":"what-if-the-functional-form-is-wrong","chapter":"16 Observational designs for causal inference","heading":"16.1.5 What if the functional form is wrong?","text":"Oh , functional form wrong, even though ’re controlling confounders, ’s still bias.Solution: matching might better job since ’s sort “nonparametric” form covariate control.","code":"\ndesign_2 <-\n  declare_population(N = 100, \n                     U_z = rnorm(N),\n                     U_x = rnorm(N),\n                     U_y = rnorm(N),\n                     X = U_x) +\n  declare_potential_outcomes(Y ~ 0.5*Z + X + X^2 + U_y) +\n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_assignment(prob_unit = pnorm(U_z + U_x + U_x^2), simple = TRUE) +\n  declare_estimator(Y ~ Z, estimand = \"ATE\", label = \"DIM\") +\n  declare_estimator(Y ~ Z + X, model = lm, estimand = \"ATE\", label = \"OLS\")\ndx_2 <- diagnose_design(design_2, sims = sims, bootstrap_sims = b_sims)\ndx_2## \n## Research design diagnosis based on 100 simulations. Diagnosand estimates with bootstrapped standard errors in parentheses (20 replicates).\n## \n##  Design Label Estimand Label Estimator Label Term N Sims   Bias   RMSE  Power\n##      design_2            ATE             DIM    Z    100   1.32   1.36   1.00\n##                                                          (0.03) (0.03) (0.00)\n##      design_2            ATE             OLS    Z    100   0.87   0.93   0.95\n##                                                          (0.03) (0.03) (0.02)\n##  Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand\n##      0.01          1.82        0.34    0.33        0.00          0.50\n##    (0.01)        (0.03)      (0.02)  (0.00)      (0.00)        (0.00)\n##      0.24          1.37        0.33    0.35        0.00          0.50\n##    (0.04)        (0.03)      (0.02)  (0.00)      (0.00)        (0.00)"},{"path":"observational-designs-for-causal-inference.html","id":"what-if-you-have-unobserved-confounding","chapter":"16 Observational designs for causal inference","heading":"16.1.6 What if you have unobserved confounding?","text":"\nFigure 16.2: DAG unobserved confounding\n","code":"\ndesign_3 <-\n  declare_population(N = 100, \n                     U_z = rnorm(N),\n                     U_x = rnorm(N),\n                     U_y = correlate(rnorm, given = U_z, rho = 0.9),\n                     X = U_x) +\n  declare_potential_outcomes(Y ~ 0.5*Z + X + U_y) +\n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_assignment(prob_unit = pnorm(U_z + U_x), simple = TRUE) +\n  declare_estimator(Y ~ Z, estimand = \"ATE\", label = \"DIM\") +\n  declare_estimator(Y ~ Z + X, model = lm, estimand = \"ATE\", label = \"OLS\")\ndx_3 <- diagnose_design(design_3, sims = sims, bootstrap_sims = b_sims)\ndx_3## \n## Research design diagnosis based on 100 simulations. Diagnosand estimates with bootstrapped standard errors in parentheses (20 replicates).\n## \n##  Design Label Estimand Label Estimator Label Term N Sims   Bias   RMSE  Power\n##      design_3            ATE             DIM    Z    100   1.72   1.74   1.00\n##                                                          (0.02) (0.02) (0.00)\n##      design_3            ATE             OLS    Z    100   1.02   1.04   1.00\n##                                                          (0.02) (0.02) (0.00)\n##  Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand\n##      0.00          2.22        0.21    0.22        0.00          0.50\n##    (0.00)        (0.02)      (0.01)  (0.00)      (0.00)        (0.00)\n##      0.00          1.52        0.17    0.19        0.00          0.50\n##    (0.00)        (0.02)      (0.01)  (0.00)      (0.00)        (0.00)"},{"path":"observational-designs-for-causal-inference.html","id":"what-if-the-observed-covariate-is-post-treatment","chapter":"16 Observational designs for causal inference","heading":"16.1.7 What if the observed covariate is post-treatment?","text":"\nFigure 16.3: DAG one observed mediator\n","code":"\ndesign_4 <-\n  declare_population(N = 100, \n                     U_z = rnorm(N),\n                     U_m = rnorm(N),\n                     U_y = rnorm(N)) +\n  declare_potential_outcomes(M ~ 0.5*Z + U_m) +\n  declare_potential_outcomes(Y ~ 0.5*Z + (0.5*Z + U_m) + U_y) +\n  declare_assignment(prob_unit = pnorm(U_z), simple = TRUE) +\n  declare_reveal(c(M, Y), Z) +\n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_estimator(Y ~ Z, estimand = \"ATE\", label = \"DIM\") +\n  declare_estimator(Y ~ Z + M, model = lm, estimand = \"ATE\", label = \"OLS\")\ndx_4 <- diagnose_design(design_4, sims = sims, bootstrap_sims = b_sims)\ndx_4## \n## Research design diagnosis based on 100 simulations. Diagnosand estimates with bootstrapped standard errors in parentheses (20 replicates).\n## \n##  Design Label Estimand Label Estimator Label Term N Sims   Bias   RMSE  Power\n##      design_4            ATE             DIM    Z    100  -0.01   0.26   0.93\n##                                                          (0.02) (0.02) (0.03)\n##      design_4            ATE             OLS    Z    100  -0.49   0.52   0.64\n##                                                          (0.02) (0.02) (0.05)\n##  Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand\n##      0.96          0.99        0.26    0.28        0.00          1.00\n##    (0.02)        (0.02)      (0.02)  (0.00)      (0.00)        (0.00)\n##      0.32          0.51        0.18    0.21        0.00          1.00\n##    (0.05)        (0.02)      (0.01)  (0.00)      (0.00)        (0.00)"},{"path":"observational-designs-for-causal-inference.html","id":"further-reading-6","chapter":"16 Observational designs for causal inference","heading":"16.2 Further reading","text":"Rosenbaum (2002) matching","code":""},{"path":"observational-designs-for-causal-inference.html","id":"p3iv","chapter":"16 Observational designs for causal inference","heading":"16.3 Instrumental variables","text":"many observational settings, researchers ","code":""},{"path":"observational-designs-for-causal-inference.html","id":"declaration-11","chapter":"16 Observational designs for causal inference","heading":"16.3.1 Declaration","text":"explain IV estimator ratio \\(\\widehat{ITT_y} / \\widehat{ITT_d}\\)identical noncompliance experiment","code":"\ndesign <-\n  declare_population(N = 100, U = rnorm(N)) +\n  declare_potential_outcomes(D ~ if_else(Z + U > 0, 1, 0), assignment_variables = Z) + \n  declare_potential_outcomes(Y ~ 0.1 * D + 0.25 + U, assignment_variables = D) +\n  declare_estimand(LATE = mean(Y_D_1[D_Z_1 == 1 & D_Z_0 == 0] - Y_D_0[D_Z_1 == 1 & D_Z_0 == 0])) +\n  declare_assignment(prob = 0.5) +\n  declare_reveal(D, Z) + \n  declare_reveal(Y, D) + \n  declare_estimator(Y ~ D | Z, model = iv_robust, estimand = \"LATE\") "},{"path":"observational-designs-for-causal-inference.html","id":"dag-9","chapter":"16 Observational designs for causal inference","heading":"16.3.2 Dag","text":"","code":""},{"path":"observational-designs-for-causal-inference.html","id":"redesign-2","chapter":"16 Observational designs for causal inference","heading":"16.3.3 Redesign","text":"excludability violationsif , go \\(ITT_d\\) \\(ITT_y\\)","code":""},{"path":"observational-designs-for-causal-inference.html","id":"suggested-readings","chapter":"16 Observational designs for causal inference","heading":"16.3.4 Suggested readings","text":"Applications\n- Mo, Cecilia Hyunjung, Katherine Conn, Georgia Anderson-Nilsson. 2019. “Can National Service Activism Activate Women’s Political Ambition? Evidence Teach America.” Politics, Groups, Identities 7(4): 864-877.Methodological literature\n- Angrist Pischke (2008) ch. 4 instrumental variables estimation\n- Gerber Green (2012) ch. 5 ch. 6 connection IV noncompliance experiment\n\n- Deaton (2010) critique LATE estimand Imbens (2010) rejoinder.","code":""},{"path":"observational-designs-for-causal-inference.html","id":"example-12","chapter":"16 Observational designs for causal inference","heading":"16.3.5 Example","text":"","code":""},{"path":"observational-designs-for-causal-inference.html","id":"difference-in-differences","chapter":"16 Observational designs for causal inference","heading":"16.4 Difference-in-differences","text":"","code":""},{"path":"observational-designs-for-causal-inference.html","id":"declaration-12","chapter":"16 Observational designs for causal inference","heading":"16.4.1 Declaration","text":"","code":"\ndesign <- \n  declare_population(\n    unit = add_level(N = 2, X = rnorm(N, sd = 0.5)),\n    period = add_level(N = 2, nest = FALSE),\n    unit_period = cross_levels(by = join(unit, period), U = rnorm(N, sd = 0.01))\n  ) + \n  declare_potential_outcomes(Y ~ X + 0.5 * as.numeric(period) + Z + U) + \n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0), subset = period == 2) + \n  declare_step(Z = X == max(X), handler = mutate) + \n  declare_reveal(Y = if_else(Z == 0 | period == 1, Y_Z_0, Y_Z_1), handler = mutate) +\n  declare_estimator(Y ~ Z * period, model = lm_robust, se_type = \"none\")"},{"path":"observational-designs-for-causal-inference.html","id":"dag-10","chapter":"16 Observational designs for causal inference","heading":"16.4.2 Dag","text":"","code":""},{"path":"observational-designs-for-causal-inference.html","id":"example-13","chapter":"16 Observational designs for causal inference","heading":"16.4.3 Example","text":"","code":""},{"path":"observational-designs-for-causal-inference.html","id":"two-period-two-group-setting","chapter":"16 Observational designs for causal inference","heading":"16.4.4 Two-period two-group setting","text":"Show comparison T C period 2 biased comparison T period 1 2 biased, unbiased presence confounding treatment assignment (unit higher unit shock always treated) time trends","code":"\nN_units <- 2\nN_time_periods <- 2\n\ntwo_period_two_group_design <- \n  \n  declare_population(\n    units = add_level(N = N_units, unit_shock = rnorm(N, sd = 0.5)),\n    periods = add_level(N = N_time_periods, nest = FALSE,\n                        time = (1:N_time_periods) - N_time_periods + 1),\n    unit_period = cross_levels(by = join(units, periods), unit_time_shock = rnorm(N, sd = 0.01))\n  ) + \n  \n  # internal note: the unbiasedness obtains whether or not there is a unit-time shock\n  declare_potential_outcomes(\n    Y_Z_0 = unit_shock + 0.5 * time + unit_time_shock, # common pretreatment trend\n    Y_Z_1 = Y_Z_0 + 1) +\n  \n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0), subset = time == 1) + \n  \n  declare_assignment(Z = unit_shock == max(unit_shock), handler = mutate) + \n  \n  declare_reveal(\n    Y = case_when(Z == 0 | time < 1 ~ Y_Z_0, TRUE ~ Y_Z_1), handler = mutate) +\n  \n  declare_estimator(estimate = (mean(Y[Z == 1 & time == 1]) - mean(Y[Z == 0 & time == 1])) -\n                      (mean(Y[Z == 1 & time == 0]) - mean(Y[Z == 0 & time == 0])),\n                    estimator_label = \"DiD\", handler = summarize, label = \"DiD\") +\n  \n  declare_estimator(estimate = mean(Y[Z == 1 & time == 1]) - mean(Y[Z == 1 & time == 0]),\n                    estimator_label = \"Diff\", handler = summarize, label = \"Over-Time\") +\n  \n  declare_estimator(estimate = mean(Y[Z == 1 & time == 1]) - mean(Y[Z == 0 & time == 1]),\n                    estimator_label = \"DiM\", handler = summarize, label = \"DiM\")\ndiagnosis_two_period_two_group <- diagnose_design(\n  two_period_two_group_design, diagnosands = declare_diagnosands(select = bias),\n  sims = sims, bootstrap_sims = b_sims)\nkable(get_diagnosands(diagnosis_two_period_two_group))"},{"path":"observational-designs-for-causal-inference.html","id":"parallel-trends-assumption","chapter":"16 Observational designs for causal inference","heading":"16.4.5 Parallel trends assumption","text":"Introduce assumption visual testFormal test (T = -1 T = 0 periods, .e. year backward )result shows two-step procedure parallel trends assumption test passes shows poor coverage SEs final (https://arxiv.org/abs/1804.01208). Cite .","code":"\n# add an additional pretreatment time period in order to visually test for parallel pre-trends\nthree_period_two_group_design <- redesign(two_period_two_group_design, N_time_periods = 3)\ndraw_data(three_period_two_group_design) %>% \n  group_by(Z, time) %>% \n  summarize(Y = mean(Y)) %>% \n  mutate(Z_color = factor(Z, levels = c(FALSE, TRUE), labels = c(\"Untreated\", \"Treated\"))) %>% \n  ggplot(aes(time, Y, color = Z_color)) + \n  geom_line() + \n  scale_color_discrete(\"\") +\n  scale_x_discrete(\"Time\", limits = c(-1, 0, 1))"},{"path":"observational-designs-for-causal-inference.html","id":"multi-period-design","chapter":"16 Observational designs for causal inference","heading":"16.4.6 Multi-period design","text":"Switch regression context 20 periods, 100 units show results hold two-way FE (controlling one period T insufficient remove bias)Show case units switch back forth T C panel bias (point Imai Kim appear weighted FE estimator fix )","code":"\nN_units <- 20\nN_time_periods <- 20\n\nmulti_period_design <- \n  \n  declare_population(\n    units = add_level(N = N_units, \n                      unit_shock = rnorm(N), \n                      unit_treated = 1*(unit_shock > median(unit_shock)), \n                      unit_treatment_start = \n                        sample(2:(N_time_periods - 1) - N_time_periods + 1, N, replace = TRUE)),\n    periods = add_level(N = N_time_periods, nest = FALSE, \n                        time = (1:N_time_periods) - N_time_periods + 1),\n    unit_period = cross_levels(by = join(units, periods),\n                               noise = rnorm(N), \n                               pretreatment = 1*(time < unit_treatment_start))\n  ) + \n  \n  declare_potential_outcomes(\n    Y_Z_0 = unit_shock + 0.5 * time + noise, # common pretreatment trend\n    Y_Z_1 = Y_Z_0 + 0.2) +\n  \n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0), subset = time == 1) + \n  \n  declare_assignment(Z = 1*(unit_treated & pretreatment == FALSE), handler = fabricate) + \n  declare_reveal(Y, Z) + \n  \n  declare_estimator(Y ~ Z + time, fixed_effects = ~ units + periods, \n                    model = lm_robust, label = \"twoway-fe\", estimand = \"ATE\") \ndiagnosis_multi_period_multi_group <- diagnose_design(multi_period_design, diagnosands = declare_diagnosands(select = bias), sims = sims, bootstrap_sims = b_sims)\nkable(get_diagnosands(diagnosis_multi_period_multi_group))"},{"path":"observational-designs-for-causal-inference.html","id":"RDD","chapter":"16 Observational designs for causal inference","heading":"16.5 Regression Discontinuity","text":"","code":""},{"path":"observational-designs-for-causal-inference.html","id":"declaration-13","chapter":"16 Observational designs for causal inference","heading":"16.5.1 Declaration","text":"","code":"\ncutoff <- 0.5\ncontrol <- function(X) {\n  as.vector(poly(X, 4, raw = TRUE) %*% c(.7, -.8, .5, 1))}\ntreatment <- function(X) {\n  as.vector(poly(X, 4, raw = TRUE) %*% c(0, -1.5, .5, .8)) + .15}\n\ndesign <-\n  declare_population(\n    N = 1000,\n    U = rnorm(N, 0, 0.1),\n    X = runif(N, 0, 1) + U - cutoff,\n    Z = 1 * (X > 0)\n  ) +\n  declare_potential_outcomes(Y ~ Z * treatment(X) + (1 - Z) * control(X) + U) +\n  declare_estimand(LATE = treatment(0) - control(0)) +\n  declare_reveal(Y, Z) +\n  declare_estimator(Y ~ poly(X, 4) * Z, model = lm_robust, estimand = \"LATE\")"},{"path":"observational-designs-for-causal-inference.html","id":"dag-11","chapter":"16 Observational designs for causal inference","heading":"16.5.2 Dag","text":"","code":""},{"path":"observational-designs-for-causal-inference.html","id":"example-14","chapter":"16 Observational designs for causal inference","heading":"16.5.3 Example","text":"Regression discontinuity designs exploit substantive knowledge treatment assigned particular way: everyone threshold assigned treatment everyone . Even though researchers control assignment, substantive knowledge threshold serves basis strong identification claim.Thistlewhite Campbell introduced regression discontinuity design 1960s study impact scholarships academic success. insight students test score just scholarship cutoff plausibly comparable students whose scores just cutoff, differences future academic success attributed scholarship .Regression discontinuity designs identify local average treatment effect: average effect treatment exactly cutoff. main trouble design vanishingly little data exactly cutoff, answer strategy needs use data distance away cutoff. away cutoff move, larger threat bias.’ll consider application regression discontinuity design examines party incumbency advantage – effect party winning election vote margin next election.","code":""},{"path":"observational-designs-for-causal-inference.html","id":"design-declaration-3","chapter":"16 Observational designs for causal inference","heading":"16.5.4 Design Declaration","text":"Model: Regression discontinuity designs four components: running variable, cutoff, treatment variable, outcome. cutoff determines units treated depending value running variable.\nexample, running variable \\(X\\) Democratic party’s margin victory time \\(t-1\\); treatment, \\(Z\\), whether Democratic party won election time \\(t-1\\). outcome, \\(Y\\), Democratic vote margin time \\(t\\). ’ll consider population 1,000 pairs elections.\nmajor assumption required regression discontinuity conditional expectation functions treatment control potential outcomes continuous cutoff.24 satisfy assumption, specify two smooth conditional expectation functions, one potential outcome. figure plots \\(Y\\) (Democratic vote margin time \\(t\\)) \\(X\\) (margin time \\(t-1\\)). ’ve also plotted true conditional expectation functions treated control potential outcomes. solid lines correspond observed data dashed lines correspond unobserved data.Model: Regression discontinuity designs four components: running variable, cutoff, treatment variable, outcome. cutoff determines units treated depending value running variable.example, running variable \\(X\\) Democratic party’s margin victory time \\(t-1\\); treatment, \\(Z\\), whether Democratic party won election time \\(t-1\\). outcome, \\(Y\\), Democratic vote margin time \\(t\\). ’ll consider population 1,000 pairs elections.major assumption required regression discontinuity conditional expectation functions treatment control potential outcomes continuous cutoff.24 satisfy assumption, specify two smooth conditional expectation functions, one potential outcome. figure plots \\(Y\\) (Democratic vote margin time \\(t\\)) \\(X\\) (margin time \\(t-1\\)). ’ve also plotted true conditional expectation functions treated control potential outcomes. solid lines correspond observed data dashed lines correspond unobserved data.Inquiry: estimand effect Democratic win election Democratic vote margin next election, Democratic vote margin first election zero. Formally, difference conditional expectation functions control treatment potential outcomes running variable exactly zero. black vertical line plot shows difference.Inquiry: estimand effect Democratic win election Democratic vote margin next election, Democratic vote margin first election zero. Formally, difference conditional expectation functions control treatment potential outcomes running variable exactly zero. black vertical line plot shows difference.Data strategy: collect data Democratic vote share time \\(t-1\\) time \\(t\\) 1,000 pairs elections. sampling random assignment.Data strategy: collect data Democratic vote share time \\(t-1\\) time \\(t\\) 1,000 pairs elections. sampling random assignment.Answer strategy: approximate treated untreated conditional expectation functions left right cutoff using flexible regression specification estimated via OLS. particular, fit regression using fourth-order polynomial. Much literature regression discontinuity designs focuses tradeoffs among answer strategies, many analysts recommending higher-order polynomial regression specifications. use one highlight well answer strategy matches functional form model. discuss alternative estimators exercises.Answer strategy: approximate treated untreated conditional expectation functions left right cutoff using flexible regression specification estimated via OLS. particular, fit regression using fourth-order polynomial. Much literature regression discontinuity designs focuses tradeoffs among answer strategies, many analysts recommending higher-order polynomial regression specifications. use one highlight well answer strategy matches functional form model. discuss alternative estimators exercises.Now simulating, saving, loading done, can use simulations whatever want.","code":"\ncutoff <- .5\ncontrol <- function(X) {\n  as.vector(poly(X, 4, raw = TRUE) %*% c(.7, -.8, .5, 1))}\ntreatment <- function(X) {\n  as.vector(poly(X, 4, raw = TRUE) %*% c(0, -1.5, .5, .8)) + .15}\n\nrd_design <-\n  # Model -------------------------------------------------------------------\ndeclare_population(\n  N = 1000,\n  X = runif(N, 0, 1) - cutoff,\n  noise = rnorm(N, 0, .1),\n  Z = 1 * (X > 0)\n) +\n  declare_potential_outcomes(Y ~ Z * treatment(X) + (1 - Z) * control(X) + noise) +\n  \n  # Inquiry -----------------------------------------------------------------\ndeclare_estimand(LATE = treatment(0) - control(0)) +\n  \n  # Data Strategy -----------------------------------------------------------------\ndeclare_reveal(Y, Z) +\n  \n  # Answer Strategy ---------------------------------------------------------\ndeclare_estimator(formula = Y ~ poly(X, 4) * Z,\n                  model = lm_robust,\n                  estimand = \"LATE\")\nrd_diagnosis <- diagnose_design(rd_design, sims = sims, bootstrap_sims = b_sims)\nsummary(rd_diagnosis)## \n## Research design diagnosis based on 100 simulations. Diagnosand estimates with bootstrapped standard errors in parentheses (20 replicates).\n## \n##  Design Label Estimand Label Estimator Label        Term N Sims   Bias   RMSE\n##     rd_design           LATE       estimator poly(X, 4)1    100   2.08  24.82\n##                                                                 (2.36) (1.50)\n##   Power Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand\n##    0.06     0.95          2.23       24.85   26.52        0.33          0.15\n##  (0.02)   (0.02)        (2.36)      (1.54)  (0.28)      (0.21)        (0.00)"},{"path":"observational-designs-for-causal-inference.html","id":"takeaways-3","chapter":"16 Observational designs for causal inference","heading":"16.5.5 Takeaways","text":"highlight three takeaways. First, power design low: 1,000 units achieve even 10% statistical power. However, estimates uncertainty wide: coverage probability indicates confidence intervals indeed contain estimand 95% time . answer strategy highly uncertain fourth-order polynomial specification regression model gives weights data greatly increase variance estimator (Gelman Imbens (2017)). exercises explore alternative answer strategies perform better.Second, design biased polynomial approximations average effect exactly point threshold inaccurate small samples (Sekhon Titiunik (2017)), especially units farther away cutoff incorporated answer strategy. know estimated bias due simulation error examining bootstrapped standard error bias estimates.Finally, figure, can see poorly average effect threshold approximates average effect units. average treatment effect among treated (right threshold figure) negative, whereas threshold positive. clarifies estimand regression discontinuity design, difference cutoff, relevant small – possibly empty – set units close cutoff.","code":""},{"path":"observational-designs-for-causal-inference.html","id":"further-reading-7","chapter":"16 Observational designs for causal inference","heading":"16.5.6 Further Reading","text":"Since rediscovery social scientists late 1990s, regression discontinuity design widely used study diverse causal effects : prison recidivism (Mitchell et al. (2017)); China’s one child policy human capital (Qin, Zhuang, Yang (2017)); eligibility World Bank loans political liberalization (Carnegie Samii (2017)); anti-discrimination laws minority employment (Hahn, Todd, Van der Klaauw (1999)).’ve discussed “sharp” regression discontinuity design units threshold treated units untreated. fuzzy regression discontinuity designs, units cutoff remain untreated units take treatment. setting analogous experiments experience noncompliance may require instrumental variables approaches answer strategy (see Compliance Potential Outcome).Geographic regression discontinuity designs use distance border running variable: units one side border treated units untreated. Keele Titiunik (2016) use design study whether voters likely turn opportunity vote directly legislation -called ballot initiatives. complication design measure distance border two dimensions.Sekhon Titiunik (2016)De la Cuesta Imai (2016)","code":""},{"path":"observational-designs-for-causal-inference.html","id":"exercises-3","chapter":"16 Observational designs for causal inference","heading":"16.5.7 Exercises","text":"Gelman Imbens (2017) point higher order polynomial regression specifications lead extreme regression weights. One approach obtaining better estimates select bandwidth, \\(h\\), around cutoff, run linear regression. Declare sampling procedure subsets data bandwidth around threshold, well first order linear regression specification, analyze power, bias, RMSE, coverage design vary function bandwidth.Gelman Imbens (2017) point higher order polynomial regression specifications lead extreme regression weights. One approach obtaining better estimates select bandwidth, \\(h\\), around cutoff, run linear regression. Declare sampling procedure subsets data bandwidth around threshold, well first order linear regression specification, analyze power, bias, RMSE, coverage design vary function bandwidth.rdrobust estimator rdrobust package implements local polynomial estimator automatically selects bandwidth RD analysis bias-corrected confidence intervals. Declare another estimator using rdrobust function add design. coverage bias estimator compare regression approaches declared ?rdrobust estimator rdrobust package implements local polynomial estimator automatically selects bandwidth RD analysis bias-corrected confidence intervals. Declare another estimator using rdrobust function add design. coverage bias estimator compare regression approaches declared ?Reduce number polynomial terms treatment() control() functions assess bias design changes potential outcomes become increasingly linear function running variable.Reduce number polynomial terms treatment() control() functions assess bias design changes potential outcomes become increasingly linear function running variable.Redefine population function units higher potential outcome likely locate just cutoff . Assess whether affects bias design.Redefine population function units higher potential outcome likely locate just cutoff . Assess whether affects bias design.","code":""},{"path":"observational-designs-for-causal-inference.html","id":"bayesianprocesstracing","chapter":"16 Observational designs for causal inference","heading":"16.6 Bayesian process-tracing","text":"Process-tracing qualitative method uses evidence -depth interviews written records test causal theories. Process-tracing designs often focus “causes--effects” inquiries (e.g., presence strong middle class cause revolution?), rather “effects--causes” inquiries (e.g., average effect strong middle class probability revolution happening?) Goertz Mahoney (2012).Causes--effects inquiries imply hypothesis – “strong middle class caused revolution,” say. One widely-promoted answer strategy suggests evaluating whether hypotheses correct given presence absence different “clues” found archives interviews [Collier, Brady, Seawright (2004); Mahoney (2012); Bennett Checkel (2015); fairfield2013going]. Van Evera (1997) categorizes different kinds clues according whether one believe hypothesis one observed clue (necessity) whether observing clue suffice infer hypothesis correct (sufficiency).25Of course, rare certainty—typically attach varying degrees belief statements truth. Bayesian process-tracing uses probability theory form posterior belief hypothesis, given beliefs whether observe different pieces evidence right wrong~. Extending Van Evera (1997), “hoop tests”\" clues nearly certain seen hypothesis true, likely either way, “smoking-guns” unlikely seen general extremely unlikely hypothesis false, “straws---wind” likely hypothesis true still somewhat likely , ``doubly-decisive’’ clues likely seen hypothesis true unlikely false.","code":""},{"path":"observational-designs-for-causal-inference.html","id":"declaration-14","chapter":"16 Observational designs for causal inference","heading":"16.6.1 Declaration","text":"\\(M\\) Model: posit population 195 cases, exhibit presence outcome, \\(Y \\\\{0,1\\}\\). sake illustration, suppose \\(Y\\) represents presence absence civil war. case also exhibits presence absence potential cause, \\(Z \\\\{0,1\\}\\). example, might suppose \\(Z\\) represents presence absence natural resources. random, 30% cases get \\(Z=1\\). also assume researchers observe clue, \\(C\\), informative whether \\(Z\\) causal relationship \\(Y\\).\npotential outcomes \\(Y\\) depend whether cause, \\(Z\\), present case “type” causal relation case exhibits. Conceptually, exactly four distinct types causal relations. First, presence \\(Z\\) might cause \\(Y\\): \\(Z = 0\\), \\(Y = 0\\) \\(Z = 1\\) \\(Y = 1\\). words, civil wars happen cases country natural resources. Second, absence \\(Z\\) might cause \\(Y\\): \\(Z = 0\\) \\(Y = 1\\) \\(Z = 1\\) \\(Y = 0\\). cases, civil war breaks country natural resources, break country natural resources. Finally, \\(Y\\) might present irrespective \\(Z\\) \\(Y\\) might absent irrespective \\(Z\\). Continuing analogy, countries civil war peace, irrespective whether also natural resources (.e., war related causal process). specify model civil war governed causal pathway 1 (\\(Z\\) causes \\(Y\\)) roughly 20% cases, pathway 2 (\\(\\neg Z\\) causes \\(Y\\)) 10% cases, pathway 3 (\\(Y\\) irrespective \\(z\\)) 20% countries, pathway 4 (\\(\\neg Y\\) irrespective \\(Z\\)) half countries.\nalso potential outcomes clue, \\(C\\). depend case’s causal type. Specifically, clue appears .25 probability case one \\(Z\\) causes \\(Y\\), probability .005 one \\(Y\\) occurs regardless. clue appear types cases. Crucially, outcome, cause, clue observable researcher, type .\\(M\\) Model: posit population 195 cases, exhibit presence outcome, \\(Y \\\\{0,1\\}\\). sake illustration, suppose \\(Y\\) represents presence absence civil war. case also exhibits presence absence potential cause, \\(Z \\\\{0,1\\}\\). example, might suppose \\(Z\\) represents presence absence natural resources. random, 30% cases get \\(Z=1\\). also assume researchers observe clue, \\(C\\), informative whether \\(Z\\) causal relationship \\(Y\\).potential outcomes \\(Y\\) depend whether cause, \\(Z\\), present case “type” causal relation case exhibits. Conceptually, exactly four distinct types causal relations. First, presence \\(Z\\) might cause \\(Y\\): \\(Z = 0\\), \\(Y = 0\\) \\(Z = 1\\) \\(Y = 1\\). words, civil wars happen cases country natural resources. Second, absence \\(Z\\) might cause \\(Y\\): \\(Z = 0\\) \\(Y = 1\\) \\(Z = 1\\) \\(Y = 0\\). cases, civil war breaks country natural resources, break country natural resources. Finally, \\(Y\\) might present irrespective \\(Z\\) \\(Y\\) might absent irrespective \\(Z\\). Continuing analogy, countries civil war peace, irrespective whether also natural resources (.e., war related causal process). specify model civil war governed causal pathway 1 (\\(Z\\) causes \\(Y\\)) roughly 20% cases, pathway 2 (\\(\\neg Z\\) causes \\(Y\\)) 10% cases, pathway 3 (\\(Y\\) irrespective \\(z\\)) 20% countries, pathway 4 (\\(\\neg Y\\) irrespective \\(Z\\)) half countries.also potential outcomes clue, \\(C\\). depend case’s causal type. Specifically, clue appears .25 probability case one \\(Z\\) causes \\(Y\\), probability .005 one \\(Y\\) occurs regardless. clue appear types cases. Crucially, outcome, cause, clue observable researcher, type .\\(\\) Inquiry: wish know answer sample-specific “cause effects” question: given specific case sampled, probability \\(Z\\) caused \\(Y\\)? formally, want know \\(\\Pr(Y_i(Z_i=0)=0| Z_i=1, Y_i(Z_i=1)=1)\\)—, chances \\(Y\\) 0 \\(Z\\) 0 unit \\(\\) \\(Z\\) 1 \\(Y\\) 1. equivalent asking probability case type 1. inquiry thus takes value 0 1 depending type case.\\(\\) Inquiry: wish know answer sample-specific “cause effects” question: given specific case sampled, probability \\(Z\\) caused \\(Y\\)? formally, want know \\(\\Pr(Y_i(Z_i=0)=0| Z_i=1, Y_i(Z_i=1)=1)\\)—, chances \\(Y\\) 0 \\(Z\\) 0 unit \\(\\) \\(Z\\) 1 \\(Y\\) 1. equivalent asking probability case type 1. inquiry thus takes value 0 1 depending type case.\\(D\\) Data Strategy: fundamental problem researcher faces observational equivalence: different causal types can cause data patterns. issue mitigated following controversial sampling strategy: selecting \\(Y\\). selecting random one case \\(Z\\) \\(Y\\) present, researcher can narrow uncertainty two candidate types: second fourth causal types incapable producing data \\(Z = 1, Y = 1\\). natural resources cause peace (type 2), peace happened irrespective natural resources (type 4), country civil war natural resources.\\(D\\) Data Strategy: fundamental problem researcher faces observational equivalence: different causal types can cause data patterns. issue mitigated following controversial sampling strategy: selecting \\(Y\\). selecting random one case \\(Z\\) \\(Y\\) present, researcher can narrow uncertainty two candidate types: second fourth causal types incapable producing data \\(Z = 1, Y = 1\\). natural resources cause peace (type 2), peace happened irrespective natural resources (type 4), country civil war natural resources.\\(\\) Answer Strategy: researcher uses Bayes’ rule update probability \\(Z\\) caused \\(Y\\) given \\(C\\).\\(\\) Answer Strategy: researcher uses Bayes’ rule update probability \\(Z\\) caused \\(Y\\) given \\(C\\).","code":"\ntypes <- c('Z_caused_Y', 'Z_caused_not_Y', 'always_Y', 'always_not_Y')\n\ndesign <-\n  declare_population(N = 195,\n                     Z = draw_binary(prob = .3, N = N),\n                     type = sample(x = types, size = N, \n                                   replace = TRUE, prob = c(.2, .1, .2, .5)))  +\n  declare_potential_outcomes(\n    Y ~ Z * (type == \"Z_caused_Y\") + (1 - Z) * (type == \"Z_caused_not_Y\") + (type == \"always_Y\"),\n    conditions = list(Z = c(0, 1), type = types)) +\n  declare_potential_outcomes(\n    pr_C_1 ~ Z * (.25 * (type == \"Z_caused_Y\") + .005 * (type == \"always_Y\")),\n    conditions = list(Z = c(0, 1), type = types)) +\n  declare_reveal(c(Y, pr_C_1), c(Z, type)) +\n  declare_measurement(C = draw_binary(prob = pr_C_1)) +\n  declare_sampling(handler = function(data) data %>% filter(Z==1 & Y==1) %>% sample_n(size = 1)) +\n  declare_estimand(did_Z_cause_Y = type == 'Z_caused_Y') +\n  declare_estimator(\n    pr_type_Z_caused_Y = .5,\n    pr_C_1_type_Z_caused_Y = .25,\n    pr_C_1_type_always_Y = .005,\n    pr_C_type_Z_caused_Y = C * pr_C_1_type_Z_caused_Y + (1 - C) * (1 - pr_C_1_type_Z_caused_Y),\n    pr_C_type_always_Y = C * pr_C_1_type_always_Y + (1 - C) * (1 - pr_C_1_type_always_Y),\n    posterior =\n      pr_type_Z_caused_Y * pr_C_type_Z_caused_Y / (pr_type_Z_caused_Y * pr_C_type_Z_caused_Y + pr_C_type_always_Y * (1 - pr_type_Z_caused_Y)),\n    estimator_label = \"Smoking Gun\",\n    estimand_label = \"did_Z_cause_Y\",\n    handler = summarize) "},{"path":"observational-designs-for-causal-inference.html","id":"dag-12","chapter":"16 Observational designs for causal inference","heading":"16.6.2 Dag","text":"","code":""},{"path":"observational-designs-for-causal-inference.html","id":"exercises-4","chapter":"16 Observational designs for causal inference","heading":"16.6.3 Exercises","text":"Inspect DAG. removing arrow pointing type C affect inferences researcher draw?Inspect DAG. removing arrow pointing type C affect inferences researcher draw?Run draw_data(design).\nInterpret value type variable.\nLook values Z Y. Explain types produce values .\nvariable pr_C_1_Z_1_type_Z_caused_Y indicates “potential outcome clue probability, given case type Z causes Y,” pr_C_1_Z_1_type_always_Y gives corresponding potential outcome cases whose causal type one Y always happens regardless Z. potential outcomes make clue smoking gun?\nExplain potential outcome Y_Z_0_type_always_Y takes value 1 whereas potential outcome Y_Z_0_type_Z_caused_Y takes value 0.\nRun draw_data(design).Interpret value type variable.Interpret value type variable.Look values Z Y. Explain types produce values .Look values Z Y. Explain types produce values .variable pr_C_1_Z_1_type_Z_caused_Y indicates “potential outcome clue probability, given case type Z causes Y,” pr_C_1_Z_1_type_always_Y gives corresponding potential outcome cases whose causal type one Y always happens regardless Z. potential outcomes make clue smoking gun?variable pr_C_1_Z_1_type_Z_caused_Y indicates “potential outcome clue probability, given case type Z causes Y,” pr_C_1_Z_1_type_always_Y gives corresponding potential outcome cases whose causal type one Y always happens regardless Z. potential outcomes make clue smoking gun?Explain potential outcome Y_Z_0_type_always_Y takes value 1 whereas potential outcome Y_Z_0_type_Z_caused_Y takes value 0.Explain potential outcome Y_Z_0_type_always_Y takes value 1 whereas potential outcome Y_Z_0_type_Z_caused_Y takes value 0.Using code , diagnose design interpret diagnosands.Using code , diagnose design interpret diagnosands.Look estimator declaration.\nprior beliefs pr_C_1_type_Z_caused_Y pr_C_1_type_always_Y represent?\nsetting beliefs value affect design, ?\nimplication clue selection Bayesian Process Tracing designs?\nLook estimator declaration.prior beliefs pr_C_1_type_Z_caused_Y pr_C_1_type_always_Y represent?prior beliefs pr_C_1_type_Z_caused_Y pr_C_1_type_always_Y represent?setting beliefs value affect design, ?setting beliefs value affect design, ?implication clue selection Bayesian Process Tracing designs?implication clue selection Bayesian Process Tracing designs?Declare new_design modify prior belief case belongs first causal type changing pr_type_Z_caused_Y different value (say, .7).\nDiagnose new_design. bias parameter change, ?\nKeeping modification just made, modify code new_design clue potential outcomes function pr_C_1 ~ Z * (.9999 * (type == \"Z_caused_Y\") + .0001 * (type == \"always_Y\" clue priors pr_C_1_type_Z_caused_Y = .9999 pr_C_1_type_always_Y = .0001. kind clue researcher now?\nDiagnose new_design. changes clue reduce bias?\nDeclare new_design modify prior belief case belongs first causal type changing pr_type_Z_caused_Y different value (say, .7).Diagnose new_design. bias parameter change, ?Diagnose new_design. bias parameter change, ?Keeping modification just made, modify code new_design clue potential outcomes function pr_C_1 ~ Z * (.9999 * (type == \"Z_caused_Y\") + .0001 * (type == \"always_Y\" clue priors pr_C_1_type_Z_caused_Y = .9999 pr_C_1_type_always_Y = .0001. kind clue researcher now?Keeping modification just made, modify code new_design clue potential outcomes function pr_C_1 ~ Z * (.9999 * (type == \"Z_caused_Y\") + .0001 * (type == \"always_Y\" clue priors pr_C_1_type_Z_caused_Y = .9999 pr_C_1_type_always_Y = .0001. kind clue researcher now?Diagnose new_design. changes clue reduce bias?Diagnose new_design. changes clue reduce bias?","code":"\ndiagnose_design(design,\n                diagnosands = declare_diagnosands(\n                  bias = mean(posterior - estimand),\n                  rmse = sqrt(mean((posterior - estimand) ^ 2)),\n                  mean_estimand = mean(estimand),\n                  mean_posterior = mean(posterior),\n                  keep_defaults = FALSE), \n                sims = 1000)"},{"path":"observational-designs-for-causal-inference.html","id":"online-appendix-applied-example","chapter":"16 Observational designs for causal inference","heading":"16.6.4 Online Appendix Applied Example","text":"Point explore cases look multiple sources \nevidencePoint explore cases look multiple sources \nevidenceIn many parts world, people rely non-state institutions construct social order. Sometimes, institutions persist long time [e.g., mourides], whereas sometimes break . non-state institutions fail? scholars African societies think traditional institutions declined 1960s 1970s due forceful efforts post-independence leaders saw alternative authorities threat young state. scholars Ensminger (1990) point instead internal political economy rural societies, emphasize role economic interests decline traditional institutions.many parts world, people rely non-state institutions construct social order. Sometimes, institutions persist long time [e.g., mourides], whereas sometimes break . non-state institutions fail? scholars African societies think traditional institutions declined 1960s 1970s due forceful efforts post-independence leaders saw alternative authorities threat young state. scholars Ensminger (1990) point instead internal political economy rural societies, emphasize role economic interests decline traditional institutions.Ensminger (1990) economic anthropologist bunch cool work Orma Kenya, nomadic pastoralist group. points , whereas Orma able avoid tragedy commons policing access scarce water resources competing somali pastoralists throughout 1960s 1970s, 1980s power council elders weakened, evidenced frequent defection individual orma sold water somalis, thus hurting interests group.Ensminger (1990) economic anthropologist bunch cool work Orma Kenya, nomadic pastoralist group. points , whereas Orma able avoid tragedy commons policing access scarce water resources competing somali pastoralists throughout 1960s 1970s, 1980s power council elders weakened, evidenced frequent defection individual orma sold water somalis, thus hurting interests group.study presents case causal process tracing scholar presents evidence rich fieldwork support inference effect produced specific cause. useful formalize provides lessons kinds clues qualitative researchers might seek order maximize probative value answer strategy.study presents case causal process tracing scholar presents evidence rich fieldwork support inference effect produced specific cause. useful formalize provides lessons kinds clues qualitative researchers might seek order maximize probative value answer strategy.key pieces outcome (breakdown council elders – measured defections orma selling water somalis), potential cause (economic diversification – measured move away cattle-based pastoralism towards sendentary economic activity – teaching, shops – Orma), pieces evidence “clues” support notion relationship outcome cause causal.key pieces outcome (breakdown council elders – measured defections orma selling water somalis), potential cause (economic diversification – measured move away cattle-based pastoralism towards sendentary economic activity – teaching, shops – Orma), pieces evidence “clues” support notion relationship outcome cause causal.set imaginary study inspired Ensminger (1990)set imaginary study inspired Ensminger (1990)researcher seeks test claim economic diversification leads breakdown informal institutions. look group Kenya experienced diversification whose institutions managing commons failed. [think cool empirical implications]. want test claim idea informal institutions failed due deliberate attempt state authorities supplant traditional leaders.researcher seeks test claim economic diversification leads breakdown informal institutions. look group Kenya experienced diversification whose institutions managing commons failed. [think cool empirical implications]. want test claim idea informal institutions failed due deliberate attempt state authorities supplant traditional leaders.answer strategy diverges one book insofar seek one two clues test prior explanation.answer strategy diverges one book insofar seek one two clues test prior explanation.time researcher needs think typology clues terms Van Evera stuff, also joint probability distribution clues.time researcher needs think typology clues terms Van Evera stuff, also joint probability distribution clues.make use joint_prob function book R package, calculates joint probability distribution two correlated events, given marginal probabilities correlationWe make use joint_prob function book R package, calculates joint probability distribution two correlated events, given marginal probabilities correlationResearcher imposes monotonicity (Z_caused_not_Y types) – economic diversity either\nnegative effect institutionsResearcher imposes monotonicity (Z_caused_not_Y types) – economic diversity either\nnegative effect institutionsNote: variation outcome. informal institutions declined, just want \nknow economic diversification caused . quant study provide leverage.Note: variation outcome. informal institutions declined, just want \nknow economic diversification caused . quant study provide leverage.Note: joint probabilities observing clues depend type, () type ZNote: joint probabilities observing clues depend type, () type Z","code":"\ntypes <- c('Z_caused_Y', 'Z_caused_not_Y', 'always_Y', 'always_not_Y')\n\n\n\n\ndesign <-\n  declare_population(N = 20,\n                     Z = draw_binary(prob = .5, N = N),\n                     type = sample(x = types, size = N, replace = TRUE, prob = c(.5, 0, .5, 0)))  +\n  declare_potential_outcomes(\n    Y ~ Z * (type == \"Z_caused_Y\") + (1 - Z) * (type == \"Z_caused_not_Y\") + (type == \"always_Y\"),\n    conditions = list(Z = c(0, 1), type = types)) +\n  declare_potential_outcomes(\n    pr_C1C2_00 ~ (joint_prob(.75,.3,0,\"00\") * (type == \"Z_caused_Y\") + joint_prob(.25,.005,0,\"00\") * (type == \"always_Y\")),\n    conditions = list(Z = c(0, 1), type = types)) +\n    declare_potential_outcomes(\n    pr_C1C2_01 ~ (joint_prob(.75,.3,0,\"01\") * (type == \"Z_caused_Y\") + joint_prob(.25,.005,0,\"01\") * (type == \"always_Y\")),\n    conditions = list(Z = c(0, 1), type = types)) +\n    declare_potential_outcomes(\n    pr_C1C2_10 ~ (joint_prob(.75,.3,0,\"10\") * (type == \"Z_caused_Y\") + joint_prob(.25,.005,0,\"10\") * (type == \"always_Y\")),\n    conditions = list(Z = c(0, 1), type = types)) +\n    declare_potential_outcomes(\n    pr_C1C2_11 ~ (joint_prob(.75,.3,0,\"11\") * (type == \"Z_caused_Y\") + joint_prob(.25,.005,0,\"11\") * (type == \"always_Y\")),\n    conditions = list(Z = c(0, 1), type = types)) +\n  declare_reveal(c(Y, pr_C1C2_00,pr_C1C2_01,pr_C1C2_10,pr_C1C2_11), c(Z, type)) +\n  declare_assignment(blocks = ID, block_prob_each = cbind(pr_C1C2_00,pr_C1C2_01,pr_C1C2_10,pr_C1C2_11),\n                     conditions = c(\"00\",\"01\",\"10\",\"11\"), \n                     assignment_variable = \"C1C2\") +  \n  declare_sampling(handler = function(data) data %>% filter(Z==1 & Y==1) %>% sample_n(size = 1)) +\n  declare_estimand(did_Z_cause_Y = type == 'Z_caused_Y') +\n  declare_measurement(\n  C1 = ifelse(C1C2 == \"10\" | C1C2 == \"11\", 1, 0),\n  C2 = ifelse(C1C2 == \"01\" | C1C2 == \"11\", 1, 0),\n  handler = fabricate) +\n  declare_estimator(\n    pr_type_Z_caused_Y = .5,\n    pr_C_1_type_Z_caused_Y = .75,\n    pr_C_1_type_always_Y = .25,\n    C = C1,\n    pr_C_type_Z_caused_Y = C * pr_C_1_type_Z_caused_Y + (1 - C) * (1 - pr_C_1_type_Z_caused_Y),\n    pr_C_type_always_Y = C * pr_C_1_type_always_Y + (1 - C) * (1 - pr_C_1_type_always_Y),\n    posterior =\n      pr_type_Z_caused_Y * pr_C_type_Z_caused_Y / (pr_type_Z_caused_Y * pr_C_type_Z_caused_Y + pr_C_type_always_Y * (1 - pr_type_Z_caused_Y)),\n    label = \"Straw in the Wind\",\n    estimand_label = \"did_Z_cause_Y\",\n    handler = summarize) +\n  declare_estimator(\n    pr_type_Z_caused_Y = .5,\n    pr_C_1_type_Z_caused_Y = .30,\n    pr_C_1_type_always_Y = .005,\n    C = C2,\n    pr_C_type_Z_caused_Y = C * pr_C_1_type_Z_caused_Y + (1 - C) * (1 - pr_C_1_type_Z_caused_Y),\n    pr_C_type_always_Y = C * pr_C_1_type_always_Y + (1 - C) * (1 - pr_C_1_type_always_Y),\n    posterior =\n      pr_type_Z_caused_Y * pr_C_type_Z_caused_Y / (pr_type_Z_caused_Y * pr_C_type_Z_caused_Y + pr_C_type_always_Y * (1 - pr_type_Z_caused_Y)),\n    label = \"Smoking Gun\",\n    estimand_label = \"did_Z_cause_Y\",\n    handler = summarize) +\n  declare_estimator(\n    pr_type_Z_caused_Y = .5,\n    pr_C1_1_type_Z_caused_Y = .30,\n    pr_C1_1_type_always_Y = .005,\n    pr_C2_1_type_Z_caused_Y = .75,\n    pr_C2_1_type_always_Y = .25,\n    rho = 0,\n    pr_C_type_Z_caused_Y = joint_prob(pr_C1_1_type_Z_caused_Y, pr_C2_1_type_Z_caused_Y, \n                                      rho, which_prob = C1C2),\n    pr_C_type_always_Y = joint_prob(pr_C1_1_type_always_Y, pr_C2_1_type_always_Y, \n                                      rho, which_prob = C1C2),\n    posterior =\n      pr_type_Z_caused_Y * pr_C_type_Z_caused_Y / (pr_type_Z_caused_Y * pr_C_type_Z_caused_Y + pr_C_type_always_Y * (1 - pr_type_Z_caused_Y)),\n    label = \"Joint Updating\",\n    estimand_label = \"did_Z_cause_Y\",\n    handler = summarize) \n\n\n# diagnose_design(design,\n#                 diagnosands = declare_diagnosands(\n#                   bias = mean(posterior - estimand),\n#                   rmse = sqrt(mean((posterior - estimand) ^ 2)),\n#                   mean_estimand = mean(estimand),\n#                   mean_posterior = mean(posterior),\n#                   keep_defaults = FALSE), \n#                 sims = 500)\n\n# design_sims <- simulate_design(design, sims = 500)"},{"path":"observational-designs-for-causal-inference.html","id":"dag-13","chapter":"16 Observational designs for causal inference","heading":"16.6.5 Dag","text":"","code":"\ndag <- dagify(C1 ~ type,\n              C2 ~ type,\n              Y ~ Z + type)\n\n\nnodes <-\n  tibble(\n    name = c( \"type\",\"Z\",\"C1\", \"C2\", \"Y\"),\n    label = name,\n    annotation = c(\n      \"**Type**<br>Causal relationship<br>between economic diversification<br>and breakdown of institutions\",\n      \"**Cause**<br>Economic Diversification\",\n      \"**Clue 1**<br>Smoking gun interview evidence\",\n      \"**Clue 2**<br>Straw-in-the-wind archival evidence\",\n      \"**Outcome**<br>Breakdown of traditional<br>institutions\"),\n    x = c(1, 1, 5, 5, 5),\n    y = c(1.5,3.5,2,1.5,3.5),\n    nudge_direction = c(\"S\", \"N\", \"N\", \"S\",\"N\"),\n    answer_strategy = \"uncontrolled\"\n  )\n\n\nggdd_df <- make_dag_df(dag, nodes, design)\n\nbase_dag_plot %+% ggdd_df"},{"path":"observational-designs-for-causal-inference.html","id":"synthetic-controls","chapter":"16 Observational designs for causal inference","heading":"16.7 Synthetic controls","text":"","code":""},{"path":"observational-designs-for-causal-inference.html","id":"declaration-15","chapter":"16 Observational designs for causal inference","heading":"16.7.1 Declaration","text":"","code":"\ndesign <- \n  declare_population(\n    unit = add_level(N = 10, units = 1:N, X = rnorm(N, sd = 0.5)),\n    period = add_level(N = 3, time = 1:N, nest = FALSE),\n    unit_period = cross_levels(by = join(unit, period), U = rnorm(N))\n  ) + \n  declare_potential_outcomes(Y ~ X + 0.5 * as.numeric(period) + Z + U) + \n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0), subset = period == 3) + \n  declare_step(handler = mutate, Z = unit == \"01\") + \n  declare_reveal(Y = if_else(Z == 0 | period < 3, Y_Z_0, Y_Z_1), handler = mutate) +\n  declare_step(predictors = \"X\",\n               time.predictors.prior = 1:2,\n               dependent = \"Y\",\n               unit.variable = \"units\",\n               time.variable = \"time\",\n               treatment.identifier = 1,\n               controls.identifier = 2:10, \n               handler = synth_weights_tidy) +\n  declare_estimator(Y ~ Z, subset = time >= 3, weights = synth_weights, model = lm_robust, label = \"synth\")"},{"path":"observational-designs-for-causal-inference.html","id":"dag-14","chapter":"16 Observational designs for causal inference","heading":"16.7.2 Dag","text":"","code":"\n# Simulation --------------------------------------------------------------\n\n# simulations <- simulate_design(design, sims = 100)\n# ggplot(simulations, aes(estimate)) + geom_histogram()\n\n\n# Synth plot --------------------------------------------------------------\n\n# data <- draw_data(design)\n# summary_df <- data %>%\n#   group_by(Z, time) %>%\n#   summarize(Y = weighted.mean(Y, w = synth_weights))\n# ggplot(summary_df, aes(x = time, y = Y, color = Z)) +\n#   geom_line(size = 2, alpha = 0.5) +\n#   geom_line(data = data, aes(x = time, y = Y, group = units), color = \"black\", alpha = 0.3) +\n#   geom_point(data = data, aes(x = time, y = Y, size = synth_weights^2), alpha = 0.3) +\n#   geom_vline(xintercept = 2.5)\n\ndag <- dagify(Y ~ X + period + Z + U,\n              Z ~ X)\n\nnodes <-\n  tibble(\n    name = c(\"U\", \"X\", \"period\", \"Z\", \"Y\"),\n    label = c(\"U\", \"X\", \"T\", \"Z\", \"Y\"),\n    annotation = c(\n      \"**Unknown heterogeneity**\",\n      \"**Unit effect**\",\n      \"**Time period**\",\n      \"**Treatment assignment**\",\n      \"**Outcome variable**\"\n    ),\n    \n    x = c(5, 1, 1, 3, 5),\n    y = c(1.5,3.5, 1, 2.5, 2.5), \n    nudge_direction = c(\"S\", \"N\", \"S\", \"S\",\"N\"),\n    answer_strategy = \"uncontrolled\")\n    \n    \n\nggdd_df <- make_dag_df(dag, nodes, design)\n\nbase_dag_plot %+% ggdd_df"},{"path":"observational-designs-for-causal-inference.html","id":"example-15","chapter":"16 Observational designs for causal inference","heading":"16.7.3 Example","text":"Modeled example :https://www.mitpressjournals.org/doi/abs/10.1162/REST_a_00429?casa_token=o-zWqCima50AAAAA:yiEERZfdhAUoHV0-xBYNjgdljvgfRXrriR8foG7X8nHSUAMFrLcw2vWY8e9pHzmRT24MMAIv9hvKpQDid 2007 Legal Arizona Workers Act Reduce State’s Unauthorized Immigrant Population?\nSarah Bohn, Magnus Lofstrom, Steven Raphael\nReview Economics Statistics 2014 96:2, 258-269\nAbstract: test effect Arizona’s 2007 Legal Arizona Workers Act (LAWA) proportion state’s population characterized noncitizen Hispanic. use synthetic control method select group states Arizona’s population trends can compared. document notable statistically significant reduction proportion Hispanic noncitizen population Arizona. decline observed matches timing LAWA’s implementation, deviates time series synthetic control group, stands relative distribution placebo estimates states nation.Outline:\n(1) synth work?\n- declaration: set states time trends levels correlated type following linear model assumed SCM\n- try three estimators: (1) difference--difference; (2) single difference treated period; (3) difference treated period weighted Synth weights.\n- show synth works assumptions; plot time series treat synthetic control; plot time series units illustrate picked (sorted weights)\n(2) synth’s assumptions?\n- linear model; treated unit convex hull control units’ pretreatment time series\n(3) diagnose outside convex hull\n- declaration outside convex hull use Abadie diagnostic demonstrating poor match. (possibly explore power diagnostic)\n- show synth biased setting. augsynth .Now simulating, saving, loading done, can use simulations whatever want.see Synth outperforms either method","code":"\n# tidy function that takes data and just adds the synthetic control weights to it\nsynth_weights_tidy <- function(data) {\n  dataprep.out <- dataprep(\n    foo = data,\n    predictors = \"prop_non_hispanic_below_hs\",\n    predictors.op = \"mean\",\n    time.predictors.prior = 1998:2006,\n    dependent = \"prop_non_hispanic_below_hs\",\n    unit.variable = \"state_number\",\n    time.variable = \"year\",\n    treatment.identifier = 4,\n    controls.identifier = c(1:3, 5:50), # states without Arizona\n    time.optimize.ssr = 1998:2006,\n    time.plot = 1998:2009)\n  capture.output(fit <- synth(data.prep.obj = dataprep.out))\n  tab <- synth.tab(dataprep.res = dataprep.out, synth.res = fit) \n  \n  data %>% \n    left_join(tab$tab.w %>% mutate(synth_weights = w.weights) %>% dplyr::select(synth_weights, unit.numbers), by = c(\"state_number\" = \"unit.numbers\")) %>% \n    mutate(synth_weights = replace(synth_weights, state_number == 4, 1))\n}\n\naugsynth_tidy <- function(data) {\n  fit <- augsynth(prop_non_hispanic_below_hs ~ legal_worker_act, state, year, t_int = 2007, data = data)\n  res <- summary(fit)$att %>% filter(Time == 2007) %>% select(Estimate, Std.Error)\n  names(res) <- c(\"estimate\", \"std.error\")\n  res$p.value <- 2 * pt(-abs(res$estimate/res$std.error), df = nrow(data) - 15)\n  res$conf.low <- res$estimate - 1.96 * res$std.error\n  res$conf.high <- res$estimate + 1.96 * res$std.error\n  res\n}\n\n# note need to clean up the range of the data, currently over 1\ndesign <-\n  declare_population(\n    states = add_level(\n      N = 50, \n      state = state.abb,\n      state_number = as.numeric(as.factor(state)),\n      state_shock = runif(N, -.15, .15),\n      border_state = state %in% c(\"AZ\", \"CA\", \"NM\", \"TX\"),\n      state_shock = ifelse(border_state, .2, state_shock)\n    ),\n    years = add_level(\n      N = 12, nest = FALSE,\n      year = 1998:2009,\n      post_treatment_period = year >= 2007,\n      year_shock = runif(N, -.025, .025), \n      year_trend = year - 1998\n    ),\n    obs = cross_levels(\n      by = join(states, years),\n      # treatment indicator:\n      legal_worker_act = if_else(post_treatment_period == TRUE & state == \"AZ\", 1, 0),\n      state_year_shock = runif(N, -.025, .025),\n      prop_non_hispanic_below_hs_baseline = \n        0.4 + state_shock + year_shock + (.01 + .05 * border_state) * year_trend + state_year_shock\n    )\n  ) +\n  declare_potential_outcomes(\n    prop_non_hispanic_below_hs ~ prop_non_hispanic_below_hs_baseline + 0.25 * legal_worker_act, \n    assignment_variable = legal_worker_act) +\n  declare_estimand(\n    ATE_AZ = mean(prop_non_hispanic_below_hs_legal_worker_act_1 - \n                    prop_non_hispanic_below_hs_legal_worker_act_0), \n    subset = legal_worker_act == TRUE) +\n  declare_reveal(prop_non_hispanic_below_hs, legal_worker_act) +\n  declare_step(handler = synth_weights_tidy) +\n  declare_estimator(\n    prop_non_hispanic_below_hs ~ legal_worker_act, \n    subset = year >= 2007, weights = synth_weights, model = lm_robust, label = \"synth\") +\n  declare_estimator(\n    prop_non_hispanic_below_hs ~ legal_worker_act, subset = year >= 2007, \n    model = lm_robust, label = \"unweighted\") +\n  declare_estimator(\n    prop_non_hispanic_below_hs ~ I(state == \"AZ\") + post_treatment_period + legal_worker_act, term = \"legal_worker_act\", \n    model = lm_robust, label = \"unweighted_did\") +\n  declare_estimator(handler = tidy_estimator(augsynth_tidy), label = \"augsynth\")\nstate_data <- draw_data(design)\n\nstate_data %>% dplyr::select(state, synth_weights) %>% distinct %>% arrange(-synth_weights) %>% head##   state synth_weights\n## 1    AZ         1.000\n## 2    NM         0.990\n## 3    TX         0.007\n## 4    CA         0.001\n## 5    AL         0.000\n## 6    AK         0.000\nstate_data %>% \n  ggplot() +\n  geom_line(aes(year, prop_non_hispanic_below_hs)) +\n  facet_wrap(~ state)\nstate_data %>% \n  mutate(treatment_state = factor(state == \"AZ\", levels = c(FALSE, TRUE), labels = c(\"Synthethic Control\", \"Arizona\"))) %>% \n  group_by(treatment_state, year) %>% \n  summarize(prop_non_hispanic_below_hs = weighted.mean(prop_non_hispanic_below_hs, w = synth_weights)) %>% \n  ggplot(aes(x = year, y = prop_non_hispanic_below_hs, color = treatment_state)) +\n  geom_line() + \n  geom_vline(xintercept = 2007) + \n  scale_x_continuous(breaks = scales::pretty_breaks()) + \n  annotate(\"text\", x = 2006.7, y = 1.7, label = \"Law Introduced in 2007\", hjust = \"right\", family = \"Palatino\") +\n  labs(color = \"\") +\n  xlab(\"\") + ylab(\"Proportion Non-Hispanic Below H.S. Education\") +\n  dd_theme()\nsimulations <- simulate_design(design, sims = sims)\nsynth_diagnosands <- declare_diagnosands(select = c(\"bias\", \"rmse\", \"coverage\"))\n\ndiagnosis <- diagnose_design(simulations, diagnosands = synth_diagnosands, bootstrap_sims = b_sims)\n\nkable(reshape_diagnosis(diagnosis))"},{"path":"observational-designs-for-causal-inference.html","id":"when-there-are-not-good-controls-standard-synth-will-get-the-wrong-answer","chapter":"16 Observational designs for causal inference","heading":"16.7.4 When there are not good controls, standard synth will get the wrong answer","text":"Now simulating, saving, loading done, can use simulations whatever want.","code":"\n# declaration outside the convex hull\ndesign_outside_hull <- replace_step(\n  design, \n  step = 2, \n  new_step = declare_potential_outcomes(\n    prop_non_hispanic_below_hs ~ prop_non_hispanic_below_hs_baseline + 0.25 * legal_worker_act + 0.2 * (state == \"AZ\"), \n    assignment_variable = legal_worker_act))\n\nstate_data_outside_hull <- draw_data(design_outside_hull)\nsimulations_outside_hull <- simulate_design(design_outside_hull, sims = sims)\ndiagnosis_outside_hull <- diagnose_design(simulations_outside_hull, diagnosands = synth_diagnosands, bootstrap_sims = b_sims)\n\nkable(reshape_diagnosis(diagnosis_outside_hull))\n# plot the synthetic control constructed in this way (it usually picks just texas and is highly biased)\nstate_data_outside_hull %>% \n  mutate(treatment_state = factor(state == \"AZ\", levels = c(FALSE, TRUE), labels = c(\"Synthethic Control\", \"Arizona\"))) %>% \n  group_by(treatment_state, year) %>% \n  summarize(prop_non_hispanic_below_hs = weighted.mean(prop_non_hispanic_below_hs, w = synth_weights)) %>% \n  ggplot(aes(x = year, y = prop_non_hispanic_below_hs, color = treatment_state)) +\n  geom_line() + \n  geom_vline(xintercept = 2007) + \n  scale_x_continuous(breaks = scales::pretty_breaks()) + \n  annotate(\"text\", x = 2006.7, y = 1.7, label = \"Law Introduced in 2007\", hjust = \"right\", family = \"Palatino\") +\n  labs(color = \"\") +\n  xlab(\"\") + ylab(\"Proportion Non-Hispanic Below H.S. Education\") +\n  dd_theme()"},{"path":"observational-designs-for-causal-inference.html","id":"references-1","chapter":"16 Observational designs for causal inference","heading":"16.7.5 References","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"experimental-designs-for-causal-inference","chapter":"17 Experimental designs for causal inference","heading":"17 Experimental designs for causal inference","text":"Section introduction","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"two-arm-trials-1","chapter":"17 Experimental designs for causal inference","heading":"17.1 Two arm trials","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"declaration-16","chapter":"17 Experimental designs for causal inference","heading":"17.1.1 Declaration","text":"","code":"\ndesign <-\n  declare_population(N = 100, U = rnorm(N)) +\n  declare_potential_outcomes(Y ~ Z + U) +\n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_assignment(prob = 0.5) +\n  declare_estimator(Y ~ Z, estimand = \"ATE\")"},{"path":"experimental-designs-for-causal-inference.html","id":"dag-15","chapter":"17 Experimental designs for causal inference","heading":"17.1.2 Dag","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"example-16","chapter":"17 Experimental designs for causal inference","heading":"17.1.3 Example","text":"can use global bib file via rmarkdown cites like : Imai, King, Stuart (2008)chunk set echo = TRUE eval = do_diagnosisRight simulations, want save simulations rds.Now simulating, saving, loading done, can use simulations whatever want.","code":"\ndesign <-\n  declare_population(N = 100, u = rnorm(N)) +\n  declare_potential_outcomes(Y ~ Z + u) +\n  declare_assignment(prob = 0.5) +\n  declare_reveal(Y, Z) +\n  declare_estimator(Y ~ Z, model = difference_in_means)\nsimulations_pilot <- simulate_design(design, sims = sims)\nkable(head(simulations_pilot))"},{"path":"experimental-designs-for-causal-inference.html","id":"blocked-trials","chapter":"17 Experimental designs for causal inference","heading":"17.2 Blocked trials","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"declaration-17","chapter":"17 Experimental designs for causal inference","heading":"17.2.1 Declaration","text":"","code":"\ndesign <-\n  declare_population(N = 100,\n                     X = rbinom(N, 1, 0.3),\n                     U = rnorm(N)) +\n  declare_potential_outcomes(Y ~ Z + X + U) +\n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_assignment(blocks = X, block_prob = c(0.1, 0.5)) +\n  declare_estimator(Y ~ Z, estimand = \"ATE\", label = \"Naive DIM\") +\n  declare_estimator(Y ~ Z,\n                    blocks = X,\n                    estimand = \"ATE\",\n                    label = \"Blocked DIM\")"},{"path":"experimental-designs-for-causal-inference.html","id":"dag-16","chapter":"17 Experimental designs for causal inference","heading":"17.2.2 Dag","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"example-17","chapter":"17 Experimental designs for causal inference","heading":"17.2.3 Example","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"two-arm-trials-and-designs-with-blocking-and-clustering","chapter":"17 Experimental designs for causal inference","heading":"17.2.4 Two-arm trials and designs with blocking and clustering","text":"","code":"\ndesign <-\n  declare_population(\n    villages = add_level(N = 100, X = rbinom(N, 1, 0.3), Q = rnorm(N)),\n    people = add_level(N = 5, U = rnorm(N))\n  ) + \n  declare_potential_outcomes(Y ~ Z * X + U + Q) +\n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_assignment(clusters = villages, blocks = X, block_prob = c(0.1, 0.5)) +\n  declare_estimator(\n    Y ~ Z, model = difference_in_means, estimand = \"ATE\", label = \"Naive DIM\") +\n  declare_estimator(\n    Y ~ Z, clusters = villages, blocks = X, model = difference_in_means, \n    estimand = \"ATE\", label = \"Blocked DIM\"\n  ) + \n  declare_estimator(\n    Y ~ Z, clusters = villages, fixed_effects = X, model = lm_robust, \n    estimand = \"ATE\", label = \"Naive FE\"\n  )"},{"path":"experimental-designs-for-causal-inference.html","id":"multiarm-designs","chapter":"17 Experimental designs for causal inference","heading":"17.3 Multiarm Designs","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"declaration-18","chapter":"17 Experimental designs for causal inference","heading":"17.3.1 Declaration","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"dag-17","chapter":"17 Experimental designs for causal inference","heading":"17.3.2 Dag","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"example-18","chapter":"17 Experimental designs for causal inference","heading":"17.3.3 Example","text":"can use global bib file via rmarkdown cites like : Imai, King, Stuart (2008)chunk set echo = TRUE eval = do_diagnosisRight simulations, want save simulations rds.Now simulating, saving, loading done, can use simulations whatever want.","code":"\ndesign <-\n  declare_population(N = 100, u = rnorm(N)) +\n  declare_potential_outcomes(Y ~ Z + u) +\n  declare_assignment(prob = 0.5) +\n  declare_reveal(Y, Z) +\n  declare_estimator(Y ~ Z, model = difference_in_means)\nsimulations_pilot <- simulate_design(design, sims = sims)\nkable(head(simulations_pilot))"},{"path":"experimental-designs-for-causal-inference.html","id":"encouragement-designs","chapter":"17 Experimental designs for causal inference","heading":"17.4 Encouragement designs","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"declaration-19","chapter":"17 Experimental designs for causal inference","heading":"17.4.1 Declaration","text":"","code":"\ndirect_effect_of_encouragement <- 0.0\nproportion_defiers <- 0.0\n\n\ndesign <-\n  declare_population(\n    N = 100,\n    type = sample(\n      x = c(\"Always-Taker\", \"Never-Taker\", \"Complier\", \"Defier\"),\n      prob = c(0.1, 0.1, 0.8, 0.0),\n      size = N, replace = TRUE\n    ),\n    U = rnorm(N)\n  ) +\n  declare_potential_outcomes(\n    D ~ case_when(\n      Z == 1 & type %in% c(\"Always-Taker\", \"Complier\") ~ 1,\n      Z == 1 & type %in% c(\"Never-Taker\", \"Defier\") ~ 0,\n      Z == 0 & type %in% c(\"Never-Taker\", \"Complier\") ~ 0,\n      Z == 0 & type %in% c(\"Always-Taker\", \"Defier\") ~ 1\n    )\n  ) +\n  declare_potential_outcomes(\n    Y ~ 0.5 * (type == \"Complier\") * D +\n      0.25 * (type == \"Always-Taker\") * D +\n      0.75 * (type == \"Defier\") * D +\n      # Building in NO excludability violation\n      0 * Z + U,\n    assignment_variables = c(\"D\", \"Z\")\n  ) +\n  declare_estimand(CACE = mean(Y_D_1_Z_1 - Y_D_0_Z_0),\n                   subset = type == \"Complier\") +\n  declare_assignment(prob = 0.5) +\n  declare_reveal(D, assignment_variable = \"Z\") +\n  declare_reveal(Y, assignment_variables = c(\"D\", \"Z\")) +\n  declare_estimator(Y ~ D | Z, model = iv_robust, estimand = \"CACE\")"},{"path":"experimental-designs-for-causal-inference.html","id":"dag-18","chapter":"17 Experimental designs for causal inference","heading":"17.4.2 Dag","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"example-19","chapter":"17 Experimental designs for causal inference","heading":"17.4.3 Example","text":"Idea one show violations defiers excludability lead bias.","code":"\ntypes <- c(\"Always-Taker\", \"Never-Taker\", \"Complier\", \"Defier\")\ndirect_effect_of_encouragement <- 0.0\nproportion_defiers <- 0.0\n\ndesign <-\n  declare_population(\n    N = 500,\n    type = sample(\n      types,\n      N,\n      replace = TRUE,\n      prob = c(0.1, 0.1, 0.8 - proportion_defiers, proportion_defiers)\n    ),\n    noise = rnorm(N)\n  ) +\n  declare_potential_outcomes(\n    D ~ case_when(\n      Z == 0 & type %in% c(\"Never-Taker\", \"Complier\") ~ 0,\n      Z == 1 & type %in% c(\"Never-Taker\", \"Defier\") ~ 0,\n      Z == 0 & type %in% c(\"Always-Taker\", \"Defier\") ~ 1,\n      Z == 1 & type %in% c(\"Always-Taker\", \"Complier\") ~ 1\n    )\n  ) +\n  declare_potential_outcomes(\n    Y ~ 0.5 * (type == \"Complier\") * D +\n      0.25 * (type == \"Always-Taker\") * D +\n      0.75 * (type == \"Defier\") * D +\n      direct_effect_of_encouragement * Z + noise,\n    assignment_variables = c(\"D\", \"Z\")\n  ) +\n  declare_estimand(CACE = mean((Y_D_1_Z_1 + Y_D_1_Z_0) / 2 -\n                                 (Y_D_0_Z_1 + Y_D_0_Z_0) / 2),\n                   subset = type == \"Complier\") +\n  declare_assignment(prob = 0.5) +\n  declare_reveal(D, assignment_variable = \"Z\") +\n  declare_reveal(Y, assignment_variables = c(\"D\", \"Z\")) +\n  declare_estimator(Y ~ D | Z, model = iv_robust, estimand = \"CACE\")\ndesigns <- redesign(\n  design,\n  proportion_defiers = seq(0, 0.3, length.out = 5),\n  direct_effect_of_encouragement = seq(0, 0.3, length.out = 5)\n)\n\nsimulations <- simulate_design(designs, sims = sims)\ngg_df <-\n  simulations %>%\n  group_by(proportion_defiers,\n           direct_effect_of_encouragement) %>%\n  summarize(bias = mean(estimate - estimand))\n\n\nggplot(gg_df,\n       aes(\n         proportion_defiers,\n         bias,\n         group = direct_effect_of_encouragement,\n         color = direct_effect_of_encouragement\n       )) +\n  geom_point() +\n  geom_line() "},{"path":"experimental-designs-for-causal-inference.html","id":"references-2","chapter":"17 Experimental designs for causal inference","heading":"17.4.4 References","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"stepped-wedge-designs","chapter":"17 Experimental designs for causal inference","heading":"17.5 Stepped wedge designs","text":"stepped wedge design, individuals randomly assigned enter treatment different stages stage outcomes remeasured. Figure ?? illustrates study gets name: two units randomly added treatment group three waves, treatment group increases “steps” control group diminishes.Often, stepped wedge designs vaunted policy appeal allow everyone (eventually) treated context experiment. However, achieve goal regular two-armed trial treating everyone final wave measurement, stepped wedge designs involve treating everyone. real advantage stepped wedge ethical appeal: ’s ability squeeze power small sample treating wave though study.","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"declaration-20","chapter":"17 Experimental designs for causal inference","heading":"17.5.1 Declaration","text":"Model: Eight units randomized remeasured three time points. Importantly, unit given point time reveals one two potential outcomes—treated untreated. untreated potential outcomes consist unit- unit-period-specific shock. Treated potential outcomes increase relative control potential outcome rate 1 period—words, bigger treatment effects later periods.Model: Eight units randomized remeasured three time points. Importantly, unit given point time reveals one two potential outcomes—treated untreated. untreated potential outcomes consist unit- unit-period-specific shock. Treated potential outcomes increase relative control potential outcome rate 1 period—words, bigger treatment effects later periods.Inquiry: estimand average treatment effect units periods sample. Averaged three periods, ATE (1 + 2 + 3) / 3 = 2.Inquiry: estimand average treatment effect units periods sample. Averaged three periods, ATE (1 + 2 + 3) / 3 = 2.Data strategy: assignment strategy adds two units (one-quarter total sample) treatment random wave, leaving two units treated wave —see remaining two orange squares top-right corner Figure ??. Notice , every unit assigned treatment wave equal probability, wave reveals treated untreated potential outcomes unit-periods different probabilities. first wave, one-quarter units reveal treated potential outcomes; second wave, one-half units reveal treated potential outcomes, final wave, three-quarters units reveal treated potential outcomes. essence, experiment like block-randomized trial, unit-periods grouped period-specific blocks differential probabilities assignment.Data strategy: assignment strategy adds two units (one-quarter total sample) treatment random wave, leaving two units treated wave —see remaining two orange squares top-right corner Figure ??. Notice , every unit assigned treatment wave equal probability, wave reveals treated untreated potential outcomes unit-periods different probabilities. first wave, one-quarter units reveal treated potential outcomes; second wave, one-half units reveal treated potential outcomes, final wave, three-quarters units reveal treated potential outcomes. essence, experiment like block-randomized trial, unit-periods grouped period-specific blocks differential probabilities assignment.Answer strategy: Just block-randomized trial differential probabilities, need take account fact assignment strategy “-represents” treated potential outcomes first wave “-represents” last wave. Inverse-propensity weights (IPWs) one way correct disparities. unit’s IPW represented Figure ??. numbers mean, change one wave another? sampling context, IPW tells us many units population sampled unit represents. experimental context, treatment assignment randomly samples units’ potential outcomes. , first wave, two treated units stand four units’ treated potential outcomes (4 + 4 = 8 units). potential outcomes revealed six units assigned control stand 1.33 units’ potential outcomes (1.33 + 1.33 + 1.33 + 1.33 + 1.33 + 1.33 = 8 units). Since second wave comprised one-half treated potential outcomes one-half control potential outcomes, unit represents two units’ potential outcomes. Using weights, get “representative” estimate average control treatment potential outcomes wave. didn’t apply weights, wouldn’t get representative view unobserved potential outcomes ’re sampling, estimates biased. buy us? also declare “two-arm” estimator focuses wave 2 , half units control half treatment. Diagnosis shows remeasuring randomizing gets us (.52 - .41) / .41 = 27% higher power.Answer strategy: Just block-randomized trial differential probabilities, need take account fact assignment strategy “-represents” treated potential outcomes first wave “-represents” last wave. Inverse-propensity weights (IPWs) one way correct disparities. unit’s IPW represented Figure ??. numbers mean, change one wave another? sampling context, IPW tells us many units population sampled unit represents. experimental context, treatment assignment randomly samples units’ potential outcomes. , first wave, two treated units stand four units’ treated potential outcomes (4 + 4 = 8 units). potential outcomes revealed six units assigned control stand 1.33 units’ potential outcomes (1.33 + 1.33 + 1.33 + 1.33 + 1.33 + 1.33 = 8 units). Since second wave comprised one-half treated potential outcomes one-half control potential outcomes, unit represents two units’ potential outcomes. Using weights, get “representative” estimate average control treatment potential outcomes wave. didn’t apply weights, wouldn’t get representative view unobserved potential outcomes ’re sampling, estimates biased. buy us? also declare “two-arm” estimator focuses wave 2 , half units control half treatment. Diagnosis shows remeasuring randomizing gets us (.52 - .41) / .41 = 27% higher power.","code":"\ndesign <- \n  declare_population(\n    unit = add_level(N = 8, X = rnorm(N)),\n    period = add_level(N = 3, time = as.numeric(period), \n                       p = c(1/4, 1/4 + 1/4, 1/4 + 1/4 + 1/4), \n                       nest = FALSE),\n    obs = cross_levels(by = join(unit, period), U = rnorm(N))) + \n  declare_potential_outcomes(Y ~ X + U + Z * time) +\n  declare_assignment(clusters = unit, conditions = 1:4, assignment_variable = \"wave\") + \n  declare_assignment(Z = as.numeric(time >= wave), ipw = 1 / (Z * p + (1 - Z) * (1 - p)), handler = fabricate) + \n  declare_reveal(Y, Z) + \n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + \n  declare_estimator(Y ~ Z, model = lm_robust, estimand = \"ATE\", label = \"1: Stepped Wedge\", weights = ipw, clusters = unit) +\n  declare_estimator(Y ~ Z, model = lm_robust, estimand = \"ATE\", label = \"2: Wave 2 Only\", subset = period == 2) "},{"path":"experimental-designs-for-causal-inference.html","id":"dag-19","chapter":"17 Experimental designs for causal inference","heading":"17.5.2 Dag","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"exercises-5","chapter":"17 Experimental designs for causal inference","heading":"17.5.3 Exercises","text":"Add answer strategy design doesn’t include IPWs diagnose design.affect bias estimates? Explain answer.Now change potential outcomes function treatment effect constant across periods. affect bias? Explain answer.relative power stepped-wedge single-period, two-arm trial change treatment effect restricted constant across periods? Explain answer.design makes subtle crucial assumption potential outcomes: namely, treated potential outcome revealed one period irrespective whether unit treated previous (subsequent) periods. violation assumption look like?","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"online-appendix-applied-example-1","chapter":"17 Experimental designs for causal inference","heading":"17.5.4 Online Appendix Applied Example","text":"FIND ADD EXAMPLE.","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"references-3","chapter":"17 Experimental designs for causal inference","heading":"17.5.5 References","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"randomized-saturation-design","chapter":"17 Experimental designs for causal inference","heading":"17.6 Randomized Saturation Design","text":"Randomized saturation designs used measure diffusion treatment effects throughout pre-defined area network (Baird et al. (2018)). design works two phases. First, entire areas networks randomly assigned saturations, e.g. 0%, 25%, 75% saturation. , network, units individually assigned proportions determined saturations: e.g., groups randomly assigned 0%, none units assigned treatment, groups assigned 25% 75%, one-quarter three-quarters units assigned treatment, respectively. comparing untreated units 0% groups untreated counterparts higher-saturation networks, researchers can estimate indirect treatment effects result saturation-induced spillovers.","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"declaration-21","chapter":"17 Experimental designs for causal inference","heading":"17.6.1 Declaration","text":"Model: four networks comprised four units. Units’ treatment assignment function network’s saturation assignment, turn defines amount spillover receive. spillover equal proportion units treated network. Potential outcomes thus defined terms S—unit’s group’s saturation—Z—whether unit treated.Model: four networks comprised four units. Units’ treatment assignment function network’s saturation assignment, turn defines amount spillover receive. spillover equal proportion units treated network. Potential outcomes thus defined terms S—unit’s group’s saturation—Z—whether unit treated.Inquiry: two inquiries. First, want know spillover, defined average difference outcomes untreated unit high versus low saturation network: \\(\\mathbb{E}[Y_i(Z_i = 0, S_i = \\text{high})-Y_i(Z_i = 0, S_i = \\text{low})]\\). also want know “direct effect”–e.g. happens directly treated disregard spillovers. defined potential outcomes experiment reveal, since one treated low-saturation constituencies: \\(\\mathbb{E}[Y_i(Z_i = 1, S_i = \\text{low})-Y_i(Z_i = 0, S_i = \\text{low})]\\).Inquiry: two inquiries. First, want know spillover, defined average difference outcomes untreated unit high versus low saturation network: \\(\\mathbb{E}[Y_i(Z_i = 0, S_i = \\text{high})-Y_i(Z_i = 0, S_i = \\text{low})]\\). also want know “direct effect”–e.g. happens directly treated disregard spillovers. defined potential outcomes experiment reveal, since one treated low-saturation constituencies: \\(\\mathbb{E}[Y_i(Z_i = 1, S_i = \\text{low})-Y_i(Z_i = 0, S_i = \\text{low})]\\).Data strategy: assign entire networks low (0%) high (75%) saturation. randomize individuals within groups treatment control proportions dictated saturation. Thus, saturation cluster-randomized, whereas treatment block-randomized.Data strategy: assign entire networks low (0%) high (75%) saturation. randomize individuals within groups treatment control proportions dictated saturation. Thus, saturation cluster-randomized, whereas treatment block-randomized.Answer strategy: weight individual inverse probability find condition ’re . estimator conditions saturation effect direct treatment effect. Note standard errors clustered network level account clustering saturation assignment.Answer strategy: weight individual inverse probability find condition ’re . estimator conditions saturation effect direct treatment effect. Note standard errors clustered network level account clustering saturation assignment.","code":"\ndesign <- \n  declare_population(network = add_level(N = 4), unit = add_level(N = 4, U = rnorm(N))) +\n  declare_assignment(assignment_variable = \"S\", clusters = network, conditions = c(\"low\",\"high\")) +\n  declare_assignment(prob = .25, blocks = network, assignment_variable = \"Z_S_low\") +\n  declare_assignment(prob = .75, blocks = network, assignment_variable = \"Z_S_high\") +\n  declare_step(spillover_low = ave(Z_S_low, network, FUN = mean), \n               spillover_high = ave(Z_S_high, network, FUN = mean),handler = fabricate)  +\n  declare_potential_outcomes(Y ~ Z + spillover_low * (S == \"low\") + Z * spillover_high * (S == \"high\") + U,\n                             conditions = list(Z = c(0,1), S = c(\"low\",\"high\"))) +\n  declare_estimand(ate_saturation = mean(Y_Z_0_S_high - Y_Z_0_S_low),\n                   ate_no_spill = mean(Y_Z_1_S_low - Y_Z_0_S_low)) +\n  declare_reveal(Z,S) +\n  declare_step(ipw = 1 / (S_cond_prob * (Z_S_low_cond_prob * (S == \"low\") + Z_S_high_cond_prob * (S == \"high\"))),\n    handler = fabricate) +\n  declare_reveal(Y,c(Z, S)) +\ndeclare_estimator(Y ~ Z * S, clusters = network, model = lm_robust, term = c(\"Z\", \"Shigh\"), \n                  estimand = c(\"ate_no_spill\", \"ate_saturation\"),weight = ipw)"},{"path":"experimental-designs-for-causal-inference.html","id":"dag-20","chapter":"17 Experimental designs for causal inference","heading":"17.6.2 Dag","text":"","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"exercises-6","chapter":"17 Experimental designs for causal inference","heading":"17.6.3 Exercises","text":"Increase saturation “low” condition 0% 25% diagnose design. Notice changes mean saturation direct effect estimands. Now set saturations equal 25%. Explain two estimands change.Increase saturation “low” condition 0% 25% diagnose design. Notice changes mean saturation direct effect estimands. Now set saturations equal 25%. Explain two estimands change.Setting saturations back original values, remove ipw estimator. happens bias? Explain answer.Setting saturations back original values, remove ipw estimator. happens bias? Explain answer.potential outcomes declaration Y, add interaction high saturation condition Z, diagnose design saturations set original values 0 .75.\nestimate ATE treatment biased whereas estimate saturation effect ?\ncan researcher simply estimate interaction design?\nNow increase low saturation 25% add interaction Z S estimator. make design unbiased ?\npotential outcomes declaration Y, add interaction high saturation condition Z, diagnose design saturations set original values 0 .75.estimate ATE treatment biased whereas estimate saturation effect ?can researcher simply estimate interaction design?Now increase low saturation 25% add interaction Z S estimator. make design unbiased ?","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"online-appendix-applied-example-2","chapter":"17 Experimental designs for causal inference","heading":"17.6.4 Online Appendix Applied Example","text":"Asunka et al. (2019), example, wanted know presence election monitors ballot stations displace violence fraud ballot stations. randomized constituencies low, medium, high levels saturation, randomized ballot stations election monitoring low, medium, high concentrations, depending randomized saturation. original study, authors include zero-saturation condition. , declare simplified version design zero-saturation condition included.Main points develop:Randomized saturation great get model right. Though, show IPW reduces power detect main effect, especially ’s spillover.Randomized saturation assumes model may wrong. particular, spillovers restricted containers. might correct.","code":""},{"path":"experimental-designs-for-causal-inference.html","id":"design-declaration-4","chapter":"17 Experimental designs for causal inference","heading":"17.6.5 Design Declaration","text":"Model:\nPotential outcomes defined terms S—saturation—Z—whether ballot station treated. model spillovers two ways. first, amount spillover affects unit determined many units network treated. second, amount spillover unit receives determined whether unit’s geographic neighbor treated, irrespective whether share network.Model:\nPotential outcomes defined terms S—saturation—Z—whether ballot station treated. model spillovers two ways. first, amount spillover affects unit determined many units network treated. second, amount spillover unit receives determined whether unit’s geographic neighbor treated, irrespective whether share network.Inquiry:\nwant know effect high medium levels saturation versus low saturation control: \\(\\mathbb{E}[Y_i(Z_i = 0, S_i = \\text{high})-Y_i(Z_i = 0, S_i = \\text{low})]\\) \\(\\mathbb{E}[Y_i(Z_i = 0, S_i = \\text{medium})-Y_i(Z_i = 0, S_i = \\text{low})]\\). also want know “direct effect”–e.g. happens directly treated disregard spillovers. defined potential outcomes experiment reveal, since one treated low-saturation constituencies: \\(\\mathbb{E}[Y_i(Z_i = 1, S_i = \\text{low})-Y_i(Z_i = 0, S_i = \\text{low})]\\).Inquiry:\nwant know effect high medium levels saturation versus low saturation control: \\(\\mathbb{E}[Y_i(Z_i = 0, S_i = \\text{high})-Y_i(Z_i = 0, S_i = \\text{low})]\\) \\(\\mathbb{E}[Y_i(Z_i = 0, S_i = \\text{medium})-Y_i(Z_i = 0, S_i = \\text{low})]\\). also want know “direct effect”–e.g. happens directly treated disregard spillovers. defined potential outcomes experiment reveal, since one treated low-saturation constituencies: \\(\\mathbb{E}[Y_i(Z_i = 1, S_i = \\text{low})-Y_i(Z_i = 0, S_i = \\text{low})]\\).Data strategy: assign entire groups individual ballot stations one three saturations: low (0%), medium (50%), high (75%). randomize individuals within groups treatment control proportions dictated saturation. Thus, saturation cluster-randomized, whereas treatment block-randomized.Data strategy: assign entire groups individual ballot stations one three saturations: low (0%), medium (50%), high (75%). randomize individuals within groups treatment control proportions dictated saturation. Thus, saturation cluster-randomized, whereas treatment block-randomized.Answer strategy: weight individual inverse probability find condition ’re . estimate spillovers, run one regression comparing high one regression comparing medium low saturation control units. estimate direct effect, run regression outcome treatment indicatior full sample, controlling saturation.Answer strategy: weight individual inverse probability find condition ’re . estimate spillovers, run one regression comparing high one regression comparing medium low saturation control units. estimate direct effect, run regression outcome treatment indicatior full sample, controlling saturation.’s hypothetical country looks like:Let’s diagnoseOur diagnosis shows design pretty great job, model spillovers:’s particularly nice, since ’re able estimate direct effect (whose constitutive POs never observe) partialling spillovers.’s particularly nice, since ’re able estimate direct effect (whose constitutive POs never observe) partialling spillovers.Show : power tradeoffs main effects versus spillovers, terms \nproportion sample allocated “low” versus conditionsShow : power tradeoffs main effects versus spillovers, terms \nproportion sample allocated “low” versus conditionsand also terms IPW (equivalent sample size everyone .5 condition)also terms IPW (equivalent sample size everyone .5 condition)Now, consider model spillovers fraud displaced latitudinally,\none neighbor next. Say, roads traveling north \nfraudsters disregard boundaries (reality, unlikely ).next-neighbor spillovers ignore boundaries, estimator biased ","code":"\nN_individuals <- 60\nN_groups <- 15\nG_per_saturation <- c(5,5,5)\n\ndesign <- \n  declare_population(N = N_individuals, X = 1:N, U = rnorm(N), G = ntile(X, N_groups)) + \n  declare_assignment(assignment_variable = \"S\", \n                     clusters = G, \n                     conditions = c(\"low\",\"med\",\"high\"),\n                     m_each = G_per_saturation) +\n  declare_assignment(prob = 0,\n                     blocks = G,\n                     assignment_variable = \"Z_S_low\") +\n  declare_assignment(prob = .5,\n                     blocks = G,\n                     assignment_variable = \"Z_S_med\") +\n  declare_assignment(prob = .75,\n                     blocks = G,\n                     assignment_variable = \"Z_S_high\") +\n  declare_step(\n    spillover_low = ave(Z_S_low, G, FUN = sum) * .1,\n    spillover_med = ave(Z_S_med, G, FUN = sum) * .1,\n    spillover_high = ave(Z_S_high, G, FUN = sum) * .1,\n    handler = fabricate,\n    label = \"spillover\")  +\n  declare_potential_outcomes(\n    Y ~ Z * -.20 + U + \n      spillover_low * (S == \"low\") + \n      spillover_med * (S == \"med\") + \n      spillover_high * (S == \"high\"),\n    conditions = list(Z = c(0,1), S = c(\"low\",\"med\",\"high\"))) +\n  declare_estimand(high = mean(Y_Z_0_S_high - Y_Z_0_S_low),\n                   med =  mean(Y_Z_0_S_med - Y_Z_0_S_low), \n                   ate_no_spill = mean(Y_Z_1_S_low - Y_Z_0_S_low)) +\n  declare_reveal(Z,S) +\n  declare_step(\n    w = 1 / (S_cond_prob * (Z_S_low_cond_prob * (S == \"low\") + \n                   Z_S_med_cond_prob * (S == \"med\") + \n                   Z_S_high_cond_prob * (S == \"high\"))),\n    handler = fabricate) +\n  declare_reveal(Y,c(Z, S)) +\n  declare_estimator(model = lm_robust, \n                    formula = Y ~ S,\n                    subset = Z == 0 & S %in% c(\"high\",\"low\"), \n                    estimand = \"high\",\n                    weights = w,\n                    label = \"high vs low\") +\n  declare_estimator(model = lm_robust, \n                    formula = Y ~ S,\n                    subset = Z == 0 & S %in% c(\"med\",\"low\"), \n                    weights = w,\n                    estimand = \"med\",\n                    label = \"med vs low\") +\n  declare_estimator(model = lm_robust, \n                    formula = Y ~ Z + S,\n                    term = \"Z\", \n                    weights = w,\n                    estimand = \"ate_no_spill\",\n                    label = \"main effect\")\ndraw_data(design) %>% \n  ggplot(aes(x = 1, y = X, color = as.factor(G))) +\n  geom_point() +\n  scale_color_discrete(\"Ballot station\") +\n  scale_y_continuous(\"Latitude\") +\n  scale_x_continuous(\"Longitude\") +\n  geom_hline(yintercept = seq(1,N_individuals,by = N_individuals / N_groups) - .5)\ndiagnosis <- diagnose_design(design, sims = sims)\ndiagnosis %>% reshape_diagnosis() %>% kable()\ndistal_design <- replace_step(design = design, step = \"spillover\",\n                              new_step = declare_step(next_neighbor = c(N,1:(N-1)),\n                                                      spillover_low = Z_S_low[next_neighbor],\n                                                      spillover_med = Z_S_med[next_neighbor],\n                                                      spillover_high = Z_S_high[next_neighbor],\n                                                      handler = fabricate) )\ndistal_diagnosis <- diagnose_design(distal_design, sims = sims)\ndistal_diagnosis %>% reshape_diagnosis() %>% kable()"},{"path":"multi-study-designs.html","id":"multi-study-designs","chapter":"18 Multi-study designs","heading":"18 Multi-study designs","text":"section introduction","code":""},{"path":"multi-study-designs.html","id":"papers-with-multiple-studies","chapter":"18 Multi-study designs","heading":"18.1 Papers with multiple studies","text":"many research projects, seek evaluate multiple observable implications single theory. [Examples: Psychology; APSR articles experiment plus observational work; replication efforts.]\ncases, single piece evidence constitute sufficient evidence validate theory whole.\nRather, believe theory multiple pieces evidence support .Conventions around constitutes convincing pattern evidence vary. researchers believe theory unless piece evidence support statistically significant.\n\n\nLess stringent approaches simply seek evidence “consistent” theory, observation effects signed predicted direction., declare \\(N\\)-study design, examine consequences two different approaches evaluating theory light multiple studies. show , multiple observable implications generated process, conditioning significance can lead one strongly astray. Generally speaking, looking sign effects probative much less prone false negatives. small numbers studies, however, risks false positives high.","code":""},{"path":"multi-study-designs.html","id":"design-declaration-5","chapter":"18 Multi-study designs","heading":"18.1.1 Design Declaration","text":"Model: declare \\(N\\) populations, governed \ndata-generating process: X exogenous standard normally-distributed,\nM standard-normally distributed correlated X rho, Y function main effect X well interaction X M, size sign direct interactive effects determined tau gamma, respectively.Model: declare \\(N\\) populations, governed \ndata-generating process: X exogenous standard normally-distributed,\nM standard-normally distributed correlated X rho, Y function main effect X well interaction X M, size sign direct interactive effects determined tau gamma, respectively.Inquiry: want know, global sense, theory “right.”\n, means effect X Y positive increasing M, X affects M.\ntau positive, theory correct. zero negative, theory incorrect.Inquiry: want know, global sense, theory “right.”\n, means effect X Y positive increasing M, X affects M.\ntau positive, theory correct. zero negative, theory incorrect.Data strategy: conduct collect independent datasets \\(N\\) datasets size n. example , conduct three studies, assuming observe X Y first, M X second, Y, M, X, third.Data strategy: conduct collect independent datasets \\(N\\) datasets size n. example , conduct three studies, assuming observe X Y first, M X second, Y, M, X, third.Answer strategy: Using linear regression, estimate bivariate correlation X Y study 1, bivariate correlation X M study 2, interaction X M Y study 3.Answer strategy: Using linear regression, estimate bivariate correlation X Y study 1, bivariate correlation X M study 2, interaction X M Y study 3.","code":"\nn1 <- 100\nn2 <- 100\nn3 <- 100\nrho <- .5\ngamma <- tau <- .2\n\ngenerate_study_sample <- function(n, rho, tau, gamma, data){\n  fabricate(N = n, X = rnorm(N), M = rnorm(N, X * rho, sqrt(1 - rho^2)), \n            U = rnorm(N), Y = tau * X + gamma * M * X + U)\n}\n\nthree_study_design <- \n  # Study 1 -- Bivariate correlation between X and Y\n  declare_population(n = n1, tau = tau, gamma = gamma, rho = rho, handler = generate_study_sample) +\n  declare_estimator(Y ~ X, term = \"X\", model = lm_robust, label = \"Study 1\") +\n  # Study 2 -- Bivariate correlation between M and X\n  declare_population(n = n2, tau = tau, gamma = gamma, rho = rho, handler = generate_study_sample) +\n  declare_estimator(M ~ X, term = \"X\", model = lm_robust, label = \"Study 2\") +\n  # Study 3 -- Interaction in X and M \n  declare_population(n = n3, tau = tau, gamma = gamma, rho = rho, handler = generate_study_sample) +\n  declare_estimator(Y ~ X + M + X:M, term = \"X:M\", model = lm_robust, label = \"Study 3\") "},{"path":"multi-study-designs.html","id":"dag-21","chapter":"18 Multi-study designs","heading":"18.1.2 DAG","text":"","code":"\nggdd_df <- make_dag_df(dag, nodes, design2)\n\nbase_dag_plot %+% ggdd_df\nggdd_df <- make_dag_df(dag, nodes, design3)\n\nbase_dag_plot %+% ggdd_df"},{"path":"multi-study-designs.html","id":"takeaways-4","chapter":"18 Multi-study designs","heading":"18.1.3 Takeaways","text":"Let us compare performance “significant” versus “signed”\napproaches theory confirmation theory “correct” (tau gamma positive),\nversus “incorrect” (parameters zero).\nfirst approach, theory deemed “supported” evidence effects\nsignificant. second, theory supported evidence signs effects positive.first three rows table, theory correct tau,gamma, rho positive, second three rows incorrect \nmain effect interaction zero. “power” column tells us \nproportion simulations effect significant \\(\\alpha = .05\\) level, “significant” column tells us proportion simulations \nstudies significant effects, “positive” column tells us often\nstudy found positively signed result, “positive column” tells\nus proportion simulations studies positively signed result.Notice , studies independent, probability \nsignificant equal product power:\nPr(studies significant) = Pr(Study 1 significant) \\(\\times\\)…\\(\\times\\) Pr(Study \\(N\\) significant). Thus, believe theory studies conducted test yield significant results, studies powered conventionally accepted level 80%, erroneously reject theory probability \\(.8^N\\). three, conventionally well-powered, randomized studies shooting right quantity interest, almost half cases right, think\nwrong.Notice , studies independent, probability \nsignificant equal product power:\nPr(studies significant) = Pr(Study 1 significant) \\(\\times\\)…\\(\\times\\) Pr(Study \\(N\\) significant). Thus, believe theory studies conducted test yield significant results, studies powered conventionally accepted level 80%, erroneously reject theory probability \\(.8^N\\). three, conventionally well-powered, randomized studies shooting right quantity interest, almost half cases right, think\nwrong.Furthermore, notice detrimental addition small study can metric, even gets important mechanism. soon condition inference theory correct significance observable implications, low-powered test can sharply increase risk false rejection.Furthermore, notice detrimental addition small study can metric, even gets important mechanism. soon condition inference theory correct significance observable implications, low-powered test can sharply increase risk false rejection.can say risk false positives? power individual studies : stated error rate $= $5%, studies every slightly anti-conservative. However, “significant” desideratum creates rejection rate high.can say risk false positives? power individual studies : stated error rate $= $5%, studies every slightly anti-conservative. However, “significant” desideratum creates rejection rate high.problems, though , alleviated disregard significance just look signs. theory right, good chance effects estimate positive: surmise theory correct roughly 94% time actually .problems, though , alleviated disregard significance just look signs. theory right, good chance effects estimate positive: surmise theory correct roughly 94% time actually .theory correct sense true effects zero, random error means positive half time negative half. Consequently, probability erroneously accepting theory based sign effects true underlying effects zero equal \\(.5^N\\). , means erroneously infer right relatively high rate 12% simulations.theory correct sense true effects zero, random error means positive half time negative half. Consequently, probability erroneously accepting theory based sign effects true underlying effects zero equal \\(.5^N\\). , means erroneously infer right relatively high rate 12% simulations.design , rates rejected accepted theories seemed\ndepend number studies considered. graph , look\ntwenty-nine different \\(N\\)-study designs, seek confirm theory\nreplicating evidence \\(N\\) times. first design made two studies,\nindependtly evaluating hypothesis \\(Y\\) positively correlated \\(X\\).\n, consider conditioning inference theory whether\nresults significant results positive affects error rates., see significance probative way look observable\ntheoretical implications. soon four studies,\nvirtually chance confirming even true theory metric.\nsee sort reverse multiple-comparisons problem: increasing number\ntests makes false rejections increasingly likely interested\njoint probability tests saying thing.\nUnless number studies small, whether produce significant results\nessentially yields information whether theory correct., see significance probative way look observable\ntheoretical implications. soon four studies,\nvirtually chance confirming even true theory metric.\nsee sort reverse multiple-comparisons problem: increasing number\ntests makes false rejections increasingly likely interested\njoint probability tests saying thing.\nUnless number studies small, whether produce significant results\nessentially yields information whether theory correct.contrast, looking signs can highly probative. \napplication, optimal number studies eight. point,\nvirtually chance erroneously inferring theory \ncorrect effects zero, theory correct \ngood chance (almost 75%) available evidence signed\naccordingly. number studies increases, probability\ndiscordant results, using unanimity signs judge whether\ntheory correct becomes increasingly unwise.contrast, looking signs can highly probative. \napplication, optimal number studies eight. point,\nvirtually chance erroneously inferring theory \ncorrect effects zero, theory correct \ngood chance (almost 75%) available evidence signed\naccordingly. number studies increases, probability\ndiscordant results, using unanimity signs judge whether\ntheory correct becomes increasingly unwise.","code":"\n# Simulate design\nsimulations <- simulate_design(three_study_design)\n# Simulate null design\nnull_three_study_design <- redesign(three_study_design, tau = 0, gamma = 0, rho = 0)\nnull_simulations <- simulate_design(null_three_study_design)"},{"path":"multi-study-designs.html","id":"multi-site-studies","chapter":"18 Multi-study designs","heading":"18.2 Multi-site studies","text":"","code":""},{"path":"multi-study-designs.html","id":"the-metaketa-design","chapter":"18 Multi-study designs","heading":"18.3 The Metaketa design","text":"worry \\(I_A\\) \\(I_B\\) etc fundamentally different inquiries different dags[faceted figure 5 different DAGs nodes different languages roman, greek, numeric, windings]. purpose theory tell us \\(\\)’s across context different, HARD research gone YEARS.metaketa design says WAIT – LET’S agree \\(MIDA_{MK}\\) implement 5 places, mutatis mutandis. , everyone “localizes” design things work slightly differently different places, try hard, can implement “important” treatment “important” outcome, even though details (like language treatment materials, specifics measurement strategies.) differ place place.Coordinating trials like metaketa design means causing teams implement designs otherwise . Collaboration can bring many benefits shared struggle, fundamentally, pulls designs towards common \\(MIDA_{MK}\\) design. strength pull given \\(\\delta\\) parameters.metaketa design success? (1) \\(MIDA_{MK}\\) design strong one correct \\(Y\\) \\(Z\\) relate. \\(MIDA_{MK}\\) design poor, however, studies failure. Instead learning \\(I_A\\) place \\(\\) learning \\(I_B\\) place \\(B\\), learn nothing anywhere.trouble standard approach (separate teams investigating phenomena later theory tries label instances thing) theory hard can never really sure \\(I_A\\) means thing \\(M_A\\) \\(I_B\\) means \\(M_B\\). metaketa design tries solve theoretical problem front end – let’s agree \\(M\\) mind (local variations), run study. approach works well \\(M\\) agree approximately correct, works worse standard approach ’re wrong.","code":""},{"path":"multi-study-designs.html","id":"unbiased-estimates-of-out-of-sample-sites-in-presence-of-heterogeneous-effects","chapter":"18 Multi-study designs","heading":"18.3.1 Unbiased estimates of out-of-sample sites in presence of heterogeneous effects","text":"starting point fixed budget ’re thinking two possible designs: (1) single large study one context (2) set five studies five different contexts intervention outcome measuresWhen heterogeneous effects, can get good predictions sample even average effects differ substantially (better multiple sites sites population different proportions subject types correlated het fx)Two notable features design:\n- must het fx work (otherwise estimates get biased toward zero overfitting het variables)\n- information covariate population sample (used proportion people het type)Findings:\n- two strategies unbiased\n- design five sites half RMSE one-site design. variation proportions types across sites.\n- interestingly poor coverage (anti-conservative) use single site design(contextual variation well, .e. effect differs across sites reasons captured het fx, coverage designs. keep point , seems like much don’t need contextual effects get different effects across sites, come different proportions types)","code":"\nmeta_re_estimator <- function(data){\n  site_estimates_df <- data %>% \n    group_by(site) %>% \n    do(tidy(lm_robust(Y ~ Z, data = .))) %>% \n    filter(term == \"Z\") %>% \n    ungroup \n  \n  meta_fit <- rma(estimate, std.error, data = site_estimates_df, method = \"REML\")\n  \n  with(meta_fit, tibble(\n    estimate = as.vector(beta), std.error = se, p.value = pval, conf.low = ci.lb, conf.high = ci.ub))\n}\n\npost_strat_estimator <- function(data, pr_types_population) {\n  if(length(unique(data$site)) > 1) {\n    fit <- lm_robust(Y ~ Z*as.factor(subject_type) + as.factor(site), data = data)\n    tidy(fit)\n  } else {\n    fit <- lm_robust(Y ~ Z*as.factor(subject_type), data = data)\n  }\n  \n  alpha <- .05\n  \n  lh_fit <- try({ linearHypothesis(\n    fit, \n    hypothesis.matrix = paste(paste(paste(pr_types_population[91:100][-1], \"*\", matchCoefs(fit, \"Z\"), sep = \"\"), collapse = \" + \"), \" = 0\"), \n    level = 1 - alpha) })\n  \n  if(!inherits(lh_fit, \"try-error\")) {\n    tibble(estimate = drop(attr(lh_fit, \"value\")), \n           std.error = sqrt(diag(attr(lh_fit, \"vcov\"))),\n           df = fit$df.residual, \n           statistic = estimate / std.error, \n           p.value = 2 * pt(abs(statistic), df, lower.tail = FALSE),\n           conf.low = estimate + std.error * qt(alpha / 2, df),\n           conf.high = estimate + std.error * qt(1 - alpha / 2, df))\n  } else {\n    tibble(error = TRUE)\n  }\n}\n# need to have biased sampling to get bias here\n# two kinds of populations, one in which the study type determines the subject types and you select on study type\n#   a second kind where study type determines study shock \n#   in second type if you adjust for subject type then you will be able to unbiased recover global\n\nmulti_site_designer <- function(\n  N_sites = 10,\n  n_study_sites = 5,\n  n_subjects_per_site = 1000,\n  feasible_effect = 0,\n  subject_type_effects = seq(from = -0.1, to = 0.1, length.out = 10),\n  pr_types = c( # rows are sites, columns are types\n    0.005, 0.005, 0.09, 0.15, 0.25, 0.1, 0, 0.1, 0.15, 0.15,\n    0.1, 0.15, 0.15, 0.15, 0.25, 0.005, 0, 0.1, 0.09, 0.005,\n    0.15, 0.15, 0.15, 0.005, 0.005, 0, 0.25, 0.09, 0.1, 0.1,\n    0, 0.15, 0.005, 0.09, 0.005, 0.15, 0.25, 0.1, 0.1, 0.15,\n    0.005, 0.1, 0.09, 0.25, 0.15, 0.15, 0.005, 0, 0.1, 0.15,\n    0.005, 0.15, 0.25, 0.1, 0, 0.1, 0.005, 0.15, 0.09, 0.15,\n    0.15, 0.15, 0.005, 0.25, 0.1, 0.15, 0.09, 0.005, 0.1, 0,\n    0.25, 0.1, 0.15, 0, 0.005, 0.15, 0.15, 0.1, 0.005, 0.09,\n    0.005, 0.1, 0.1, 0.15, 0, 0.25, 0.15, 0.09, 0.005, 0.15,\n    0.005, 0.09, 0.15, 0.1, 0, 0.1, 0.15, 0.005, 0.25, 0.15)\n) {\n  declare_population(\n    site = add_level(N = N_sites, feasible_site = sample(c(rep(1, 8), rep(0, 2)), N, replace = FALSE)),\n    subject_types = add_level(\n      N = 10,\n      subject_type = 1:10,\n      subject_type_effect = subject_type_effects,\n      type_proportion = pr_types,\n      N_subjects = ceiling(2500 * type_proportion)\n    ),\n    subjects = add_level(N = N_subjects, noise = rnorm(N))\n  ) + \n    declare_potential_outcomes(Y ~ Z * (0.1 + subject_type_effect + feasible_effect * feasible_site) + noise) +\n    declare_estimand(ATE_feasible = mean(Y_Z_1 - Y_Z_0), subset = feasible_site == FALSE) + # true effect for feasible sites\n    declare_sampling(clusters = site, strata = feasible_site, strata_n = c(0, n_study_sites)) + \n    declare_sampling(strata = site, n = n_subjects_per_site) + \n    declare_assignment(blocks = site, prob = 0.5) + \n    declare_estimand(study_site_ATE = mean(Y_Z_1 - Y_Z_0)) +\n    declare_estimator(handler = tidy_estimator(post_strat_estimator), pr_types_population = pr_types, label = \"post-strat\")\n}\n\nsingle_site_large_design <- multi_site_designer(n_study_sites = 1, n_subjects_per_site = 2500)\n\nsmall_study_five_sites <- multi_site_designer(n_study_sites = 5, n_subjects_per_site = 500)\nsimulations_small_large <- simulate_design(single_site_large_design, small_study_five_sites, sims = sims)\ndiagnosis_small_large <- diagnose_design(simulations_small_large %>% filter(!is.na(estimate) & !is.na(std.error) & !is.na(statistic) & !is.na(p.value) & !is.na(conf.low) & !is.na(conf.high)), bootstrap_sims = b_sims)\nkable(get_diagnosands(diagnosis_small_large))"},{"path":"multi-study-designs.html","id":"bayesian-estimation-can-improve-estimates-of-effects-for-sampled-sites","chapter":"18 Multi-study designs","heading":"18.3.2 Bayesian estimation can improve estimates of effects for sampled sites","text":"can improve site-level effect estimates analyzing simple Bayesian model shrinkage property, even Bayesian model wrong distribution effects populationthis point blog post; modify design can also make point, switching normal distribution uniform distribution fx distribution","code":"\nstan_model <- \" \ndata {\n  int<lower=0> J;         // number of sites \n  real y[J];              // estimated effects\n  real<lower=0> sigma[J]; // s.e. of effect estimates \n}\nparameters {\n  real mu; \n  real<lower=0> tau;\n  real eta[J];\n}\ntransformed parameters {\n  real theta[J];\n  real tau_sq = tau^2;\n  for (j in 1:J)\n    theta[j] = mu + tau * eta[j];\n}\nmodel {\n  target += normal_lpdf(eta | 0, 1);\n  target += normal_lpdf(y | theta, sigma);\n}\n\"\n\nstan_re_estimator <- function(data) {\n  site_estimates_df <- data %>% \n    group_by(site) %>% \n    do(tidy(lm_robust(Y ~ Z, data = .))) %>% \n    filter(term == \"Z\") %>% \n    ungroup \n  \n  J      <- nrow(site_estimates_df)\n  df     <- list(J = J, y = site_estimates_df$estimate, sigma = site_estimates_df$std.error)\n  fit    <- stan(model_code = stan_model, data = site_estimates_df)\n  fit_sm <- summary(fit)$summary\n  data.frame(estimate = fit_sm[,1][c(\"mu\", \"tau\", \"theta[1]\", \"prob_pos\")])\n}\n\nbayes_estimator <- declare_estimator(handler = stan_re_estimator)"},{"path":"multi-study-designs.html","id":"when-things-break-down-confounded-sampling","chapter":"18 Multi-study designs","heading":"18.3.3 when things break down: confounded sampling","text":"none designs work ’re trying make predictions sites systematically different, .e. population sampling framethe design set include several sites researchers feasibly set experiments. original design, effects depend whether sites feasible experiment. effects vary, systematic differences target sites. differences might come three sources: mean effect size differs places sampled vs sampled; individual-level het fx sizes systematically differ places sampled study vs others; covariate profiles exist sites outside sampling frame. introduce effects first way show substantial bias.points decided abandon keep simple:\n- tradeoff: context-specific interventions comparability intervention effects\n- tradeoff: comparability fidelity context outcome measurement","code":"\nsmall_study_five_sites_feasible_effects <- multi_site_designer(n_study_sites = 5, n_subjects_per_site = 500, feasible_effect  = -0.25)\nsimulations_feasible_effects <- simulate_design(small_study_five_sites_feasible_effects, sims = sims)\ndiagnosis_feasible_effects <- diagnose_design(simulations_feasible_effects %>% filter(!is.na(estimate) & !is.na(std.error) & !is.na(statistic) & !is.na(p.value) & !is.na(conf.low) & !is.na(conf.high)), bootstrap_sims = b_sims)\nkable(get_diagnosands(diagnosis_feasible_effects))"},{"path":"part-iii-exercises.html","id":"part-iii-exercises","chapter":"19 Part III Exercises","heading":"19 Part III Exercises","text":"Measuring sensitive traits direct questions may biased due sensitivity bias: subjects may perceive (rightly wrongly) someone enumerator, neighbors, authorities impose costs provide dispreferred answer. sensitive settings, direct questions provide comparatively precise answer, biased. One alternative direct questions list experiment (see section XXX). assumptions, list experiments unbiased, can quite imprecise. Thus, choice list experiments direct questions includes bias-variance tradeoff.Following formulas given _____, variance direct question estimator following, \\(\\pi^*\\) true prevalence rate \\(\\delta\\) sensitivity bias.\\[\n\\frac{1}{N - 1} \\bigg\\{ \\pi^* (1 - \\pi^*) + \\delta (1 - \\delta) - 2(\\delta - \\pi^*\\delta) \\bigg\\}\n\\]variance list experiment given expression, \\(\\mathbb{V}(Y_i(0)\\) variance control item count \\(\\mathrm{cov}(Y_i(0), D_i^*)\\) covariance control item count densitive trait.\\[\n\\frac{1}{N-1} \\bigg\\{ \\pi^*(1-\\pi^*) + 4 \\mathbb{V}(Y_i(0))  + 4 \\mathrm{cov}(Y_i(0), D_i^*) \\bigg\\}\n\\]goal compare direct question list experiment designs respect RMSE diagnosand. Recall RMSE equals square root variance plus bias squared: \\(RMSE = \\sqrt{Variance + Bias^2}\\). Assume following design parameters: \\(\\delta = 0.10\\), \\(\\pi^* = 0.50\\), \\(\\mathbb{V}(Y_i(0) = 0.075\\), \\(\\mathrm{cov}(Y_i(0), D_i^*) = 0.025\\).RMSE direct question \\(N\\) = 100?RMSE list experiment \\(N\\) = 100?Make figure \\(N\\) horizontal axis RMSE vertical axis. Plot RMSE designs range sample sizes 100 2000. Hint: ’ll need write function design takes \\(N\\) input returns RMSE. can get started filling starter function: direct_rmse <- function(N){ # stuff goes }large sample size need list experiment preferred direct question RMSE grounds?Comment answer (d) change \\(\\delta\\) equal 0.2? implications choice list experiments direct questions?","code":""},{"path":"research-design-lifecycle.html","id":"research-design-lifecycle","chapter":"20 Research Design Lifecycle","heading":"20 Research Design Lifecycle","text":"Empirical results think now know result conducting study. Research design think know . set reasons study means think means – research design – central importance throughout research design lifecycle. process begins research idea continues many phases implementation, writing publishing piece, beyond, integration acquired knowledge collective scientific understanding world. stage process, research design – specification \\(M\\), \\(\\), \\(D\\), \\(\\) – shapes choices well others learn work.part book works discrete stages lifecycle. presented linear fashion, stages intertwined common connection MIDA. example, show many disputes among scholars proper interpretation come differing understandings part M, , D, . preanalysis plan sufficiently precise beliefs features design, disputes can specified precisely, better resolve . also explore reason disputes preanalysis plan.every research project explicitly feature stages. example, prospective research designs like experiments surveys often included pilot studies learn important unknown features \\(M\\) implementing full studies. Retrospective studies, like textual analyses speeches delivered Parliament, might .","code":""},{"path":"research-design-lifecycle.html","id":"p4planning","chapter":"20 Research Design Lifecycle","heading":"20.1 Planning","text":"many research communities, becoming standard practice publicly register pre-analysis plan (PAP) prior implementation data strategy. PAPs serve many functions, importantly, clarify design choices made data collection made afterward. Sometimes – perhaps every time! – conduct research study, aspects \\(M\\), \\(\\), \\(D\\), \\(\\) shift along way. concern shift ways invalidate apparent conclusions study. example, “\\(p\\)-hacking” shady practice trying many regression specifications \\(p\\)-value associated important test attains statistical significance. PAPs protect researchers communicating skeptics design decisions made: regression specification detailed PAP posted data collected, test result \\(p\\)-hack.PAPs sometimes misinterpreted binding commitment report pre-registered analyses nothing . view unrealistic unnecessarily rigid. think researchers report pre-registered analyses somewhere (see Section 20.7 “populated PAPs”), study write-ups inevitably deviate way PAP – ’s good thing. Researchers learn conducting research; learning can reflected finalized answer strategy. Even binding, main point publicly posting pre-analysis plan communicate stage research process choices made.hunch main consequence actually writing PAP improvement research design . Just like research design declaration forces us think details model, inquiry, data strategy, answer strategy, describing choices publicly-posted document surely causes deeper reflection design. way, main audience PAP study authors .belongs PAP? Recommendations set decisions specified PAP remain remarkably unclear inconsistent across research communities. PAP templates checklists proliferating, number items suggest range nine sixty. PAPs becoming longer detailed, American Economic Association (AEA) Evidence Governance Politics (EGAP) study registries reaching hundreds pages researchers seek ever comprehensive. registries emphasize registration hypotheses tested, others emphasize registration tests used. read many PAPs found hard assess whether detailed plans actually contain key analytically-relevant details.view , minimally, PAP include design declaration. good deal discussion goes PAP centers answer strategy \\(\\) – estimator use, covariates condition , subsets data include. course also need know details \\(D\\) – units sampled, treatments assigned, outcomes measured. need details assess properties design also gauge whether principles analyze randomize followed. need know \\(\\) need know target inference.26 need enough \\(M\\) describe \\(\\) sufficient detail. short, design declaration belongs PAP, design declaration specifies analytically-relevent design decisions.addition design declaration, PAP include mock analyses conducted simulated data. design declaration done formally code, creating simulated data resemble eventual realized data quite straightforward. think researchers run answer strategy mock data, creating mock figures tables eventually made real data. experience, step really causes researchers think hard aspects design.Strictly speaking, preanalysis plans include design declaration, require design diagnosis. since design finally settled design implemented usually chosen result diagnosis, can informative describe, preanalysis plan, reasons particular design chosen. reason, PAP might include estimates diagnosands like power, RMSE, bias. researcher writes PAP power detect small effect large, study comes back null, eventual writeup can much credibly rule “low power” explanation null. Moreover, ex ante design diagnosis communicates assumptions thought design good one ran study. reasons often basis convince skeptics value design, writing results known increases faith put .","code":""},{"path":"research-design-lifecycle.html","id":"example-20","chapter":"20 Research Design Lifecycle","heading":"20.1.1 Example","text":"section, provide example supplement PAP design declaration. follow actual PAP Bonilla Tillery (2020), posted Predicted registry : https://aspredicted.org/q56qq.pdf. goal study estimate causal effects alternative framings Black Lives Matter (BLM) movement support movement among Black Americans overall well among subsets Black community. study authors models research transparency: prominently link PAP published article, conduct non-preregistered analyses except requested review process, replication archive includes materials required confirm analyses, able reproduce exactly minimal effort. goal section show design declaration can supplement complement existing planning practices.","code":""},{"path":"research-design-lifecycle.html","id":"model-1","chapter":"20 Research Design Lifecycle","heading":"20.1.1.1 Model","text":"authors write PAP:hypothesize : H1: Black Nationalist frames BLM movement increase perceived effectiveness BLM among African American test subjects. H2: Feminist frames BLM movement increase perceived effectiveness BLM among African American women, decrease perceived effectiveness male subjects. H3: LGBTQ Intersectional frames BLM movement effect (demobilizing effect) perceived effectiveness BLM African American subjects.hypotheses reflect model coalition politics emphasizes tensions induced overlapping identities. Framing BLM movement feminist pro-LGBTQ may increase support among Black women Black LGBTQ identifiers, increase may come expense support among Black men Blacks identify LGBTQ. Similarly, model predicts subjects stronger attachment Black identity larger response Black nationalist framing BLM weaker attachements.model also includes beliefs distributions gender, LGBTQ status, Black identity strength. Data strategy, Black identity measured standard linked fate measure. background characteristics may correlated support BLM include age, religiosity, income, eductation, familiarity movement, included model well.focus study causal effects nationalism, feminism, intersectional frames relative general description Black Lives Matter movement. Model beliefs treatment effect heterogeneity embedded declare_potential_outcomes call. effect nationalism treatment hypothesized stronger, greater subjects’ sense linked fate; effect feminism treatment negative men postive women; effect intersectionality treatment positive LGBTQ identifies, negative non-identifiers.","code":"\nmodel <- \n  declare_population(\n    N = 800,\n    female = rbinom(N, 1, prob = 0.51),\n    lgbtq = rbinom(N, 1, prob = 0.05),\n    linked_fate = sample(1:5, N, replace = TRUE, prob = c(0.05, 0.05, 0.15, 0.25, 0.5)),\n    age = sample(18:80, N, replace = TRUE),\n    religiosity = sample(1:6, N, replace = TRUE),\n    income = sample(1:12, N, replace = TRUE),\n    college = rbinom(N, 1, prob = 0.5),\n    blm_familiarity = sample(1:4, N, replace = TRUE),\n    U = runif(N),\n    blm_support_latent = rescale(\n      U + 0.1 * blm_familiarity + 0.45 * linked_fate + 0.001 * age + \n        0.25 * lgbtq + 0.01 * income + 0.1 * college + -0.1 * religiosity)\n  ) + \n  declare_potential_outcomes(\n    blm_support_Z_general = \n      likert_cut(blm_support_latent),\n    blm_support_Z_nationalism = \n      likert_cut(blm_support_latent + 0.01 + 0.01 * linked_fate + 0.01 * blm_familiarity),\n    blm_support_Z_feminism = \n      likert_cut(blm_support_latent - 0.02 + 0.07 * female + 0.01 * blm_familiarity),\n    blm_support_Z_intersectional = \n      likert_cut(blm_support_latent  - 0.05 + 0.15 * lgbtq + 0.01 * blm_familiarity)\n  )"},{"path":"research-design-lifecycle.html","id":"inquiry-1","chapter":"20 Research Design Lifecycle","heading":"20.1.1.2 Inquiry","text":"inquiries study naturally include average affects three treatments relative “general” framing, well differences average effects subgroups. describing planned analyses, authors write:also look differences responses indicating pre-treatment familiarity BLM (4-Extensive knowledge 1-Never heard BLM), gender (particularly Feminist treatment), linked fate (particularly Nationalist treatment), LGBT+ affiliation (particularly LGBT+ treatment), though necessarily expecting moderations strong effect samples may lack adequate representation.code , specify treatment effect changes corresponding covariate \\(X\\) \\(\\frac{cov(\\tau_i, X)}{X}\\), identical difference--difference binary covariates (female lgbtq) slope best linear predictor effect changes range linked_fate, blm_familiarity treating quasi-continuous .","code":"\ninquiry <-  \n  declare_estimands(\n    \n    # Average effects\n    ATE_nationalism = mean(blm_support_Z_nationalism - blm_support_Z_general),\n    ATE_feminism = mean(blm_support_Z_feminism - blm_support_Z_general),\n    ATE_intersectional = mean(blm_support_Z_intersectional - blm_support_Z_general),\n    \n    # Overall heterogeneity w.r.t. blm_familiarity\n    DID_nationalism_familiarity = \n      cov(blm_support_Z_nationalism - blm_support_Z_general, blm_familiarity)/var(blm_familiarity),\n    DID_feminism_familiarity = \n      cov(blm_support_Z_feminism - blm_support_Z_general, blm_familiarity)/var(blm_familiarity),\n    DID_intersectional_familiarity = \n      cov(blm_support_Z_intersectional - blm_support_Z_general, blm_familiarity)/var(blm_familiarity),\n    \n    # Treatment-specific heterogeneity\n    DID_nationalism_linked_fate = \n      cov(blm_support_Z_nationalism - blm_support_Z_general, linked_fate)/var(linked_fate),\n    DID_feminism_gender = \n      cov(blm_support_Z_feminism - blm_support_Z_general, female)/var(female),\n    DID_intersectional_lgbtq = \n      cov(blm_support_Z_intersectional - blm_support_Z_general, lgbtq)/var(lgbtq)\n  )"},{"path":"research-design-lifecycle.html","id":"data-strategy-1","chapter":"20 Research Design Lifecycle","heading":"20.1.1.3 Data strategy","text":"subjects study 800 Black Americans recruited survey firm Qualtrics using quota sampling procedure. elide sampling step declaration – 800 subjects described declare_population call. reason , common practice analysis survey experiments convenience samples, authors formally extrapolate data make generalizations population Black Americans. inquiries study sample average effects. authors used different sampling strategy, using random sampling random digit dialing example, defined population sampling random sampling procedure.subjects’ background characteristics measured, assigned one four treatment conditions. Since survey conducted Qualtrics, assume authors used built-randomization tools, typically use simple (Bernoulli) random assignment.","code":"\ndata_strategy <- \n  declare_assignment(conditions = c(\"general\", \"nationalism\", \"feminism\", \"intersectional\"), simple = TRUE) + \n  declare_reveal(blm_support, Z) "},{"path":"research-design-lifecycle.html","id":"answer-strategy-1","chapter":"20 Research Design Lifecycle","heading":"20.1.1.4 Answer strategy","text":"authors write:run OLS regression predicting support , effectiveness , trust BLM treatment condition. […] also look differences responses indicating pre-treatment familiarity BLM (4-Extensive knowledge 1-Never heard BLM), gender (particularly Feminist treatment), linked fate (particularly Nationalist treatment), LGBT+ affiliation (particularly LGBT+ treatment), though necessarily expecting moderations strong effect samples may lack adequate representation. plan conduct analyses without controls. check group balance, may also run OLS analyses demographic controls (age, linked fate, gender, sexual orientation, religiosity, income, education, ethnic multi-racial backgrounds), report differences OLS results.DeclareDesign, corresponds five estimators, two shooting ATEs three shooting differences--CATEs. use OLS five – majority code bookkeeping ensure match right regression coefficient appropriate estimand.","code":"\nanswer_strategy <-\n  declare_estimator(\n    blm_support ~ Z,\n    term = c(\"Znationalism\", \"Zfeminism\", \"Zintersectional\"),\n    model = lm_robust,\n    estimand = c(\"ATE_nationalism\", \"ATE_feminism\", \"ATE_intersectional\"),\n    label = \"OLS\"\n  ) +\n  declare_estimator(\n    blm_support ~ Z + age + female + as.factor(linked_fate) + lgbtq,\n    term = c(\"Znationalism\", \"Zfeminism\", \"Zintersectional\"),\n    estimand = c(\"ATE_nationalism\", \"ATE_feminism\", \"ATE_intersectional\"),\n    model = lm_robust,\n    label = \"OLS with controls\"\n  ) +\n    declare_estimator(\n    blm_support ~ Z*blm_familiarity,\n    term = c(\"Znationalism:blm_familiarity\", \"Zfeminism:blm_familiarity\", \"Zintersectional:blm_familiarity\"),\n    model = lm_robust,\n    estimand = c(\"DID_nationalism_familiarity\", \"DID_feminism_familiarity\", \"DID_intersectional_familiarity\"),\n    label = \"DID_familiarity\"\n  ) +\n  declare_estimator(\n    blm_support ~ Z * linked_fate,\n    term = \"Zfeminism:linked_fate\",\n    model = lm_robust,\n    estimand = \"DID_nationalism_linked_fate\",\n    label = \"DID_nationalism_linked_fate\"\n  ) +\n  declare_estimator(\n    blm_support ~ Z * female,\n    term = \"Zfeminism:female\",\n    model = lm_robust,\n    estimand = \"DID_feminism_gender\",\n    label = \"DID_feminism_gender\"\n  ) +\n  declare_estimator(\n    blm_support ~ Z * lgbtq,\n    term = \"Zintersectional:lgbtq\",\n    model = lm_robust,\n    estimand = \"DID_intersectional_lgbtq\",\n    label = \"DID_intersectional_lgbtq\"\n  )"},{"path":"research-design-lifecycle.html","id":"mock-analysis","chapter":"20 Research Design Lifecycle","heading":"20.1.1.5 Mock analysis","text":"Putting together, can declare complete design draw mock data .\nTable 20.1: Mock analysis Bonilla Tillery design.\nTable ?? shows mock analysis average effects (estimated without covariate adjustment) well heterogeneous effects analyses respect quasicontinuous moderators.\nMock regression table Bonilla Tillery design.\n\nFigure 20.1: Mock coefficient plot Bonilla Tillery design.\n","code":"\ndesign <- model + inquiry + data_strategy + answer_strategy\nmock_data <- draw_data(design)"},{"path":"research-design-lifecycle.html","id":"design-diagnosis","chapter":"20 Research Design Lifecycle","heading":"20.1.1.6 Design diagnosis","text":"Finally, design diagnosis necessary component preanalysis plan, can useful show readers particular design chosen others. diagnosis shows design produces unbiased estimates estimands, better powered inquires others (assumptions effect size, original authors’). well-powered average effects, power increases include covariate controls. design probably small heterogeneous effect analyses, point directly conceded authors’ original PAP.\nTable 20.2: Design diagnosis Bonilla Tillery design.\nreading.Casey, Glennerster, Miguel (2012): early entryOlken (2015): halfway skepticalGreen Lin (2016): Standard operating proceduresChristensen Miguel (2018): reviewCoffman Niederle (2015): skeptical take (prefer replications).Humphreys, Sierra, Windt (2013): nonbindingMiguel et al. (2014): distinguishes disclosure PAPOfosu Posner (2020): apparently paps hinder publication?Rennie (2004)Zarin Tse (2008)Nosek et al. (2015)Findley et al. (2016) results-blind review","code":""},{"path":"research-design-lifecycle.html","id":"ethical-review","chapter":"20 Research Design Lifecycle","heading":"20.2 Ethical Review","text":"Ethical review institutional review boards (IRBs) required many countries, additional review required entities governing health specific types research. Social scientists also increasingly focused meeting ethical standards go beyond requirements national laws. Ethical appendices required journals describing protections human subjects many funding agencies now require defenses ethics research study funds disbursed.Common ethical principles include respect persons, beneficence (researchers must welfare participants mind designing research), informed consent participants, minimizing harm participants others.\nEthical considerations extend well beyond elements captured design declaration. instance declaration analytic relevant components design may tell little level care respect accorded subjects. Nevertheless think useful identify design relevant features can informative ethical judgements.","code":""},{"path":"research-design-lifecycle.html","id":"ethical-principles-as-diagnosands","chapter":"20 Research Design Lifecycle","heading":"20.2.1 Ethical principles as diagnosands","text":"Researchers can use declare-diagnose-redesign process help inform ethical judgements. particular, diagnostic-statistic defined relevant ethical criteria. example, design can scored based total cost participants, many participants harmed (.e., many retraumatized asked past experience violence), average level informed consent measured survey comprehension study goals, risks adverse events.Consider two examples.Costs. common concern measurement imposes cost subjects, wasting time. Subjects’ time valuable resource – often donate willingly scientific enterprise taking survey similar. Sometimes generosity repaid financial compensation time. Sometimes subjects unknowing participants research study obtaining informed consent distort behavior hinder ability researchers study .Risks just realizations. subtly, different realizations data data strategy may also differ ethical status. result, ethics study determined looking fact happened study — many participants many treated, many harmed, many raised complaints. Instead, ethical status project depends judgments potential harms potential participants: happen, happened.design diagnosed, diagnosands can constructed summarize level ethical encumberance across possible realizations design. first way ethical diagnosands can used determine whether study design meets set ethical thresholds. probability harm minimal enough? average level informed consent sufficient? Given characteristics vary across designs across realizations design, writing concretely measure ethical characteristic threshold design ethical can help structure thinking. (diagnoses threshold determinations can also shared ethical writeups design.)Among ethical designs, researchers must select single design implement. Often, ethical threshold met, select among feasible designs based research design criteria statistical power bias. Instead, continue assess ethical considerations alongside quality research design. Among ethical designs, still often tradeoffs much time asked subjects risk harm. select designs weight considerations (perhaps highly!) power designs. , can simply continue include diagnosands related ethical criteria diagnoses redesign studies.difficult challenge process order weigh ethical criteria research design criteria power cost, must put able measure two common scale. must able think value society research weigh risks participants others. Similarly, must able weigh precise estimates question ethical considerations also change based number units proportion treated among design features. moving forward research must implicitly weigh considerations. IRB applications, often directly asked weigh costs subjects benefits research subjects well society whole.–>–>Scholars increasingly calling reporting ethical considerations study design beyond IRB approval. declaring expectations ethical outcomes experiment terms diagnosands time participants devote study probability harm individuals, declared research design can input ethical reporting. Readers can review considered ethical outcomes design judge mitigation efforts undertook relation expectations. scholars proposed including ethical assessments preanalysis plans. Declarations ethical diagnosands natural complement preregistered assessments.readings.– history: belmont report. tuskegee experiment. stanford prison experiment.\n– laws IRBs: revised common rule, paper IRBs\n– guidelines econ, poli sci, soc, psych.\n– ethics experiments: macartan’s paper, dawn teele’s chapter; /B illusion;\n– ethics outcome data collection: https://link.springer.com/epdf/10.1007/s11133-020-09458-9?sharing_token=ZXNJXewvrZN0ouaL02wFDPe4RwlQNchNByi7wbcMAY4-EQXsmvNo8IbQcOueStzgc77hZU2onyyln_mVDwcUpdOR-pwRaPn3QvTGa3Im4FtNdYCpNVCfjs7pObv1rfyp-y7KtLBchLUMAHPMkKVeCSMwqZH6G5ldDK9hxy4NRoQ%3D\n– new ideas: jay lyall paper; lauren young paper","code":""},{"path":"research-design-lifecycle.html","id":"partners","chapter":"20 Research Design Lifecycle","heading":"20.3 Partners","text":"Partnering third-party organizations research — cooperating intervene world measure outcomes — increasingly common social sciences. Researchers seek produce (publish) scientific knowledge; work political parties, government agencies, non-profit organizations, businesses learn worked independently. groups consent working researchers order learn achieve organizational goals. example, government may want learn expand access healthcare corporation may want learn improve ad targeting.best case scenario, goals researchers partner organizations aligned. scientific question answered (inquiry) practical question organization cares , gains cooperation clear. research team gains access financial logistical capacity organization act world partner organziation gains access scientific expertise researchers. Finding good research partner almost always amounts finding organization common – least conflicting – goal. Understanding private goals partner researcher essential selecting research design amenable parties. Research design declaration diagnosis can help problem formalizing tradeoffs two sets goals.One common divergence partner researcher goals partner organizations often want learn, care primary mission. business settings, dynamic sometimes referred “learn versus earn” “exploration-exploitation” tradeoff. aid organization (donors) cares delivering program many people possible; learning whether programme intended effects outcomes interest obviously also important, resources spent evaluation resources spent progam delivery.Research design diagnosis can help navigate learning-tradeoff. One instance tradeoff proportion units receive treatment (e.g., medicine) represents rate “” also affects amount learning extreme units treated can (typically) learning effect treatment. tradeoff represented graph power study vs. proportion treated (top facet), utility partner (bottom facet) increasing proportion treated. researchers power cut-standard 80% threshold. partner also strict cut-: need treat least 2/3 sample order fulfill donor requirement.absence partners, researchers might simply ignore proportion treated axis select design highest power. partner organization, researcher might use graph conversation partner jointly select design highest power sufficiently high proportion treated meet partner’s needs. represented “zone agreement” gray: region, design least 80% power least 2/3 sample treated. Deciding within region involves trade-power (decreasing proportion treated ) utility partner (increasing proportion treated). diagnosis surfaces zone agreement clarifies choice designs zone.\nFigure 20.2: Navigating research partnerships.\nChoosing proportion treated one example integrating partner constraints research designs generate feasible designs. second common problem set units must treated, ethical political reasons (e.g., home district government partner must receive treatment), must treated. constraints discovered treatment assignment, lead noncompliance, may substantially complicate analysis experiment even prevent providing answer original inquiry. Considerable thought given avoiding type noncompliance. Gerber Green recommend, randomizing treatment, exploring possible treatment assignments partner organization using exercise elicit set units must treated. King et al. (cite) describe “politically-robust” design, uses pair-matched block randomization unit dropped due political constraints pair dropped study.27Design diagnosis can help circumstances providing mechanism specify possible patterns noncompliance diagnosing alternative designs mitigate negative consequences. addition, can used communicate partners consequences noncompliance research help make better decisions together avoid noncompliance treatment randomized research progress.best advice working partners involve design declaration diagnosis process. can develop intuitions means, variances, covariances important variables measured? Ask partner best guesses, may far educated . experimental studies, solicit partner’s beliefs magnitude treatment effect outcome variable, subgroup subgroup possible. specific – ask think average control group average treatment group. Sorting beliefs quickly sharpens discussion key design details. Share design diagnoses mock analyses study launched order build consensus around goals study.Sometimes partnership simply work . Indeed partnerships never even initiated researcher organizational goals far apart. find setting partnership much violence research design, find way walk away project.","code":""},{"path":"research-design-lifecycle.html","id":"funding","chapter":"20 Research Design Lifecycle","heading":"20.4 Funding","text":"design, tradeoffs diagnosands quality research well costs. Costs function data strategy (expensive others!) data realized time. Collecting original data expensive analyzing existing data, collecting new data may less expensive depending easy reach specific individuals interview . result, diagnosing research designs including cost diagnosands important, diagnosands may usefully include average cost maximum cost. Researchers may make different decisions cost: cases, researcher select “best” design terms research design quality subject budget constraint, others choose cheapest among similar quality designs order save money future research. Diagnosis can help identify set decide among .relax budget constraint, researchers apply funding. Funding applicants wish obtain large grant possible design, difficulty credibly communicating quality design given subjectivity exercise. Funders, flip side, wish get value money set proposals decide fund, difficulty assessing quality proposed research. MIDA design declaration provide tool speaking common language can easily verified sides design proposed value knowledge set assumptions can interrogated funders.Funding applications often aim communicate research design proposed; learning answers design useful, important, interesting scholars, public, policymakers, another audience; research design provides credible answers question; researcher capable executing design; value--money design answers provides.new section funding applications aid communicating questions declaring MIDA design presenting diagnosis design. addition common diagnosands bias efficiency, two special diagnosands may valuable: cost , related, value--money. Cost can included design variant function design features sample size, number treated units, duration survey interviews. cost may vary parameters may vary across possible designs , example, number treated units random number. Simulating design across possible realizations design, thus, provides distribution costs function choices researcher makes. Value money diagnosand function cost also amount learned design. RMSE might one value criterion, another average difference priors posteriors Bayesian answer strategy (direct measure learning).cases, funders request applicants provide multiple options multiple price points make clear design altered funded lower (higher) level. Redesigning design differing sample sizes communicate researcher conceptualizes options, also provide funder understanding tradeoffs amount learning cost design variants. Applicants use redesign justify high cost request ask additional funding.Ex ante power analyses, required increasing number funders, illustrate crux misaligned incentives applicants funders. power analysis can demonstrate almost design “sufficiently powered” changing expected effect sizes noise. clarifying assumptions power analysis code, researchers can easily defend choices. Funders can easily interrogate assumptions. Power analyses using standard power calculators online difficult--interrogate assumptions built accommodate specifics many common designs. result, many return incorrect estimates power designs (Blair et al. 2020).Funders request design declarations can compare funding applications common scales: root mean-squared error, bias, power. course, also want weigh considerations like importance question fit funding program. moving design considerations onto common scale takes guesswork process reduces reliance researcher claims properties.","code":""},{"path":"research-design-lifecycle.html","id":"p4piloting","chapter":"20 Research Design Lifecycle","heading":"20.5 Piloting","text":"designs results past studies important guides selecting M, , D, . understanding nodes edges causal graph M, expected effect sizes, distribution outcomes, feasible randomization schemes, many features directly selected past research chosen based literature review distribution past studies. However, researchers face problem guided past research: research context inquiries often differ least subtle ways past study. Even replicating past study, collecting data different time period effects vary time aspects M may differ original study. deal , often run pilot studies. take many forms: focus groups learn features M learn ask survey questions; small-scale tests measurement tools verify data collection technology works; mini studies planned design smaller scale.Pilot studies constrained time money. constrained, run full study learn wrong design run corrected design main study. Since due constraints, run either smaller mini studies test subset elements planned design. places us bind: running design smaller less complete study imagine conducting, properties pilot design measure .MIDA provides framework thinking can learned pilot research design diagnosis. Just like full study, can define inquiries decisions make parameter estimates draw designing full study.Figure 20.3, display results diagnosis 50-unit pilot study conducting prepare larger main study. consider two strategies: (1) determining sample size power analysis main study, selecting minimum \\(N\\) study 80% powered detect pilot study’s effect size); (2) setting fixed \\(N\\) determined budget constraint, case 500, using standard deviation units treated control group pilot determine minimum detectable effect size 500-unit main study.left panel sampling distribution effect size estimates, .e., histogram effect estimates pilot. design, standard deviation outcome set one, effect estimates standard deviation units. true effect size set 0.2. can see sampling distribution huge range, nearly -0.5 nearly 0.75. first problem sampling distribution many estimates, fact nearly quarter , negative (wrong sign!). might lead us choose wrong sample size choose one-sided tests wrong direction. second high likelihood guessing effect size much higher really . obtain one estimates 0.75 even 0.5, choose \\(N\\) small detect true effect size 0.2. short, estimates effect size 50-person pilot study simple variable useful designing main study.However, good news: can learn lot power main study pilot study, just effect estimates. right panel Figure 20.3, estimate minimum detectable effect size 500-unit main study, relying estimated standard deviation control group estimated standard deviation treatment group calculate estimated standard error effect estimate main study. calculate minimum detectable effect size using approximation Gelman Hill (2006): 2.8 times estimated standard error (pg. 441). find estimates MDE full study much precise, tightly centered around 0.25. Since don’t know larger smaller true effect size, must make argument based past studies’ effect sizes justify whether minimum size sufficiently large whether increase sample size order detect even smaller effects. reason MDE precisely estimated standard deviation control group much less variable estimate true standard deviation control potential outcome effect size estimate true effect size.\nFigure 20.3: Learning pilot studies.\ndiagnosing pilot studies way, can learn decisions can made confidence pilot data shaped instead expectations past studies qualitative knowledge. Diagnosis can also help us decide large pilot study need order estimate quantities like MDE full study precision.Beyond estimating MDE studies, facts can often usefully learned pilot studies take form existence proofs. often wish study variation \\(D\\) (treatment) affects variation \\(Y\\) (outcome), absence past data two variables may know even variation \\(Y\\) explain. experimental studies, can learn whether treatment can implemented, observational study can learn whether variation treatment variable.Baseline measurement may often used instead pilot study learn empirical features. sample size fixed interested learning whether outcome measures vary across units covary, can measure baseline make adjustments posttreatment survey. still control imperfect measures baseline improve efficiency.","code":""},{"path":"research-design-lifecycle.html","id":"implementation","chapter":"20 Research Design Lifecycle","heading":"20.6 Implementation","text":"design declaration road map implementing study. data strategy tells procedure use sample units; assign treatments; variables measure. answer strategy function translates realized data set answers inquiries statistics communicate confidence answers. specified data answer strategies sufficient detail code, can directly run functions declared sample units assign treatment analyze data.road map useful tool learn go things go right, also identify take wrong turn need make decisions get answer. sense, design declaration living document, updated reflect set decisions make along way twists turns research road. model world inquiry declared, unable collect variable, treat subset units treatment, reach units followup surveys, can compare alternative ways handling deviation original plan terms originally designed experiment. changes data strategy \\(D\\). can assess options also guide decisionmaking whether continue study use money another better purpose. can also use comparison diagnosands alternative options tool communciate research partners change practices needed. can also use defend intermediate data strategy choices finished reviewers readers.make changes \\(D\\), changes \\(\\) may also required order follow principle analyzing sample, assign treatment, measure. switch individual randomization cluster randomization logistically possible individually assign units treatment, typically want adjust answer strategy account clustering calculation standard errors. keeping data strategy answer strategy date implement study, may also identify new data must collected new steps take order still able provide credible answers.also learn design go along, anything goes wrong natural progression research. example, may know many units per cluster per block, key details assigning treatments analyzing data experiments. learn details, change data answer strategies reflect new details — diagnose new design sure still agree original choices. Beyond data answer strategies, may also learn new nodes edges model course research. learn new confounders mediators, update model, also consider whether changes data answer strategy necessitated ensure can answer original inquiry.short, design declaration living document can keep updated use tool guide along research path, just document write beginning study revisit writing . advice apparent tension idea preanalysis plans, precommit analysis choices data collected. need . useful keep original design declaration preregister , also useful keep declaration updated make changes along way inevitably happen. better position make good choices things go awry, also communicate made changes design.Related readings.Failure (Karlan Appel (2018))","code":""},{"path":"research-design-lifecycle.html","id":"p4populatedpap","chapter":"20 Research Design Lifecycle","heading":"20.7 Populated Preanalysis Plan","text":"Inevitably, authors pre-analysis plans fail anticipate , eventually, data generated study analyzed. Many reasons discrepancy discussed previous section implementation, reasons intervene well. common reason PAPs promise many analyses – process writing paper, analyses dropped, others combined, still others added writing revision process. next section, ’ll describe reconcile analyses--planned analyses--implemented, present section analysis plan immediately getting data back.echo proposals made Banerjee et al. (2020) Alrababa’h et al. (2020) researchers produce short reports fulfill promises made PAPs. Banerjee et al. (2020) emphasize writing PAPs difficult usually time constrained, natural final paper reflect thinking full set empirical approaches. “populated PAP” serves simply communicate results promised analyses. Alrababa’h et al. (2020) cite tendency researchers abandon publication studies return null results. order address resulting publication bias, recommend “null results reports” share results pre-registered analyses.recommend authors include mock analyses PAPs using mock data. major benefit quite specific details answer strategy. benefit comes time produce populated PAP, since realized data can quite straightforwardly swapped mock data. Given time invested producing mock analyses PAP, writing populated PAP takes much effort needed clean data, need done case.","code":""},{"path":"research-design-lifecycle.html","id":"example-21","chapter":"20 Research Design Lifecycle","heading":"20.7.1 Example","text":"Section 20.1, declared design Bonilla Tillery (2020) following preanalysis plan. , declared answer strategy code. populated PAP, can run answer strategy code, swap simulated data real data collected study. present first regression table Table ?? coefficient plot Figure 20.4.\nStatistical models\n\nFigure 20.4: Coefficient plot Bonilla Tillery design based study’s realized data.\n","code":""},{"path":"research-design-lifecycle.html","id":"reconciliation","chapter":"20 Research Design Lifecycle","heading":"20.8 Reconciliation","text":"Inevitably, research design implemented differ way research design planned. Treatments implemented conceived, people found interview, sometimes learn baseline measures informs measure later. understanding research design changed conception implementation crucial understanding learned design.Suppose original design described three-arm trial: one control two treatments, design implemented drops subjects assigned second treatment. Sometimes entirely appropriate reasonable design modification: perhaps turns due implementation failure, second treatment simply delivered. times, modification less benign – perhaps estimate effect second treatment achieve statistical significance, author simply omits analysis.reason, explicitly reconciling design planned design implemented first step writing paper. publicly-posted preanalysis plan can make reconciliation process especially credible – know sure planned design preanalysis plan describes pre-implementation. However, preanalysis plan prerequisite engaging reconciliation. scientific enterprise built large measure trust: ready believe researchers say, design though implement due unanticipated developements, design ended implementing.cases, reconciliation lead additional learning beyond can inferred final design . units refused included study sample units refused measurement, learn important features units. Understanding sample exclusions, noncompliance, attrition may inform future research design planning choices, contribute substantively understanding social setting. policy implemented way study likely also able work units refused participate, future research examine convince policy’s benefits.belongs reconciliation? minimum, need full description planned design, full description implemented design, list differences. can made explicit declaration design computer code, comparing two design objects line--line.DeclareDesign take first steps comparing designs :","code":"\ndesign1 <- declare_population(N = 100, u = rnorm(N)) +\n  declare_potential_outcomes(Y ~ Z + u) +\n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_sampling(n = 75) +\n  declare_assignment(m = 50) +\n  declare_reveal(Y, Z) +\n  declare_estimator(Y ~ Z, estimand = \"ATE\")\n\ndesign2 <- declare_population(N = 200, u = rnorm(N)) +\n  declare_potential_outcomes(Y ~ 0.5*Z + u) +\n  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_sampling(n = 100) +\n  declare_assignment(m = 25) +\n  declare_reveal(Y, Z) +\n  declare_estimator(Y ~ Z, model = lm_robust, estimand = \"ATE\")\n\ncompare_designs(design1, design2)\ncompare_design_code(design1, design2)\ncompare_design_summaries(design1, design2)\ncompare_design_data(design1, design2)\ncompare_design_estimates(design1, design2)\ncompare_design_estimands(design1, design2)"},{"path":"research-design-lifecycle.html","id":"example-22","chapter":"20 Research Design Lifecycle","heading":"20.8.1 Example","text":"Section 20.1, described preanalysis plan registered Bonilla Tillery (2020). reconcile set conditional average treatment effect analyses planned PAP, analyses reported paper, reported appendix request reviewers Table 20.3. column two, see authors planned four CATE estimates: effects familiarity Black Lives Matter; gender; LGBT status; linked fate. two reported paper, others may excluded space reasons. two excluded analyses especially informative; excluded basis statistical significance. Another way handle uninteresting results presented populated PAP posted Web site paper’s appendix.appendix, authors report set analyses requested reviewers. see perfect example transparently presenting set planned analyses highlighting analyses added afterward added. write:asked consider pertinent moderations beyond gender LGBTQ+ status. contained four following sections.Table 20.3:  Reconciliation Bonilla Tillery preanalysis plan.","code":""},{"path":"research-design-lifecycle.html","id":"writing","chapter":"20 Research Design Lifecycle","heading":"20.9 Writing","text":"writing empirical paper, authors must convince reviewers readers question important research design selected provides useful answers question. MIDA comes . Elements MIDA appear every section paper, every element MIDA described somewhere paper.common model social science empirical papers five sections: introduction, theory hypotheses, research design, results, discussion. outline elements MIDA can usefully fit section , providing example paper highlighting element described.introduction section capitulation aspect MIDA brief. reader brought quickly speed whole research design, well expectations actual findings.next section, theory hypotheses, contains information causal model world (\\(M\\)) research question world (\\(\\)) well guesses \\(^M\\) (.e., hypotheses!). section lay features model necessary describe \\(\\).motivate hypotheses, theory section outline prior beliefs \\(^W\\), true answer inquiry, based past literature. meta-analysis systematic review past evidence provide systematic summary past answers \\(^W\\), informal literature review offered. priors used along study results construct posterior beliefs reported discussion section, .e., know conducting study. summarizing literature, important consider research designs past studies (see Synthesis section). Meta-analyses often formally account quality research designs past studies weighting inverse precision (upweighting informative studies -weighting uninformative ones). Literature reviews may informally. Moreover, summary past literature research design, try prevent common research design issues selection dependent variable ensuring discuss literature present views consonant hypotheses.Though research questions nearly ubiquitously included theory section, details model often left . Expected effect sizes, effects vary subgroups, expected proportions subgroups, variables expected correlated, amount variability outcome features model important effects reviewers readers judge quality research design. Without specifying portions \\(M\\), diagnosis conducted. Moreover, describe Reanalysis, define \\(M\\) paper, can clarify terms debate alternative analysis strategies. \\(M\\) undefined, author reanalysis must infer model make , may agree.research design methods section description \\(D\\) \\(\\) description diagnosis design. section, defend choice \\(D\\) \\(\\) \\(M\\) \\(\\). Papers commonly one \\(D\\) associated \\(\\) answering related questions; \\(D\\) \\(\\) described section.results section describes \\(^D\\). Increasingly, results presented visually well text, order effectively communicate results reader. Visualizing modeled results well raw data can improve results communicated simultaneously help readers connect research design, data, results.discussion section update understanding model form posteriors \\(^W\\) given prior expectations results \\(^D\\). may learn study new variables new edges matter model — new outcomes affected treatment. also often learn functional form causal relationships two variables, example new moderators precision size moderation. discussion section also chance point new MIDAs implemented learn \\(M\\). might relate new nodes discovered, parts model yet learn enough .Figure , illustrate elements MIDA incorporated Mousa (2020). study reports outcome randomized experiment Iraqi Christians assigned either -Christian soccer team team play alongside Muslims. experiment tested whether mixed team affected intergroup attitudes behaviors, among teammates back home games . highlight color areas discussing model \\(M\\) yellow, inquiry \\(\\) green, data strategy \\(D\\) blue, answer strategy \\(\\) pink.Paper MIDA elements highlighted (Mousa 2020)model inquiry largely appear abstract introductory portion paper, though aspects model discussed later . Much first three pages devoted data strategy, answer strategy appears briefly. division makes sense: paper, action experimental design whereas answer strategy follows straightforwardly using principles analyze randomize. paper mostly describes \\(M\\) \\(D\\), small portion devoted \\(\\) \\(\\). Finally, notable data strategy interspersed aspects model. reason author justifying choices randomization measurement using features model. highlights deep connection two discussed Part II.Papers often report results multiple data strategies others answers multiple related inquiries. Typically, single \\(M\\) described theory section, describes nodes edges used \\(\\)’s fit together. data strategy described, inquiries answer strategy targeting. estimate linked one inquiries. cases, reason multiple inquiries studied single paper aim paper falsify model world. case, model described diagnosis design include assessment good overall design, data answer strategies generate answers inquiry, falsifying model. Psychology three-study papers often take form: study 1 asks correlation X Y, study 2 X randomized, study 3 analysis mechanisms lead X Y examined.","code":""},{"path":"research-design-lifecycle.html","id":"publication","chapter":"20 Research Design Lifecycle","heading":"20.10 Publication","text":"Declaring MIDA research design can help help editors reviewers assess quality research design. reviewers request changes study, declaration proposed changes diagnosis can used compare two possible designs .","code":""},{"path":"research-design-lifecycle.html","id":"reviewing-papers","chapter":"20 Research Design Lifecycle","heading":"20.10.1 Reviewing papers","text":"Reviewers editors must decide whether devote scarce space editing bandwidth publishing paper. Criteria may include topic fit journal, importance question, much learned research. problem publication filter — publishing studies statistically-significant splashy results — long recognized cause “false” findings making way literature bias literature simply missing null findings. One remedy problem results-blind review studies: selecting studies without knowing nature results.Results-blind review can take place multiple ways. Journals instituted alternative submission paths “registered reports,” authors lay analyze data often include mock tables figures based simulated data. reports can constructed data collected. closely related preanalysis plans, fact often PAPs consist registered report.results-blind review require institutional setup: reviewers can review papers without regard results . Reviewers can print papers hide results simply aim write comments basis importance question quality research design.cases, needed authors clear statement research design report interpret results. Results-blind review need ignore features research design data displayed whether authors overclaiming based results. Authors can write plans visualize claims make based different patterns findings. Journals institute results-blind review can ensure reviewers able judge quality research design requesting details \\(M\\), \\(\\), \\(D\\), \\(\\) authors.","code":""},{"path":"research-design-lifecycle.html","id":"responding-to-reviewers","chapter":"20 Research Design Lifecycle","heading":"20.10.2 Responding to reviewers","text":"research design study set stone final version post journal Web site published print. , journal editors reviewers may ask changes answer strategies even data strategies inquiries , view, improve paper.reviewer may make three kinds requests: alternative analysis strategy replace augment original; additional data collection; addition new inquiry, based realized data new data collection; change model form adding dropping node edge, related changes parts design; change inquiry reviewer feels can better answered data answer strategy author selected.can understand making changes requested reviewers alters design. change improves design, adopting suggestions easy. changes irrelevant design – like reporting reviewer’s preferred descriptive statistics – case, following advice also easy trouble comes reviewers propose changes actively undermine design? , diagnosing reviewer’s alternative design can effective way demonstrate proposed changes harm research design.","code":""},{"path":"research-design-lifecycle.html","id":"archiving","chapter":"20 Research Design Lifecycle","heading":"20.11 Archiving","text":"One biggest successes push greater research transparency changing norms surrounding sharing data analysis code studies published. become de rigeur many journals post materials publicly-available repositories like OSF Dataverse. development undoubtedly good thing. older manuscripts, sometimes data analyses described “available upon request” course requests sometimes ignored. Furthermore, century now, study authors longer us even wanted respond requests. Public repositories much better chance preserving study information future.belongs replication archive? First, data \\(d\\) . Sometimes raw data, sometimes “cleaned” data actually called analysis scripts. ethically possible, think preferable post much raw data possible, example removing information like IP address geographic location used identify subject. usually consider data processing scripts clean prepare data analysis part data strategy \\(D\\) sense complete measurement procedures laid \\(D\\). Cleaning scripts might also considered part answer strategy sense apply interpretation data provided world. output cleaning scripts – cleaned data – included replication archive well.Replication archives also include \\(\\), set functions applied \\(d\\) produce \\(^D\\). vitally important actual analysis code archived natural-language descriptions \\(\\) typically given papers imprecise. small example, many articles describe answer strategies “ordinary least squares” fully describe set covariates used flavor standard errors estimated. differences can substantively affect quality research design. actual analysis code makes \\(\\) explicit.typical replication archives include \\(d\\) \\(\\), think future replication archives also include design declaration fully describes \\(M\\), \\(\\), \\(D\\), \\(\\) – , archive designs, just data analysis code. done code words. addition, diagnosis included, demonstrating properties understood author also indicating diagnosands author considered judging quality design.Figure shows file structure example replication. view replication archives shares much common TIER protocal, can found : https://www.projecttier.org/. includes raw data platform-independent format (.csv) cleaned data language-specifc format (.rds), data features like labels, attributes, factor levels preserved imported analysis scripts. analysis scripts labeled outputs create, figures tables. master script included runs cleaning analysis scripts correct order. documents folder includes paper, supplemental appendix, pre-analysis plan, populated analysis plan, codebooks describe data. README file explains part replication archive. also suggest authors include script includes design declaration diagnosis.File structure archiving","code":""},{"path":"research-design-lifecycle.html","id":"reanalysis","chapter":"20 Research Design Lifecycle","heading":"20.12 Reanalysis","text":"reanalysis existing study followup study \\(d\\), original realized data, fixed changes \\(\\) sometimes \\(M\\) \\(\\) proposed. Given \\(d\\) fixed, data strategy \\(D\\). results given new MIDA, may differ original study’s results, reported.can learn reanalyses least five ways. can confirm errors analysis strategy. Many reanalyses correct simple mathematical errors, typos data transcription, failures analyze following data strategy faithfully. reanalyses show whether results depend corrections.can reassess known \\(\\), using new information world learned original study published. , may learn new confounders alternative causal channels undermine credibility original answer strategy. reanalyzed, demonstrating results () change improves understanding \\(^W\\).Many reanalyses show original findings “robust” alternative answer strategies. better conceptualized claims robustness alternative models: one model may imply one answer strategy different model, another confounder, implies another. models plausible, good answer strategy robust even help distinguish reanalysis uncover robustness alternative models lack thereof.Reanalyses may also aim answer new questions considered original study, realized data can provide useful answers. example, authors may analyze outcomes originally analyzed.Reanalyses research designs. Whether reanalysis good design, much can contribute knowledge original inquiry, depend possible realizations data. \\(d\\) fixed reanalysis, analysts often instead tempted judge reanalysis based whether overturns confirms results original study. successful reanalysis way thinking demonstrates, showing original results changed alternative \\(\\), results robust plausible models. way thinking can lead incorrect assessments reanalyses. need consider answers obtain original answer strategy \\(\\) reanalysis strategy \\(^{\\prime}\\) many possible realizations data. good reanalysis strategy reveals high probability set models world can make credible claims \\(\\). Whether results fixed \\(d\\) realized change \\(\\) \\(^{\\prime}\\) tells us little probability. one draw.diagnose reanalysis, need define two answer strategies — \\(\\) \\(^{\\prime}\\) — also new diagnostic-statistic. need decide summarize answers two answer strategies. one returns TRUE one FALSE, conclude inquiry? function define summarize two results depends inquiry goals reanalysis. diagnosis reanalysis assess properties summary two studies possible realizations data. goal reanalysis instead learn new question, simply construct new MIDA altogether, holding constant \\(D\\) original study, change already collected \\(d\\) using .","code":""},{"path":"research-design-lifecycle.html","id":"replication","chapter":"20 Research Design Lifecycle","heading":"20.13 Replication","text":"study completed, may one day replicated. Replication differs reanalysis replication study involves specification new MIDA collection new data study inquiry. discussed previous, reanalysis may re-specify parts research design, always re-uses original data \\(d\\) way.-called “exact” replications hold key features , D, fixed, draw new dataset \\(d_{\\rm new}\\) \\(D()\\) apply \\(\\) new \\(d\\) order produce fresh answer \\(a_{\\rm new}^D\\). Replications said “succeed” \\(a_{\\rm old}^D\\) \\(a_{\\rm new}^D\\) similar “fail” . Dichotomizing replication attempts successes failures usually helpful, better simply characterize similar \\(a_{\\rm old}^D\\) \\(a_{\\rm new}^D\\) .course, exact replication impossible: least elements M changed first study replication. Specifying might changed, e.g., outcomes vary time, help judge differences observed \\(a_{\\rm old}^D\\) \\(a_{\\rm new}^D\\). Statistical noise also play role.Replication studies benefit enormously knowledge gains produced original studies. example, learn large amount \\(M\\) likely value \\(^M\\) original study. \\(M\\) replication study can incorporate new information. example, learn original study \\(^M\\) positive might small, replication study respond changing \\(D\\) order increase sample size. Design diagnosis can help learn change design replication study light original study.changes \\(D\\) \\(\\) can made produce informative answers \\(\\), exact replication may preferred. Holding treatment outcomes may required provide answer \\(\\), increasing sample size sampling individuals rather villages changes may preferable exact replication. Replication designs can take advantage new best practices research design.designing original studies, anticipate someday work replicated. improves ex ante incentives. extent want future replication studies arrive similar answers original study produce (.e., want \\(a_{\\rm new}^D\\) match \\(a_{\\rm old}^D\\) closely possible), want choose designs bring \\(a_{\\rm old}^D\\) close \\(^M\\) possible, presupposition faithful replicators also design studies way \\(a_{\\rm new}^D\\) also close \\(^M\\).Replication studies necessarily differ original studies – literally impossible reproduce exact conditions original study way ’s impossible step river twice. Another way putting statement \\(D_{\\rm new}\\) necessarily different \\(D_{\\rm old}\\). Theory (.e., beliefs \\(M\\)) tool use say \\(D_{\\rm old}\\) similar enough \\(D_{\\rm new}\\) consititute close enough replication study. concrete example, many survey experimental replications involve using exact experimental stimuli changing study sample, e.g., nationally representative sample convenience sample.-called “conceptual” replications alter \\(M\\) \\(D\\), keep \\(\\) \\(\\) similar possible. , conceptual replication tries ascertain whether relationship one context (\\((M_{\\rm old})\\)) also holds new context (\\((M_{\\rm new}\\)). trouble promise conceptual replications lies success designer holding \\(\\) constant. often, conceptual replication fails changing \\(M\\), much changes \\(\\) much changes “concept” replication.summary function interpret difference \\(a_{\\rm old}^D\\) \\(a_{\\rm new}^D\\). may take new one throw old MIDA poor first. may taking average. may precision-weighted average. Specifying function ex ante may useful, avoid choice summary depending results replication. summary function reflected discussion section replication paper.reading.(???) distinctions replication reanalysis","code":""},{"path":"research-design-lifecycle.html","id":"resolving-disputes","chapter":"20 Research Design Lifecycle","heading":"20.14 Resolving Disputes","text":"Disputes arise reanalyses replication studies conducted claims made past studies learn pair. realized data two studies, \\(d\\) \\(d^{\\prime}\\), well two designs \\(MIDA\\) \\(MIDA^{\\prime}\\), together inform learn two studies. Disputes arise whether changes \\(M^{\\prime}\\), \\(^{\\prime}\\), \\(D^{\\prime}\\), \\(^{\\prime}\\) new study mean \\(d^{\\prime}\\) can informative original \\(\\).\\(M\\) always changes. reanalysis replication conducted original study, definition learned least \\(^w\\) \\(^d\\) first study. often learned much , distribution variables, existence new nodes edges, sometimes much related studies published . However, original author may agree new author updated \\(M\\). disputes substantive.offer five rules resolving disputes changes \\(\\), \\(D\\), \\(\\).Replacing alternative practices justified design simulation\n1. M always changes! (information tau sd(tau))\n2. Home ground dominance: Change D--’ > M\n3. Robustness alternative models: Change D--’ ≥ M ’ > M’ E.g. change simple complete RA\n4. Model plausibility:’ < M ’ > M’, change ’ D--IFF M’ plausible M E.g. switching balanced design believe variances equal across treatment groups\n5. Undefined inquiries. Change ’ undefined M defined M: can’t change ’, can’t change D D’ means unidentifiable.","code":""},{"path":"research-design-lifecycle.html","id":"synthesis","chapter":"20 Research Design Lifecycle","heading":"20.15 Synthesis","text":"One last, last, stage lifecyle research design eventual incorportation common scientific understanding world. Research findings specific – specific \\(^D\\)s need synthesized broader scientific understanding. research synthesis comprises new research design summarizes past research.Research synthesis takes two basic forms. first meta-analysis, series \\(^D\\)s analyzed together order better understand features distribution answers obtained literature. Traditional meta-analysis typically focuses average k answers: \\(a_1^D\\),\\(a_2^D\\),…\\(a_k^D\\). Studies can averaged together many ways better worse. Sometimes answers averaged together according precision – precision weighted average estimates many studies equivalent fixed-effects meta analysis. Sometimes studies “averaged” counting many estimates positive significant, many negative significant, many null. typical averaging approach taken literature review. Regardless averaging approach, goal kind synthesis learn much possible particular \\(\\) drawing evidence many studies.second kind synthesis attempt bring together many \\(^D\\), targets different inquiry common model. kind synthesis takes place across entire research literature. Different scholars focus different nodes edges common model, synthesis needs incorporate diverse sources evidence.can best anticipate research findings synthesized? first kind synthesis – meta-analysis – must cognizant keeping commonly understood \\(\\) mind. want select inquiries novelty, commonly-understood importance. want many studies effects women versus men elected officials public goods want understand particular \\(\\) great detail specificity. specifics models \\(M\\) might differ study study, fact \\(\\)s similar enough synthesized allows specific kind knowledge accumulation.second kind synthesis – literature-wide progress full causal model – even greater care required. Specific studies make bespoke models \\(M\\) instead must understand specific \\(M\\) adopted study specical case master \\(M\\) principle agreed wider research community. nonstop, neverending proliferation study-specific theories threat kind knowledge accumulation.\nDeclaring diagnosing properties meta design can informative planning individual study. first step every research synthesis process collecting past studies. Search strategies sampling strategies, can biased ways convenience samples individuals. Conducting Census past literature topic impossible: much research conducted published yet published. Selecting studies major journals alone may induce additional publication bias sample. Collecting working papers soliciting unpublished abandoned research topic strategies mitigate risks. choice answer strategy research synthesis typically driven assumptions model studies related contexts units within selected. model declaring research synthesis thus must include assumptions studies reach synthesizer, contexts units selected original studies. Diagnosis can help assess conditions analysis strategies provide unbiased, efficient estimates true effects either subset contexts studies broader population.","code":""},{"path":"part-iv-exercises.html","id":"part-iv-exercises","chapter":"21 Part IV Exercises","heading":"21 Part IV Exercises","text":"","code":""},{"path":"references-4.html","id":"references-4","chapter":"References","heading":"References","text":"Alrababa’h, Ala’, Scott Williamson, Andrea Dillon, Jens Hainmueller, Dominik Hangartner, Michael Hotard, David Laitin, Duncan Lawrence, Jeremy Weinstein. 2020. “Learning Null Effects: Bottom-Approach.” SocArXiv. https://doi.org/10.31235/osf.io/5ebpy.Angrist, Joshua D., Jörn-Steffen Pischke. 2008. Mostly Harmless Econometrics: Empiricist’s Companion. Princeton: Princeton University Press.Aronow, Peter M., Cyrus Samii. 2016. “Regression Produce Representative Estimates Causal Effects?” American Journal Political Science 60 (1): 250–67.Asunka, Joseph, Sarah Brierley, Miriam Golden, Eric Kramon, George Ofosu. 2019. “Electoral Fraud Violence: Effect Observers Party Manipulation Strategies.” British Journal Political Science 49 (1): 129–51. https://doi.org/10.1017/S0007123416000491.Baird, Sarah, J. Aislinn Bohren, Craig McIntosh, Berk Ozler. 2018. “Optimal Design Experiments Presence Interference.” Review Economics & Statistics 5 (100): 844–60.Banerjee, Abhijit, Esther Duflo, Amy Finkelstein, Lawrence F Katz, Benjamin Olken, Anja Sautmann. 2020. “Praise Moderation: Suggestions Scope Use Pre-Analysis Plans Rcts Economics.” Working Paper 26993. Working Paper Series. National Bureau Economic Research. https://doi.org/10.3386/w26993.Baumgartner, Michael, Alrik Thiem. 2017. “Often Trusted Never (Properly) Tested: Evaluating Qualitative Comparative Analysis.” Sociological Methods & Research.Bennett, Andrew. 2015. “Appendix.” Process Tracing: Metaphor Analytic Tool, edited Andrew Bennett Jeffrey Checkel. New York: Cambridge University Press.Bennett, Andrew, Jeffrey T Checkel. 2015. Process Tracing. New York: Cambridge University Press.Björkman, Martina, Jakob Svensson. 2009. “Power People: Evidence Randomized Field Experiment Community-Based Monitoring Project Uganda.” Quarterly Journal Economics 124 (2): 735–69.Blair, Graeme, Jasper Cooper, Alexander Coppock, Macartan Humphreys. 2020. “Declaring Diagnosing Research Designs.” American Political Science Review.Blair, Graeme, Alexander Coppock, Margaret Moor. 2018. “Worry Sensitivity Bias: Evidence 30 Years List Experiments.” Unpublished Manuscript.Blair, Graeme, Kosuke Imai. 2012. “Statistical Analysis List Experiments.” Political Analysis 20 (1): 47–77.Bonilla, Tabitha, Alvin B. Tillery. 2020. “Identity Frames Boost Support Mobilization #Blacklivesmatter Movement? Experimental Test.” American Political Science Review 114 (4): 947–62. https://doi.org/10.1017/S0003055420000544.Brady, Henry E., David Collier. 2010. Rethinking Social Inquiry: Diverse Tools, Shared Standards. Lanham, Maryland: Rowman & Littlefield Publishers.Carnegie, Allison, Cyrus Samii. 2017. “International Institutions Political Liberalization: Evidence World Bank Loans Program.” British Journal Political Science, 1–23.Casey, Katherine, Rachel Glennerster, Edward Miguel. 2012. “Reshaping Institutions: Evidence Aid Impacts Using Pre-Analysis Plan.” Quarterly Journal Economics 127 (4): 1755–1812.Christensen, Garret, Edward Miguel. 2018. “Transparency, Reproducibility, Credibility Economics Research.” Journal Economic Literature 56 (3): 920–80. https://doi.org/10.1257/jel.20171350.Clemens, Michael . 2017. “Meaning Failed Replications: Review Proposal.” Journal Economic Surveys 31 (1): 326–42.Coffman, Lucas C., Muriel Niederle. 2015. “Pre-Analysis Plans Limited Upside, Especially Replications Feasible.” Journal Economic Perspectives 29 (3): 81–97.Collier, David. 2011. “Understanding Process Tracing.” PS: Political Science & Politics 44 (4): 823–30.Collier, David, Henry E. Brady, Jason Seawright. 2004. “Sources Leverage Causal Inference: Toward Alternative View Methodology.” Rethinking Social Inquiry: Diverse Tools, Shared Standards, edited David Collier Henry E. Brady. Lanham, Maryland: Rowman; Littlefield.Coppock, Alexander. 2017. “Shy Trump Supporters Bias 2016 Polls? Evidence Nationally-Representative List Experiment.” Statistics, Politics Policy 8 (1): 29–40.Cronbach, Lee J., Karen. Shapiro. 1982. Designing Evaluations Educational Social Programs. San Francisco: Jossey-Bass.Dawid, . Philip. 2000. “Causal Inference Without Counterfactuals.” Journal American Statistical Association 95 (450): 407–24.Deaton, Angus S. 2010. “Instruments, Randomization, Learning Development.” Journal Economic Literature 48 (2): 424–55.De la Cuesta, Brandon, Kosuke Imai. 2016. “Misunderstandings Regression Discontinuity Design Study Close Elections.” Annual Review Political Science 19: 375–96.Dunning, Thad. 2012. Natural Experiments Social Sciences: Design-Based Approach. Cambridge: Cambridge University Press.Ensminger, Jean. 1990. “Co-Opting Elders: Political Economy State Incorporation Africa.” American Anthropologist 92 (3): 662–75. http://www.jstor.org/stable/680341.Fairfield, Tasha. 2013. “Going Money : Strategies Taxing Economic Elites Unequal Democracies.” World Development 47: 42–57.Fairfield, Tasha, Andrew E. Charman. 2017. “Explicit Bayesian Analysis Process Tracing: Guidelines, Opportunities, Caveats.” Political Analysis 25 (3): 363–80.Fearon, James D, David D Laitin. 2008. “Integrating Qualitative Quantitative Methods.” Oxford Handbook Political Science.Findley, Michael G., Nathan M. Jensen, Edmund J. Malesky, Thomas B. Pepinsky. 2016. “Can Results-Free Review Reduce Publication Bias? Results Implications Pilot Study.” Comparative Political Studies 49 (13): 1667–1703.Geddes, Barbara. 2003. Paradigms Sand Castles: Theory Building Research Design Comparative Politics. Ann Arbor, Michigan: University Michigan Press.Gelman, Andrew, John Carlin. 2014. “Beyond Power Calculations Assessing Type S (Sign) Type M (Magnitude) Errors.” Perspectives Psychological Science 9 (6): 641–51.Gelman, Andrew, Jennifer Hill. 2006. Data Analysis Using Regression Multilevel/Hierarchical Models. Cambridge: Cambridge University Press.Gelman, Andrew, Guido Imbens. 2017. “High-Order Polynomials Used Regression Discontinuity Designs.” Journal Business & Economic Statistics, . Forthcoming.Gerber, Alan S., Donald P. Green. 2012. Field Experiments: Design, Analysis, Interpretation. New York: W.W. Norton.Gerring, John, Lee Cojocaru. 2016. “Selecting Cases Intensive Analysis: Diversity Goals Methods.” Sociological Methods & Research 45 (3): 392–423.Glynn, Adam N. 2013. “Can Learn Statistical Truth Serum? Design Analysis List Experiment.” Public Opinion Quarterly 77 (S1): 159–72.Goertz, Gary, James Mahoney. 2012. Tale Two Cultures: Qualitative Quantitative Research Social Sciences. Princeton: Princeton University Press.Green, Donald P., Winston Lin. 2016. “Standard Operating Procedures: Safety Net Pre-Analysis Plans.” PS: Political Science & Politics 49 (3): 495–99.Gulzar, Saad, Muhammad Yasir Khan. n.d. “Motivating Political Candidacy Performance: Experimental Evidence Pakistan.”Hahn, Jinyong, Petra Todd, Wilbert Van der Klaauw. 1999. “Evaluating Effect Antidiscrimination Law Using Regression-Discontinuity Design.” National Bureau Economic Research.Halpern, Joseph Y. 2000. “Axiomatizing Causal Reasoning.” Journal Artificial Intelligence Research 12: 317–37.Herron, Michael C., Kevin M. Quinn. 2016. “Careful Look Modern Case Selection Methods.” Sociological Methods & Research 45 (3): 458–92.Herron, Michael C, Kevin M Quinn. 2016. “Careful Look Modern Case Selection Methods.” Sociological Methods & Research 45 (3): 458–92.Huber, John. 2013. “Theory Getting Lost ‘Identification Revolution’?”Humphreys, Macartan, Alan M. Jacobs. 2015. “Mixing Methods: Bayesian Approach.” American Political Science Review 109 (4): 653–73.Humphreys, Macartan, Raul de la Sierra, Peter van der Windt. 2013. “Fishing, Commitment, Communication: Proposal Comprehensive Nonbinding Research Registration.” Political Analysis 21 (1): 1–20.Imai, Kosuke, Gary King, Elizabeth . Stuart. 2008. “Misunderstandings Experimentalists Observationalists Causal Inference.” Journal Royal Statistical Society: Series (Statistics Society) 171 (2): 481–502.Imbens, Guido W. 2010. “Better Late Nothing: Comments Deaton (2009) Heckman Urzua (2009).” Journal Economic Literature 48 (2): 399–423.Imbens, Guido W., Donald B. Rubin. 2015. Causal Inference Statistics, Social, Biomedical Sciences. Cambridge: Cambridge University Press.Ingram, Matthew C, Imke Harbers. 2020. “Spatial Tools Case Selection: Using Lisa Statistics Design Mixed-Methods Research.” Political Science Research Methods 8 (4): 747–63.Karlan, Dean, Jacob Appel. 2018. Failing Field: Can Learn Field Research Goes Wrong. Princeton University Press.Keele, Luke, Rocı́o Titiunik. 2016. “Natural Experiments Based Geography.” Political Science Research Methods 4 (1): 65–95.Li, Yimeng. 2019. “Relaxing Liars Assumption List Experiment Analyses.” Political Analysis 27 (4): 540–55.Lieberman, Evan S. 2005. “Nested Analysis Mixed-Method Strategy Comparative Research.” American Political Science Review 99 (3): 435–52.Lohr, Sharon. 2010. Sampling: Design Analysis. Boston: Brooks Cole.Mahoney, James. 2012. “Logic Process Tracing Tests Social Sciences.” Sociological Methods Research 41 (4): 570–97.Mahoney, James, Gary Goertz. 2004. “Possibility Principle: Choosing Negative Cases Comparative Research.” American Political Science Review 98 (4): 653–69.Martin, Lisa. 1992. Coercive Cooperation: Explaining Multilateral Economic Sanctions. Princeton: Princeton University Press.Miguel, Edward, Colin Camerer, Katherine Casey, Joshua Cohen, Kevin M Esterling, Alan Gerber, Rachel Glennerster, et al. 2014. “Promoting Transparency Social Science Research.” Science 343 (6166): 30.Mill, John Stuart. 1884. System Logic, Ratiocinative Inductive: Connected View Principles Evidence Methods Scientific Investigation. Harper.Miller, Judith Droitcour. 1984. “New Survey Technique Studying Deviant Behavior.” PhD thesis, George Washington University.Mitchell, Ojmarrh, Joshua C. Cochran, Daniel P. Mears, William D. Bales. 2017. “Examining Prison Effects Recidivism: Regression Discontinuity Approach.” Justice Quarterly 34 (4): 571–96.Morris, Tim P., Ian R. White, Michael J. Crowther. 2019. “Using Simulation Studies Evaluate Statistical Methods.” Statistics Medicine.Mousa, Salma. 2020. “Building Social Cohesion Christians Muslims Soccer Post-Isis Iraq.” Science 369 (6505): 866–70.Nosek, Brian ., George Alter, George C. Banks, Denny Borsboom, Sara D. Bowman, Steven J. Breckler, Stuart Buck, et al. 2015. “Promoting Open Research Culture: Author Guidelines Journals Help Promote Transparency, Openness, Reproducibility.” Science 348 (6242): 1422.Ofosu, George K., Daniel N. Posner. 2020. “Pre-Analysis Plans Hamper Publication?” AEA Papers Proceedings 110 (May): 70–74.Olken, Benjamin . 2015. “Promises Perils Pre-Analysis Plans.” Journal Economic Perspectives 29 (3): 61–80.Pearl, Judea. 1999. “Probabilities Causation: Three Counterfactual Interpretations Identification.” Synthese 121 (1-2): 93–149.———. 2009. Causality. Cambridge: Cambridge: Cambridge University Press.Pearl, Judea, Dana Mackenzie. 2018. Book : New Science Cause Effect. Basic Books.Qin, Xuezheng, Castiel Chen Zhuang, Rudai Yang. 2017. “One-Child Policy Improve Children’s Human Capital Urban China? Regression Discontinuity Design.” Journal Comparative Economics 45 (2): 287–303.Ragin, Charles. 1987. Comparative Method. Moving Beyond Qualitative Quantitative Strategies. Berkeley: University California Press.Rennie, Drummond. 2004. “Trial registration.” JAMA: Journal American Medical Association 292 (11): 1359–62.Rohlfing, Ingo. 2018. “Power False Negatives Qualitative Comparative Analysis: Foundations, Simulation Estimation Empirical Studies.” Political Analysis 26 (1): 72–89.Rosenbaum, Paul R. 2002. Observational Studies. New York: Springer.Rubin, Donald B. 1984. “Bayesianly Justifiable Relevant Frequency Calculations Applied Statistician.” Annals Statistics 12 (4): 1151–72.Seawright, Jason, John Gerring. 2008a. “Case Selection Techniques Case Study Research: Menu Qualitative Quantitative Options.” Political Research Quarterly 61 (2): 294–308.———. 2008b. “Case Selection Techniques Case Study Research: Menu Qualitative Quantitative Options.” Political Research Quarterly 61 (2): 294–308.Sekhon, Jasjeet S. 2004. “Quality Meets Quantity: Case Studies, Conditional Probability, Counterfactuals.” Perspectives Politics 2 (2): 281–93.Sekhon, Jasjeet S., Rocıo Titiunik. 2016. “Understanding Regression Discontinuity Designs Observational Studies.” Observational Studies 2: 173–81.Sekhon, Jasjeet S., Rocı́o Titiunik. 2017. “Interpreting Regression Discontinuity Design Local Experiment.” Regression Discontinuity Designs, 1–28.Shadish, William, Thomas D. Cook, Donald Thomas Campbell. 2002. Experimental Quasi-Experimental Designs Generalized Causal Inference. Boston: Houghton Mifflin.Skocpol, Theda. 1979. States Social Revolutions: Comparative Analysis France, Russia China. Cambridge University Press.Swank, Duane. 2002. Global Capital, Political Institutions, Policy Change Developed Welfare States. New York: Cambridge University Press.Van Evera, Stephen. 1997. Guide Methods Students Political Science. Ithaca: Cornell University Press.Wang, Wei, David Rothschild, Sharad Goel, Andrew Gelman. 2015. “Forecasting Elections Non-Representative Polls.” International Journal Forecasting 31 (3): 980–91.Weaver, Vesla, Gwen Prowse, Spencer Piston. 2019. “Much Knowledge, Little Power: Assessment Political Knowledge Highly Policed Communities.” Journal Politics 81 (3): 1153–66.Weingast, Barry R. 1998. “Political Stability Civil War: Institutions, Commitment, American Democracy.” Analytic Narratives, edited Robert H. Bates, Avner Greif, Margaret Levi, Jean-Laurent Rosenthal, Barry R. Weingast, 148–93. Princeton University Press.Yamamoto, Teppei. 2012. “Understanding Past: Statistical Analysis Causal Attribution.” American Journal Political Science 56 (1): 237–56.Zarin, Deborah ., Tony Tse. 2008. “Moving Towards Transparency Clinical Trials.” Science 319 (5868): 1340–2.Zhang, Junni L., Donald B. Rubin. 2003. “Estimation Causal Effects via Principal Stratification Outcomes Truncated ‘Death’.” Journal Educational Behavioral Statistics 28 (4): 353–68.","code":""}]
