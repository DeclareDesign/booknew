<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 23 During | Research Design: Declare, Diagnose, Redesign</title>
<meta name="author" content="Graeme Blair, Alexander Coppock, and Macartan Humphreys">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.3.9000/tabs.js"></script><script src="libs/bs3compat-0.2.3.9000/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<link href="libs/bs4_book-1.0.0/dd_imgpopup.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/bs4_book-1.0.0/dd_imgpopup.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://hypothes.is/embed.js" async></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Research Design: Declare, Diagnose, Redesign</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li class="book-part">Introduction</li>
<li><a class="" href="preamble.html"><span class="header-section-number">1</span> Preamble</a></li>
<li><a class="" href="improving-research-designs.html"><span class="header-section-number">2</span> Improving research designs</a></li>
<li><a class="" href="primer.html"><span class="header-section-number">3</span> Software primer</a></li>
<li><a class="" href="part-i-exercises.html"><span class="header-section-number">4</span> Part I Exercises</a></li>
<li class="book-part">Declaration, Diagnosis, Redesign</li>
<li><a class="" href="declaration.html"><span class="header-section-number">5</span> Declaration</a></li>
<li><a class="" href="specifying-the-model.html"><span class="header-section-number">6</span> Specifying the model</a></li>
<li><a class="" href="defining-the-inquiry.html"><span class="header-section-number">7</span> Defining the inquiry</a></li>
<li><a class="" href="crafting-a-data-strategy.html"><span class="header-section-number">8</span> Crafting a data strategy</a></li>
<li><a class="" href="choosing-an-answer-strategy.html"><span class="header-section-number">9</span> Choosing an answer strategy</a></li>
<li><a class="" href="p2diagnosis.html"><span class="header-section-number">10</span> Diagnosis</a></li>
<li><a class="" href="redesign-2.html"><span class="header-section-number">11</span> Redesign</a></li>
<li><a class="" href="part-ii-exercises.html"><span class="header-section-number">12</span> Part II Exercises</a></li>
<li class="book-part">Research Design Library</li>
<li><a class="" href="research-design-library.html"><span class="header-section-number">13</span> Research Design Library</a></li>
<li><a class="" href="observational-designs-for-descriptive-inference.html"><span class="header-section-number">14</span> Observational designs for descriptive inference</a></li>
<li><a class="" href="observational-designs-for-causal-inference.html"><span class="header-section-number">15</span> Observational designs for causal inference</a></li>
<li><a class="" href="experimental-designs-for-causal-inference.html"><span class="header-section-number">16</span> Experimental designs for causal inference</a></li>
<li><a class="" href="experimental-designs-for-descriptive-inference.html"><span class="header-section-number">17</span> Experimental designs for descriptive inference</a></li>
<li><a class="" href="complex-designs-1.html"><span class="header-section-number">18</span> Complex designs</a></li>
<li><a class="" href="part-iii-exercises.html"><span class="header-section-number">19</span> Part III Exercises</a></li>
<li class="book-part">Research Design Lifecycle</li>
<li><a class="" href="research-design-lifecycle.html"><span class="header-section-number">20</span> Research Design Lifecycle</a></li>
<li><a class="" href="brainstorming.html"><span class="header-section-number">21</span> Brainstorming</a></li>
<li><a class="" href="planning.html"><span class="header-section-number">22</span> Planning</a></li>
<li><a class="active" href="during.html"><span class="header-section-number">23</span> During</a></li>
<li><a class="" href="planning-1.html"><span class="header-section-number">24</span> Planning</a></li>
<li><a class="" href="realization.html"><span class="header-section-number">25</span> Realization</a></li>
<li><a class="" href="integration.html"><span class="header-section-number">26</span> Integration</a></li>
<li><a class="" href="part-iv-exercises.html"><span class="header-section-number">27</span> Part IV Exercises</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="during" class="section level1">
<h1>
<span class="header-section-number">23</span> During<a class="anchor" aria-label="anchor" href="#during"><i class="fas fa-link"></i></a>
</h1>
<div id="p4piloting" class="section level2">
<h2>
<span class="header-section-number">23.1</span> Piloting<a class="anchor" aria-label="anchor" href="#p4piloting"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<!-- -- can't learn causal effect -->
<!-- -- what can you learn? about Y0, about M, about se -->
<!-- -- bring in blog post -->
<p>The designs and results of past studies are important guides for selecting M, I, D, and A. Our understanding of the nodes and edges in the causal graph of M, expected effect sizes, the distribution of outcomes, feasible randomization schemes, and many other features are directly selected from past research or chosen based on a literature review of the distribution over past studies. However, researchers face a problem in being guided by past research: the research context and our inquiries often differ in at least subtle ways from any past study. Even when we are replicating a past study, we are collecting data in a different time period and if effects vary over time then aspects of M may differ from the original study. To deal with this, we often run pilot studies. These take many forms: focus groups to learn about features of M or to learn how to ask survey questions; small-scale tests of measurement tools to verify our data collection technology works; up to mini studies with the planned design but on a smaller scale.</p>
<p>Pilot studies are constrained by our time and by money. If we were not constrained, we would run the full study and learn what is wrong with our design and then run a corrected design for the main study. Since we cannot due to our constraints, we run either smaller mini studies or test out only a subset of the elements of our planned design. This places us in a bind: we are running a design smaller or less complete than the study we imagine conducting, and so the properties of the pilot design will not measure up.</p>
<p>MIDA provides a framework for thinking about what can be learned from a pilot through research design diagnosis. Just like for a full study, we can define inquiries about the decisions we would make and the parameter estimates we would draw on in designing the full study.</p>
<p>In Figure <a href="during.html#fig:pilotingfig">23.1</a>, we display the results of a diagnosis of a 50-unit pilot study that we are conducting to prepare for a larger main study. We consider two strategies: (1) determining the sample size from a power analysis of the main study, selecting the minimum <span class="math inline">\(N\)</span> such that the study is 80% powered to detect the pilot study’s effect size); (2) setting a fixed <span class="math inline">\(N\)</span> determined by our budget constraint, in this case to 500, and using the standard deviation of units in the treated and control group from the pilot to determine the minimum detectable effect size of our 500-unit main study.</p>
<p>In the left panel is the sampling distribution of effect size estimates, i.e., a histogram of the effect estimates from the pilot. In the design, the standard deviation of the outcome is set to one, so effect estimates are in standard deviation units. The true effect size is set to 0.2. We can see that the sampling distribution has a huge range, from nearly -0.5 to nearly 0.75. The first problem with the sampling distribution is that many estimates, in fact nearly a quarter of them, are negative (the wrong sign!). This might lead us not only to choose the wrong sample size but to choose one-sided tests in the wrong direction. The second is that we have a high likelihood of guessing the effect size is <em>much</em> higher than it really is. If we obtain one of the estimates over 0.75 or even over 0.5, we would choose an <span class="math inline">\(N\)</span> too small to detect the true effect size of 0.2. In short, our estimates of the effect size from our 50-person pilot study are simply too variable to be useful in designing our main study.</p>
<p>However, there is good news: we can learn a lot about the power of our main study from the pilot study, just not from the effect estimates. In the right panel of Figure <a href="during.html#fig:pilotingfig">23.1</a>, we estimate the minimum detectable effect size of a 500-unit main study, relying on the estimated standard deviation in the control group and the estimated standard deviation in the treatment group to calculate the estimated standard error of the effect estimate in the main study. We then calculate the minimum detectable effect size using the approximation from <span class="citation">Gelman and Hill (<a href="references.html#ref-gelman2006data" role="doc-biblioref">2006</a>)</span>: 2.8 times the estimated standard error (pg. 441). We find that our estimates of the MDE for the full study are much more precise, tightly centered around 0.25. Since we don’t know if that is larger or smaller than the true effect size, we then must make an argument based on past studies’ effect sizes to justify whether that minimum size is sufficiently large or whether we should increase the sample size in order to detect even smaller effects. The reason the MDE is more precisely estimated is that the standard deviation of the control group is a much less variable estimate of the true standard deviation of the control potential outcome than the effect size estimate is of the true effect size.</p>
<div class="figure">
<span id="fig:pilotingfig"></span>
<img src="book_files/figure-html/pilotingfig-1.svg" alt="Learning from pilot studies." width="100%"><p class="caption">
Figure 23.1: Learning from pilot studies.
</p>
</div>
<p>By diagnosing our pilot studies in this way, we can learn what decisions can be made with confidence from pilot data and what should be shaped instead by expectations from past studies and qualitative knowledge. Diagnosis can also help us to decide how large a pilot study we need in order to estimate quantities like the MDE of the full study with precision.</p>
<p>Beyond estimating the MDE of studies, other facts that can often be usefully learned from pilot studies take the form of existence proofs. We often wish to study how variation in <span class="math inline">\(D\)</span> (a treatment) affects variation in <span class="math inline">\(Y\)</span> (an outcome), but in the absence of past data from these two variables, we may not know even if there is variation in <span class="math inline">\(Y\)</span> to explain. In experimental studies, we can learn whether a treatment <em>can</em> be implemented, and in an observational study, we can learn whether there is variation in the treatment variable.</p>
<p>Baseline measurement may often be used instead of a pilot study to learn about some empirical features. If our sample size is fixed and we are interested in learning whether some outcome measures vary across units or how they covary, we can measure them in the baseline and then make adjustments before a posttreatment survey. We will still control for our imperfect measures at baseline to improve efficiency.</p>
<!-- Diagnosing the pilot study on its own provides stark insights, which amount to: we cannot provide answers to the inquiry in the main study, and should not try to do so. There are also aspects of the logistics of research that within time and financial constraints we simply cannot learn until we run the main study. Science is imperfect, and also iterative, but these mistakes or suboptimal design choices also often lead to discoveries. -->
<!-- -- how does it help to diagnose the design together? the properties of the main study *change* when we do a pilot. This is because if we run the pilot study, we are doing so to make decisions about how to run the main study, and so our *design* of the main study and thus its results may depend on the *results* (and design) of the pilot study.  -->
<!-- In this section, we illustrate several general principles that flow from diagnosing pilot studies.  -->
<!-- Purposes of pilot studies: -->
<!-- Existence proofs: -->
<!-- -- is there variation in Y -->
<!-- -- is there variation in X -->
<!-- -- what are nodes in M -->
<!-- -- what are feasible D's, what are feasible treatments / can you implement the treatment (existence proof) -->
<!-- Harder questions requiring bigger sample sizes: -->
<!-- -- what is the distribution of X (helps select stratification proportions etc.) -->
<!-- -- what is the standard deviation of Y0 -->
<!-- #### Assessing a pilot design -->
<!-- declare pilot itself and diagnose just as if it were the main study -->
<!-- if you can't learn the answer, don't make any decisions based on it -->
<!-- #### Assessing a sequenced design -->
<!-- if you are making decisions about MIDA for main study based on pilot, diagnose the procedure of two studies, think about POs of pilot -->
<!-- #### Pilots and baselines -->
<!-- Designs can be reassessed after baselines and before treatment assignment -- so some of the questions you might do a pilot for can just be answered in a baseline -->
<!-- #### BLOG material -->
<!-- Data collection is expensive, and we often only get one bite at the apple. In response, we often conduct an inexpensive (and small) pilot test to help better design the study. Pilot studies have many virtues, including practicing the logistics of data collection and improving measurement tools. But using pilots to get noisy estimates in order to determine sample sizes for scale up comes with risks. -->
<!-- Pilot studies are often used to get a guess of the average effect size, which is then plugged into power calculators when designing the full study. -->
<!-- The procedure is: -->
<!-- 1. Conduct a small pilot study (say, N = 50) -->
<!-- 2. Obtain an estimate of the effect size (this is noisy, but better than nothing!) -->
<!-- 3. Conduct a power analysis for a larger study (say, N = 500) on the basis of the estimated effect size in the pilot -->
<!-- We show in this post that this procedure turns out to be dangerous: at common true effect sizes found in the social sciences, you are at risk of selecting an underpowered design based on the noisy effect estimate in your pilot study. -->
<!-- A different procedure has better properties: -->
<!-- 1. Conduct a small pilot study (say, N = 50) -->
<!-- 2. Obtain an estimate of the **standard deviation of the outcome variable** (again, this is a noisy estimate but better than nothing!) -->
<!-- 3. Estimate the minimum detectable effect (MDE) for a larger study (say, N = 500), using the estimated standard deviation -->
<!-- We show what happens in each procedure, using DeclareDesign. In each case, we'll think about a decision the researcher wants to make based on the pilot: should I move forward with my planned study, or should I go back to the drawing board? We'll rely on power to make that decision in the first procedure and the MDE in the second procedure. -->
<!-- [omitting code] -->
<!-- For each true effect size, the simulations will give us a distribution of estimated effects that a researcher might use as a basis for power analysis. For example, for a true effect size of 0 the researcher might still estimate an effect of 0.10, and so conduct their power analysis assuming that the true effect is 0.10. For each true effect, we can thus construct a distribution of *power estimates* a researcher might obtain from *estimated* effects. Since we know the true power for the true underlying effect, we can compare the distribution of post-hoc power estimates to the true power one would estimate if one knew the true effect size. -->
<!-- What did we find? In the plot, we show our guesses for the power of the main study based on our pilot effect size estimates.  -->
<!-- At high true effect sizes (top row), we do pretty well. Most of our guesses are above 80\% power, leading us to the correct decision that the study is powered. Indeed we often *underestimate* our power in these cases meaning that we run larger studies than we need to. -->
<!-- However, at low true effect sizes (bottom row) we show we are equally likely to find that the design is in fact powered as underpowered. We are equally likely to guess the power of the design is 90% as 10%. There is a good chance that we will falsely infer that our design is well powered just because we happened to get a high estimate from a noisy pilot. -->
<!-- ### How about estimating the standard deviation of the outcome? -->
<!-- Now, let's look at the second approach. Here, instead of using our pilot study to estimate the effect size for a power calculation, we estimate the **standard deviation of the outcome** and use this to calculate the main study's minimum detectable effect. The decision we want to make is: is this MDE small enough to be able to rule out substantively important effects? -->
<!-- We calculate the minimum detectable effect size using the approximation from [@gelman2006data, pg. 441], 2.8 times the estimated standard error. We estimate the standard error using Equation 3.6 from @gerber2012field.  -->
<!-- In summary, pilot studies can be valuable in planning research for many reasons, but power calculations based on noisy effect size estimates can be misleading. A better approach is to use the pilot to learn about the distribution of outcome variables. The variability of the outcome variable can then be plugged into MDE formulas or even power calculations with, say, the smallest effect size of political, economic, or social importance. -->
<!-- In the same spirit, pilot studies could also be used to learn the strength of the correlation between pre-treatment covariates and the outcome variable. With this knowledge in hand, researchers can develop their expectations about how much precision there is to be gained from covariate control or blocking. -->

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
<!-- start post here, do not edit above -->
</div>
<div id="criticism" class="section level2">
<h2>
<span class="header-section-number">23.2</span> Criticism<a class="anchor" aria-label="anchor" href="#criticism"><i class="fas fa-link"></i></a>
</h2>
<!-- make sure to rename the section title below -->
<p>A vital part of the research design process is gathering criticism and feedback from others. Timing is delicate here. Asking for comments on a project that is underdeveloped can sometimes lead to brainstorming sessions about what research questions one might look into – possibly quite useful, but essentially restarts the research design lifecycle from the begining. Sharing work only after a full draft has been produced is worse, since the data strategy will have already produced the realized data <span class="math inline">\(d\)</span>. Feedback can only help a project so much if the fieldwork has already been conducted, the treatments have already been allocated, or the outcomes have already been measured. While critics can always suggest changes to <span class="math inline">\(I\)</span> and <span class="math inline">\(A\)</span> post-data collection, an almost-finished project is fundamentally constrained by the data strategy as it was implemented.</p>
<p>The best moments to seek advice come before the registration of preanalysis plans or, if not pre-registereing, before the implemation of major data strategy elements. The point is not to seek advice exclusively on sampling, assignment, or measurement procedures – it’s that when there’s still time to modify those design elements, feedback about the design as a whole can inform changes to the data strategy.</p>
<p>Feedback will come in many forms. Sometimes the comments are directly about diagnosands – the critic may think the design has too many arms and so won’t be well-powered for many inquiries. The critic may be concerned about bias due to excludability violations or selection issues. Other comments are harder to pin down. A fruitful exercise in such cases is to understand how the criticism fits in to M, I, D, and A. A comment like, “I’m concerned about external validity here” might seem to be about the data strategy – since the units weren’t randomly sampled from some well-specified population, we can’t generalize from the sample to the population. But if the inquiry is not actually a population quantity, then this inability to use sample data to estimate a population quantity is irrelevant. Now the question is whether knowing the answer to your sample inquiry helps make theoretical progress, or whether we need to switch the inquiry to the population quantity in order to make headway. Critics will not usually specificy how their cricitism relates to M, I, D, or A, so it is up to the person seeking feedback to understand the implications of the criticism for the design.</p>
<p>Possibly the most common forum for feedback is the causal in-passing converation. Someone will ask you what you’re working on – “Oh, we’re doing this cool RCT on the effect of PSAs on social norms” and they’ll say “Oh neat, have you read [article they know about], it seems related.” This kind of exchange is often more useful when you can quickly convey what you’re trying to learn (your inquiry) and how you’re trying to learn about it (your data and answer strategies) while in line for coffee or milling about before a talk.</p>
<p>More formal settings include meetings with your advisor or colleagues in which you are specifically asking for help. In order to give good feedback, the advisor needs to understand M, I, D, and A – so sending documents in advance that describe these elements is crucial.</p>
<p>TIMING</p>
<p>Before implimentations of D.</p>
<p>After implementations of D, and your levers are restricted to changes in A or I– those are usually quite limited</p>
<p>WORKSHOP setting</p>
<p>don’t defend yourself.
The point is not to defeed the research. The goal is to break it before implementation.</p>
<p>Learn to interpret criticism as changes to MIDA</p>
<p>you’re saying to change to question</p>
<p>Telling me to do diffeerent analysis. How to tell whether to accept this criticms.</p>
<p>Too many arms
not enough data
I don’t think that’s how it works
wrong estimator
lots of excludability
dont by selection on obseervables
I already knew that
meansurement strategy not good for ystar
Spillovers
I dont care
This doesn’t distinguish between theories (can you rule out alternatives)</p>
<ul>
<li><p>there is a silent piece in the declare-diagnose-redesign framework: getting outside advice on the design and diagnosis <em>before</em> implementation</p></li>
<li><p>most scientific criticism takes place ex post, when</p></li>
<li><p>why before? you can’t change it afterward</p></li>
<li><p>research design document</p></li>
</ul>
<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book--><!-- start post here, do not edit above -->
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="planning.html"><span class="header-section-number">22</span> Planning</a></div>
<div class="next"><a href="planning-1.html"><span class="header-section-number">24</span> Planning</a></div>
</div></main><div id="on-this-page-nav" class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#during"><span class="header-section-number">23</span> During</a></li>
<li><a class="nav-link" href="#p4piloting"><span class="header-section-number">23.1</span> Piloting</a></li>
<li><a class="nav-link" href="#criticism"><span class="header-section-number">23.2</span> Criticism</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="download-code" href="./during.R"><i class="far fa-file-code"></i> Download R code</a></li>
          
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Research Design: Declare, Diagnose, Redesign</strong>" was written by Graeme Blair, Alexander Coppock, and Macartan Humphreys. </p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
