<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 8 Crafting a data strategy | Research Design: Declare, Diagnose, Redesign</title>
<meta name="author" content="Graeme Blair, Jasper Cooper, Alexander Coppock, and Macartan Humphreys">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.2.9000/tabs.js"></script><script src="libs/bs3compat-0.2.2.9000/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<link href="libs/bs4_book-1.0.0/dd_imgpopup.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/bs4_book-1.0.0/dd_imgpopup.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://hypothes.is/embed.js" async></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Research Design: Declare, Diagnose, Redesign</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li class="book-part">Introduction</li>
<li><a class="" href="preamble.html"><span class="header-section-number">1</span> Preamble</a></li>
<li><a class="" href="improving-research-designs.html"><span class="header-section-number">2</span> Improving research designs</a></li>
<li><a class="" href="primer.html"><span class="header-section-number">3</span> Software primer</a></li>
<li><a class="" href="part-i-exercises.html"><span class="header-section-number">4</span> Part I Exercises</a></li>
<li class="book-part">Declaration, Diagnosis, Redesign</li>
<li><a class="" href="declaration.html"><span class="header-section-number">5</span> Declaration</a></li>
<li><a class="" href="specifying-the-model.html"><span class="header-section-number">6</span> Specifying the model</a></li>
<li><a class="" href="defining-the-inquiry.html"><span class="header-section-number">7</span> Defining the inquiry</a></li>
<li><a class="active" href="crafting-a-data-strategy.html"><span class="header-section-number">8</span> Crafting a data strategy</a></li>
<li><a class="" href="choosing-an-answer-strategy.html"><span class="header-section-number">9</span> Choosing an answer strategy</a></li>
<li><a class="" href="p2diagnosis.html"><span class="header-section-number">10</span> Diagnosis</a></li>
<li><a class="" href="redesign-2.html"><span class="header-section-number">11</span> Redesign</a></li>
<li><a class="" href="part-ii-exercises.html"><span class="header-section-number">12</span> Part II Exercises</a></li>
<li class="book-part">Research Design Library</li>
<li><a class="" href="research-design-library.html"><span class="header-section-number">13</span> Research Design Library</a></li>
<li><a class="" href="observational-designs-for-descriptive-inference.html"><span class="header-section-number">14</span> Observational designs for descriptive inference</a></li>
<li><a class="" href="experimental-designs-for-descriptive-inference.html"><span class="header-section-number">15</span> Experimental designs for descriptive inference</a></li>
<li><a class="" href="observational-designs-for-causal-inference.html"><span class="header-section-number">16</span> Observational designs for causal inference</a></li>
<li><a class="" href="experimental-designs-for-causal-inference.html"><span class="header-section-number">17</span> Experimental designs for causal inference</a></li>
<li><a class="" href="multi-study-designs.html"><span class="header-section-number">18</span> Multi-study designs</a></li>
<li><a class="" href="part-iii-exercises.html"><span class="header-section-number">19</span> Part III Exercises</a></li>
<li class="book-part">Research Design Lifecycle</li>
<li><a class="" href="research-design-lifecycle.html"><span class="header-section-number">20</span> Research Design Lifecycle</a></li>
<li><a class="" href="part-iv-exercises.html"><span class="header-section-number">21</span> Part IV Exercises</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="crafting-a-data-strategy" class="section level1">
<h1>
<span class="header-section-number">8</span> Crafting a data strategy<a class="anchor" aria-label="anchor" href="#crafting-a-data-strategy"><i class="fas fa-link"></i></a>
</h1>
<!-- make sure to rename the section title below -->
<p>In order to collect information about the world, researchers must deploy a data strategy. Depending on the design, the data strategy could include decisions about any or or all of the following: sampling, assignment, and measurement. Sampling is the procedure for selecting which units will be measured; assignment is the procedure for allocating treatments to sampled units; and measurement is the procedure for turning information about the sampled units into data.</p>
<p>Sampling choices confront the fundamental problem of generalization: we want to make inferences about units <em>not</em> sampled. For this reason, we need to pay special attention to the procedure by which units are selected into the sample. We might use a random sampling procedure in order to generate a design-based justification for generalizing from samples to the specific population from which units were drawn. Nonrandom sampling procedures are also possible: convenience sampling, respondent-driven sampling, and snowball sampling are all data strategies that do not include an explicitly randomized component. Nonrandomized designs usually require model-based inference in order to generalize from the sample to a population.</p>
<p>Assignment choices confront the fundamental problem of causal inference: we want to make inferences about the conditions to which units were <em>not</em> assigned. For this reason, experimental design is focused on the assignment of treatments. How many treatment conditions should there be? Should we use a simple coin flip to decide who receives treatment, or should we use a more complicated strategy like blocking? Experimenters are of course also very concerned with sampling and measurement procedures, but it is the random assignment to treatments that make randomized experiments distinctive among research designs.</p>
<p>Measurement choices confront the fundamental problem of descriptive inference: we want to make inferences about latent values on the basis of measured values. The tools we use to measure are a critical part of the data strategy. For many social scientific studies, a main way we collect information is through surveys. A huge methodological literature on survey administration has developed to help guide questionnaire development. Bad survey questions yield distorted or noisy responses. A biased question systematically misses the true latent target it is designed to measure, in which case the question has low <em>validity</em>. A question is high variance if (hypothetically) you would obtain different answers each time you asked, in which case the question has low <em>reliability</em>. The concerns about validity and reliability do not disappear once we move out of the survey environment. For example, the information that shows up in an administrative database is itself the result of many human decisions, each of which has the possibility of increasing or decreasing the distance between the measurement and the latent measurement target.</p>
<p>These three problems are all fundamental in the sense that they cannot be surmounted and researchers should be humble in face of them. Strong research design can help, but we can never be <em>sure</em> that our sample generalizes, or that we know what would have happened in a counterfactual state of the world, or what true latent value of the outcome is (or if it even exists). Researchers have to choose good sampling, assignment, and measurement techniques that, when combined and applied to the world, will produce analysis-ready information. We will discuss answer strategies – the set of analysis choices about what to do with the data once they are collected – in the next chapter. The data and answer strategies are of course intimately interconnected. How you analyze data depends deeply on how they were collected <em>and</em> how you collect data depends just as deeply on how you plan to analyze them. For the moment, we are thinking through the many choices we might make as part of the data strategy, but in any applied research design setting, they will have to be considered in concert with the answer strategy.</p>
<p>The data strategy <span class="math inline">\(D\)</span> is a <em>set of procedures</em> that result in a dataset <span class="math inline">\(d\)</span>. It is important to keep these two concepts straight. If you apply data strategy <span class="math inline">\(D\)</span> to the world <span class="math inline">\(w\)</span>, it produces dataset <span class="math inline">\(d\)</span>. We say <span class="math inline">\(d\)</span> is “the” result of <span class="math inline">\(D\)</span>, since when we apply the data strategy to the world, we only do so once and we obtain the data we obtain. But when we are crafting a data strategy, we have to think about the many datasets that the data strategy <em>could have</em> produced. Some of the datasets might be really excellent. For example, in good datasets, we achieve good covariate balance across the treatment and control groups. Or we might draw a sample whose distribution of observable characteristics looks really similar to the population. But some of the datasets might be worse: because of the vagaries of randomization, the particular realizations of the random assignment or random sampling might be more or less balanced. We do not have to settle for data strategies that might produce weak datasets – we are in control of the procedures we choose. We want to choose a data strategy <span class="math inline">\(D\)</span> that is likely to result in a high-quality dataset <span class="math inline">\(d\)</span>.</p>
<p>In Figure <a href="crafting-a-data-strategy.html#fig:datastrategydag">8.1</a>, we illustrate the data strategy and its three elements: sampling, assignment, and measurement. Sampling is the procedure for selecting which units will be measured; assignment is the procedure for allocating treatments to sampled units; and measurement is the procedure for turning information about the sampled units into data. All empirical studies have a data strategy and every data strategy involves sampling, assignment, and measurement. Even studies in which researchers measure the full universe of cases involve sampling, since <em>future</em> units are not included. Even studies in which researchers do not apply treatments themselves involve assignment, since other forces end up assigning units to conditions. Even studies in which researchers take no new measurements involve measurement, since a choice about which measurements made by others to use must nevertheless be made.</p>
<div class="figure">
<span id="fig:datastrategydag"></span>
<img src="book_files/figure-html/datastrategydag-1.svg" alt="DAG illustrating three elements of a data strategy: sampling, assignment, and measurement." width="100%"><p class="caption">
Figure 8.1: DAG illustrating three elements of a data strategy: sampling, assignment, and measurement.
</p>
</div>
<p>In the figure, we rule out threats to inference that come from implementation: failure to treat (noncompliance), failure of inclusion in the sample (attrition), and causal relationships between random sampling and assignment as well as measurement on the latent outcome (excludability). In the last section of the chapter, we discuss when these threats are realized and how we can mitigate these risks they present by design.</p>
<div id="p2sampling" class="section level2">
<h2>
<span class="header-section-number">8.1</span> Sampling strategies<a class="anchor" aria-label="anchor" href="#p2sampling"><i class="fas fa-link"></i></a>
</h2>
<p>Sampling is the process by which units are selected from the population to be studied. Some sampling procedures involve randomization while others do not.
<!-- Sometimes -- perhaps even usually -- randomized sampling procedures face implementation challenges that results in them looking more like nonrandomized designs.  -->
Whether a sampling procedure is randomized or not has large implications for the answer strategy. Randomized designs support “design-based inference,” which refers to the idea that we rely on known features of the sampling process when producing population-level estimates – much more about this in the next chapter on Answer strategies. When randomization breaks down (e.g., if the design encounters attrition) or if a nonrandomized designs are used, then we have to fall back on model-based inference to generalize from the sample to the population. Model-based inference relies on researcher beliefs about the nature of the uncontrolled sampling process in order to make inferences about the population. When possible, design-based inference has the advantage of letting you base inferences from known rather than assumed features of the data selection process. That said, when randomly sampled individuals fail to respond or when we seek to make inferences about <em>new</em> populations, we must apply model-based inference even if the data are the result of a random process.</p>
<p>Why would we ever be content to study a sample and not the full population? For infinite populations we have no choice. For finite populations the first and best explanation is cost: it’s expensive and time-consuming to conduct a full census of the population. Even well-funded research projects face this problem, since money and effort spent answering one question could also be spent answering a second question. We tend to sample rather than measure every unit in the population because we face opportunity costs as well. A second reason reason to sample is the diminishing marginal returns of additional data collection. Increasing the number of sampled units from 1,000 to 2,000 will greatly increase the precision of our estimates. Moving from 100,000 to 101,000 will improve things too, but the scale of the improvement is much smaller.</p>
<div id="randomized-sampling-designs" class="section level3">
<h3>
<span class="header-section-number">8.1.1</span> Randomized sampling designs<a class="anchor" aria-label="anchor" href="#randomized-sampling-designs"><i class="fas fa-link"></i></a>
</h3>
<p>Owing to the natural appeal of design-based inference, we start off with randomized designs before proceeding to nonrandomized designs. Randomized sampling designs typically begin with a list of all units in a population, then choose a subset to sample using a random process. These random processes can be simple (every unit has an equal probability of inclusion) or complex (first we select regions at random, then villages at random within selected regions, then households within selected villages, then individuals within selected households).</p>
<p>Table <a href="crafting-a-data-strategy.html#tab:samplingtypes">8.1</a> collects all of these kinds of random sampling together and offers an example of functions in the <code>randomizr</code> package you can use to conduct these kinds of sampling. The most basic form is simple random sample, also called “coin flip” or Bernoulli random sampling. Under simple random assignment, all units in the population have the same probability <span class="math inline">\(p\)</span> of being included in the sample. It is sometimes called coin flip random sampling because it is as though for each unit, we flip a weighted coin that has probability <span class="math inline">\(p\)</span> of landing heads-up. While quite straightforward, a drawback of simple random sampling is that we can’t be sure of the number of sampled units in advance. On average, we’ll sample <span class="math inline">\(N*p\)</span> units, sometimes slightly more units will be sampled and sometimes fewer.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:samplingtypes">Table 8.1: </span> Kinds of random sampling</caption>
<colgroup>
<col width="6%">
<col width="74%">
<col width="19%">
</colgroup>
<thead><tr class="header">
<th>Design</th>
<th>Description</th>
<th>Randomizr function</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Simple random sampling</td>
<td>“Coin flip” or Bernoulli random sampling. All units have the same inclusion probability p</td>
<td><code>simple_rs(N = 100, p = 0.25)</code></td>
</tr>
<tr class="even">
<td>Complete random sampling</td>
<td>Exactly n of N units are sampled, and all units have the same inclusion probability n/N</td>
<td><code>complete_rs(N = 100, n = 40)</code></td>
</tr>
<tr class="odd">
<td>Stratified random sampling</td>
<td>Complete random sampling within pre-defined strata. Units within the same strata have the same inclusion probability n_s / N_s, but units in different strata might have different inclusion probabilities</td>
<td><code>strata_rs(strata = regions)</code></td>
</tr>
<tr class="even">
<td>Cluster random sampling</td>
<td>Whole groups of units are brought into the sample together.</td>
<td><code>cluster_ra(clusters = households)</code></td>
</tr>
<tr class="odd">
<td>Stratifed cluster sampling</td>
<td>Cluster random sampling within strata</td>
<td><code>strata_and_cluster_rs(strata = regions,clusters = villages)</code></td>
</tr>
<tr class="even">
<td>Multi-stage random sampling</td>
<td>First clusters, then units within clusters</td>
<td>
<code>cluster_ra(clusters = villages)</code>; <code>strata_ra(strata = villages)</code>
</td>
</tr>
</tbody>
</table></div>
<p>Complete random sampling addresses this problem. Under complete random sampling, exactly <span class="math inline">\(n\)</span> of <span class="math inline">\(N\)</span> units are sampled. Each unit still has an inclusion probability of <span class="math inline">\(p = n/N\)</span>, but in contrast to simple random sampling, we are guaranteed that the final sample will be of size <span class="math inline">\(n\)</span>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;To convince yourself of the difference between simple and complete random sampling, run &lt;code&gt;table(simple_rs(N = 100, prob = 0.5))&lt;/code&gt; a few times and compare the results with &lt;code&gt;table(complete_rs(N = 100, n = 50))&lt;/code&gt;&lt;/p&gt;"><sup>8</sup></a> Complete random sampling represents an improvement over simple random sampling because it rules out samples in which more or fewer than <span class="math inline">\(N*p\)</span> units are sampled. One circumstance in which we might nevertheless go with simple random sampling is when the size of the population is not known in advance and sampling choices have to be made “on the fly.”</p>
<p>Complete random sampling solves the problem of fixing the total number of sampled units, but it doesn’t address the problem that the total number of units with particular characteristics will not be fixed. Imagine a population with <span class="math inline">\(N_{y}\)</span> young people and <span class="math inline">\(N_{o}\)</span> old people. If we sample exactly <span class="math inline">\(n\)</span> from the population <span class="math inline">\(N_{y} + N_{o}\)</span>, the number of sampled young people (<span class="math inline">\(n_y\)</span>) and sampled old people (<span class="math inline">\(n_{o}\)</span>) will bounce around from sample to sample. We can solve this problem by conducting complete random sampling <em>within</em> each group of units. This procedure goes by the name stratified random sampling, since the sampling is conducted separately within the strata of units.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;To convince yourself of the difference between complete and stratified sampling, run &lt;code&gt;age &amp;lt;- rep(c("Y", "O"), 50); table(age, complete_rs(N = 100, n = 50))&lt;/code&gt; a few times and compare the results with &lt;code&gt;table(age, strata_rs(strata = age))&lt;/code&gt;&lt;/p&gt;'><sup>9</sup></a> In our example, our strata were formed by a dichotomoous grouping of people into “young” and “old” categories, but in general, the sampling strata can be formed by any information we have about units before they are sampled. Stratification offers at least three major benefits. First, we defend against sampling surprisingly too few units in some stratum by “bad luck.” Second (as discussed in the chapter on answer strategies) stratification tends to produce lower variance estimates of most inquiries. Finally, stratification allows researchers to “oversample” subgroups of particular interest.</p>
<p>Stratified sampling should not be confused with cluster sampling. Stratified sampling means that a fixed number of units from a particular group are drawn into the sample. Cluster sampling means that units from a particular group are brought into the sample <em>together</em>. For example, if we cluster sample households, we interview all individuals living in a sampled household. Clustering introduces dependence in the sampling procedure – if one member of the household is sampled, the other members are also always sampled. Relative to a complete random sample of the same size, cluster samples tend to produce higher variance estimates. Just as the individual sampling designs, cluster sampling comes in simple, complete, and stratified varieties with exactly parallel logics and motivations.</p>
<p>Lastly, we turn to multi-stage random sampling, in which we conduct random sampling at multiple levels of a heirarchically-structured population. For example, we might first sample regions, then villages within regions, then households within villages, then individuals within households. Each of those sampling steps might be stratified or clustered depending on researcher goals. The purpose of a multi-stage approach is typically to balance the logistical difficulties of visiting many geographic areas with the relative ease of collecting additional data once you have arrived.</p>
<p>Figure <a href="crafting-a-data-strategy.html#fig:samplingquilt">8.2</a> gives a graphical interpretation of each of these kinds of random sampling. Here, we imagine a population of 64 units with two levels of heirarchy. For concreteness, we can imagine that the units are indiviudals nested within 16 households of four people each and the 16 households are nested within four villages of four people each. Starting at the top left, we have simple random samping at the individual level. The inclusion probability was set to 0.5, so on average, we ought to sample 32 people, but in this particular draw, we actually sampled only 29. Complete random sampling (top center), fixes this problem, so exactly 32 people are sampled – but these 32 are unevenly spread across the four villages. This is addressed with stratified sampling – here we sample exactly 8 people at random from each village of 16 total people.</p>
<p>Moving down to the middle row of the figure, we have three approaches to clustered random sampling. Under simple sampling at the cluster, each cluster has the same probability <span class="math inline">\(p\)</span> of inclusion in the sample, so on average we will sample eight clusters. This time, we only sampled seven, this problem can again be fixed with complete random sampling (center facet), but again we have an uneven distribution across villages. Stratified cluster sampling ensures that exactly two households from each village are sampled.</p>
<p>The bottom row of the figure illustrates some approaches to multistage sampling. In the bottom left panel, we conduct a simple random sample of individuals in each sampled cluster. In the bottom center, we draw a complete random sample of indiviudals in each sampled household. And in the bottom left, we stratify on an individual level characteristic – we always draw one individual from each row of the household. “Row” could refer to the age of the household members. This doubly-stratified multistage random sampling procedure ensures that we sample two households from each village and within those households, one older member and one younger member.</p>
<div class="figure">
<span id="fig:samplingquilt"></span>
<img src="book_files/figure-html/samplingquilt-1.svg" alt="Nine kinds of random sampling" width="100%"><p class="caption">
Figure 8.2: Nine kinds of random sampling
</p>
</div>
<!-- There are many ways to draw a random sample, but two are particularly common in the social sciences: simple and stratified. Simple random sampling is sampling without replacement with a fixed sample size. Stratified random sampling takes a simple random sample of a fixed size from two or more subgroups, or strata, in the data. There are two different reasons you might select stratified random sampling over simple: your inquiry might involve comparisons between two subgroups, and you want to ensure you have a sufficient size in each subgroup; and there may be very different amounts of variability in two groups, and you want the most efficient estimate of a summary of that data so you oversample from the group with the higher variance to get a similar amount of precision from each group. Stratified sampling is a simple case of a larger class of sampling strategies, in which the probability of inclusion in the sample varies across units. In stratified sampling with two strata, the probability of inclusion is set for the two groups and is fixed within them. However, the probability can vary within strata as well. If you wish to have you sample include all age groups but upweight older people, you could make the probability of inclusion a function of age: the probability of sampling an 18 year old could be 0.01 ranging up to a probability of 0.1 for 80 year olds. -->
<!-- When it is impossible to get a list of all units in the target population either because it does not exist or is too costly to obtain, clustered sampling is often a substitute. In cluster sampling, you obtain a list of clusters of units, randomly sample from the list of clusters, and either collect data from all units within selected clusters, or conduct a simple random sample of units within sampled cluster (this would represent a simple form of multistage sampling). The advantage is you can still obtain a random sample of units, but it is feasible to do so because you have a list of clusters (but not of all units). The disadvantage is that units within clusters often have similar outcomes, so you don't get as much benefit from sampling more clusters as you do from sampling units from other clusters that are more different. This efficiency tradeoff -- how many clusters to choose vs. how many units within clusters -- can be explored in design diagnosis by comparing the RMSE of various choices. There are many other forms of multistage sampling -- sample provinces then sample districts within provinces, then villages within districts, and people within villages -- that fit particular aims and budgets. -->
</div>
<div id="nonrandomized-sampling-designs" class="section level3">
<h3>
<span class="header-section-number">8.1.2</span> Nonrandomized sampling designs<a class="anchor" aria-label="anchor" href="#nonrandomized-sampling-designs"><i class="fas fa-link"></i></a>
</h3>
<p>Because nonrandomized sampling procedures are defined by what they don’t do – they don’t use randomization – a hugely varied set of procedures could be described this way. We’ll consider a just a few common ones, since the idiosyncracies of each approach are hard to systemetize.</p>
<p>Convenience sampling refers to the practice of gathering units from the population in the least expensive way available to you. Convenience sampling is a good choice when generalizing to an explicit population is not a main goal of the design, for example when a sample average treatment effect is a theoretically-important inquiry. For many decades, social science undergraduates were the most abundant data source available to academics and many important theoretical claims have been established on the basis of experiments conducted with such samples. In recent years, however, online convenience samples like Mechanical Turk, Prolific, or Lucid have mostly supplanted undergraduates as the convenience sample of choice. In other (mostly nonexperimental) circumstances, however, convenience sampling is likely to lead to badly biased estimates. For example, cable news shows often conduct viewer polls that should not be taken at all seriously. While such polls might promote viewer loyalty (and so might be worth doing from the cable executives’ perspective) they do not provide credible evidence about what the population at large thinks or believes.</p>
<p>Many types of qualitative and quantitative research involve convenience sampling. Archival research often involves a convenience sample of documents on a certain topic that exist in an archive. The question of how these documents differ from those that would be in a different archive, or how the documents available in archives differ from those that do not ever make it into the archive importantly shapes what we can learn from them (Aliza Luft paper cite). With the decline of telephone survey response rates (cite), researchers can no longer rely on random digit dialing to obtain a representative sample of people in many countries, and instead must rely on convenience samples from the internet or panels who agree to have their phone numbers in a list. Sometimes, reweighting techniques in the answer strategy can, in some cases, help recover estimates for the population as a whole if sampling if a credible model of the unknown sampling process can be agreed upon.</p>
<p>Next, we consider purposive sampling. Purposive is a catch-all term for rule-based sampling strategies that do not involve random draws but also are not purely based on convenience and cost. A common example is quota sampling. Sampling purely based on convenience often means we will end up with many units of one type but very few of another type. Quota sampling addresses the problem by continuing to search for subjects until target counts (quotas) of each find of subject are found. Loosely speaking, quota sampling is to convenience sampling as stratified random sampling is to complete random sampling: it fixes the problem that not enough (or too many) subjects of particular types are sampled by employing specific quotas. Importantly, however, we have no guarantee that the sampled units <em>within</em> type are representative of that type over all: quota samples are within-stratum convenience sampes.</p>
<p>A second common form of purposive sampling is respondent-driven sampling, which is used to sample from hard-to-reach populations such as HIV-positive needle users. RDS methods often begin with a convenience sample and then systematically obtain contacts for other units who share the same characteristic in order the build a large sample.</p>
<p>Each of these three nonrandom sampling procedures – convenience, quota, and respondent-driven – are illustrated in Figure <a href="crafting-a-data-strategy.html#fig:nonrandomsamplingquilt">8.3</a>. Imagining that village A is easier to reach, we could obtain a convenience sample by contacting everyone we can reach in village A before moving on to village B. This process doesn’t yield good coverage across villages and for that we can turn to quota sampling. Under this quota sampling scheme, we talk to the five people who are easiest to reach in each of the four villages. Finally, if we conduct a respondent-driven sample, we select one seed unit in each village, and that person recruits their four closest friends (who may or may not reside in the same village).</p>
<div class="figure">
<span id="fig:nonrandomsamplingquilt"></span>
<img src="book_files/figure-html/nonrandomsamplingquilt-1.svg" alt="Three forms of non-random sampling." width="100%"><p class="caption">
Figure 8.3: Three forms of non-random sampling.
</p>
</div>
</div>
<div id="sampling-designs-for-qualitative-research" class="section level3">
<h3>
<span class="header-section-number">8.1.3</span> Sampling designs for qualitative research<a class="anchor" aria-label="anchor" href="#sampling-designs-for-qualitative-research"><i class="fas fa-link"></i></a>
</h3>
<p>Case selection is another term for a sampling strategy. In case study research, whether qualitative or quantitative, the way you select the (typically small) set of cases is of great importance, and considerable attention has been paid to developing case selection methods.</p>
<p>John Stuart <span class="citation">Mill (<a href="references.html#ref-mill1884system" role="doc-biblioref">1884</a>)</span> elaborated methods of induction that have inspired two popular case selection methods based on whether outcomes are “different” or “agree.” The method of difference involves selecting cases that have divergent outcomes but otherwise look very similar. If one characteristic covaries with the outcome, it becomes a candidate for the cause. For example, <span class="citation">Skocpol (<a href="references.html#ref-skocpol1979states" role="doc-biblioref">1979</a>)</span> compares historical periods in France, Russia, the United Kingdom, and Germany that look very similar in many regards. The first two, however, had social revolutions, while the second two did not. The presence of agrarian institutions that provided a degree of political autonomy to the peasants in France and Russia and their absence in the UK and Germany then becomes a possible clue to understanding the underlying causal structure of social revolutions. We discuss the debate around this answer strategy in the section on answer strategies. By contrast, the method of agreement involves selecting cases that share the same outcome but diverge on a host of other characteristics. Any characteristics that are <em>common</em> to the cases then become candidates for causal attribution.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Suppose you and a family member get food poisoning. You compare all the meals you ate over the course of the day. At breakfast, lunch, and dinner you ate different meals, though you shared a salad at lunch. Your sampling strategy involves selecting all meals over the past day and no other (future or past) meals. The method of agreement suggests it was the salad that caused the food poisoning.&lt;/p&gt;"><sup>10</sup></a> In both of these strategies, the selection of cases is guided by the inquiry and answer strategy.</p>
<p>Some qualitative case selection strategies borrow directly from the toolkit of quantitative, large-<span class="math inline">\(N\)</span> analysis. Where datasets on a large number of cases are are available, <span class="citation">Lieberman (<a href="references.html#ref-lieberman2005nested" role="doc-biblioref">2005</a>)</span> proposes using the predicted values from a regression model—often referred to as the “regression line”—from an initial quantitative analysis in order to select cases for in-depth analysis.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Another example of how quantitative methods can guide case selection is given by &lt;span class="citation"&gt;Ingram and Harbers (&lt;a href="references.html#ref-ingram2020spatial" role="doc-biblioref"&gt;2020&lt;/a&gt;)&lt;/span&gt;, who suggest using quantitative tools of spatial analysis to guide qualitative case selection.&lt;/p&gt;'><sup>11</sup></a> Again, the inquiry and answer strategy guide case selection. When the inquiry is focused on uncovering the same causal relationship sought in the quantitative analysis, <span class="citation">Lieberman (<a href="references.html#ref-lieberman2005nested" role="doc-biblioref">2005</a>)</span> suggests selecting cases that are relatively well-predicted and that maximize variation on the causal variable. He points to <span class="citation">Martin (<a href="references.html#ref-Martin1992" role="doc-biblioref">1992</a>)</span> and <span class="citation">Swank (<a href="references.html#ref-Swank2002" role="doc-biblioref">2002</a>)</span> as examples of designs employing this strategy. However, <span class="citation">Lieberman (<a href="references.html#ref-lieberman2005nested" role="doc-biblioref">2005</a>)</span> advocates a different case selection strategy when the goal is to expand upon the theory initially tested in the quantitative analysis. In that instance, he recommends choosing cases lying far from the regression line, which are not well-predicted and may therefore lead to insights about what alternative mechanisms were left out of the initial regression.</p>
<p><span class="citation">Seawright and Gerring (<a href="references.html#ref-SeawrightGerring2008" role="doc-biblioref">2008</a><a href="references.html#ref-SeawrightGerring2008" role="doc-biblioref">a</a>)</span> use the regression line analogy to describe seven different sampling strategies tailored to suit different inquiries, depicted below in Figure <a href="crafting-a-data-strategy.html#fig:caseselectionstrategies">8.4</a>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;See &lt;span class="citation"&gt;Gerring and Cojocaru (&lt;a href="references.html#ref-gerring2016selecting" role="doc-biblioref"&gt;2016&lt;/a&gt;)&lt;/span&gt; for a yet larger list, in which the strengths of these different strategies for different inquiries are expanded upon.&lt;/p&gt;'><sup>12</sup></a> The horizontal axis represents some characteristic of cases, <span class="math inline">\(X\)</span>, such as the degree of labor union strength. The vertical axis represents the outcome, <span class="math inline">\(Y\)</span>, such as degree of welfare state generosity. The line represents the predicted cross-case relationship, dark grey squares represent possible cases in the population, and blue squares represent the sample chosen under each strategy.</p>
<div class="figure">
<span id="fig:caseselectionstrategies"></span>
<img src="book_files/figure-html/caseselectionstrategies-1.svg" alt="Case selection Strategies" width="100%"><p class="caption">
Figure 8.4: Case selection Strategies
</p>
</div>
<p>Typical cases typify the cross-case relationship and can be chosen in order to explore and validate mediating mechanisms. If the researcher’s model implies union membership increases welfare spending in democracies through its effects on negotiations with the government, for example, then the researcher might look for evidence of such processes in the cases well-predicted by the theory. Diverse cases maximize variation on both <span class="math inline">\(X\)</span> <em>and</em> <span class="math inline">\(Y\)</span>, while extreme cases are located a maximal distance from other cases on just one dimension—in our example, the researcher chooses the two cases with the highest degree of union strength. While diverse and extreme cases might lie on the regression line, deviant cases are defined by their distance from it. Influential cases are those whose exclusion would most noticeably change the imaginary regression line (i.e., those with the highest leverage in a regression). Thus, while diverse and extreme case selection can be most useful for exploratory work, like updating the theoretical model or finding new ways to pose the inquiry, deviant and influential case selection can be useful for understanding scope conditions on a theory. Confusingly, the “most similar systems” design employs Mill’s method of <em>difference</em> in order to select cases that are similar on <span class="math inline">\(X\)</span> but different on <span class="math inline">\(Y\)</span>, while the “most different systems” design employs his method of agreement to select cases that agree on <span class="math inline">\(Y\)</span> but are different in their characteristics.</p>
<p>It is very unlikely that every strategy available will provide equally good answers in every research design, or that one method will always fare better than others. Declaring and simulating your design under some simplifying assumptions about the model, inquiry, and answer strategy clarifies how “the cases you choose affect the answers you get” <span class="citation">(Geddes <a href="references.html#ref-geddes2003paradigms" role="doc-biblioref">2003</a>)</span>.</p>
<p>For example, <span class="citation">Herron and Quinn (<a href="references.html#ref-herron2016careful" role="doc-biblioref">2016</a>)</span> used Monte Carlo simulations to study how well the seven strategies depicted above perform when the model stipulates that cases are of four causal types,<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;For a version of this model, see the process tracing entry in the Design Library&lt;/p&gt;"><sup>13</sup></a> the inquiry is the average treatment effect in the population, and the answer strategy involves, perhaps optimistically, perfectly observing the selected cases’ causal type. With these simplifying assumptions, they uncover a clear hierarchy and set of prescriptions: extreme and deviant case selection fare much worse than the other methods in terms of the three diagnosands considered (root mean square error, variance, and bias of the mean of the posterior distribution). By contrast, influential case selection outperforms the other strategies, followed closely by diverse and simple random sampling. As the authors acknowledge, however, this hierarchy might look very different if the inquiry aimed at a different, exploratory quantity (such as discovering the number of causal types that exist). <span class="citation">Humphreys and Jacobs (<a href="references.html#ref-humphreys2015mixing" role="doc-biblioref">2015</a>)</span> provide simulations where they incorporate a process tracing inferential procedure and highlight the importance of “probative value” for case selection. The point is that there is rarely a case selection strategy that fits all problems equally well—the best strategy is the one that optimizes a particular diagnosand given stipulations about the inquiry, the model, and the answer strategy. If you can justify those stipulations and the importance of the diagnosand, then defending the choice of sampling strategy is straightforward.</p>
<p>This point speaks to two of the greatest controversies in qualitative case selection: whether to randomize and whether to select using information on the dependent variable.</p>
<p>Since qualitative case analysis is labor-intensive, sample sizes are typically small (in the 1-30 case range). Scholars like <span class="citation">Seawright and Gerring (<a href="references.html#ref-SeawrightGerring2008" role="doc-biblioref">2008</a><a href="references.html#ref-SeawrightGerring2008" role="doc-biblioref">a</a>)</span> point out that random sampling therefore produces very high variance and can even lead to abberant samples that have no variation in outcomes or explanatory variables, or that share idiosyncratic features or locations. As such, they advocate for purposive selection. <span class="citation">Fearon and Laitin (<a href="references.html#ref-fearon2008integrating" role="doc-biblioref">2008</a>)</span> point out, however, that convenience samples and purposive selection can lead to severe bias, especially when the goal is to make a population-level inference. They advocate the use of stratified random sampling. Yet, the question of whether or not to randomly sample cannot be resolved by deciding which of these claims is correct, since they are both correct for different diagnosands: nonrandom sampling might minimize variance while maximizing bias, while random sampling might increase variance while minimizing bias. And even these diagnosand-specific claims are conditional on the specific model, inquiry, and answer strategy the authors had in mind.</p>
<p>This reasoning also applies to the question of whether to incorporate information about the dependent variable into sampling decisions. <span class="citation">Geddes (<a href="references.html#ref-geddes2003paradigms" role="doc-biblioref">2003</a>)</span> famously critiqued <span class="citation">Skocpol (<a href="references.html#ref-skocpol1979states" role="doc-biblioref">1979</a>)</span> for making an inference about the role of foreign threat in social revolution based on a selection of cases that all experienced revolutions.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;As we explained above, her analysis of the role of peasant autonomy arguably did not suffer from this same issue as it employed “negative” cases (those that don’t exhibit the outcome, see also: &lt;span class="citation"&gt;Mahoney and Goertz (&lt;a href="references.html#ref-mahoneygoertz2004" role="doc-biblioref"&gt;2004&lt;/a&gt;)&lt;/span&gt;).&lt;/p&gt;'><sup>14</sup></a> In essence, Geddes criticizes the resultant lack of variation in the outcome, which makes it difficult to assess whether revolutions failed to occur, for example, in countries that did not face foreign threats.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;The same problem can arise in observational quantitative research. &lt;span class="citation"&gt;Ashworth et al. (&lt;a href="references.html#ref-ashworth2008design" role="doc-biblioref"&gt;2008&lt;/a&gt;)&lt;/span&gt; discuss the bias that results from studying the factors predicting suicide terrorism by only looking at observations where suicide terrorism occurred.&lt;/p&gt;'><sup>15</sup></a> However, this critique assumes Skocpol’s answer strategy is based on Mill’s method of difference<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;A point that is debated, see: &lt;span class="citation"&gt;Sekhon (&lt;a href="references.html#ref-Sekhon2004" role="doc-biblioref"&gt;2004&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;'><sup>16</sup></a> and that the inquiry focuses on something like an average effect of foreign threat on revolution—i.e., an effects-of-cause question. In the design library, we declare a process-tracing design in which the inquiry is a cause-of-effects question: not, what is the average effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>, but, given that <span class="math inline">\(Y\)</span> happened, what is the likelihood that it happened because of <span class="math inline">\(X\)</span>? For such inquiries, selecting on the dependent variable is not only essential (it’s hard to study how <span class="math inline">\(Y\)</span> happened when it didn’t happen!), but advantageous: it cuts the space of competing causal hypotheses in half, and drastically simplifies the answer strategy. Note that this prescription does not necessarily hold for <em>quantitative</em> answer strategies: <span class="citation">Yamamoto (<a href="references.html#ref-yamamoto2012understanding" role="doc-biblioref">2012</a>)</span> has shown that “negative” cases are necessary to estimate cause-of-effects questions, even if they are not necessary to define the inquiry.</p>
<p>Thus, as with so many data strategy choices, whether or not it makes sense to sample using information on outcomes will depend on I, M, and A, not to mention the diagnosands, <span class="math inline">\(\phi\)</span>, by which we judge design performance.</p>
<!-- Need to finish this properly. Something about how simulation of research designs necessarily engages in abstraction, and this is particularly apparent in the drastic simplifications required to declare qualitative strategies on a computer. AS the examples show, however, these can yield really good insights.  -->
</div>
<div id="choosing-among-sampling-designs" class="section level3">
<h3>
<span class="header-section-number">8.1.4</span> Choosing among sampling designs<a class="anchor" aria-label="anchor" href="#choosing-among-sampling-designs"><i class="fas fa-link"></i></a>
</h3>
<p>In short, the choice of sampling strategy depends on features of the model and the inquiry, and different sampling strategies can be compared in terms of power and RMSE in design diagnosis. The model defines the population of units we are interested in making inferences about, and the target population of the sampling strategy should match that as much as possible. The model also points us to important subgroups (defined by nodes or endogenous variables) that we may wish to stratify on, depending on the variability within those subgroups. Whether we select convenience, random, or purposive sampling depends on our budget and logistical constraints as well as the efficiency (power or RMSE) of the design. If there is little bias from convenience sampling, we will often want to select it for cost reasons; if we cannot obtain a convenience sample that has the right composition, we may choose a purposive method that ensures we do. The choice between simple and stratified sampling comes down to the inquiry and to a diagnosis of the RMSE: when the inquiry involves a comparison of subgroups, we will often select stratified sampling. In either, a diagnosis of alternative designs in terms of power or RMSE will guide selection.</p>
<p>Sampling, like all data strategies, is an intervention in the world by researchers. As a result, it may have independent causal effects on outcomes. As we will see in the next section, there is no deep difference between sampling and treatment assignment, because in each case we are randomly sampling from two potential outcomes. In treatment assignment, we are assigning units to a treatment group and a control group, and obtaining a sample of the treated potential outcome and a second random sample of the control potential outcome. In random sampling, we are assigning units to be sampled or to not be sampled. We obtain one sample of the sampled potential outcome. However, the non-sampled potential outcome exists conceptually, and may differ from the sampled potential outcome. The act of including units in the sample may change outcomes, for example if you are selected to participate in a medical trial you believe you are going to receive better care and a placebo effect changes your health condition. A research design in which you conduct an experiment on sampled units, but also unobtrusively measure outcomes in nonsampled units could be used to estimate the effect of inclusion in the sample. This design would provide a random sample of the nonsampled potential outcome.</p>
<p>Design-based inference can help extrapolate from our sample data to quantities in the population, using the details of the sampling design. Whether our extrapolations are unbiased and efficient will depend on how we sampled. We are sometimes also interested in extrapolating outside the population we sampled from to other populations. Model-based inference can be used to extrapolate to other populations, with assumptions about how the population you sample from differs from the population you wish to extrapolate to.</p>
<!-- you create strata and clusters from real things in the world -->
</div>
<div id="example-wang2015forecasting" class="section level3">
<h3>
<span class="header-section-number">8.1.5</span> Example: <span class="citation">Wang et al. (<a href="references.html#ref-wang2015forecasting" role="doc-biblioref">2015</a>)</span><a class="anchor" aria-label="anchor" href="#example-wang2015forecasting"><i class="fas fa-link"></i></a>
</h3>
<p>Xbox forecasting of elections</p>
<!-- what is scraping? -->
<!-- what is going to the archive? -->
<!-- sampling on the DV -->
<!-- redefine your inquiry -->
<!-- ALSO is your inquiry really the right one - difference between target population and real population (Americans in 2016 vs Americans every year) -->
<!-- -- sample all of the population or part of it -->
<!-- -- random sampling, purposive sampling -->
<!-- - simple, complete, stratified, clustered, stratified and clustered -->
<!-- - weighted sampling (over/undersampling) -->
<!-- - finding missing populations (RDS) -->
<!-- - quota sampling -->
<!-- - hawthorne effects -->
<!-- - the inquiry and the model guide your choice of which units to sample and how to sample them -->
</div>
</div>
<div id="p2assignment" class="section level2">
<h2>
<span class="header-section-number">8.2</span> Treatment assignment<a class="anchor" aria-label="anchor" href="#p2assignment"><i class="fas fa-link"></i></a>
</h2>
<p>In many studies, researchers intervene in the world to <strong>set</strong> the level of the causal variable of interest. The procedures used to assign units to treatment are tightly analogous to the procedures explored in the previous section on sampling. Like sampling, assignment procedures fall in to two classes, randomized and nonrandomized. Among the radnomized procedures we can distinguish between a host of two arm trial designs, multiarm designs, and a set of more designs advanced designs.</p>
<div id="two-arm-trials" class="section level3">
<h3>
<span class="header-section-number">8.2.1</span> Two arm trials<a class="anchor" aria-label="anchor" href="#two-arm-trials"><i class="fas fa-link"></i></a>
</h3>
<p>The analogy between sampling and assignment runs deep. All of sampling designs discussed in the previous section have directly equivalent assignment designs. Simple random sampling is analogous to Bernoulli random assignement, stratified random sampling is analogous to blocked random assignment and so on. Many of the same design tradeoffs hold as well: just like cluster sampling generates higher variance estimates than individual sampling, clustered assignment generates higher variance estimates than individual assignment. While we usually think of randomized assignment designs only, nonrandomized designs in which the research applies treatments also occur. For example, researchers sometimes treat a convenience sample, then search out a different convenience sample to serve as a control group. Within-subject designs in which subjects are measured, then treated, then measured again are a second example of nonrandomized application of treatment.</p>
<p>The analogy between sampling and assignment runs so deep because in a sense, assignment <strong>is</strong> sampling. Instead of sampling units in or out of the study, we sample from alternative possible worlds. The treatment group represents a sample from the alternative world in which all units are treated and the control group represents a sample from the alternative world in which all units are untreated.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Strictly speaking, this claim only holds under a noninterference assumption; if the usual noninterference assumption is incorrect, we have to redefine potential outcomes in order to recover “stability.” Assignment strategies sample from possible worlds of stable potential outcomes.&lt;/p&gt;"><sup>17</sup></a> We can reencounter the fundamental problem of causal inference through this lens – if a unit is sampled from one possible world, it can’t be sampled from any other possible world. Table <a href="crafting-a-data-strategy.html#tab:assignmenttypes">8.2</a> collects together common forms of random assignment.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:assignmenttypes">Table 8.2: </span> Kinds of random assignment</caption>
<colgroup>
<col width="6%">
<col width="74%">
<col width="19%">
</colgroup>
<thead><tr class="header">
<th>Design</th>
<th>Description</th>
<th>Randomizr function</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Simple random assignment</td>
<td>“Coin flip” or Bernoulli random assignment All units have the same treatment probability p</td>
<td><code>simple_ra(N = 100, p = 0.25)</code></td>
</tr>
<tr class="even">
<td>Complete random assignment</td>
<td>Exactly n of N units are sampled, and all units have the same treatment probability n/N</td>
<td><code>complete_ra(N = 100, m = 40)</code></td>
</tr>
<tr class="odd">
<td>Block random assignment</td>
<td>Complete random assignment within pre-defined blocks. Units within the same block have the same treatment probability m_b / N_b, but units in different blocks might have different treatment probabilities</td>
<td><code>block_ra(blocks = regions)</code></td>
</tr>
<tr class="even">
<td>Cluster random assignment</td>
<td>Whole groups of units are assigned to conditions together.</td>
<td><code>cluster_ra(clusters = households)</code></td>
</tr>
<tr class="odd">
<td>Blocked cluster assignment</td>
<td>Cluster random sampling within blocks</td>
<td><code>block_and_cluster_ra(blocks = regions,clusters = villages)</code></td>
</tr>
<tr class="even">
<td>Saturation random assignment</td>
<td>First clusters, then units within clusters</td>
<td><code>cluster_ra(clusters = villages); block_ra(blocks = villages</code></td>
</tr>
</tbody>
</table></div>
<p>Figure <a href="crafting-a-data-strategy.html#fig:binaryassignmentquilt">8.5</a> visualizes nine kinds of random assignment, arranged according to whether the assignment procedure is simple, complete, or blocked and according to whether the assignment procedure is carried out at the individual, cluster, or saturation level. In the top left facet, we have simple (or Bernoulli) random assignment, in which all units have a 50% probability of treatment, but the total number of treated units could bounce around from assignment to assignment. In the top center, this problem is fixed: under complete random assignment, exactly <span class="math inline">\(m\)</span> of <span class="math inline">\(N\)</span> units are assigned to treatment and the <span class="math inline">\(N - m\)</span> are assigned to control. While complete random assignment fixes the number of units treated at exactly <span class="math inline">\(m\)</span>, the number of units that are treated within any particular group of units (defined by a pre-treatment covariate) could bounce around. Under block random assignment, we conduct complete random assignment within each block separately, so we directly control the number treated within each block. Moving from simple to complete random assignment tends to decrease sampling variablity a bit, by ruling out highly unbalanced allocations. Moving from complete to blocked can help more, so long as the blocking variable is correlated with the outcome. Blocking rules out assignments in which too many or too few units in a particular subgroup are treated. To build intuition for why the correlation of the blocking variable with the outcome is important, consider forming blocks at random; none of the assignments under complete random assignment would be ruled out, so the sampling distributions under the two assignment procedures would be equivalent.</p>
<p>The second row of Figure <a href="crafting-a-data-strategy.html#fig:binaryassignmentquilt">8.5</a> shows clustered designs in which all units within a cluster receive the same treatment assignment. Clustered designs are common for household-level, school-level, or village-level designs, where it would be impratical or infeasible to conduct indiviudal level assignment. When units within the same cluster are more alike than units in different clusters (as in most cases), clustering increases sampling variability relative to individual level assignment. Just like in individual-level designs, moving from simple to complete or from complete to blocked tends to result in lower sampling variability.</p>
<p>The final row of Figure <a href="crafting-a-data-strategy.html#fig:binaryassignmentquilt">8.5</a> shows a series of designs that are analogous to the multi-stage sampling designs shown in Figure <a href="crafting-a-data-strategy.html#fig:samplingquilt">8.2</a> – but their purpose is subtly different in spirit. Multi-stage sampling designs are employed to reduce costs – first clusters are sampled but not all units within a cluster are sampled. A saturation randomization design (sometimes called a “partial population design” cite Berk et al) uses a similar procedure both contain and learn about spillover effects. Some clusters are chosen for treatment, but some units <em>within</em> those clusters are not treated. Units that are untreated in treated clusters can be compared with units that are untreated in untreated clusters in order to suss out intra-cluster spillover effects <span class="citation">(Sinclair, McConnell, and Green <a href="references.html#ref-betsy2012detecting" role="doc-biblioref">2012</a>)</span>. The figure shows how the saturation designs comes in simple, complete, and blocked varieties.</p>
<div class="figure">
<span id="fig:binaryassignmentquilt"></span>
<img src="book_files/figure-html/binaryassignmentquilt-1.svg" alt="Nine kinds of random assignment. In the first row individuals are the sampling units, in the second row clusters are sampled, in the third clusters are sampled and then individuals within these clusters are sampled. In the first column units are sampled independently, in the second units are sampled to hit a target, in the third units are sampled to hit targets within strata." width="100%"><p class="caption">
Figure 8.5: Nine kinds of random assignment. In the first row individuals are the sampling units, in the second row clusters are sampled, in the third clusters are sampled and then individuals within these clusters are sampled. In the first column units are sampled independently, in the second units are sampled to hit a target, in the third units are sampled to hit targets within strata.
</p>
</div>
</div>
<div id="multiarm-and-factorial-trials" class="section level3">
<h3>
<span class="header-section-number">8.2.2</span> Multiarm and factorial trials<a class="anchor" aria-label="anchor" href="#multiarm-and-factorial-trials"><i class="fas fa-link"></i></a>
</h3>
<p>Thus far we have considered assignment strategies that allocate subjects to either treatment or control. All of the designs we have considered so far generalize quite nicely to multiarm trials. Trials that have three, four, or many more arms can of course be simple, complete, blocked, clustered, or feature variable saturation. Figure <a href="crafting-a-data-strategy.html#fig:multiarmquilt">8.6</a> shows blocked versions of a three-arm trial, a factorial trial, and a four-arm trial.</p>
<p>In the three-arm trial on the left, subjects can be assigned to a control condition or one of two treatments. This design enables three comparisons: a comparison of each treatment to the control condition, but also a comparison of the two treatment conditions to each other. In the four-arm trial on the right, subjects can be assigned to a control condition or one of three treatments. This design supports six comparisons: each of the treatments to control, and all three of the pairwise comparisons across treatments.</p>
<p>The two-by-two factorial design in the center panel shares similarities with both the three-arm and the four-arm trials. Like the three arm, it considers two treatments T1 and T2, but it also includes a fourth condition in which both treatments are applied. Factorial designs can be analyzed like a four-arm trial, but the structure of the design also enables more subtle analyses. In particular, the factorial structures allows researchers to investigate whether the effects of one treatment depend on the level of the other treatment.</p>
<div class="figure">
<span id="fig:multiarmquilt"></span>
<img src="book_files/figure-html/multiarmquilt-1.svg" alt="Multi-arm random assignment." width="100%"><p class="caption">
Figure 8.6: Multi-arm random assignment.
</p>
</div>
</div>
<div id="complex-designs" class="section level3">
<h3>
<span class="header-section-number">8.2.3</span> Complex designs<a class="anchor" aria-label="anchor" href="#complex-designs"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>Patient preference trials</li>
<li>Stepped-wedge assignment</li>
<li>Multi arm bandits</li>
</ul>
<div class="figure">
<span id="fig:stepwedgera"></span>
<img src="book_files/figure-html/stepwedgera-1.svg" alt="Step-wedge random assignment." width="100%"><p class="caption">
Figure 8.7: Step-wedge random assignment.
</p>
</div>
</div>
<div id="non-randomized-assignment" class="section level3">
<h3>
<span class="header-section-number">8.2.4</span> Non randomized assignment<a class="anchor" aria-label="anchor" href="#non-randomized-assignment"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>Alphabetical assignment</li>
<li>RDD</li>
<li>Bayesian optimal assignment</li>
</ul>
</div>
<div id="example-sociology-natural-experiment-from-government-or-official-cutoff" class="section level3">
<h3>
<span class="header-section-number">8.2.5</span> Example: sociology natural experiment from government or official cutoff<a class="anchor" aria-label="anchor" href="#example-sociology-natural-experiment-from-government-or-official-cutoff"><i class="fas fa-link"></i></a>
</h3>
</div>
</div>
<div id="measurement-1" class="section level2">
<h2>
<span class="header-section-number">8.3</span> Measurement<a class="anchor" aria-label="anchor" href="#measurement-1"><i class="fas fa-link"></i></a>
</h2>
<p>Measurement is the part of the data strategy in which variables are collected about the population of units to enable sampling, variables are collected about the sample before treatment assignment including those used in treatment assignment, and outcomes are collected after treatment assignment. All variables used in the answer strategy are collected in measurement, aside from the treatment assignment variable and assignment and sample inclusion probabilities.</p>
<p>The fundamental problem of description is that we can never measure the latent variables we are interested in, <span class="math inline">\(Y^*\)</span>, such as fear, support for a political candidate, and economic well-being. Instead, we use a measurement technology to imperfectly observe them, which we represent as the function <span class="math inline">\(Q\)</span> that yield the observed outcome <span class="math inline">\(Y^{\mathrm obs}\)</span>: <span class="math inline">\(Q(Y^*) = Y^{\mathrm obs}\)</span>. Our measurement strategy is a set of functions <span class="math inline">\(Q\)</span> for each variable we measure.</p>
<p>There are two basic ways to assess the quality of each function <span class="math inline">\(Q\)</span>: bias, or the difference between the observed and latent outcome, <span class="math inline">\(Y^{\mathrm obs} - Y^*\)</span>, which is given the special label <em>measurement validity</em>; and <em>measurement reliability</em>, which is the variance across multiple outcomes for a given individual, <span class="math inline">\(\mathbb{V}(Y_1^{\mathrm obs}, Y_2^{\mathrm obs}, Y_3^{\mathrm obs})\)</span>.</p>
<p>Researchers select several characteristics of <span class="math inline">\(Q\)</span>: who collects the measures; the mode of measurement; how often and when measures are taken; how many different observed measures of <span class="math inline">\(Y^*\)</span> are collected and how they are summarized into a single measure; and what information is provided to participants about how they are being measured (if any). These design characteristics may affect validity, reliability, or both.</p>
<p>Data may be collected by the researcher themselves, by the participant, or by a third party. In some forms of qualitative research such as participant-observation and interview-based research, the researcher may be the primary data collector. In survey research, the interviewer is typically a hired agent of the researcher, and in many cases multiple interviewers are hired. These interviewers may ask questions differently, leading to less reliable (more variable) answers and in some cases validity problems when they ask questions in a way that leads to biased measures of <span class="math inline">\(Y^*\)</span>. Participants themselves are often asked to collect data on themselves, either through self-administered surveys, journaling, or taking measurements of themselves using thermometers or scales. A primary concern with self-reports is validity: do respondents report their measurements truthfully. A parallel concern is raised when participants do not collect their own data but are made aware of the fact that they are being measured by others. Finally, data may be collected by agents of government or other organizations, yielding so-called administrative data. The difference between administrative data and other forms of data is only in the identity of the data collector.</p>
<p>Most of the variety in measurement strategies is how those data collectors obtain their data. Humans can code data by observation through the five senses of sight, hearing, touch, smell, and taste, and by asking other humans for self-reports about themselves in surveys. Measurement instruments can also be used to record waves of light (e.g., photos), sound (e.g., audio and seismic recordings), electromagnetism (e.g., EKGs and x-rays), and combinations of more than one (e.g., video); characteristics of the atmosphere (e.g., temperature and pressure), the water (e.g., salinity and pollution), and the soil (i.e., mercury pollution); and human and animal health (e.g., blood tests). Considerable recent progress has been made in taking advantage of all of these measurement modes due to increasing computing power and machine learning techniques that can code streams of raw data from photos, videos, and these other sources and translate them into usable data. The translation of raw data into coded data that can be used for analysis is part of <span class="math inline">\(Q\)</span> in the measurement strategy.</p>
<p>When data are collected, and how often, can also affect validity and reliability. The inquiry should guide when data is collected in relation to other events such as an election or the holiday period or the time after a treatment is delivered to research participants. The inquiry defines whether the effect of interest is a month after treatment or in the case of long-term effects a year or more. However, data need not be collected at a single time period. The model encodes beliefs about the autocorrelation (correlation over time) of data that varies over time, and this can help guide whether to collect multiple measurements or just one and whether to measure data at baseline as well as endline. If data are expected to be highly variable (low autocorrelation), then taking multiple measurements and averaging them may provide efficiency gains. When they exhibit high autocorrelation, then multiple measures will waste resources — approximately the same measure will be returned each time. However, under high autocorrelation there will be large gains from collecting a baseline measure before a treatment in an experiment, because controlling for baseline outcomes the only major change will be the response to treatment. Thus, the estimates of treatment effects will be much more efficient (more T cite). Beliefs about autocorrelation can guide decisions about the tradeoff between including more participants or more measures of a smaller set of participants.</p>
<p>Parallel considerations apply to the choice to measure the same latent <span class="math inline">\(Y_i^*\)</span> with multiple measurement tools in the same time period. When possible, taking multiple, different measures and averaging will typically yield efficiency improvements. (In some cases, there may be a tradeoff with a budget constraint in terms of survey length or the number of subjects.) The multiple measures can then be combined to construct a single measure of <span class="math inline">\(Y_i^*\)</span>. When the tools produce answers that are highly correlated, taking multiple measures is unlikely to be worth the cost but when the correlation is low it will be worth taking multiple measurements and averaging to improve efficiency.</p>
<p>Beyond what and by whom, two important features of any data collection strategy are when and how often. In experiments, data is often collected after treatment is delivered, and sometimes before at baseline to enable block randomization and before-after comparisons. In difference-in-difference studies, data is always collected before and after, and to buttress claims of parallel trends between treated and control often for multiple periods before treatment. In process tracing studies, data is typically collected at many periods in between a change in the independent variable and a change in the dependent variable to demonstrate the connection between the two. How often data are collected after the treatment (or change in the independent variable) influences reliability: measuring highly variable outcomes multiple times and averaging can increase measurement reliability. The flip side of this is when variables are fast to change, taking measurements at baseline to compare to endline measurements is not likely to increase reliability. When outcomes change slowly, however, baseline measurements are likely to substantially improve the efficiency of estimates of treatment effects (for further discussion, see cite more T).</p>
<p>One common pitfall of measurement is the sensitivity bias that affects measures self-reported by participants in the presence of social pressure or the risk of sanction from authorities who prefer to hide or wish to detect sensitive characteristics in the population <span class="citation">(Blair, Coppock, and Moor <a href="references.html#ref-blair2018list" role="doc-biblioref">2020</a>)</span>. Sensitivity bias represents an excludability violation: the latent outcome <span class="math inline">\(Y_i^*\)</span> depends on which measurement tool is used. More generally, excludability violations of measurement imply that the very act of measurement affects the latent outcome, often known as a Hawthorne effect. Typically, we want to avoid these excludability violations, because we are trying to measure a latent quantity that exists independent of how it is measured.</p>
<p>Selecting among measurement modes, data collectors, time periods, frequency, and the number of measurements reduces to tradeoffs between their validity and reliability. We want the largest set of valid, measures to combine into a measurement of <span class="math inline">\(Y_i^*\)</span> that meet our budget and logistical constraints. However, some measurement techniques will vary in both validity and reliability, so often we must decide on a weighting of the two concerns given our research goals. In some cases, validity will be important even if the measures are unreliable, but in others we may want to weight them approximately equally.</p>
<p>Learning which measurement tools are valid and reliable is ultimately guesswork, though it can be informed guesswork. We cannot measure the true <span class="math inline">\(Y_i^*\)</span>, so we cannot truly “validate” any measurement technique. Often studies present themselves as validation studies by comparing a proposed measure to a “ground truth,” measured from administrative data or a second technique to reduce measurement error. However, neither measurement is known to be exactly <span class="math inline">\(Y_i^*\)</span>, so ultimately these studies are comparisons of multiple techniques each with their own advantages and disadvantages. This does not make them useless, but rather should be used to understand the how measurements differ on average and in variability. They may also be useful for informing how to combine multiple measures.</p>
<!-- inattention -->
<!-- differential measurement -->
<!-- parallel measurement across treatment conditions -->
<!-- measurement bias subtracted off from T-C comparison -- but what are you really measuring (identification is saved yes, but what does it identify) -->
<!-- - issues with rates - y^* / x^*, then rates are messed up -->
<div id="example-weaver2019too" class="section level3">
<h3>
<span class="header-section-number">8.3.1</span> Example: <span class="citation">Weaver, Prowse, and Piston (<a href="references.html#ref-weaver2019too" role="doc-biblioref">2019</a>)</span><a class="anchor" aria-label="anchor" href="#example-weaver2019too"><i class="fas fa-link"></i></a>
</h3>
</div>
</div>
<div id="threats-to-implementation" class="section level2">
<h2>
<span class="header-section-number">8.4</span> Threats to implementation<a class="anchor" aria-label="anchor" href="#threats-to-implementation"><i class="fas fa-link"></i></a>
</h2>
<div class="figure">
<span id="fig:threatsdag"></span>
<img src="book_files/figure-html/threatsdag-1.svg" alt="DAG with exclusion restrictions." width="100%"><p class="caption">
Figure 8.8: DAG with exclusion restrictions.
</p>
</div>
<p>Data strategies are plans for how we will sample units, assign treatments, and measure outcomes. But our studies do not always go according to plan. In this section, we explore the threats to our inferences that emerge while implementing data strategies. Anticipating and planning for these threats presents two benefits: we can assess the properties of the designs we will actually run, obtaining more realistic guesses of the power and bias of our study given problems like attrition and noncompliance; and we can incorporate procedures to mitigate the risk of these threats as part of our designs.</p>
<p>Above, we adapt the figure first present in the chapter’s introduction to introduce threats that come from noncompliance (failure to treat), attrition (failure to be included in the sample or provide measures), and excludability violations (causal effects of random sampling, random assignment, or measurement on the latent outcome).</p>
<div id="noncompliance" class="section level3">
<h3>
<span class="header-section-number">8.4.1</span> Noncompliance<a class="anchor" aria-label="anchor" href="#noncompliance"><i class="fas fa-link"></i></a>
</h3>
<p>The first type of threat during implementation is noncompliance: when random assignment <span class="math inline">\(Z\)</span> imperfectly manipulates the treatment variable <span class="math inline">\(D\)</span>. In the absence of noncompliance, <span class="math inline">\(D_i = Z_i\)</span>. One-sided noncompliance is the setting in which some treated units fail to be treated (and receive the control condition instead). Two-sided noncompliance is when some units fail to be treated, but in addition some units assigned to the control group receive the treatment.</p>
<p>In randomized experiments, noncompliance may happen due to administrative error, miscommunications between researcher and partners, shortages of materials or staff, transportation problems, participants refusing treatment, and the inability of researchers to find the participant to treat, among other problems.</p>
<p>Noncompliance also affects observational designs for causal inference in which nature or a non-random administrative process affects treatment such as a threshold cut-off. In instrumental variables designs, noncompliance is at the core: the instrument <span class="math inline">\(Z\)</span> causally affects treatment <span class="math inline">\(D\)</span>, but other factors also affect <span class="math inline">\(D\)</span>. In a regression discontinuity design, a threshold determines treatment status such that above a certain value of a scale the unit is treated and below it is not. But noncompliance can also occur, leading to fuzzy regression discontinuity designs, in which the threshold is imperfect and some units below receive treatment and some units above do not.</p>
<p>Under two-sided noncompliance, there are four types of participants, determined by the potential outcomes <span class="math inline">\(D\)</span> as a function of which condition the unit is randomly assigned two. For a two-arm trial, there are two treatment potential outcomes: whether you would receive the treatment if you are assigned to control (<span class="math inline">\(D_i(Z_i = 0)\)</span>) and whether you would receive the treatment if assigned to treatment (<span class="math inline">\(D_i(Z_i = 1)\)</span>). The four types are enumerated and labeled in Table <a href="crafting-a-data-strategy.html#tab:compliancetypes">8.3</a> below.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:compliancetypes">Table 8.3: </span> Compliance types</caption>
<thead><tr class="header">
<th>Type</th>
<th><span class="math inline">\(D_i(Z_i = 0)\)</span></th>
<th><span class="math inline">\(D_i(Z_i = 1)\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Never-taker</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>Complier</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>Defier</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>Always-taker</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table></div>
<p>Never-takers are those who never take the treatment, no matter what treatment they are assigned. Compliers take exactly the treatment they are assigned. Sefiers do exactly the opposite. When assigned to treatment, they refuse it, but when assigned to control, they take treatment. Like their name suggests, always-takers take the treatment regardless of the treatment condition they are assigned to. In some settings we can rule out some types on the basis of design information. Canvassing experiments, for example, cannot feature always takers because there is no way to “take treatment” unless you are assigned treatment, but there are never-takers because subjects might not be at home when the canvassers come knocking. Such experimentsa are said to experience one-sided noncompliance.</p>
<p>In the presence of noncompliance, a change in inquiry is inevitable. The average treatment effect cannot be estimated by comparing those assigned to treatment and those assigned to control, because the assignment differs from whether treatment was received. The average difference between those assigned to the two conditions can be relabeled the intent-to-treat effect. Comparing those who <em>received</em> treatment to those that did not is also not an option, without additional herculean assumptions, because unobserved heterogeneity now jointly affects <span class="math inline">\(D\)</span> with <span class="math inline">\(Z\)</span>. The randomized experiment is broken: the treatment group is no longer comparable to the control group in expectation. Instead, a complier average treatment may be obtained using instrumental variables estimation, which implies switching to a local inquiry among complier types. This effect may differ from the average treatment effect if compliers differ systematically from other types. Estimating the complier average treatment effect requires the addition of assumptions on top of those for randomized experiments, including the ignorability of treatment assignment and, in the case of two-sided noncompliance, a monotonicity assumption that rules out defiers.</p>
<p>In the case of randomized experiments, spending budget and time to carefully design the treatment delivery protocols to avoid noncompliance will help avoid or minimize the threat from noncompliance.</p>
<p>A parallel set of decisions faces the designer of an observational study with noncompliance in treatments. Instrumental variables designs imply there is noncompliance and the inquiry is the complier average treatment effect (in some cases, the intent-to-treat effect or reduced form effect is also of interest). Researchers who adopt regression discontinuity designs also focus on a local effect among units near the threshold, and in the case of the fuzzy regression discontinuity design with noncompliance must switch to a complier local average treatment effect.</p>
<p>Compliance need not be binary: if you are assigned to treatment, you may receive partial treatment not all or none of it. However, it is typically difficult to measure partial compliance, and more difficult still to account for this possibility in analysis (we cannot separate the effects of partial compliance from full compliance without further treatments that attempt to manipulate treatment more and less strongly).</p>
<p>In multi-arm trials or with continuous rather than binary instruments, noncompliance becomes a more complex problem to define and address through the data strategy and answer strategy. We must define complier types according to all of the (potentially-infinite) possible treatment conditions. For multiarm trials, the complier types for the first treatment may not be the same for the second treatment; in other words, units will comply at different rates to different treatments. Apparent differences in complier average treatment effects and intent-to-treat effects, as a result, may reflect not differences in treatment effects but different rates of compliance.</p>
</div>
<div id="attrition" class="section level3">
<h3>
<span class="header-section-number">8.4.2</span> Attrition<a class="anchor" aria-label="anchor" href="#attrition"><i class="fas fa-link"></i></a>
</h3>
<p>Attrition is the special form of noncompliance to sampling and measurement, when we do not have measures for all units who are sampled. There are two types of missing data that may result: when a single measure is missing, commonly known as item nonresponse; and when all measures are missing for a participant, known as survey nonresponse. Though these terms were coined by survey researchers, the problems are identical when not collected by surveys but by passive measurement or by instrumentation. Item nonresponse may result from many causes, but often because respondents do not wish to answer a specific questions because they are sensitive or intrusive. Survey nonresponse in which no questions are answered may be instead due to lack of interest in the measurement as presented, inability to contact to a subject, or an inability of the data collector to build rapport and gain the trust of the participant.</p>
<p>Attrition occurs when at least one unit does not respond to at least one measurement item. Whether attrition is a problem depends on whether a participant response <span class="math inline">\(R\)</span> is causally affected by variables other than sampling or is completely random. Though missingness completely at random is rare, it is possible, often due to administrative or computer error. If attrition is completely at random, there is no effect of any variable on <span class="math inline">\(R\)</span>, and there is a loss of sample size but no distortion to estimates.</p>
<p>For descriptive inquiries, if attrition is not completely at random, then the types of units who respond are different in some way from those that do not respond. Model-based inference is required to adjust estimates to match the characteristics of the original sample, upweighting types of respondents who were less likely to respond and downweighting those that were more likely to respond. However, if we guess the wrong set of variables to reweight in our model <span class="math inline">\(M\)</span>, then we will miss the target.</p>
<p>For causal inquiries, we also need to know whether random assignment <span class="math inline">\(Z\)</span> affects <span class="math inline">\(R\)</span>, as well as <span class="math inline">\(U\)</span>. If <span class="math inline">\(Z\)</span> affects <span class="math inline">\(R\)</span> (there is a lower/higher rate of attrition in one treatment group than the other), but <span class="math inline">\(U\)</span> does not affect <span class="math inline">\(R\)</span>, then there is just a loss of sample size but no distortion to estimates. We simply have a more (less) precise estimate of the potential outcomes from one group than we do from the other. However, when <span class="math inline">\(Z\)</span> and <span class="math inline">\(U\)</span> affect <span class="math inline">\(R\)</span>, then we do not have a random sample of either the treated potential outcome or the control potential outcome from the treatment or control group. Therefore, we have to resort to model-based inference for both groups, reweighting each group based on our assumptions (in <span class="math inline">\(M\)</span>) about how <span class="math inline">\(U\)</span> and <span class="math inline">\(Z\)</span> affect <span class="math inline">\(R\)</span>. Alternatively, we can construct bounds of our estimates of the causal effect, which incorporate our uncertainty due to the non-random attrition.</p>
<p>In the case of descriptive and of causal inquiries, there is also a design-based way to estimate effects in the presence of uncertainty that does not rely on assumptions in <span class="math inline">\(M\)</span>: double sampling. When there are missing outcomes, we can take a second sample of those who do not respond and conduct additional measurement to try to obtain those units’ outcomes. If we spend more resources on making sure we can obtain measures for all of these randomly-sampled units, then we can use this random sample of non-responders in combination with the original sample to construct a full sample. There will still be a loss of sample size, because we take a subsample in the second round, and do not attempt to interview all nonresponders. We can also use double sampling for causal inquiries, conducting a random sample of nonresponders in the treatment group to stand in for those missing observations and a second in the control group.</p>
</div>
<div id="excludability" class="section level3">
<h3>
<span class="header-section-number">8.4.3</span> Excludability<a class="anchor" aria-label="anchor" href="#excludability"><i class="fas fa-link"></i></a>
</h3>
<p>The final type of threat to the implementation of research is excludability violations. Excludability means that when we define potential outcomes, we can exclude a variable from our definition of the potential outcome. When we define the treated potential outcome for the latent outcome as <span class="math inline">\(Y_i^*(D_i = 0)\)</span>, we invoke four important excludability assumptions: there is no effect of sampling <span class="math inline">\(S\)</span>, treatment assignment <span class="math inline">\(Z\)</span> (except through treatment <span class="math inline">\(D\)</span>!), or measurement <span class="math inline">\(Q\)</span> on the latent outcome. If we do not invoke these assumptions, we must define the treated potential outcome as <span class="math inline">\(Y_i^*(S_i, Z_i, D_i = 0, Q_i)\)</span>.</p>
<p>The problem is not simply one of definitions: in order to define an inquiry such as the average treatment effect we would have to average over possible values of <span class="math inline">\(S_i\)</span>, <span class="math inline">\(Z_i\)</span>, and <span class="math inline">\(Q_i\)</span>. Many of the potential outcomes, such as for unsampled units (<span class="math inline">\(S_i = 0\)</span>), are impossible to obtain and are known as complex potential outcomes. We cannot average over these impossible-to-observe outcomes, and thus cannot identify even with infinite data a quantity like the average treatment effect in the absence of this excludability assumption.</p>
<p>Excludability assumptions are required not for identification of causal and descriptive claims from data, but in order to interpret effects in the way we want to. We can often <em>identify</em> an average treatment effect but not separate (exclude!) the effects of treatment assignment from measurement.</p>
<p>The three excludability assumptions of <span class="math inline">\(S_i\)</span>, <span class="math inline">\(Z_i\)</span>, and <span class="math inline">\(Q_i\)</span> are representd as gray dotted lines in Figure <a href="crafting-a-data-strategy.html#fig:threatsdag">8.8</a>, These are strong assumptions that are often not met in practice.</p>
<p>The first excludability assumption is that there is no causal effect of sampling on <span class="math inline">\(Y_i^*\)</span>. This assumption will be violated if the fact of being <em>included in the sample</em> changes your attitudes. By being asked to be in a focus group, even before being assigned to treatment or being asked survey questions, if you reflect on your political beliefs and change your mind, the sampling excludability assumption may be violated.</p>
<p>The excludability of <span class="math inline">\(Z\)</span> on <span class="math inline">\(Y^*\)</span> is more subtle. When studying causal effects, we certainly believe that treatment assignment has an effect on the outcomes. What the treatment assignment excludability assumption says is that this effect only works through the received treatment <span class="math inline">\(D\)</span>. In other words, there is no independent effect of being assigned apart from actually being treated. There are many ways in which this assumption might be violated. In observational studies using instrumental variables, the excludability of <span class="math inline">\(Z\)</span> on <span class="math inline">\(Y^*\)</span> is an assumption of no alternative channels in which the instrument affects outcomes except the endogenous variable. In studies of the effect of economic growth on civil conflict using rainfall as an instrument for economic growth, we must invoke an excludability assumption that rainfall has no independent effect on civil conflict besides through economic growth.</p>
<p>Equally worrying is the excludability of measurement assumption, that <span class="math inline">\(Q\)</span> does not affect <span class="math inline">\(Y^*\)</span>. Hawthorne effects, in which the fact of being measured changes outcomes, are another word for a violation of this excludability assumption. If outcomes depend on whether subjects know they are being measured or do not, then we cannot exclude the effect of measurement from our effects.</p>
<p>A final excludability assumption we did represent on the DAG is that <span class="math inline">\(Z\)</span> must have no effect on <span class="math inline">\(Q\)</span>. How and whether we measure outcomes cannot depend on whether a unit is assigned to treatment. This excludability assumptional is commonly referred to as the requirement that measurement be parallel across treatment conditions. If we measure outcomes using a survey in the treatment group and using a remote measurement device in control, then we cannot separate (exclude!) the effect of measurement from the effect of treatment.</p>
<p>Researchers should avoid excludability violations when possible and report them when not. In some cases, the best we can do is to produce unbiased estimates of an effect for which we cannot exclude multiple causes. Future research can be used to disentangle them and more credibly invoke an excludability assumption.</p>
</div>
<div id="interference" class="section level3">
<h3>
<span class="header-section-number">8.4.4</span> Interference<a class="anchor" aria-label="anchor" href="#interference"><i class="fas fa-link"></i></a>
</h3>
<p>We have four endogenous outcomes in the DAG of a research design above: <span class="math inline">\(R\)</span>, whether a participant responds to data collection; <span class="math inline">\(D\)</span>, whether a respondent receives treatment; <span class="math inline">\(Y^*\)</span>, the latent outcome; and <span class="math inline">\(Y\)</span>, the observed outcome. Setting aside attrition and noncompliance for the moment, <span class="math inline">\(R\)</span> is a function only of sampling; <span class="math inline">\(D\)</span> of treatment assignment; <span class="math inline">\(Y^*\)</span> of <span class="math inline">\(D\)</span>; and <span class="math inline">\(Y\)</span> of measurement strategy <span class="math inline">\(Q\)</span>. Interference is the problem in which these endogneous variables depend not only on whether and how <em>they</em> are sampled, assigned to treatment, and measured, but whether and how <em>other units</em> are sampled, assigned to treatment, and measured. We assume for example that <span class="math inline">\(Y_i(Z_i) = Y_i(\mathbf{Z})\)</span>. In other words, <span class="math inline">\(Y_i\)</span> the outcome for unit <span class="math inline">\(i\)</span>, is a function of its own treatment assignment status <span class="math inline">\(Z_i\)</span> not those of other units (<span class="math inline">\(\mathbf{Z}\)</span>. If there is noncompliance with assignment, we assume instead that <span class="math inline">\(Y_i(Z_i, D_i) = Y_i(\mathbf{Z}, \mathbf{D})\)</span> <em>and</em> <span class="math inline">\(D_i(Z_i) = D_i(\mathbf{Z})\)</span>, or that we only need to know the assignment and received treatment status for individual <span class="math inline">\(i\)</span> to know its outcome and we only need to know whether a unit was assigned to treatment to know if they received treatment.</p>
<p>The no interference assumption is strong. Interference is common, perhaps even ubiquitous, and is often referred to as spillovers. Interference may occur when participants tell each other about the treatment status, but it also may occur in more subtle ways. When treatments displace outcomes such as crime, they reduce crime in treated locales but the crime is simply moved to other localities. The overall crime rate may not change. Displacement is a form of interference. Resource allocation decisions that are affected by a treatment also often suffer from interference. If citizens are treated with information about how to report potholes and do so, the roads agency may move resources from untreated areas to treated areas, increasing the number of potholes in control areas.</p>
<p>Interference can also be induced by sampling. When potential outcomes depend on whether other units are included in the sample, this is sampling interference, i.e. <span class="math inline">\(Y_i(S_i = 1) \neq Y_i(\mathbf{S})\)</span>. When subjects are asked questions that induce them to consider their relative status with respect to other people, which people are in the sample may affect their comparison group and their potential outcomes.</p>
<p>Interference can affect measurement, just as it affects sampling and treatment assignment. Measurement interference occurs when <span class="math inline">\(Y_i^*\)</span> depends on whether and how <em>other</em> units are measured. If participants in an experiment discuss the measurement technique itself and what researchers are asking about with other participants, this may lead to a violation. Psychology researchers on academic campuses worry about this possibility and encourage subjects not to discuss what they do in the lab with potential participants. They are worried about interference in treatment assignment but also of measurement, because often implicit measurement techniques can be defeated (and measure something other than <span class="math inline">\(Y_i^*\)</span>) when the concept is known before the session. Other researchers worry that when measuring sensitive questions that the sensitive topic will be revealed to participants either by earlier participants or local leaders who discover its purpose.</p>
<p>In the presence of interference, we are unable to obtain a random sample of a potential outcome such as <span class="math inline">\(Y_i(Z_i = 1)\)</span>. The reason is that, even in a simplified setting, units will no longer either receive the treatment or not, but they may be directly treated, indirectly treated by being near a directly treated, or if far from others truly receive no treatment. Thus, the group assigned to control will be made up of units that receive indirect treatment and no treatment. Which they receive is not purely determined by the randomization scheme: whether you are indirectly treated depends on how close you are in distance or social connections to other units. Because we cannot obtain a random sample of one or more potential outcomes, we cannot produce unbiased estimates of quantities like the average treatment effect or even the average effect that depend on them. We may, however, be able to change our inquiry, by switching to a set of units for which we can credibly assume no interference.</p>
<p>In some cases, interference between individuals within a household may be highly likely (we all read our family’s mail), but interference between households may be less likely. In others, interference may be likely across households in a neighborhood but not between neighborhoods. When this is the case, we may be able to random sample from <span class="math inline">\(Y_j(Z_j = 0)\)</span>, the potential outcome for neighborhood <span class="math inline">\(j\)</span>, by randomly assigning households within a neighborhoods to all receive the same treatment condition (i.e., cluster random assignment). This represents a change in our inquiry – the potential outcomes of individuals are not the same as the potential outcomes of their neighborhood – but in the presence of interference this change may be required. Similar solutions are available in the presence of sampling and measurement interference.</p>
<p>Another way we could change our inquiry is to sample units for inclusion in the study who are separated by physical or social distance such that interference is unlikely. We might restrict our sample to include only those people who are separated by at least one mile or by at least two social connections (friends and friends-of-friends are ruled out). Here, we will sample from the individual potential outcomes, but our research design will be complicated by heterogeneous sampling probabilities. In high density areas, we will have low sampling inclusion probabilities and in low density areas high probabilities, because it will be easier to find samples of people separated in distance in areas of low density. We have not fixed the problem of interference, because there are still units that are indirectly treated, but we have constructed a sampling procedure in which we prevent sampling those units by design. We only sample units that will then either be randomly assigned to treatment or control and by separating the units none <em>in our sample</em> will be indirectly treated.</p>
<p>Interference can also occur over time. Potential outcomes can depend not only on whether a unit was treated in the present time period, but also in the past. Time interference occurs when the contemporaneous potential outcome depends on sampling, treatment, or measurement in the past. Design can help to avoid sampling from partially treated units by introducing washout periods in which the researcher assumes subjects forget about (are no longer affected by) past treatments or research activities.</p>
</div>
</div>
<div id="further-reading-4" class="section level2">
<h2>
<span class="header-section-number">8.5</span> Further reading<a class="anchor" aria-label="anchor" href="#further-reading-4"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li>
<span class="citation">Lohr (<a href="references.html#ref-lohr2009sampling" role="doc-biblioref">2010</a>)</span> on sampling.</li>
<li>
<span class="citation">Shadish, Cook, and Campbell (<a href="references.html#ref-cook2002experimental" role="doc-biblioref">2002</a>)</span>, <span class="citation">Gerber and Green (<a href="references.html#ref-Gerber2012" role="doc-biblioref">2012</a>)</span> on experimental design</li>
<li>
<span class="citation">Seawright and Gerring (<a href="references.html#ref-seawright2008case" role="doc-biblioref">2008</a><a href="references.html#ref-seawright2008case" role="doc-biblioref">b</a>)</span> and <span class="citation">Lieberman (<a href="references.html#ref-lieberman2005nested" role="doc-biblioref">2005</a>)</span> on case selection</li>
<li>
<span class="citation">Collier (<a href="references.html#ref-collier2011understanding" role="doc-biblioref">2011</a>)</span>, <span class="citation">Fairfield (<a href="references.html#ref-fairfield2013going" role="doc-biblioref">2013</a>)</span>, and <span class="citation">Humphreys and Jacobs (<a href="references.html#ref-humphreys2015mixing" role="doc-biblioref">2015</a>)</span> on process tracing</li>
</ul>
<!-- - ethics belongs in data strategy --><!-- - just downloading the data.  Did you offload the data strategy --><!-- - possibly ambiguous where the data strategy ends and the analysis strategy ends. --><!-- - SOMEone did parts of the data strategy --><!-- - Answer strategy section distinguishes qual from quant; should the data strategy section do the same? Make the point that lots of kinds of activities are data strategies. Interviews are a complex interaction of data strategy and answer strategy make them hard to declare. --><!-- - Data strategy is a main locus of ethical problems. That's not to say that ethical challenges don't occur when we have bad models or inquiries, or when our answer strategies violate subject privacy. Data strategies change what outcomes are revealed (as in an experiment) or measured (as in a survey). The act of treating or measure can induce trauma or other bad outcomes.  --><!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book--><!-- start post here, do not edit above -->
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="defining-the-inquiry.html"><span class="header-section-number">7</span> Defining the inquiry</a></div>
<div class="next"><a href="choosing-an-answer-strategy.html"><span class="header-section-number">9</span> Choosing an answer strategy</a></div>
</div></main><div id="on-this-page-nav" class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#crafting-a-data-strategy"><span class="header-section-number">8</span> Crafting a data strategy</a></li>
<li>
<a class="nav-link" href="#p2sampling"><span class="header-section-number">8.1</span> Sampling strategies</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#randomized-sampling-designs"><span class="header-section-number">8.1.1</span> Randomized sampling designs</a></li>
<li><a class="nav-link" href="#nonrandomized-sampling-designs"><span class="header-section-number">8.1.2</span> Nonrandomized sampling designs</a></li>
<li><a class="nav-link" href="#sampling-designs-for-qualitative-research"><span class="header-section-number">8.1.3</span> Sampling designs for qualitative research</a></li>
<li><a class="nav-link" href="#choosing-among-sampling-designs"><span class="header-section-number">8.1.4</span> Choosing among sampling designs</a></li>
<li><a class="nav-link" href="#example-wang2015forecasting"><span class="header-section-number">8.1.5</span> Example: Wang et al. (2015)</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#p2assignment"><span class="header-section-number">8.2</span> Treatment assignment</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#two-arm-trials"><span class="header-section-number">8.2.1</span> Two arm trials</a></li>
<li><a class="nav-link" href="#multiarm-and-factorial-trials"><span class="header-section-number">8.2.2</span> Multiarm and factorial trials</a></li>
<li><a class="nav-link" href="#complex-designs"><span class="header-section-number">8.2.3</span> Complex designs</a></li>
<li><a class="nav-link" href="#non-randomized-assignment"><span class="header-section-number">8.2.4</span> Non randomized assignment</a></li>
<li><a class="nav-link" href="#example-sociology-natural-experiment-from-government-or-official-cutoff"><span class="header-section-number">8.2.5</span> Example: sociology natural experiment from government or official cutoff</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#measurement-1"><span class="header-section-number">8.3</span> Measurement</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#example-weaver2019too"><span class="header-section-number">8.3.1</span> Example: Weaver, Prowse, and Piston (2019)</a></li></ul>
</li>
<li>
<a class="nav-link" href="#threats-to-implementation"><span class="header-section-number">8.4</span> Threats to implementation</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#noncompliance"><span class="header-section-number">8.4.1</span> Noncompliance</a></li>
<li><a class="nav-link" href="#attrition"><span class="header-section-number">8.4.2</span> Attrition</a></li>
<li><a class="nav-link" href="#excludability"><span class="header-section-number">8.4.3</span> Excludability</a></li>
<li><a class="nav-link" href="#interference"><span class="header-section-number">8.4.4</span> Interference</a></li>
</ul>
</li>
<li><a class="nav-link" href="#further-reading-4"><span class="header-section-number">8.5</span> Further reading</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Research Design: Declare, Diagnose, Redesign</strong>" was written by Graeme Blair, Jasper Cooper, Alexander Coppock, and Macartan Humphreys. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>Supported by the Laura and John Arnold Foundation and Evidence in Governance and Politics.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
