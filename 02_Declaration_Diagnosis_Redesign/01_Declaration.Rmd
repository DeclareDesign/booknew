---
title: "Declaration"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
source("scripts/vayr.R")
```

<!-- start post here, do not edit above -->

# Declaration

<!-- make sure to rename the section title below -->

```{r research_questions, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

In Chapter 2, we gave a high-level overview of our framework for describing research designs in terms of their models, inquiries, data strategies, and answer strategies, our process for diagnosing their properties, and a general purpose approach for improving them to better fit research tasks. Now in this chapter, we place our approach on firmer formal footing. To do so, we employ elements from Pearl's [-@pearl2009causality] approach to causal modeling (directed acyclic graphs, or DAGs for short), which provides a syntax for mapping design inputs to design outputs. We also use the potential outcomes framework as presented, for example, in @Imbens2015, which many social scientists use to clarify their inferential targets. 

Our goal here is not formalization for formalization's sake. Describing a research design as a DAG helps us to see the fundamental symmetries across the theoretical (M and I) and empirical (D and A) halves of a research design. A recurring theme of our book is that research designs tend to be stronger when the relationship of M to I is mirrored by the relationship of D to A; the aim of this chapter is to make this somewhat mystical claim more concrete.

We define a research design as $\Delta$, with four elements $<M,I,D,A>$. Describing a research design entails "declaring" each of these four elements. 

$M$ is a reference **model**, or set of reference models, of how the world works. Following Pearl's definition of a probabilistic causal model, a model in $M$ contains three core elements. The first is a specification of the variables $X$ about which research is being conducted. This includes endogenous and exogenous variables ($V$ and $U$ respectively) and the ranges of these variables. In the formal literature this is sometimes called the *signature* of a model [@halpern2000]. The second element ($F$) a specification of how each endogenous variable depends on other variables. These can be considered functional relations or, as in @Imbens2015, potential outcomes. The third and final element is a probability distribution over exogenous variables, written $P(U)$. Sometimes it is useful to think of the draws from $U$ as implying distinct models of their own, in which case we might think of $M$ as a family of models and a particular model $m$ as an element of $M$. 

The **inquiry** $I$ is a summary of the variables $X$, perhaps given interventions on some variables. An inquiry might be the average value of an outcome $Y$: $\E[Y] = \int{y\times Pr(Y=y)}$, or the average value of the outcome conditional on the value of a treatment $Z$: $\E[Y|Z=1] = \int{y\times Pr(Y=y|Z=1)}$. Using Pearl's notation we can distinguish between descriptive inquiries and causal inquiries. Causal inquiries are those that summarise distributions that would arise under interventions, as indicated by the $do()$ operator, e.g., $\Pr(Y | do(Z = 1))$. Descriptive inquiries summarise distributions that arise without intervention, such as $\Pr(Y | Z =1)$.

We let $a^m$ denote the answer to $I$ *under the model*. Conditional on the model, $a^m$ is the value of the estimand, the quantity that the researcher wants to learn about. The connection of $a^m$ to the model is given by: $a^m = I(m)$. 

As the saying goes, models are wrong but some may perhaps be useful. We denote the *true* causal model of the world as $W$ and the realized world $w$ as a draw from this true causal model---a reference model for a particular case. The true answer, then, is $a^w = I(w)$. The answer under a reference model $a^m$ may be close or far from the true value $a^w$, which is to say it could be wrong. If the model $m$ is far from $w$, then of course $a^m$ need not be correct. We note moreover that $a^w$ might be even be undefined, since inquiries can only be stated in terms of theoretical models. If the theoretical model is wrong enough---for instance conditioning on events that do not in fact arise---then the inquiry might be nonsensical when applied to the real world.

A **data** strategy, $D$, generates data $d$. Data $d$ arises, under model $M$ with probability $P_M(d|D)$. The data strategy includes sampling strategies and assignment strategies, which we denote with $P_S$ and $P_Z$ respectively. Measurement techniques are also a part of data strategies and can be thought of as a selection of observable variables that carry information about unobservable variables. The data strategy operates on $w$ to produce the observed data: $D(w) = d$.

While $M$ reflects our beliefs about true underlying causal processes at work where $D$ is the procedure that results in the collection or creation of data. In this light, the phrase "data generating process" is imprecise. Scholars usually use the phrase "true DGP" to refer to $M$, even though $M$ doesn't create data, $D$ does when it is applied to the real world.

The **answer** strategy, $A$ generates answer $a^d$ using data $d$. We encode this relationship as $A(d) = a^d$. 

The full set of causal relationships between $M$, $I$, $D$, and $A$ with respect to $m$, $a^m$, $d$, $a^d$, $w$, and $a^w$ can be seen in the DAG schematic representation of a research design.


```{r worldandtheory, echo = FALSE, fig.cap = "MIDA as a DAG", fig.width = 7, fig.height = 2.75}
dag <-
  dagify(aw ~ w + I,
         m ~ M,
         am ~ m + I,
         d ~ D + w,
         ad ~ A + d,
         w ~ W)

dag_base <- tidy_dagitty(dag) %>%
  select(name, direction, to, circular) %>%
  as_tibble

nodes_df <-
  tibble(
    name = c("M", "I", "D", "A", 
             "m", "am", "aw", "d", 
             "ad", "w", "W"),
    label = c("M", "I", "D", "A", "m", "a<sup>m</sup>", "a<sup>w</sup>", "d", "a<sup>d</sup>", "w", "W"),
    long_label = c("Theoretical<br>causal model", "Inquiry<br>", "Data<br>strategy", "Answer<br>strategy", "Model<br>draw", "Theoretical<br>answer", "True<br>answer", "Realized<br>data", "Empirical<br>answer", "Real<br>world", "True causal<br>model"),
    lbl_direction = c("N", "N", "N", "N", "S", "S", "S", "S", "S", "S", "N"),
    x = c(1.5, 3.5, 11.5, 13.5, 
          1.5, 3.5, 8.5, 11.5, 
          13.5, 6.5, 6.5),
    y = c(3.5, 3.5, 3.5, 3.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 3.5)
  )

endnodes_df <-
  nodes_df %>%
  transmute(to = name, xend = x, yend = y)

gg_df <-
  dag_base %>%
  left_join(nodes_df, by = "name") %>%
  left_join(endnodes_df, by = "to")

gg_df <-
  gg_df %>%
  mutate(arced1 = (name == "w" & to == "d"),
         arced2 = (name == "I" & to == "aw")) %>%
  arrange(name)

rect_df <-
  tibble(
    xmin = c(0, 10),
    xmax = c(5, 15),
    ymin = c(0, 0),
    ymax = c(5, 5)
  )

g <-
  ggplot(data = filter(gg_df, !arced1 & !arced2), aes(
    x = x,
    y = y,
    xend = xend,
    yend = yend
  )) +
  geom_point(color = gray(.1), fill = NA, size = 14, stroke = 0.5, pch = 1) +
  geom_dag_edges(edge_width = 0.35) +
  geom_dag_edges_arc(data = filter(gg_df, arced1), curvature = -0.25, edge_width = 0.35) +
  geom_dag_edges_arc(data = filter(gg_df, arced2), curvature = .64, edge_width = 0.35) +
  geom_richtext(color = "black",
                aes(label = label, 
                    y = y + if_else(name %in% c("M", "I", "D", "A", "W"), -0.025, 0),),
                fill = NA,
                label.color = NA,
                label.padding = grid::unit(rep(0, 4), "pt"),
                size = 4) +
  geom_richtext(aes(y = y + if_else(lbl_direction == "N", 0.75, -0.75),
        vjust = if_else(lbl_direction == "N", "bottom", "top"),
        label = long_label),
    color = gray(0.5),
    fill = NA,
    label.color = NA,
    label.padding = grid::unit(rep(0, 4), "pt"),
    size = 3) +
  geom_rect(data = rect_df, aes(x = NULL, y = NULL, 
                                xend = NULL, yend = NULL,
                                xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax),
            alpha = 0.15) +
  annotate("text", x = 2.5, y = 5.35, label = "Theory") +
  annotate("text", x = 12.5, y = 5.35, label = "Empirics") +
  annotate("text", x = 7.5, y = 5.35, label = "Reality") +
  scale_x_continuous(breaks = 0:15) +
  scale_y_continuous(breaks = 0:5) +
  coord_fixed(clip = "off") + 
  theme_dag()
g
```

Figure \@ref(fig:worldandtheory) illustrates  how a research design is a correspondence $I(m) = a^m$ and $A(d) = a^d$. The theoretical half of a research design produces an answer to the inquiry **in theory**. The empirical half of a research design produces an *empirical estimate* of the answer to the inquiry. 

Neither answer is necessarily close to the truth $a^w$, of course. And, as shown in the figure, the truth is not directly accessible either to us in theory or in empirics. Our gamble in empirical research however is that our theoretical models are close enough to the truth: that the truth is like the set of models we imagine in our model set. 

<!-- MH: Having some trouble understanding the distrinction between W and w. If w includes potential outcome functions, isn't it the true causal model? If W is the set of possible worlds, is that a subject of inquiry. For instance say our estimand is sd(ate) would that be a(W) rather than a(w). -->

<!-- Think it worth having a second figure which shows just two panels:if indeed w \in M, then: -->
<!-- We moved this graph to "diagnosis". Can we handle the epistemological point in text -- we hope that one of the models in M is the true causal model W... but of course we don't think that's ever true because of an "all models are wrong, some are useful" type thinking? -->

| Relationship  | Description                                                                              |
| ------------- | ---------------------------------------------------------------------------------------  |
| $m = M()$     | $m$ is a draw from the theoretical causal model $M$                                      |
| $a^m = I(m)$  | the answer under the model $a^m$ is the inquiry applied to a draw from the model $m$     |
| $w = W()$     | the real world $w$ is a draw from the true causal model $W$ (unobservable)               |
| $a^w = I(w)$  | the true answer $a^w$ is the inquiry were it applied to the real world $w$ (unobservable)|
| $d = D(w)$    | the realized dataset $d$ results from the data strategy applied to the real world $w$    |
| $a^d = A(d)$  | the empirical answer $a^d$ is the answer strategy applied to the dataset $d$             |

Table: (\#tab:elementsofresearchdesign) Elements of research design.

From this discussion we see a striking analogy between the $M$, $I$ relationship and the $D$, $A$ relationship. The answer we aim for is gotten  by applying $I$ to a draw from $M$. But the answer we have access to is  gotten  by applying $A$ to a draw from $D$. And our hope, usually, is that these two answers are the same. In some cases this suggests that $A$ should be "like" $I$: for instance if we are interested in the mean of a population and we have access to a random sample, the data available to us from $D$ is like the ideal data we would have if we could observe $M$ directly. And indeed taking the mean of the data is likely a good strategy. But this principle does not hold as general matter. For instance if data is sampled with non uniform probabilities, a simple mean of observed data is likely not a good strategy to calculate the average of a population. Rather, the focus should be on the extent to which the two sets of answers line up. 

<!-- The central crux of good research design is generated by the deep analogy of Models and Inquiries to Data and Answer Strategies. This analogy can guide many design decisions. The guidance that measurement procedures should be theoretically informed is an instance of the general point that data strategies should parallel theoretical models. The general preference for nonparametric estimation approaches over parametric procedures is an instance of the general point that answer strategies should parallel inquiries.  -->

<!-- $$ -->
<!-- M : I : : D : A -->
<!-- $$ -->

<!-- The DAG also clarifies the main difference between the theoretical and emprical halves of research design is the introduction of reality. When we draw from $M$ to produce $m$, we write $M() = m$ -- the function has no inputs from outside the model because it is purely theoretical. When we draw from the $D$ to produce $d$, we write $D(w) = d$ -- the dataset $d$ that results from the data strategy $D$ depends on the actual state of the world $w$. Empirical researchers must be theorists too: our goal is to choose our theoretical model $M$ such that $m$ is as close to $w$ as possible.  -->

<!-- Idea we want to express here: M:I::D:A, but M and D are different. How are they different? The first way is that M is theoretical and D is empirical -- D() has w as an argument. The second way is that there are two "fundamental" problems facing empirical research. The first is the fundamental problem of causal inference -- we can't observe units in counterfactual states. The second is the fundmental problem of descriptive inference -- we can't observe the latent Y*, only Y as measured. -->

<!-- With a design declared in terms of MIDA, we can then diagnose its properties, and redesign to find an optimal design subject to constraints. Design diagnosis is the process of simulating both $I(m) = a^m$ and $A(d) = a^d$ over many draws from $M()$ and $D(m)$, and comparing them. In other words, diagnosis is the process of learning the value of a diagnosand for a single design, often under multiple specifications of M to assess robustness to alternative models. Redesign is the process of *changing* parts of D and A in order to learn how the diagnosands change. The redesign process is complete when a research selects the best (or one of the best) D and A among the feasible set, as measured by the changing values of the diagnosand. -->

## Example {#p2gulzarkhanrevisited}

In this section, we declare the @Gulzar2020 example in finer detail. The aim is to capture all of the analytically-relevant features of the design. In Chapter 2, we declared in words and in code a simplified version of the design, in order to communicate its key features. We will make this move again throughout the book: we will present simplified versions of canonical designs in the text, and also provide worked examples with the details of specific studies. Moving back and forth between the two will, we hope, enable you to both learn about the general principles and how to declare your own designs which always involve the fine details of research implementation.

We display the DAG for the design in Figure \@ref(fig:p2gulzarkhandag).

```{r p2gulzarkhandag, echo=FALSE, fig.height = 3.5, fig.width = 7, fig.cap = "DAG for Gulzar-Khan design."}
design <-
  declare_population(N = 100,
                     X = rbinom(N, 1, 0.3),
                     U = rnorm(N)) +
  declare_potential_outcomes(Y ~ Z + X + U) +
  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +
  declare_sampling(strata = X, prob = 1) + 
  declare_assignment(blocks = X, block_prob = c(0.1, 0.5)) +
  declare_estimator(Y ~ Z, estimand = "ATE", label = "Naive DIM") +
  declare_estimator(Y ~ Z,
                    blocks = X,
                    estimand = "ATE",
                    label = "Blocked DIM")

dag <- dagify(
  Y ~ Z + X + U,
  Z ~ X + S,
  S ~ X
)

nodes <-
  tibble(
    name = c("Y", "S", "Z", "U", "X"),
    label = c("Y", "S", "Z", "U", "X"),
    annotation = c(
      "**Outcome**<br>",
      "**Sampling**",
      "**Random assignment**<br>",
      "**Unknown heterogeneity**",
      "**villages**<br>Stratification, cluster assignment"),
    x = c(5, 1, 3, 5, 1),
    y = c(2.5, 2.5, 2.5, 4, 4), 
    nudge_direction = c("S", "S", "S", "N", "N"),
    answer_strategy = "uncontrolled"
  )
ggdd_df <- make_dag_df(dag, nodes, design)

base_dag_plot %+% ggdd_df + coord_fixed(ylim = c(2.05, 4.6), xlim = c(0.25 - epsilon, 5.75 + epsilon)) 
```


### Model

The model for the study describes the set of units: citizens living in the two Pakistani districts the study took place in, Haripur and Abbottabad, each of which comprises over 300 villages. Approximately 6500 citizens who are eligible to run for elected office live in each village. The main outcome of interest is whether the citizen filed papers to run for office. The outcome depends on the level of three treatments. A neutral appeal simply informs potential candidates of how to run; the other two emphasize either the social or personal benefits to holding public office. We expect the neutral appeal to have some effect and the social and personal appeals to have an additional effect over and above the neutral appeal.

```{r}
model <-
  declare_population(
    districts = add_level(
      N = 2,
      district_name = c("Haripur", "Abbottabad"),
      N_villages = c(311, 359)
    ),
    villages = add_level(N = N_villages),
    citizens = add_level(N = 6500,
                         U = runif(N))
  ) +
  declare_potential_outcomes(
    Y_Z_control = U,
    Y_Z_neutral = U + 0.02,
    Y_Z_personal = U + 0.04,
    Y_Z_social = U + 0.05
  )
```

### Inquiry

This model supports many inquiries, but here we'll focus on two: the average treatment effect of *any* appeal versus the pure control condition. This inquiry is theoretically important because it refers to the mere act of inviting citizens to stand for office. A second inquiry is more subtle: the average difference in effectiveness between appeals to run that focus on the pro-social reasons to stand for office versus appeals that emphasize the personal returns.
  

```{r}
inquiry <- 
  declare_estimand(
    ATE = mean((Y_Z_neutral + Y_Z_personal + Y_Z_social) / 3 -
                 Y_Z_control),
    ATE_social_vs_personal = mean(Y_Z_social - Y_Z_personal)
  ) 
```

### Data strategy

The data strategy entailed three steps. First, 192 of the 670 villages were selected, via cluster sampling, to serve as treatment villages. Of these 48 were assigned to the neutral appeal, 72 to the social appeal, and 72 to the personal appeal. The 478 unsampled villages did not receive any treatment, so can serve as a pure control. Second, 48 citizens from each treated village were sampled via random walk, and were the individuals to whom the candidacy appeals were made. Third, the measurement procedure for this study was to obtain, via administrative record, the names of all candidates who stood for office. In this way, the outcome is observed for all subjects in the experiment, sampled and not.

```{r}
data_strategy <- 
  declare_assignment(
    m_each = c(478, 48, 72, 72),
    clusters = villages,
    conditions = c("control", "neutral", "social", "personal")
  ) + 
  reveal_outcomes(Y, Z) +
  declare_step(any_treatment = if_else(Z == "control", 0, 1),
               handler = fabricate) +
  declare_sampling(strata = villages, 
                   n_unit = if_else(Z == "control", 0, 48), 
                   drop_nonsampled = FALSE) 
```

### Answer strategy

The @Gulzar2020 design supports many alternative answer strategies. Here, we focus on two answer strategies in particular. First, we compare the rate of running in untreated villages to the rate among the sampled units among all treatment villages, using an OLS regression with standard errors clustered at the village level.^[The implemented design also included a village-level treatment that complicates (in interesting ways!) the interpretation of the untreated units in the treated villages]. Second, we compare the rates of running in the personal and social conditions for the sampled units only, again using an OLS regression with standard errors clustered at the village level.

```{r}
answer_strategy <- 
  declare_estimator(
    Y ~ any_treatment,
    clusters = villages,
    model = lm_robust,
    se_type = "CR0",
    subset = (Z == "control" | S == 1),
    estimand = "ATE",
    label = "ATE"
  ) +
  declare_estimator(
    Y ~ Z,
    clusters = villages,
    model = lm_robust,
    se_type = "CR0",
    subset = (S == 1 & Z != "neutral"),
    estimand = "ATE_social_vs_personal",
    label = "ATE_social_vs_personal"
  )
```

With these elements in hand, we can declare the full design and display one draw of the design:

```{r}
design <- 
  model + inquiry + data_strategy + answer_strategy
```

```{r, eval = FALSE}
dat <- draw_data(design)
```

```{r, echo = FALSE, eval = do_diagnosis}
dat <- draw_data(design)

dat_head <- head(dat, 5)

estimates <- run_design(design) 

summary_df <- dat %>% group_by(Z) %>%
  summarise(
    n = n(),
    n_sampled = sum(S),
    n_villages = n_distinct(villages)
  ) 
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path_head <- paste0(get_dropbox_path("01_Declaration.Rmd"), "/dat_head.RDS")
rds_file_path_estimates <- paste0(get_dropbox_path("01_Declaration.Rmd"), "/estimates.RDS")
rds_file_path_summary_df <- paste0(get_dropbox_path("01_Declaration.Rmd"), "/summary_df.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(dat_head, path = rds_file_path_head)
  write_rds(estimates, path = rds_file_path_estimates)
  write_rds(summary_df, path = rds_file_path_summary_df)
}
dat_head <- read_rds(rds_file_path_head)
estimates <- read_rds(rds_file_path_estimates)
summary_df <- read_rds(rds_file_path_summary_df)
```

We print the first six rows of the simulated data in Table \@ref(tab:gulzarkhansimdata):

```{r gulzarkhansimdata, echo = FALSE}
dat_head %>% kable(digits = 3, caption = "Six rows of simulated data from the Gulzar-Khan design.", booktabs = TRUE)
```

From this data, we present simulated estimates of the two inquiries as well as the simulated estimand values in Table \@ref(tab:gulzarkhansimestimates):

```{r, eval = FALSE}
estimates <- run_design(design)
```

```{r gulzarkhansimestimates, echo = FALSE}
estimates$estimates_df %>% left_join(estimates$estimands_df) %>% 
select(estimator_label, estimate, std.error, estimand) %>% 
kable(digits = 3, caption = "Simulated estimates and estimand from one run of the Gulzar-Khan design.", booktabs = TRUE)
```

In Table \@ref(tab:gulzarkhansimcrosstab), we display the number of people and villages in each treatment assignment and sampling condition to illustrate the data strategy:

```{r gulzarkhansimcrosstab, echo = FALSE}
summary_df %>% 
  kable(caption = "Count of units by treatment assignment and sampling status.", booktabs = TRUE)
```

## Further reading


- @Imbens2015 on potential outcomes
- @halpern2000 on causal models



<!-- ## Brief illustration -->

<!-- To illustrate, consider the following design.  -->

<!-- * A model *M* specifies three variables $X$, $Y$ and $Z$ (all defined on the reals). These form the signature. In additional we assume functional relationships between them that allow for the possibility of confounding (for example, $Y =  bX + Z + \epsilon_Y; X = Z+ \epsilon_X$, with $Z, \epsilon_X, \epsilon_Z$ distributed standard normal).  -->

<!-- * The inquiry $I$ is ``what would be the average effect of a unit increase in $X$ on $Y$ in the population?'' Note that this question depends on the signature of the model, but not the functional equations of the model (the answer provided by the model does of course depend on the functional equations).  -->

<!-- * Under data strategy, $D$,  data is gathered on $X$ and $Y$ for $n$ randomly selected units.  -->

<!-- * Answer $a^d$, is then generated using ordinary least squares as the answer strategy, $A$. -->

<!-- We have specified all the components of MIDA. We now ask: How strong is this research design? One way to answer this question is with respect to the diagnosand "expected error." Here the model's functional equations provide an answer, $a^m$ to the inquiry (for any draw of $\beta$), and so the distribution of the expected error, *given the model*, $a^d-a^m$, can be calculated. In this example the expected performance of the design may be poor, as measured by this diagnosand, because the data and analysis strategy do not handle the confounding described by the model. -->

<!-- In comparison, better performance may be achieved through an alternative data strategy (e.g., where $D'$ randomly assigned $X$ to $n$ units before recording $X$ and $Y$) or an alternative analysis strategy (e.g., $A'$  conditions on $Z$). These design evaluations depend on the model, and so one might reasonably ask how performance would look were the model different (for example if the underlying process involved nonlinearities). -->













