---
title: "Declaration"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
source("scripts/vayr.R")
```

<!-- start post here, do not edit above -->

# Declaration

<!-- make sure to rename the section title below -->

```{r research_questions, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

In Chapter 2, we gave a high-level overview of our framework for describing research designs in terms of their models, inquiries, data strategies, and answer strategies, our process for diagnosing their properties, and a general purpose approach for improving them to better fit research tasks. Now in this chapter, we place our approach on firmal formal footing. To do so, we employ elements from Pearl's [-@Pearl2009] approach to causal modeling (directed acyclic graphs, or DAGs for short), which provides a syntax for mapping design inputs to design outputs. We also use the potential outcomes framework as presented, for example, in @Imbens2015, which many social scientists use to clarify their inferential targets. 

Our goal here is not formalization for formalization's sake. Describing a research design as a DAG helps us to see the fundamental symmetries across the theoretical (M and I) and empirical (D and A) halves of a research design. A recurring theme of our book is that research designs tend to be stronger when the relationship of M to I is mirrored by the relationship of D to A; the aim of this chapter is to make this somewhat mystical claim more concrete.

We define a research design as $\Delta$, with four elements $<M,I,D,A>$. Describing a research design entails "declaring" each of these four elements. 

$M$ is a reference **model**, or set of reference models, of how the world works. Following Pearl's definition of a probabilistic causal model, a model in $M$ contains three core elements. The first is a specification of the variables $X$ about which research is being conducted. This includes endogenous and exogenous variables ($V$ and $U$ respectively) and the ranges of these variables. In the formal literature this is sometimes called the *signature* of a model [@halpern2000]. The second element ($F$) a specification of how each endogenous variable depends on other variables. These can be considered functional relations or, as in @Imbens2015, potential outcomes. The third and final element is a probability distribution over exogenous variables, written $P(U)$. Since the model is probabalistic, we can think of $m$ as an element of $M$. 

The **inquiry** $I$ is a summary of the variables $X$, perhaps given interventions on some variables. An inquiry might be the average value of an outcome $Y$: $\E[Y] = \int{y\times Pr(Y=y)}$, or the average value of the outcome conditional on the value of a treatment $Z$: $\E[Y|Z=1] = \int{y\times Pr(Y=y|Z=1)}$. Using Pearl's notation we can distinguish between descriptive inquiries and causal inquiries. Causal inquiries are those that summarise distributions that would arise under interventions, as indicated by the $do()$ operator, e.g., $\Pr(Y | do(Z = 1))$. Descriptive inquiries summarise distributions that arise without intervention, such as $\Pr(Y | Z =1)$.

We let $a^m$ denote the answer to $I$ *under the model*. Conditional on the model, $a^m$ is the value of the estimand, the quantity that the researcher wants to learn about. The connection of $a^m$ to the model can be seen in the following relationship: $a^m = I(m)$. 

As the saying goes, models are wrong but some may perhaps be useful. We denote the *true* causal model of the world as $W$ and the realized world $w$ as a draw from this true causal model. The true answer, then, is $a^w = I(w)$. The answer under the model $a^m$ may be close or far from the true value $a^w$, which is to say it could be wrong. If the model $M$ is far from the $W$, then of course $a^m$ need not be correct. We note that $a^w$ might be undefined, since inquiries can only be stated in terms of theoretical models. If the theoretical model is wrong enough, then the inquiry might be nonsensical when applied to the real world.

A **data** strategy, $D$, generates data $d$. Data $d$ arises, under model $M$ with probability $P_M(d|D)$. The data strategy includes sampling strategies and assignment strategies, which we denote with $P_S$ and $P_Z$ respectively. Measurement techniques are also a part of data strategies and can be thought of as a selection of observable variables that carry information about unobservable variables. The data strategy operates on $w$ to produce the observed data: $D(w) = d$.

While $M$ reflects our beliefs about true underlying causal processes at work where $D$ is the procedure that results in the collection or creation of data. In this light, the phrase "data generating process" is imprecise. Scholars usually use the phrase "true DGP" to refer to $M$, even though $M$ doesn't create data, $D$ does when it is applied to the real world.

The **answer** strategy, $A$ generates answer $a^d$ using data $d$. We encode this relationship as $A(d) = a^d$. 

The full set of causal relationships between $M$, $I$, $D$, and $A$ with respect to $m$, $a^m$, $d$, $a^d$, $w$, and $a^w$ can be seen in the DAG schematic representation of a research design.

```{r worldandtheory, echo = FALSE, fig.cap = "MIDA as a DAG", fig.width = 6.5, fig.height = 3}
dag <-
  dagify(aw ~ w + I,
         m ~ M,
         am ~ m + I,
         d ~ D + w,
         ad ~ A + d,
         w ~ W)

dag_base <- tidy_dagitty(dag) %>%
  select(name, direction, to, circular) %>%
  as_tibble

nodes_df <-
  tibble(
    name = c("M", "I", "D", "A", "m", "am", "aw", "d", "ad", "w", "W"),
    label = c("M", "I", "D", "A", "m", "a<sup>m</sup>", "a<sup>w</sup>", "d", "a<sup>d</sup>", "w", "W"),
    long_label = c("Theoretical<br>causal model", "Inquiry", "Data<br>strategy", "Answer<br>strategy", "Model<br>draw", "Theoretical<br>answer", "True answer", "Realized<br>data", "Empirical<br>answer", "Real<br>world", "True<br>causal model"),
    lbl_direction = c("N", "N", "N", "N", "S", "S", "S", "S", "S", "S", "N"),
    x = c(1, 2, 5.5, 6.5, 1, 2, 4.25, 5.5, 6.5, 3.25, 3.25),
    y = c(3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3)
  )

endnodes_df <-
  nodes_df %>%
  transmute(to = name, xend = x, yend = y)

gg_df <-
  dag_base %>%
  left_join(nodes_df, by = "name") %>%
  left_join(endnodes_df, by = "to")

gg_df <-
  gg_df %>%
  mutate(arced1 = (name == "w" & to == "d"),
         arced2 = (name == "I" & to == "aw")) %>%
  arrange(name)

rect_df <-
  tibble(
    xmin = c(.4, 4.9),
    xmax = c(2.6, 7.1),
    ymin = c(1.15, 1.15),
    ymax = c(3.85, 3.85)
  )

g <-
  ggplot(data = filter(gg_df, !arced1 & !arced2), aes(
    x = x,
    y = y,
    xend = xend,
    yend = yend
  )) +
  geom_point(color = gray(.1), fill = NA, size = 15, stroke = 0.5, pch = 1) +
  geom_dag_edges(edge_width = 0.35) +
  geom_dag_edges_arc(data = filter(gg_df, arced1), curvature = -0.28, edge_width = 0.35) +
  geom_dag_edges_arc(data = filter(gg_df, arced2), curvature = .7, edge_width = 0.35) +
  geom_richtext(color = "black",
                parse = TRUE,
                aes(label = label),
                fill = NA,
                label.color = NA,
                label.padding = grid::unit(rep(0, 4), "pt"),
                size = 4) +
  geom_richtext(
    aes(y = y + if_else(lbl_direction == "N", 0.4, -0.4),
        vjust = if_else(lbl_direction == "N", "bottom", "top"),
        label = long_label),
    color = gray(0.5),
    parse = TRUE,
    fill = NA,
    label.color = NA,
    label.padding = grid::unit(rep(0, 4), "pt"),
    size = 4) +
  coord_fixed(ylim = c(1.15, 4)) + 
  geom_rect(data = rect_df, aes(x = NULL, y = NULL, 
                                xend = NULL, yend = NULL,
                                xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax),
            alpha = 0.15) +
  annotate("text", x = 1.5, y = 4.05, label = "Theory") +
  annotate("text", x = 6, y = 4.05, label = "Empirics") +
  annotate("text", x = 3.75, y = 4.05, label = "Reality") +
  # annotate("text", x = 3, y = 1.6, label = "Truth") +
  theme_dag()
g
```

Figure \@ref(fig:worldandtheory) illustrates  how a research design is a correspondence $I(m) = a^m$ and $A(d) = a^d$. The theoretical half of a research design produces an answer to the inquiry **in theory**. The emprical half of a research design produces an *empirical estimate* of the answer to the inquiry. 

Neither answer is necessarily close to the truth $a^w$, of course. And, as shown in the figure, the truth is not directly accessible either to us in theory or in empirics. Our gamble in empirical research however is that our theoretical models are close enough to the truth: that the truth is like the set of models we imagine in our model set. If indeed the truth can be thought of as one of the possible worlds in our model set then we have a simpler set of relations between theory and empirics, as shown in Figure \@ref(fig:worldandtheory2).  

<!-- MH: Having some trouble understanding the distrinction between W and w. If w includes potential outcome functions, isn't it the true causal model? If W is the set of possible worlds, is that a subject of inquiry. For instance say our estimand is sd(ate) would that be a(W) rather than a(w). -->

<!-- Think it worth having a second figure which shows just two panels:if indeed w \in M, then: -->



```{r worldandtheory2, echo=FALSE, fig.cap="MIDA as a DAG: In which we pretend the world is included in the set of models we stipulate.", fig.width = 6.5, fig.height = 3}
dag <-
  dagify(aw ~ w + I,
         w ~ M,
         d ~ D + w,
         ad ~ A + d)

dag_base <- tidy_dagitty(dag) %>%
  select(name, direction, to, circular) %>%
  as_tibble

nodes_df <- nodes_df[-c(7, 10, 11),]
nodes_df[1, 3] <- c("Possible worlds") 
nodes_df[5, 1] <- c("w") 
nodes_df[5, 2] <- c("w") 
nodes_df[5, 3] <- c("World") 
nodes_df[6, 1] <- c("aw") 
nodes_df[6, 2] <- c("a<sup>w</sup>") 
nodes_df[6, 3] <- c("True answer") 

endnodes_df <-
  nodes_df %>%
  transmute(to = name, xend = x, yend = y)

gg_df <-
  dag_base %>%
  left_join(nodes_df, by = "name") %>%
  left_join(endnodes_df, by = "to")

gg_df <-
  gg_df %>%
  mutate(arced1 = (name == "w" & to == "d"),
         arced2 = (name == "I" & to == "aw")) %>%
  arrange(name)

rect_df <-
  tibble(
    xmin = c(.4, 4.9),
    xmax = c(2.6, 7.1),
    ymin = c(1.15, 1.15),
    ymax = c(3.85, 3.85)
  )

g <-
  ggplot(data = filter(gg_df, !arced1), aes(
    x = x,
    y = y,
    xend = xend,
    yend = yend
  )) +
  geom_point(color = gray(.1), fill = NA, size = 15, stroke = 0.5, pch = 1) +
  geom_dag_edges(edge_width = 0.35) +
  geom_dag_edges_arc(data = filter(gg_df, arced1), curvature = -0.28, edge_width = 0.35) +
#  geom_dag_edges_arc(data = filter(gg_df, arced2), curvature = .7, edge_width = 0.35) +
  geom_richtext(color = "black",
                parse = TRUE,
                aes(label = label),
                fill = NA,
                label.color = NA,
                label.padding = grid::unit(rep(0, 4), "pt"),
                size = 4) +
  geom_richtext(
    aes(y = y + if_else(lbl_direction == "N", 0.4, -0.4),
        vjust = if_else(lbl_direction == "N", "bottom", "top"),
        label = long_label),
    color = gray(0.5),
    parse = TRUE,
    fill = NA,
    label.color = NA,
    label.padding = grid::unit(rep(0, 4), "pt"),
    size = 4) +
  coord_fixed(ylim = c(1.15, 4)) + 
  geom_rect(data = rect_df, aes(x = NULL, y = NULL, 
                                xend = NULL, yend = NULL,
                                xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax),
            alpha = 0.15) +
  annotate("text", x = 1.5, y = 4.05, label = "Theory") +
  annotate("text", x = 6, y = 4.05, label = "Empirics") +
  # annotate("text", x = 3, y = 1.6, label = "Truth") +
  theme_dag()
g
```


| Relationship  | Description                                                                              |
| ------------- | ---------------------------------------------------------------------------------------  |
| $m = M()$     | $m$ is a draw from the theoretical causal model $M$                                      |
| $a^m = I(m)$  | the answer under the model $a^m$ is the inquiry applied to a draw from the model $m$     |
| $w = W()$     | the real world $w$ is a draw from the true causal model $W$ (unobservable)               |
| $a^w = I(w)$  | the true answer $a^w$ is the inquiry were it applied to the real world $w$ (unobservable)|
| $d = D(w)$    | the realized dataset $d$ results from the data strategy applied to the real world $w$    |
| $a^d = A(d)$  | the empirical answer $a^d$ is the answer strategy applied to the dataset $d$             |

Table: (\#tab:elementsofresearchdesign) Elements of research design.

From this discussion we see a striking analogy between the $M$, $I$ relationship and the $D$, $A$ relationship. The answer we aim for is gotten  by applying $I$ to a draw from $M$. But the answer we have access to is  gotten  by applying $A$ to a draw from $D$. And our hope, usually, is that these two answers are the same. In some cases this suggests that $A$ should be "like" $I$: for instance if we are interested in the mean of a population and we have access to a random sample, the data available to us from $D$ is like the ideal data we would have if we could observe $M$ directly. And indeed taking the mean of the data is likely a good strategy. But this principle does not hold as general matter. For instance if data is sampled with non uniform probabilities, a simple mean of observed data is likely not a good strategy to calculate the average of a population. Rather, the focus should be on the extent to which the two sets of answers line up. 

<!-- The central crux of good research design is generated by the deep analogy of Models and Inquiries to Data and Answer Strategies. This analogy can guide many design decisions. The guidance that measurement procedures should be theoretically informed is an instance of the general point that data strategies should parallel theoretical models. The general preference for nonparametric estimation approaches over parametric procedures is an instance of the general point that answer strategies should parallel inquiries.  -->

<!-- $$ -->
<!-- M : I : : D : A -->
<!-- $$ -->

<!-- The DAG also clarifies the main difference between the theoretical and emprical halves of research design is the introduction of reality. When we draw from $M$ to produce $m$, we write $M() = m$ -- the function has no inputs from outside the model because it is purely theoretical. When we draw from the $D$ to produce $d$, we write $D(w) = d$ -- the dataset $d$ that results from the data strategy $D$ depends on the actual state of the world $w$. Empirical researchers must be theorists too: our goal is to choose our theoretical model $M$ such that $m$ is as close to $w$ as possible.  -->

<!-- Idea we want to express here: M:I::D:A, but M and D are different. How are they different? The first way is that M is theoretical and D is empirical -- D() has w as an argument. The second way is that there are two "fundamental" problems facing empirical research. The first is the fundamental problem of causal inference -- we can't observe units in counterfactual states. The second is the fundmental problem of descriptive inference -- we can't observe the latent Y*, only Y as measured. -->

<!-- With a design declared in terms of MIDA, we can then diagnose its properties, and redesign to find an optimal design subject to constraints. Design diagnosis is the process of simulating both $I(m) = a^m$ and $A(d) = a^d$ over many draws from $M()$ and $D(m)$, and comparing them. In other words, diagnosis is the process of learning the value of a diagnosand for a single design, often under multiple specifications of M to assess robustness to alternative models. Redesign is the process of *changing* parts of D and A in order to learn how the diagnosands change. The redesign process is complete when a research selects the best (or one of the best) D and A among the feasible set, as measured by the changing values of the diagnosand. -->

## Example {#p2gulzarkhanrevisited}

In this section, we declare the @Gulzar2020 example in finer detail. The aim is to capture all of the analytically-relevant features of the design. In Chapter 2, we declared in words and in code a simplified version of the design, in order to communicate its key features. We will make this move again throughout the book: we will present simplified versions of canonical designs in the text, and also provide worked examples with the details of specific studies. Moving back and forth between the two will, we hope, enable you to both learn about the general principles and how to declare your own designs which always involve the fine details of research implementation.

We first describe the design in words and then declare in code:

The *model* for the study describes the set of units: citizens living in the two Pakistani districts the study took place in, Haripur and Abbottabad. The endogenous outcomes of interest are whether the citizen filed papers to run for office; whether the citizen was elected; and the Euclidean distance of the citizen's preferences from average citizen preferences. The outcomes are a function in their causal model of three treatments, which emphasize either the social or personal benefits to holding public office or do not encourage anyone to run for office at all. Their model would include an expected treatment effect magnitude for each treatment and their suppositions about how correlated outcomes are within villages that they study. 

The *inquiry* is the effect of an encouragement to run for office focused on prosocial motivations for officeholding (rather than encouragements that emphasized  personal benefits) on the rate of filing papers to run for office between people living in villages randomly assigned to receive.

The *data strategy* entailed three steps: (1) randomly sampling 192 villages from among all villages in Haripur and Abbottabad districts, and using a random walk procedure to select 48 citizens in each village to participate in the experiment; (2) randomly-assigning each village with equal probability to one of three conditions (neutral, social benefits, or personal benefits); and (3) collecting administrative data on who filed papers to run and matching that back to pretreatment survey data on the 9,216 citizens. Sampling, treatment assignment, and measurement are the three common data strategy steps in an experiment; some experiments, instead, do not include a sampling step and instead assign treatments within a convenience sample. 

@Gulzar2020 have a two-step *answer strategy*, fitting a linear model predicting whether a citizen ran for office (the outcome) with indicators for the social benefits and the personal benefits treatments, and then calculating the difference between the two coefficients as an estimate of whether social benefits are more or less effective than the personal benefits. Their answer strategy includes presenting a table with estimated difference, a standard error clustered on village to account for village-level random assignment, and a *p*-value calculated using permutation inference.

```{r}
gulzar_khan_design <- 
  
  declare_population(
    # study is conducted in two districts in Pakistan, with 311 and 359 villages in them
    districts = add_level(N = 2, name = c("Haripur", "Abbottabad"), N_villages = c(311, 359)),
    
    # villages nested within districts
    villages = add_level(N = N_villages),
    
    # avg. 6500 citizens per village
    citizens = add_level(N = 6500)
  ) +
  
  # main outcome is whether a citizen filed papers to run for office
  # we define potential outcomes in response to being assigned to a social, personal, or neutral appeal to run
  declare_potential_outcomes(
    filed_papers ~ rbinom(N, 1, prob = 0.05 + 0.05 * (Z_appeal == "social") + 0.01 * (Z_appeal == "personal")),
    assignment_variable = Z_appeal, conditions = c("neutral", "social", "personal")
  ) + 
  
  # inquiry is the difference in rates of filing papers between the social and personal appeal conditions
  declare_estimand(ATE = mean(filed_papers_Z_appeal_social - filed_papers_Z_appeal_personal)) + 
  
  # sample 192 villages
  declare_sampling(clusters = villages, n = 192, sampling_variable = "S_villages") + 
  
  # sample 48 citizens in each village via random walk
  declare_sampling(strata = villages, n = 48, sampling_variable = "S_citizens") + 
  
  # assign villages to three arms with equal probabilities for three types of appeals to run for office
  declare_assignment(
    m_each = c(48, 72, 72),
    clusters = villages,
    conditions = c("neutral", "social", "personal"),
    assignment_variable = Z_appeal
  ) + 
  
  # recode treatment assignment for analysis into indicators for the two conditions of interest
  declare_step(
    Z_social_village = if_else(Z_appeal == "social", 1, 0),
    Z_personal_village = if_else(Z_appeal == "personal", 1, 0),
    handler = mutate
  ) +
  
  # 1. run a linear regression with condition indicators
  # 2. calculate the difference in effects between people in villages assigned to social appeals compared
  #    to those assigned to personal appeals
  # 3. calculate robust standard errors clustered on village
  declare_estimator(
    filed_papers ~ Z_social_village + Z_personal_village, 
    linear_hypothesis = "Z_social_village - Z_personal_village = 0",
    term = "Z_social_village - Z_personal_village = 0",
    clusters = villages,
    model = lh_robust
  )
```

```{r p2gulzarkhandag,echo=FALSE, fig.height = 3.5, fig.width = 7, fig.cap = "DAG for Gulzar-Khan design."}
design <-
  declare_population(N = 100,
                     X = rbinom(N, 1, 0.3),
                     U = rnorm(N)) +
  declare_potential_outcomes(Y ~ Z + X + U) +
  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +
  declare_sampling(strata = X, prob = 1) + 
  declare_assignment(blocks = X, block_prob = c(0.1, 0.5)) +
  declare_estimator(Y ~ Z, estimand = "ATE", label = "Naive DIM") +
  declare_estimator(Y ~ Z,
                    blocks = X,
                    estimand = "ATE",
                    label = "Blocked DIM")

dag <- dagify(
  Y ~ Z + X + U,
  Z ~ X + S,
  S ~ X
)

nodes <-
  tibble(
    name = c("Y", "S", "Z", "U", "X"),
    label = c("Y", "S", "Z", "U", "X"),
    annotation = c(
      "**Outcome**<br>",
      "**Sampling**",
      "**Random assignment**<br>",
      "**Unknown heterogeneity**",
      "**villages**<br>Stratification, cluster assignment"),
    x = c(5, 1, 3, 5, 1),
    y = c(2.5, 2.5, 2.5, 4, 4), 
    nudge_direction = c("S", "S", "S", "N", "N"),
    answer_strategy = "uncontrolled"
  )
ggdd_df <- make_dag_df(dag, nodes, design)

base_dag_plot %+% ggdd_df + coord_fixed(ylim = c(2.05, 4.6), xlim = c(0.25 - epsilon, 5.75 + epsilon)) 
```

## Further reading


- @Imbens2015 on potential outcomes
- @halpern2000 on causal models



<!-- ## Brief illustration -->

<!-- To illustrate, consider the following design.  -->

<!-- * A model *M* specifies three variables $X$, $Y$ and $Z$ (all defined on the reals). These form the signature. In additional we assume functional relationships between them that allow for the possibility of confounding (for example, $Y =  bX + Z + \epsilon_Y; X = Z+ \epsilon_X$, with $Z, \epsilon_X, \epsilon_Z$ distributed standard normal).  -->

<!-- * The inquiry $I$ is ``what would be the average effect of a unit increase in $X$ on $Y$ in the population?'' Note that this question depends on the signature of the model, but not the functional equations of the model (the answer provided by the model does of course depend on the functional equations).  -->

<!-- * Under data strategy, $D$,  data is gathered on $X$ and $Y$ for $n$ randomly selected units.  -->

<!-- * Answer $a^d$, is then generated using ordinary least squares as the answer strategy, $A$. -->

<!-- We have specified all the components of MIDA. We now ask: How strong is this research design? One way to answer this question is with respect to the diagnosand "expected error." Here the model's functional equations provide an answer, $a^m$ to the inquiry (for any draw of $\beta$), and so the distribution of the expected error, *given the model*, $a^d-a^m$, can be calculated. In this example the expected performance of the design may be poor, as measured by this diagnosand, because the data and analysis strategy do not handle the confounding described by the model. -->

<!-- In comparison, better performance may be achieved through an alternative data strategy (e.g., where $D'$ randomly assigned $X$ to $n$ units before recording $X$ and $Y$) or an alternative analysis strategy (e.g., $A'$  conditions on $Z$). These design evaluations depend on the model, and so one might reasonably ask how performance would look were the model different (for example if the underlying process involved nonlinearities). -->













