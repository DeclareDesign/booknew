---
title: "Crafting a data strategy"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# Crafting a data strategy

<!-- make sure to rename the section title below -->

```{r crafting_a_data_strategy, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

The data strategy is what researchers do in the world in order to collect information about it. Depending on the design, it could include decisions about any or or all of the following: how to sample or select cases, how to assign treatments, or how to measure outcomes.   

These choices apply to all kinds of research. In experimental research, a large focus is given to the assignment of treatments. How many treatment conditions should there be? Should we use a simple coin flip to decide who receives treatment, or should we use a more complicated strategy like blocking? Experimenters are of course also very concerned with sampling and measurement procedures, but it is the random assignment to treatments that make experiments distinctive among research designs. 

Quantitative descriptive research, on the other hand, often has an inquiry like the population average of some outcome variable. Since the goal here is to draw inferences about a population on the basis of a sample, we need to pay special attention to the procedure by which units are selected into the sample. We might use a random sampling procedure in order to generate a design-based justification for generalizing from samples to population. Nonrandom sampling procedures are also possible: convenience sampling, respondent-driven sampling, and snowball sampling are all data strategies that do not include an explicitly randomized component.

Once we have selected units into the sample, we need to measure them in some way. The tools we use to measure are a critical part of the data strategy. For many social scientific studies, a main way we collect information is through surveys. A huge methodological literature on survey administration has developed to help guide researchers who have to design questionnaires. Bad survey questions yield distorted or noisy responses. A biased question systematically misses the true latent target it is designed to measure, in which case the question has low *validity*. A question is high variance if (hypothetically) you would obtain different answers each time you asked, in which case the question has low *reliability*. 

The concerns about validity and reliability do not disappear once we move out of the survey environment. For example, the information that shows up in an administrative database is itself the result of many human decisions, each of which has the possibility of increasing or decreasing the distance between the measurement and the latent measurement target.

Researchers have to choose good sampling, assignment, and measurement techniques that, when combined and applied to the world, will produce information that is ready for analysis. We will discuss answer strategies -- the set of analysis choices about what to do with the data once they are collected -- in the next chapter. The data and answer strategies are of course intimately interconnected. How you analyze data depends deeply on how they were collected *and* how you collect data depends just as deeply on how you plan to analyze them. For the moment, we are thinking through the many choices we might make as part of the data strategy, but in any applied research design setting, they will have to be considered in concert with the answer strategy .

The data strategy is a *set of procedures* that result in a dataset. It is important to keep these two concepts straight. If you apply data strategy $D$ to the world $w$, it produces dataset $d$. We say $d$ is "the" result of $D$, since when we apply the data strategy to the world, we only do so once and we obtain the data we obtain. But when we are crafting a data strategy, we have to think about the many datasets that the data strategy *could have* produced. Some of the datasets might be really excellent. For example, in good datasets, we achieve good covariate balance across the treatment and control groups. Or we might draw a sample whose distribution of observable characteristics looks really similar to the population. But some of the datasets might be worse: because of the vagaries of randomization, the particular realizations of the random assignment or random sampling might be more or less balanced. We do not have to settle for data strategies that might produce weak datasets -- we are in control of the procedures we choose. We want to choose a data strategy $D$ that is likely to result in a high-quality dataset $d$.

The data strategy has three elements: sampling, assignment, and measurement. Sampling is the procedure for selecting which units will be measured; assignment is the procedure for allocating treatments to sampled units; and measurement is the procedure for turning information about the sampled units into data. All empirical studies have a data strategy and every data strategy involves sampling, assignment, and measurement. Even studies in which researchers measure the full universe of cases involve sampling, since *future* units are not included. Even studies in which researchers do not apply treatments themselves involve assignment, since other forces end up assigning units to conditions. Even studies in which researchers take no new measurements involve measurement, since a choice about which measurements made by others to use must nevertheless be made.

Sampling choices confront the fundamental problem of generalization: we want to make inferences about units *not* sampled. Assignment choices confront the fundamental problem of causal inference: we want to make inferences about the conditions to which units were *not* assigned. Measurement choices confront the fundamental problem of descriptive inference: we want to make inferences about latent values on the basis of measured values. These problems are all fundamental in the sense that they cannot be surmounted and researchers should be humble in face of them. Strong research design can help, but we can never be *sure* that our sample generalizes, or that we know what would have happened in a counterfactual state of the world, or what true latent value of the outcome is (or if it even exists). 


<!-- All studies will have a measurement strategy (otherwise nothing is measured and no data results from the data strategy), but only some will have sampling and treatment assignment procedures. A survey to measure satisfaction in a class might not sample units and instead measure all students in the class and no treatment would be assigned (we could, of course, define the sampling strategy as sampling all units). An experiment conducted in a class might have measurement and a treatment assignment strategy but no sampling. A survey of voters in a city might have a measurement strategy and a sampling strategy (random digit dialing, for example), but no treatment assignment (of course, here, we could define the treatment assignment procedure as assigning all units to a no-intervention condition). -->

## Sampling

Sampling is the process by which units are selected from the population to be studied. Some sampling procedures involve randomization while others do not. Sometimes -- even oftentimes -- randomized sampling procedures break in ways that turn them into non randomized designs. Whether a sampling procedure is randomized or not has large implications for the answer stratey. Randomized designs support "design-based inference," which refers to the that we rely on known features of the sampling process when producing population-level estimates. When randomization breaks down (e.g., if the design encounters attrition) or if a nonrandomized design is used, then we have to fall back on model-based inference. Model-based inference relies on beliefs about the process by which units came to be in the sample in order to make inferences about the population. When possible, design-based inference is preferable, because it's easier to be correct about about the features of the sampling process directly under researcher control than about the uncontrolled processes.

The model defines the target population and the sampling strategy defines which units from that population will be sampled. Why would we ever be content to study a sample and not the full population? The first and best explanation is cost: it's expensive and time-consuming to conduct a full census of the population. The second reason is diminishing marginal returns of additional data collection. Increasing the number of sampled units from 1,000 to 2,000 will greatly increase the precision of our estimates. Moving from 100,000 to 101,000 will improve things too, but the scale of the improvement is much smaller.

### Randomized sampling designs

Owing to the natural appeal of design-based inference, we start off with randomized designs. Randomized sampling designs typically begin with a list of all units in a population, then choose a subset to sample using a random process. These random processes can be simple (every unit has an equal probability of inclusion) or complex (first we select regions at random, then villages at random within selected regions, then households within selected villages, then individuals within selected households).



| Design                      | Description                                                                                                                                                                                                | Randomizr function                |
|-----------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------|
| Simple random sampling      | "Coin flip" or Bernoulli random sampling. All units have the same inclusion probability p                                                                                                                  | simple_rs(N = 100, p = 0.25)      |
| Complete random sampling    | Exactly n of N units are sampled, and all units have the same inclusion probability n/N                                                                                                                    | complete_rs(N = 100, n = 40)      |
| Stratified random sampling  | Complete random sampling within pre-defined strata. Units within the same strata have the same inclusion probability n_s / N_s, but units in different strata might have different inclusion probabilities | strata_rs(strata = regions)       |
| Cluster random sampling     | Whole groups of units are brought into the sample together.                                                                                                                                                | cluster_ra(clusters = households) |
| Stratifed cluster sampling  | Cluster random sampling within strata                                                                                                                                                                      |                                   |
| Multi-stage random sampling | First clusters, then units within clusters                                                                                                                                                                 |                                   |
|                             |                                                                                                                                                                                                            |                                   |
|                             |                                                                                                                                                                                                            |                                   |

There are many ways to draw a random sample, but two are particularly common in the social sciences: simple and stratified. Simple random sampling is sampling without replacement with a fixed sample size. Stratified random sampling takes a simple random sample of a fixed size from two or more subgroups, or strata, in the data. There are two different reasons you might select stratified random sampling over simple: your inquiry might involve comparisons between two subgroups, and you want to ensure you have a sufficient size in each subgroup; and there may be very different amounts of variability in two groups, and you want the most efficient estimate of a summary of that data so you oversample from the group with the higher variance to get a similar amount of precision from each group. Stratified sampling is a simple case of a larger class of sampling strategies, in which the probability of inclusion in the sample varies across units. In stratified sampling with two strata, the probability of inclusion is set for the two groups and is fixed within them. However, the probability can vary within strata as well. If you wish to have you sample include all age groups but upweight older people, you could make the probability of inclusion a function of age: the probability of sampling an 18 year old could be 0.01 ranging up to a probability of 0.1 for 80 year olds.

When it is impossible to get a list of all units in the target population either because it does not exist or is too costly to obtain, clustered sampling is often a substitute. In cluster sampling, you obtain a list of clusters of units, randomly sample from the list of clusters, and either collect data from all units within selected clusters, or conduct a simple random sample of units within sampled cluster (this would represent a simple form of multistage sampling). The advantage is you can still obtain a random sample of units, but it is feasible to do so because you have a list of clusters (but not of all units). The disadvantage is that units within clusters often have similar outcomes, so you don't get as much benefit from sampling more clusters as you do from sampling units from other clusters that are more different. This efficiency tradeoff -- how many clusters to choose vs. how many units within clusters -- can be explored in design diagnosis by comparing the RMSE of various choices. There are many other forms of multistage sampling -- sample provinces then sample districts within provinces, then villages within districts, and people within villages -- that fit particular aims and budgets. 


### Nonrandomized sampling designs

Convenience sampling means you obtain a set of units from the population, but how you do so is not governed by any rule. Instead, you find units in the cheapest way possible. Convenience sampling is a good choice in two circumstances: it is the only strategy that is affordable, or when sampling is ignorable. Ignorability refers to the independence of missingness and characteristics of units. For many purposes, convenience sampling is sufficient, such as conducting experiments where treatment effects are not expected to vary by characteristics of the units in the sample. In other circumstances, such as measuring a sample mean like the proportion of people who are unmarried, convenience sampling is likely to lead to badly biased estimates due to a lack of ignorability. The bias comes from the fact that the types of people you are able to sample may have different outcomes than the types of people you are unable to sample. Many types of qualitative and quantitative research involve convenience sampling. Archival research often involves a convenience sample of documents on a certain topic that exist in an archive. The question of how these documents differ from those that would be in a different archive, or how the documents available in archives differ from those that do not ever make it into the archive importantly shapes what we can learn from them (Aliza Luft paper cite). With the decline of telephone survey response rates (cite), researchers can no longer rely on random digit dialing to obtain a representative sample of people in many countries, and instead must rely on convenience samples from the internet or panels who agree to have their phone numbers in a list. Reweighting techniques in the answer strategy can, in some cases, help recover estimates for the population as a whole if sampling is ignorable conditional on variables you can measure and weight on.

Purposive sampling is a catch-all term for rule-based sampling strategies that do not involve random draws but also are not purely based on convenience and cost. A common example is quota sampling. In studies where there is treatment effect heterogeneity, getting samples highly imbalanced in the characteristics by which effects vary will lead to bias for the population average treatment effect. If effects vary by gender and the study is conducted on Mechanical Turk, which skews male, there may be a problem. Quota sampling addresses the problem by ensuring through a convenience sample that fixed proportions of types of people are selected. In Mechanical Turk, we might set a quota of 50% men and 50% women. We would continue to select people in the convenience sample until we reached this threshold, by rejecting men from the sample after we reached 50%. Another common form of purposive sampling is respondent-driven sampling, which is used to sample from hard-to-reach populations such as HIV-positive needle users. RDS methods often begin with a convenience sample and then systematically obtain contacts for other units who share the same characteristic in order the build a large sample.

Case selection is another term for a sampling strategy. In case study research, whether qualitative or quantitative, the way you select the (typically small) set of cases is of great importance, and considerable attention has been paid to developing purposive case selection methods. Mills (cite) method of difference is a common case selection procedure, in which cases are selected that have different outcomes and the cases are inspected to identify characteristics that differ that may have led to the divergence. Lieberman (2005) proposes a method for selecting cases for indepth study from an initial regression study: cases on the regression line, well-predicted by it, are selected to determine whether the hypothesized causal relationship took place in those cases. Off-the-line cases, that are not well-predicted, are also selected to determine what factors may be left out of the regression model that provide alternative mechanisms. In each method, the inferential goal determines how cases are selected. Research design diagnosis can also help compare the efficiency and bias of these methods to select cases.

Selection on the dependent variable is purposive sampling method that has recieved substantial attention. The procedure is to select positive outcomes --- units in which an event such as a revolution took place, for example --- and study what led to that outcome. Geddes (XXXX) lays out the problem in this method for case study research: we cannot know whether the factors identified from investigating these cases led to the event, because we do not know whether those factors are also present in units where the outcome did not happen. If the factor is present in both, it is unlikely to have caused the positive outcome. The same problem plagues observational quantitative research. Ramsay et al. (XXXX) discuss the bias that results from studying the factors predicting suicide terrorism by only looking at observations where suicide terrorism occurred. In order to avoid these problems accidentally creeping into our research, it is important to diagnose the bias of our designs and not just the efficiency. 

### Choosing among sampling designs

In short, the choice of sampling strategy depends on features of the model and the inquiry, and different sampling strategies can be compared in terms of power and RMSE in design diagnosis. The model defines the population of units we are interested in making inferences about, and the target population of the sampling strategy should match that as much as possible. The model also points us to important subgroups (defined by nodes or endogenous variables) that we may wish to stratify on, depending on the variability within those subgroups. Whether we select convenience, random, or purposive sampling depends on our budget and logistical constraints as well as the efficiency (power or RMSE) of the design. If there is little bias from convenience sampling, we will often want to select it for cost reasons; if we cannot obtain a convenience sample that has the right composition, we may choose a purposive method that ensures we do. The choice between simple and stratified sampling comes down to the inquiry and to a diagnosis of the RMSE: when the inquiry involves a comparison of subgroups, we will often select stratified sampling. In either, a diagnosis of alternative designs in terms of power or RMSE will guide selection.

Sampling, like all data strategies, is an intervention in the world by researchers. As a result, it may have independent causal effects on outcomes. As we will see in the next section, there is no deep difference between sampling and treatment assignment, because in each case we are randomly sampling from two potential outcomes. In treatment assignment, we are assigning units to a treatment group and a control group, and obtaining a sample of the treated potential outcome and a second random sample of the control potential outcome. In random sampling, we are assigning units to be sampled or to not be sampled. We obtain one sample of the sampled potential outcome. However, the non-sampled potential outcome exists conceptually, and may differ from the sampled potential outcome. The act of including units in the sample may change outcomes, for example if you are selected to participate in a medical trial you believe you are going to receive better care and a placebo effect changes your health condition. A research design in which you conduct an experiment on sampled units, but also unobtrusively measure outcomes in nonsampled units could be used to estimate the effect of inclusion in the sample. This design would provide a random sample of the nonsampled potential outcome.

### Pitfalls
<!-- interference, exludability, noncompliance -->

- interference: whether you are sampled depends on whether other people are sampled
- excludability: 
- noncompliance: failure to participate when sampled == survey nonresponse

<!-- you create strata and clusters from real things in the world -->

### Example: @wang2015forecasting

Xbox forecasting of elections

<!-- what is scraping? -->
<!-- what is going to the archive? -->
<!-- sampling on the DV -->

<!-- redefine your inquiry -->

<!-- ALSO is your inquiry really the right one - difference between target population and real population (Americans in 2016 vs Americans every year) -->

<!-- -- sample all of the population or part of it -->
<!-- -- random sampling, purposive sampling -->

<!-- - simple, complete, stratified, clustered, stratified and clustered -->
<!-- - weighted sampling (over/undersampling) -->
<!-- - finding missing populations (RDS) -->
<!-- - quota sampling -->
<!-- - hawthorne effects -->
<!-- - the inquiry and the model guide your choice of which units to sample and how to sample them -->

## Treatment assignment

Treatment assignment is closely related to sampling. A sampling strategy assigns units in a population either to be included in the sample or excluded. A simple treatment assignment strategy assigns units in the sample to one of two conditions, treatment or control. Both assign units from a group into one of two groups. The difference is only in the fact that we do not measure outcomes for nonsampled units (though we could, to learn about how nonsampled units differ from sampled units). The mechanics and statistics of sampling and simple treatment assignments are identical but for this difference. Assignment procedures may assign units into more than two treatment conditions. In a factorial design, two treatments are assigned to a set of units, and a unit can be assigned to neither treatment (this is a "pure control"), just one, the other, or both. In this case, there are four groups a unit could be assigned to.

- simple, complete, blocked, clustered, blocked and clustered
- point restricted randomization
- no assignment procedure at all
- multiple arms
- adaptive
- the inquiry and the model guide your choices of which treatments to assign and how to assign them

### Example: sociology natural experiment from government or official cutoff


## Measurement

Measurement is the part of the data strategy in which all variables used in sampling and treatment assignment as well as the answer strategy are collected. Researchers should measure all the variables needed to conduct other parts of $D$ and $A$. (This is similar to how we identified the variables that should be declared in $M$, but in that case we need to define all the variables needed for $I$ and the measurement strategy some of which are not measured. The latent outcomes are defined in the model but not measured directly.)

The fundamental problem of description is that we can never measure the latent variables we are interested in, $Y^*$, such as fear, support for a political candidate, and economic wellbeing. Instead, we use a measurement technology to imperfectly observe them, which we represent as the function $Q$ that yield the observed outcome $Y^{\mathrm obs}$: $Q(Y^*) = Y^{\mathrm obs}$. Our measurement strategy is a set of functions $Q$ for each variable we measure.

There are two basic ways to assess the quality of each function $Q$: bias, or the difference between the observed and latent outcome, $Y^{\mathrm obs} - Y^*$, which is given the special label *measurement validity*; and *measurement reliability*, which is the variance across multiple outcomes for a given individual, $\Var(Y_1^{\mathrm obs}, Y_2^{\mathrm obs}, Y_3^{\mathrm obs})$.

Researchers select several characteristics of $Q$: the mode of measurement; who collects the measures; how often and when measures are taken; how many different observed measures of $Y^*$ are collected and how they are summarized into a single measure; and what (if any) information is provided to participants about how they are being measured. 

A wide variety of measurement tools are in use in the social sciences, but an even wider are possible. Measures can be collected through human observation: from the five senses, sight, hearing, touch, smell, and taste (the latter three are less frequently used!) as well as self-reports of emotions, attitudes, and perceptions. Recordings can be taken of waves of light (e.g., photos), sound (e.g., audio and seismic recordings), electromagnetic waves (e.g., EKGs and x-rays), and combinations of more than one (e.g., video). Instruments such as thermometers and barometers can provide measures of temperature and atmospheric pressures and medical devices can measure the presence of disease in blood and other fluids. 

For many of these data collection modes, measures could be taken by researchers, research participants themselves, or by a third party (e.g., hired survey interviewers or governments and firms in the case of administrative data). The identity of data collectors may affect both the validity and reliability of estimates. Excludability violations may be introduced if respondents' $Y_i^{obs}$ depends on the data collector, which may happen when respondents infer the goal of the researcher (demand effects) or dishonestly respond to hide sensitive characteristics (sensitivity bias). 

When data are collected is often also important, as is the frequency of data collection. The inquiry should guide when data is collected in relation to other events such as an election or the holiday period or the time after a treatment is delivered to research participants. The inquiry defines whether the effect of interest is a month after treatment or in the case of longterm effects a year or more. However, data need not be collected at a single time period. The model encodes beliefs about the autocorrelation (correlation over time) of data that varies over time, and this can help guide whether to collect multiple measurements or just one and whether to measure data at baseline as well as endline. If data are expected to be highly variable (low autocorrelation), then taking multiple measurements and averaging them may provide efficiency gains. When they exhibit high autocorrelation, then multiple measures will waste resources --- approximately the same measure will be returned each time. However, under high autocorrelation there will be large gains from collecting a baseline measure before a treatment in an experiment, because controlling for baseline outcomes the only major change will be the response to treatment. Thus, the estimates of treatment effects will be much more efficient (more T cite). Beliefs about autocorrelation can guide decisions about the tradeoff between including more participants or more measures of a smaller set of participants. 

Parallel considerations apply to the choice to measure the same latent $Y_i^*$ with multiple measurement tools in the same time period. Each measure comes with a cost, but taking multiple imperfect measures of the latent outcome may yield more reliable estimates. When the tools produce answers that are highly correlated, taking multiple measures is unlikely to be worth the cost but when the correlation is low it will be worth taking multiple measurements and averaging to improve efficiency.

Selecting among measurement modes, data collectors, time periods, frequency, and the number of measurements reduces to tradeoffs between their validity and reliability. We want the largest set of valid, measures to combine into a measurement of $Y_i^*$ that meet our budget and logistical constraints. However, some measurement techniques will vary in both validity and reliability, so often we must decide on a weighting of the two concerns given our research goals. In some cases, validity will be important even if the measures are unreliable, but in others we may want to weight them approximately equally. 

Learning which measurement tools are valid and reliable is ultimately guesswork, though it can be informed guesswork. We cannot measure the true $Y_i^*$, so we cannot truly "validate" any measurement technique. Often studies present themselves as validation studies by comparing a proposed measure to a "ground truth," measured from administrative data or a second technique to reduce measurement error. However, neither measurement is known to be exactly  $Y_i^*$, so ultimately these studies are comparisons of multiple techniques each with their own advantages and disadvantages. This does not make them useless, but rather should be used to understand the how measurements differ on average and in variability. They may also be useful for informing how to combine multiple measures.

### Pitfalls

Perhaps the most common pitfall of measurement is the sensitivity bias that affects measures self-reported by participants in the presence of social pressure or the risk of sanction from authorities who prefer to hide or wish to detect sensitive characteristics in the population (BCM2020). Sensitivity bias represents an excludability violation: the latent outcome $Y_i^*$ depends on which measurement tool is used. More generally, excludability violations of measurement imply that the very act of measurement affects the latent outcome, often known as a Hawthorne effect. Typically, we want to avoid these excludability violations, because we are trying to measure a latent quantity that exists independent of how it is measured. 

Interference can affect measurement, just as it affects sampling and treatment assignment. Measurement interference occurs when $Y_i^*$ depends on whether and how *other* units are measured. If participants in an experiment discuss the measurement technique itself and what researchers are asking about with other participants, this may lead to a violation. Psychology researchers on academic campuses worry about this possibility and encourage subjects not to discuss what they do in the lab with potential participants. They are worried about intereference in treatment assignment but also of measurement, because often implicit measurement techniques can be defeated (and measure something other than $Y^i^*$) when the concept is known before the session. Other researchers worry that when measuring sensitive questions that the sensitive topic will be revealed to participants either by earlier participants or local leaders who discover its purpose (Lyall Blair Imai).  

The final pitfall of measurement is attrition, or nonresponse. There are two types of missing data that may result: when a single measure is missing, commonly known as item nonresponse; and when all measures are missing for a participant, known as survey nonresponse. Though these terms were coined by survey researchers, the problems are identical when not collected by surveys but by passive measurement or by instrumentation. Item nonresponse may result from many causes, but often because respondents do not wish to answer a specific questions because they are sensitive or intrusive. Survey nonresponse in which no questions are answered may be instead due to lack of interest in the measurement as presented, inability to contact to a subject, or an inability of the data collector to build rapport and gain the trust of the participant. Attrition causes problems because we can no longer make inferences about the average value of $Y_i^*$ in the sample let alone a population, because we do not observe outcomes for some units without further assumptions. Under the assumption that $Y_i^{\rm obs}$ is missing completely at random, we can treat $Y_i^{\rm obs}$ for nonmissing participants as a random sample from all participants. When nonresponse is not completely random, then respondents who respond may be different than those who do not respond in terms of $Y_i^*$ --- and we don't learn anything about $Y_i^*$ for nonresponders from the missing data. However, there are design-based techniques for addressing this problem. One is to take a random sample of nonresponders and undertake additional data collection. If you are able to reach all nonresponders, then you can use the random sample as a standin for $Y_i^{\rm obs}$ from all nonresponders and, with some efficiency loss, still obtain summaries of $Y_i^*$ for the whole sample. There are also model-based strategies, in which the probability of nonresponse is estimated from a model, and estimates are weighted by the inverse of the (estimated) probability of responding (cite cite).

<!-- - issues with rates - y^* / x^*, then rates are messed up -->

### Example: @weaver2019too


