---
title: "Crafting a data strategy"
output: html_document
bibliography: ../bib/book.bib
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# Crafting a data strategy

<!-- make sure to rename the section title below -->

```{r crafting_a_data_strategy, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

In order to collect information about the world, researchers must deploy a data strategy. Depending on the design, the data strategy could include decisions about any or or all of the following:  sampling, assignment, and measurement. Sampling is the procedure for selecting which units will be measured; assignment is the procedure for allocating treatments to sampled units; and measurement is the procedure for turning information about the sampled units into data.

Sampling choices confront the fundamental problem of generalization: we want to make inferences about units *not* sampled. For this reason, we need to pay special attention to the procedure by which units are selected into the sample. We might use a random sampling procedure in order to generate a design-based justification for generalizing from samples to the specific population from which units were drawn. Nonrandom sampling procedures are also possible: convenience sampling, respondent-driven sampling, and snowball sampling are all data strategies that do not include an explicitly randomized component. Nonrandomized designs usually require model-based inference in order to generalize from the sample to a population.

Assignment choices confront the fundamental problem of causal inference: we want to make inferences about the conditions to which units were *not* assigned. For this reason, experimental design is focused on the assignment of treatments. How many treatment conditions should there be? Should we use a simple coin flip to decide who receives treatment, or should we use a more complicated strategy like blocking? Experimenters are of course also very concerned with sampling and measurement procedures, but it is the random assignment to treatments that make randomized experiments distinctive among research designs.

Measurement choices confront the fundamental problem of descriptive inference: we want to make inferences about latent values on the basis of measured values. The tools we use to measure are a critical part of the data strategy. For many social scientific studies, a main way we collect information is through surveys. A huge methodological literature on survey administration has developed to help guide questionnaire development. Bad survey questions yield distorted or noisy responses. A biased question systematically misses the true latent target it is designed to measure, in which case the question has low *validity*. A question is high variance if (hypothetically) you would obtain different answers each time you asked, in which case the question has low *reliability*. The concerns about validity and reliability do not disappear once we move out of the survey environment. For example, the information that shows up in an administrative database is itself the result of many human decisions, each of which has the possibility of increasing or decreasing the distance between the measurement and the latent measurement target.

These three problems are all fundamental in the sense that they cannot be surmounted and researchers should be humble in face of them. Strong research design can help, but we can never be *sure* that our sample generalizes, or that we know what would have happened in a counterfactual state of the world, or what true latent value of the outcome is (or if it even exists). Researchers have to choose good sampling, assignment, and measurement techniques that, when combined and applied to the world, will produce analysis-ready information. We will discuss answer strategies -- the set of analysis choices about what to do with the data once they are collected -- in the next chapter. The data and answer strategies are of course intimately interconnected. How you analyze data depends deeply on how they were collected *and* how you collect data depends just as deeply on how you plan to analyze them. For the moment, we are thinking through the many choices we might make as part of the data strategy, but in any applied research design setting, they will have to be considered in concert with the answer strategy.

The data strategy $D$ is a *set of procedures* that result in a dataset $d$. It is important to keep these two concepts straight. If you apply data strategy $D$ to the world $w$, it produces dataset $d$. We say $d$ is "the" result of $D$, since when we apply the data strategy to the world, we only do so once and we obtain the data we obtain. But when we are crafting a data strategy, we have to think about the many datasets that the data strategy *could have* produced. Some of the datasets might be really excellent. For example, in good datasets, we achieve good covariate balance across the treatment and control groups. Or we might draw a sample whose distribution of observable characteristics looks really similar to the population. But some of the datasets might be worse: because of the vagaries of randomization, the particular realizations of the random assignment or random sampling might be more or less balanced. We do not have to settle for data strategies that might produce weak datasets -- we are in control of the procedures we choose. We want to choose a data strategy $D$ that is likely to result in a high-quality dataset $d$.

```{r, echo = FALSE}
# design <-
#   declare_population(N = 100) +
#   declare_potential_outcomes(D ~ Z) +
#   declare_potential_outcomes(Ystar ~ D, assignment_variables = D) +
#   declare_sampling(n = 99) +
#   declare_assignment() +
#   declare_measurement(Y = Ystar)

dag <- dagify(Y ~ Ystar + Q + R,
              R ~ S + U,
              Ystar ~ D + U,
              D ~ Z + U,
              Z ~ S)
nodes <-
  tibble(
    name = c("S", "R", "Z", "Q", "D", "Ystar", "Y", "U"),
    label = c("S", "R", "Z", "Q", "D", "Y<sup>*</sup>", "Y", "U"),
    annotation = c(
      "**Sampling**",
      "**Response**",
      "**Treatment assignment**",
      "**Measurement tool**",
      "**Treatment received**",
      "**Latent outcome**",
      "**Observed outcome**",
      "**Unobserved heterogeneity**"
    ),
    x = c(1, 3, 1, 5, 3, 4, 5, 2.5),
    y = c(4, 4, 1, 4, 1, 2.5, 2.5, 2.5),
    nudge_direction = c("N", "N", "S", "N", "S", "N", "S", "S"),
    answer_strategy = "uncontrolled"
  )

endnodes <-
  nodes %>%
  transmute(to = name, xend = x, yend = y)

ggdd_df <-
  dag %>%
  tidy_dagitty() %>%
  select(name, direction, to, circular) %>%
  as_tibble %>%
  mutate(
    data_strategy = case_when(
      name == "D" ~ "unmanipulated",
      name == "Q" ~ "measurement",
      name == "S" ~ "sampling",
      name == "Ystar" ~ "unmanipulated",
      name == "Z" ~ "assignment",
      name == "Y" ~ "unmanipulated",
      name == "U" ~ "unmanipulated",
      name == "R" ~ "unmanipulated"
    ),
    exclusion_restriction = "no"
  ) %>%
  # left_join(design_nodes, by = "name") %>%
  left_join(nodes, by = "name") %>%
  left_join(endnodes, by = "to") %>%
  left_join(nudges_df, by = c("x", "y", "nudge_direction")) %>%
  left_join(aes_df, by = c("data_strategy", "answer_strategy")) %>%
  mutate(shape_y = y + shape_nudge_y)

base_dag_plot %+% ggdd_df
```

We illustrate above the data strategy and its three elements: sampling, assignment, and measurement. Sampling is the procedure for selecting which units will be measured; assignment is the procedure for allocating treatments to sampled units; and measurement is the procedure for turning information about the sampled units into data. All empirical studies have a data strategy and every data strategy involves sampling, assignment, and measurement. Even studies in which researchers measure the full universe of cases involve sampling, since *future* units are not included. Even studies in which researchers do not apply treatments themselves involve assignment, since other forces end up assigning units to conditions. Even studies in which researchers take no new measurements involve measurement, since a choice about which measurements made by others to use must nevertheless be made.

In the figure, we rule out threats to inference that come from implementation: failure to treat (noncompliance), failure of inclusion in the sample (attrition), and causal relationships between random sampling and assignment as well as measurement on the latent outcome (excludability). In the last section of the chapter, we discuss when these threats are realized and how we can mitigate these risks they present by design.


## Sampling

Sampling is the process by which units are selected from the population to be studied. Some sampling procedures involve randomization while others do not. Sometimes -- perhaps even usually -- randomized sampling procedures break in ways that turn them into nonrandomized designs. Whether a sampling procedure is randomized or not has large implications for the answer stratey. Randomized designs support "design-based inference," which refers to the idea that we rely on known features of the sampling process when producing population-level estimates -- much more about this in the next chapter on Answer strategies. When randomization breaks down (e.g., if the design encounters attrition) or if a nonrandomized design is used, then we have to fall back on model-based inference to generalize from the sample to the population. Model-based inference relies on researcher beliefs about the nature of the uncontrolled sampling process in order to make inferences about the population. When possible, design-based inference is preferable, because it's easier to be correct about about the features of controlled processes than uncontrolled ones. That said, when randomly sampled individuals fail to respond or when we seek to make inferences about *new* populations, we must apply model-based inference even if the data are the result of a random process.

Why would we ever be content to study a sample and not the full population? The first and best explanation is cost: it's expensive and time-consuming to conduct a full census of the population. Even well-funded research projects face this problem, since money and effort spent answering one question could also be spent answering a second question. We tend to sample rather than measure every unit in the population because we face opportunity costs as well. A second reason reason to sample is the diminishing marginal returns of additional data collection. Increasing the number of sampled units from 1,000 to 2,000 will greatly increase the precision of our estimates. Moving from 100,000 to 101,000 will improve things too, but the scale of the improvement is much smaller.

### Randomized sampling designs

Owing to the natural appeal of design-based inference, we start off with randomized designs before proceeding to nonrandomized designs. Randomized sampling designs typically begin with a list of all units in a population, then choose a subset to sample using a random process. These random processes can be simple (every unit has an equal probability of inclusion) or complex (first we select regions at random, then villages at random within selected regions, then households within selected villages, then individuals within selected households).

This table describes XX common forms of sampling. The most basic form is simple random sample, also called "coin flip" or Bernoulli random sampling. Under simple random assignment, all units in the population have the same probability $p$ of being included in the sample. It is sometimes called coin flip random sampling because it is as though for each unit, we flip a weighted coin that has probability $p$ of landing heads-up. While quite straightforward, a drawback of simple random sampling is that we can't be sure of the number of sampled units in advance. On average, we'll sample $N*p$ units, sometimes slightly more units will be sampled and sometimes fewer.

Complete random sampling addresses this problem. Under complete random sampling, exactly $n$ of $N$ units are sampled. Each unit still has an inclusion probability of $p = n/N$, but in contrast to simple random sampling, we are guaranteed that the final sample will be of size $n$.^[To convince yourself of the difference between simple and complete random sampling, run `table(simple_rs(N = 100, prob = 0.5))` a few times and compare the results with `table(complete_rs(N = 100, n = 50))`]. Complete random sampling represents an improvement over simple random sampling because it rules out samples in which more or fewer than $N*p$ units are sampled. One circumstance in which we might nevertheless go with simple random sampling is when the size of the population is not known in advance and sampling choices have to be made "on the fly."

Complete random sampling solves the problem of fixing the total number of sampled units, but it doesn't address the problem that the total number of units with particular characteristics will not be fixed.  Imagine a population with $N_{y}$ young people and $N_{o}$ old people. If we sample exactly $n$ from the population $N_{y} + N_{o}$, the number of sampled young people ($n_y$) and sampled old people ($n_{o}$) will bounce around from sample to sample. We can solve this problem by conducting complete random sampling *within* each group of units. This procedure goes by the name stratified random sampling, since the sampling is conducted separately within the strata of units.^[To convince yourself of the difference between complete and stratified sampling, run `age <- rep(c("Y", "O"), 50); table(age, complete_rs(N = 100, n = 50))` a few times and compare the results with `table(age, strata_rs(strata = age))`] In our example, our strata were formed by a dichotomoous grouping of people into "young" and "old" categories, but in general, the sampling strata can be formed by any information we have about units before they are sampled. Stratification offers at least three major benefits. First, we defend against sampling surprisingly too few units in some stratum by "bad luck." Second (as discussed in the chapter on answer strategies) stratification tends to produce lower variance estimates of most inquiries. Finally, stratification allows researchers to "oversample"  subgroups of particular interest.

Stratified sampling should not be confused with cluster sampling. Stratified sampling means that a fixed number of units from a particular group are drawn into the sample. Cluster sampling means that units from a particular group are brought into the sample *together*. For example, if we cluster sample households, we interview all individuals living in a sampled household. Clustering introduces dependence in the sampling procedure -- if one member of the household is sampled, the other members are also always sampled. Relative to a complete random sample of the same size, cluster samples tend to produce higher variance estimates. Just as the individual sampling designs, cluster sampling comes in simple, complete, and stratified varieties with exactly parallel logics and motivations.

Lastly, we turn to multi-stage random sampling, in which we conduct random sampling at multiple levels of a heirarchically-structured population. For example, we might first sample regions, then villages within regions, then households within villages, then individuals within households. Each of those sampling steps might be stratified or clustered depending on researcher goals. The purpose of a multi-stage approach is typically to balance the logistical difficulties of visiting many geographic areas with the relative ease of collecting additional data once you have arrived.

Table XXX collects all of these kinds of random sampling together and offers an example of functions in the `randomizr` package you can use to conduct these kinds of sampling.

| Design | Description | Randomizr function |
|-----------|------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------|
| Simple random sampling      | "Coin flip" or Bernoulli random sampling. All units have the same inclusion probability p | simple_rs(N = 100, p = 0.25)      |
| Complete random sampling    | Exactly n of N units are sampled, and all units have the same inclusion probability n/N | complete_rs(N = 100, n = 40)      |
| Stratified random sampling  | Complete random sampling within pre-defined strata. Units within the same strata have the same inclusion probability n_s / N_s, but units in different strata might have different inclusion probabilities | strata_rs(strata = regions) |
| Cluster random sampling     | Whole groups of units are brought into the sample together. | cluster_ra(clusters = households) |
| Stratifed cluster sampling  | Cluster random sampling within strata| strata_and_cluster_rs(strata = regions,clusters = villages)|
| Multi-stage random sampling | First clusters, then units within clusters| cluster_ra(clusters = villages); strata_ra(strata = villages|


Figure XXX gives a graphical interpretation of each of these kinds of random sampling. Here, we imagine a population of 64 units with two levels of heirarchy. For concreteness, we can imagine that the units are indiviudals nested within 16 households of four people each and the 16 households are nested within four villages of four people each. Starting at the top left, we have simple random samping at the individual level. The inclusion probability was set to 0.5, so on average, we ought to sample 32 people, but in this particular draw, we actually sampled only 29. Complete random sampling (top center), fixes this problem, so exactly 32 people are sampled -- but these 32 are unevenly spread across the four villages. This is addressed with stratified sampling -- here we sample exactly 8 people at random from each village of 16 total people.

Moving down to the middle row of the figure, we have three approaches to clustered random sampling. Under simple sampling at the cluster, each cluster has the same probability $p$ of inclusion in the sample, so on average we will sample eight clusters. This time, we only sampled seven, this problem can again be fixed with complete random sampling (center facet), but again we have an uneven distribution across villages. Stratified cluster sampling ensures that exactly two households from each village are sampled.

The bottom row of the figure illustrates some approaches to multistage sampling. In the bottom left panel, we conduct a simple random sample of individuals in each sampled cluster. In the bottom center, we draw a complete random sample of indiviudals in each sampled household. And in the bottom left, we stratify on an individual level characteristic -- we always draw one individual from each row of the household. "Row" could refer to the age of the household members. This doubly-stratified multistage random sampling procedure ensures that we sample two households from each village and within those households, one older member and one younger member.  


```{r, fig.width = 6.5, fig.height = 7, echo = FALSE}
set.seed(348)
gg_df <- fabricate(
  villages = add_level(N = 4, village_num = 1:4 + 1:4 * 0.1),
  households = add_level(N = 4,
                         household_num = 1:4 + 1:4 * 0.1),
  individuals = add_level(
    N = 4,
    X = village_num + c(0.25,-0.25, 0.25,-0.25),
    Y = household_num + c(0.25, 0.25,-0.25,-0.25),
    a = simple_rs(N),
    b = complete_rs(N),
    c = strata_rs(strata = villages),
    d = cluster_rs(clusters = households, simple = TRUE),
    e = cluster_rs(clusters = households),
    f = strata_and_cluster_rs(clusters = households, strata = villages),
    g = d * simple_rs(N),
    h = e * strata_rs(strata = households),
    i = f * strata_rs(strata = paste0(households, Y))
  )) %>%
  pivot_longer(cols = letters[1:9], names_to = "procedure", values_to = "sampled") %>%
  mutate(procedure = factor(
    procedure,
    levels = letters[1:9],
    labels = c(
      "Individual Random Sampling (simple)",
      "Individual Random Sampling (complete)",
      "Individual Random Sampling (stratified)",
      "Cluster Random Sampling (simple)",
      "Cluster Random Sampling (complete)",
      "Cluster Random Sampling (stratified)",
      "Multistage Random Sampling (simple)",
      "Multistage Random Sampling (complete)",
      "Multistage Random Sampling (stratified)"
    )
  ),
  sampled = as.factor(sampled),
  unit = case_when(
    str_detect(procedure, "Individual") ~ "Individual",
    str_detect(procedure, "Cluster") ~ "Cluster",
    str_detect(procedure, "Multistage") ~ "Multistage"
  ),
  sampling_type = case_when(
    str_detect(procedure, "simple") ~ "Simple",
    str_detect(procedure, "complete") ~ "Complete",
    str_detect(procedure, "stratified") ~ "Stratified"
  ),
  unit = factor(unit, levels = c("Individual", "Cluster", "Multistage")),
  sampling_type = factor(sampling_type, levels = c("Simple", "Complete", "Stratified"))
  )

cluster_df <- gg_df %>%
  group_by(households, unit, sampling_type) %>%
  summarize(hh_sampled = any(sampled == 1), X = mean(X), Y = mean(Y)) %>%
  filter(hh_sampled == TRUE, unit != "Individual")

ggplot(gg_df, aes(X, Y)) +
  geom_tile(aes(fill = sampled), color = NA, width = 0.46, height = 0.46) +
  geom_tile(data = cluster_df, fill = NA, color = gray(0.6), size = 0.25, width = 1.03, height = 1.03) +
  coord_fixed() +
  facet_grid(unit  ~ sampling_type, switch = "y") +
  dd_theme() +
  scale_fill_manual(values = c(gray(0.95), dd_light_blue)) +
  scale_x_continuous(name = "Stratum (e.g., locality)", breaks = 1:4 + 1:4 * 0.1, labels = LETTERS[1:4]) +
  theme(legend.position = "none",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank())

#gg_df %>% group_by(unit, sampling_type) %>% summarise(n = n(), sampled = sum(sampled == 1))
```



<!-- There are many ways to draw a random sample, but two are particularly common in the social sciences: simple and stratified. Simple random sampling is sampling without replacement with a fixed sample size. Stratified random sampling takes a simple random sample of a fixed size from two or more subgroups, or strata, in the data. There are two different reasons you might select stratified random sampling over simple: your inquiry might involve comparisons between two subgroups, and you want to ensure you have a sufficient size in each subgroup; and there may be very different amounts of variability in two groups, and you want the most efficient estimate of a summary of that data so you oversample from the group with the higher variance to get a similar amount of precision from each group. Stratified sampling is a simple case of a larger class of sampling strategies, in which the probability of inclusion in the sample varies across units. In stratified sampling with two strata, the probability of inclusion is set for the two groups and is fixed within them. However, the probability can vary within strata as well. If you wish to have you sample include all age groups but upweight older people, you could make the probability of inclusion a function of age: the probability of sampling an 18 year old could be 0.01 ranging up to a probability of 0.1 for 80 year olds. -->

<!-- When it is impossible to get a list of all units in the target population either because it does not exist or is too costly to obtain, clustered sampling is often a substitute. In cluster sampling, you obtain a list of clusters of units, randomly sample from the list of clusters, and either collect data from all units within selected clusters, or conduct a simple random sample of units within sampled cluster (this would represent a simple form of multistage sampling). The advantage is you can still obtain a random sample of units, but it is feasible to do so because you have a list of clusters (but not of all units). The disadvantage is that units within clusters often have similar outcomes, so you don't get as much benefit from sampling more clusters as you do from sampling units from other clusters that are more different. This efficiency tradeoff -- how many clusters to choose vs. how many units within clusters -- can be explored in design diagnosis by comparing the RMSE of various choices. There are many other forms of multistage sampling -- sample provinces then sample districts within provinces, then villages within districts, and people within villages -- that fit particular aims and budgets. -->


### Nonrandomized sampling designs

Because nonrandomized sampling procedures are defined by what they don't do -- they don't use randomization -- a hugely varied set of procedures could be described this way. We'll consider a just a few common ones, since the idiosyncracies of each approach are hard to systemetize.

Convenience sampling refers to the practice of gathering units from the population in the least expensive way available to you. Convenience sampling is a good choice when generalizing to an explicit population is not a main goal of the design, for example when a sample average treatment effect is a theoretically-important inquiry. For many decades, social science undergraduates were the most abundant data source available to academics and many important theoretical claims have been established on the basis of experiments conducted with such samples. In recent years, however, online convenience samples like Mechanical Turk, Prolific, or Lucid have mostly supplanted undergraduates as the convenience sample of choice. In other (mostly nonexperimental) circumstances, however, convenience sampling is likely to lead to badly biased estimates. For example, cable news shows often conduct viewer polls that should not be taken at all seriously. While such polls might promote viewer loyalty (and so might be worth doing from the cable executives' perspective) they do not provide credible evidence about what the population at large thinks or believes.

Many types of qualitative and quantitative research involve convenience sampling. Archival research often involves a convenience sample of documents on a certain topic that exist in an archive. The question of how these documents differ from those that would be in a different archive, or how the documents available in archives differ from those that do not ever make it into the archive importantly shapes what we can learn from them (Aliza Luft paper cite). With the decline of telephone survey response rates (cite), researchers can no longer rely on random digit dialing to obtain a representative sample of people in many countries, and instead must rely on convenience samples from the internet or panels who agree to have their phone numbers in a list. Sometimes, reweighting techniques in the answer strategy can, in some cases, help recover estimates for the population as a whole if sampling if a credible model of the unknown sampling process can be agreed upon.

Next, we consider purposive sampling. Purposive is a catch-all term for rule-based sampling strategies that do not involve random draws but also are not purely based on convenience and cost. A common example is quota sampling. Sampling purely based on convenience often means we will end up with many units of one type but very few of another type. Quota sampling addresses the problem by continuing to search for subjects until target counts (quotas) of each find of subject are found. Loosely speaking, quota sampling is to convenience sampling as stratified random sampling is to complete random sampling: it fixes the problem that not enough (or too many) subjects of particular types are sampled by employing specific quotas. Importantly, however, we have no guarantee that the sampled units *within* type are representative of that type over all: quota samples are within-stratum convenience sampes.

A second common form of purposive sampling is respondent-driven sampling, which is used to sample from hard-to-reach populations such as HIV-positive needle users. RDS methods often begin with a convenience sample and then systematically obtain contacts for other units who share the same characteristic in order the build a large sample.

Each of these three nonrandom sampling procedures -- convenience, quota, and respondent-driven -- are illustrated in Figure XXX. Imagining that village A is easier to reach, we could obtain a convenience sample by contacting everyone we can reach in village A before moving on to village B. This process doesn't yield good coverage across villages and for that we can turn to quota sampling. Under this quota sampling scheme, we talk to the five people who are easiest to reach in each of the four villages. Finally, if we conduct a respondent-driven sample, we select one seed unit in each village, and that person recruits their four closest friends (who may or may not reside in the same village).


```{r, fig.width = 6.5, fig.height = 2.5, echo = FALSE}

# convenience
# quota (strata)
# RDS


convenience <-
c(1, 1, 1, 1, 1, 1, 1, 1,
  1, 1, 1, 1, 1, 0, 0, 0,
  1, 1, 1, 0, 0, 0, 0, 0,
  0, 0, 0, 0, 0, 0, 0, 0,
  0, 0, 0, 0, 0, 0, 0, 0,
  0, 0, 0, 0, 0, 0, 0, 0,
  0, 0, 0, 0, 0, 0, 0, 0,
  0, 0, 0, 0, 0, 0, 0, 0)

quota <-
c(1, 1, 1, 1, 1, 0, 0, 0,
  0, 0, 0, 0, 0, 0, 0, 0,
  0, 0, 1, 1, 1, 1, 1, 0,
  0, 0, 0, 0, 0, 0, 0, 0,
  1, 1, 1, 0, 1, 1, 0, 0,
  0, 0, 0, 0, 0, 0, 0, 0,
  0, 0, 0, 1, 1, 0, 0, 0,
  0, 0, 0, 0, 1, 1, 1, 0)

rds <-
c(0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
  0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
  1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
  1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
  1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
  0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
  1, 1, 1, 0)


gg_df <- fabricate(
  villages = add_level(N = 4, village_num = 1:4 + 1:4 * 0.1),
  households = add_level(N = 4,
                         household_num = 1:4 + 1:4 * 0.1),
  individuals = add_level(
    N = 4,
    X = village_num + c(-0.25,0.25, -0.25,0.25),
    Y = household_num + c(-0.25, -0.25,0.25,0.25),
    a = convenience,
    b = quota,
    c = rds
  )) %>%
  pivot_longer(cols = letters[1:3], names_to = "procedure", values_to = "sampled") %>%
  mutate(procedure = factor(
    procedure,
    levels = letters[1:3],
    labels = c(
      "Convenience",
      "Quota",
      "Respondent-driven"
    )
  ),
  sampled = as.factor(sampled)
  )


ggplot(gg_df, aes(X, Y)) +
  geom_tile(aes(fill = sampled), color = NA, width = 0.46, height = 0.46) +
  # geom_text(aes(label = individuals)) +
  coord_fixed() +
  facet_wrap(~procedure) +
  dd_theme() +
  scale_fill_manual(values = c(gray(0.95), dd_light_blue)) +
  scale_x_continuous(name = "Stratum (e.g., locality)", breaks = 1:4 + 1:4 * 0.1, labels = LETTERS[1:4]) +
  theme(legend.position = "none",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank())
```



### Sampling designs for qualitative research

Case selection is another term for a sampling strategy. In case study research, whether qualitative or quantitative, the way you select the (typically small) set of cases is of great importance, and considerable attention has been paid to developing purposive case selection methods. Mills (cite) method of difference is a common case selection procedure, in which cases are selected that have different outcomes and the cases are inspected to identify characteristics that differ that may have led to the divergence. Lieberman (2005) proposes a method for selecting cases for indepth study from an initial regression study: cases on the regression line, well-predicted by it, are selected to determine whether the hypothesized causal relationship took place in those cases. Off-the-line cases, that are not well-predicted, are also selected to determine what factors may be left out of the regression model that provide alternative mechanisms. In each method, the inferential goal determines how cases are selected. Research design diagnosis can also help compare the efficiency and bias of these methods to select cases.

Selection on the dependent variable is purposive sampling method that has recieved substantial attention. The procedure is to select positive outcomes --- units in which an event such as a revolution took place, for example --- and study what led to that outcome. Geddes (XXXX) lays out the problem in this method for case study research: we cannot know whether the factors identified from investigating these cases led to the event, because we do not know whether those factors are also present in units where the outcome did not happen. If the factor is present in both, it is unlikely to have caused the positive outcome. The same problem plagues observational quantitative research. Ramsay et al. (XXXX) discuss the bias that results from studying the factors predicting suicide terrorism by only looking at observations where suicide terrorism occurred. In order to avoid these problems accidentally creeping into our research, it is important to diagnose the bias of our designs and not just the efficiency.


### Choosing among sampling designs

In short, the choice of sampling strategy depends on features of the model and the inquiry, and different sampling strategies can be compared in terms of power and RMSE in design diagnosis. The model defines the population of units we are interested in making inferences about, and the target population of the sampling strategy should match that as much as possible. The model also points us to important subgroups (defined by nodes or endogenous variables) that we may wish to stratify on, depending on the variability within those subgroups. Whether we select convenience, random, or purposive sampling depends on our budget and logistical constraints as well as the efficiency (power or RMSE) of the design. If there is little bias from convenience sampling, we will often want to select it for cost reasons; if we cannot obtain a convenience sample that has the right composition, we may choose a purposive method that ensures we do. The choice between simple and stratified sampling comes down to the inquiry and to a diagnosis of the RMSE: when the inquiry involves a comparison of subgroups, we will often select stratified sampling. In either, a diagnosis of alternative designs in terms of power or RMSE will guide selection.

Sampling, like all data strategies, is an intervention in the world by researchers. As a result, it may have independent causal effects on outcomes. As we will see in the next section, there is no deep difference between sampling and treatment assignment, because in each case we are randomly sampling from two potential outcomes. In treatment assignment, we are assigning units to a treatment group and a control group, and obtaining a sample of the treated potential outcome and a second random sample of the control potential outcome. In random sampling, we are assigning units to be sampled or to not be sampled. We obtain one sample of the sampled potential outcome. However, the non-sampled potential outcome exists conceptually, and may differ from the sampled potential outcome. The act of including units in the sample may change outcomes, for example if you are selected to participate in a medical trial you believe you are going to receive better care and a placebo effect changes your health condition. A research design in which you conduct an experiment on sampled units, but also unobtrusively measure outcomes in nonsampled units could be used to estimate the effect of inclusion in the sample. This design would provide a random sample of the nonsampled potential outcome.

we're often interested in extrapolating from our sample to the population the sample was drawn from, or to new populations. there are design-based inference solutions for making inferences about the population you directly sampled from, but model-based inference will be required to extrapolate to new populations and the same population in another time period.

<!-- you create strata and clusters from real things in the world -->

### Example: @wang2015forecasting

Xbox forecasting of elections

<!-- what is scraping? -->
<!-- what is going to the archive? -->
<!-- sampling on the DV -->

<!-- redefine your inquiry -->

<!-- ALSO is your inquiry really the right one - difference between target population and real population (Americans in 2016 vs Americans every year) -->

<!-- -- sample all of the population or part of it -->
<!-- -- random sampling, purposive sampling -->

<!-- - simple, complete, stratified, clustered, stratified and clustered -->
<!-- - weighted sampling (over/undersampling) -->
<!-- - finding missing populations (RDS) -->
<!-- - quota sampling -->
<!-- - hawthorne effects -->
<!-- - the inquiry and the model guide your choice of which units to sample and how to sample them -->

## Treatment assignment

Treatment assignment is closely related to sampling. A sampling strategy assigns units in a population either to be included in the sample or excluded. A simple treatment assignment strategy assigns units in the sample to one of two conditions, treatment or control. Both assign units from a group into one of two groups. The difference is only in the fact that we do not measure outcomes for nonsampled units (though we could, to learn about how nonsampled units differ from sampled units). The mechanics and statistics of sampling and simple treatment assignments are identical but for this difference. Assignment procedures may assign units into more than two treatment conditions. In a factorial design, two treatments are assigned to a set of units, and a unit can be assigned to neither treatment (this is a "pure control"), just one, the other, or both. In this case, there are four groups a unit could be assigned to.

- simple, complete, blocked, clustered, blocked and clustered
- point restricted randomization
- no assignment procedure at all
- multiple arms
- adaptive
- the inquiry and the model guide your choices of which treatments to assign and how to assign them

```{r, fig.width = 6.5, fig.height = 7, echo = FALSE}
set.seed(343)
gg_df <- fabricate(
  villages = add_level(N = 4, village_num = 1:4 + 1:4 * 0.1),
  households = add_level(N = 4,
                         household_num = 1:4 + 1:4 * 0.1),
  individuals = add_level(
    N = 4,
    X = village_num + c(0.25,-0.25, 0.25,-0.25),
    Y = household_num + c(0.25, 0.25,-0.25,-0.25),
    a = simple_ra(N),
    b = complete_ra(N),
    c = block_ra(blocks = villages),
    d = cluster_ra(clusters = households, simple = TRUE),
    e = cluster_ra(clusters = households),
    f = block_and_cluster_ra(clusters = households, blocks = villages),
    g = d * simple_ra(N),
    h = e * block_ra(blocks = households),
    i = f * block_ra(blocks = paste0(households, Y))
  )) %>%
  pivot_longer(cols = letters[1:9], names_to = "procedure", values_to = "sampled") %>%
  mutate(procedure = factor(
    procedure,
    levels = letters[1:9],
    labels = c(
      "Individual Random Assignment (simple)",
      "Individual Random Assignment (complete)",
      "Individual Random Assignment (blocked)",
      "Cluster Random Assignment (simple)",
      "Cluster Random Assignment (complete)",
      "Cluster Random Assignment (blocked)",
      "Saturation Design (simple)",
      "Saturation Design (complete)",
      "Saturation Design (blocked)"
    )
  ),
  unit = case_when(
    str_detect(procedure, "Individual") ~ "Individual",
    str_detect(procedure, "Cluster") ~ "Cluster",
    str_detect(procedure, "Saturation") ~ "Saturation"
  ),
  sampling_type = case_when(
    str_detect(procedure, "simple") ~ "Simple",
    str_detect(procedure, "complete") ~ "Complete",
    str_detect(procedure, "blocked") ~ "Blocked"
  ),
  unit = factor(unit, levels = c("Individual", "Cluster", "Saturation")),
  sampling_type = factor(sampling_type, levels = c("Simple", "Complete", "Blocked"))
  )

gg_df <-
  gg_df %>%
  group_by(households, unit, sampling_type) %>%
  mutate(treated_cluster = any(sampled == 1),
         condition = case_when(
           sampled == 1 & unit == "Individual" ~ "T",
           sampled == 0 & unit == "Individual" ~ "C",
           sampled == 1 & unit == "Cluster" ~ "T",
           sampled == 0 & unit == "Cluster" ~ "C",
           sampled == 1 & unit == "Saturation" ~ "T",
           sampled == 0 & unit == "Saturation" & treated_cluster  ~ "Sp",
           sampled == 0 & unit == "Saturation" & !treated_cluster  ~ "C"
         ))

cluster_df <- gg_df %>%
  group_by(households, unit, sampling_type) %>%
  summarize(hh_sampled = any(sampled == 1), X = mean(X), Y = mean(Y)) %>%
  filter(hh_sampled == TRUE, unit != "Individual")

ggplot(gg_df, aes(X, Y)) +
  geom_tile(aes(fill = condition), color = NA, width = 0.46, height = 0.46) +
  geom_tile(data = cluster_df, fill = NA, color = gray(0.6), size = 0.25, width = 1.03, height = 1.03) +
  geom_text(aes(label = condition), size = 2, color = "white") +
  coord_fixed() +
  facet_grid(unit  ~ sampling_type, switch = "y") +
  dd_theme() +
  scale_fill_manual(values = c(dd_light_gray, "#72B4F366", dd_light_blue)) +
  scale_x_continuous(name = "Block (e.g., locality)", breaks = 1:4 + 1:4 * 0.1, labels = LETTERS[1:4]) +
  theme(legend.position = "none",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank())
```


```{r, fig.width = 6.5, fig.height = 2.5, echo = FALSE}
set.seed(343)
gg_df <-
  fabricate(
  villages = add_level(N = 4, village_num = 1:4 + 1:4 * 0.1),
  households = add_level(N = 4, household_num = 1:4 + 1:4 * 0.1),
  individuals = add_level(
    N = 4,
    X = village_num + c(0.25, -0.25, 0.25, -0.25),
    Y = household_num + c(0.25, 0.25, -0.25, -0.25),
    a = block_ra(households, conditions = c("C", "T1", "T2")),
    b = block_ra(households, conditions = c("C", "T1", "T2", "T1\nT2")),
    c = if_else(b == "T1\nT2", "T3", as.character(b))
    # c = block_ra(households, conditions = c("C", "T1", "T2", "T3"))
  )
) %>%
  pivot_longer(cols = letters[1:3],
               names_to = "procedure",
               values_to = "assignment") %>%
  mutate(procedure = factor(
    procedure,
    levels = letters[1:3],
    labels = c("Three-arm", "Factorial", "Four-arm")
  ))

sort(unique(gg_df$assignment))

gg_df %>%
  ggplot(aes(X, Y)) +
  geom_tile(aes(fill = assignment), width = 0.46, height = 0.46) +
  geom_text(aes(label = assignment), size = 2, color = "white") +
  coord_fixed() +
  facet_grid(~ procedure, switch = "y") +
  dd_theme() +
  scale_fill_manual(values = c(dd_light_gray, dd_pink, dd_purple , dd_light_blue, dd_orange)) +
  # scale_fill_manual(values = c("#72B4F333", dd_orange, dd_light_blue)) +
  # scale_x_continuous(name = "Block (e.g., locality)", breaks = 1:4 + 1:4 * 0.1, labels = LETTERS[1:4]) +
  theme(legend.position = "none",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.title = element_blank(),
        axis.text = element_blank())
```

```{r, fig.width = 6.5, fig.height = 2.5, echo = FALSE}
set.seed(343)
gg_df <- fabricate(
  villages = add_level(N = 4, village_num = 1:4 + 1:4 * 0.1),
  households = add_level(N = 4,
                         household_num = 1:4 + 1:4 * 0.1),
  individuals = add_level(
    N = 4,
    X = village_num + c(0.25,-0.25, 0.25,-0.25),
    Y = household_num + c(0.25, 0.25,-0.25,-0.25),
    assignment = block_ra(blocks = households, conditions = c("P1", "P2", "P3")),
    assignment1 = assignment == "P1",
    assignment2 = assignment %in% c("P1", "P2"),
    assignment3 = 1
  )) %>%
  select(-assignment) %>%
  pivot_longer(cols = contains("assignment"), names_to = "procedure", values_to = "assignment") %>%
  mutate(procedure = factor(procedure, levels = paste0("assignment", 1:3), labels = c("Period 1", "Period 2", "Period 3")),
         assignment = as.factor(assignment))

gg_df %>%
  ggplot(aes(X, Y)) +
  geom_tile(aes(fill = assignment), color = NA, width = 0.46, height = 0.46) +
  # geom_text(aes(label = assignment), size = 2.5) +
  coord_fixed() +
  facet_grid(~ procedure, switch = "y") +
  dd_theme() +
  scale_fill_manual(values = c(dd_light_gray, dd_light_blue)) +
  # scale_x_continuous(name = "Block (e.g., locality)", breaks = 1:4 + 1:4 * 0.1, labels = LETTERS[1:4]) +
  theme(legend.position = "none",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.title = element_blank(),
        axis.text = element_blank())
```


### Example: sociology natural experiment from government or official cutoff


## Measurement

Measurement is the part of the data strategy in which variables are collected about the population of units to enable sampling, variables are collected about the sample before treatment assignment including those used in treatment assignment, and outcomes are collected after treatment assignment. All variables used in the answer strategy are collected in measurement, aside from the treatment assignment variable and assignment and sample inclusion probabilities.

The fundamental problem of description is that we can never measure the latent variables we are interested in, $Y^*$, such as fear, support for a political candidate, and economic wellbeing. Instead, we use a measurement technology to imperfectly observe them, which we represent as the function $Q$ that yield the observed outcome $Y^{\mathrm obs}$: $Q(Y^*) = Y^{\mathrm obs}$. Our measurement strategy is a set of functions $Q$ for each variable we measure.

There are two basic ways to assess the quality of each function $Q$: bias, or the difference between the observed and latent outcome, $Y^{\mathrm obs} - Y^*$, which is given the special label *measurement validity*; and *measurement reliability*, which is the variance across multiple outcomes for a given individual, $\V(Y_1^{\mathrm obs}, Y_2^{\mathrm obs}, Y_3^{\mathrm obs})$.

Researchers select several characteristics of $Q$: who collects the measures; the mode of measurement; how often and when measures are taken; how many different observed measures of $Y^*$ are collected and how they are summarized into a single measure; and what information is provided to participants about how they are being measured (if any). These design characteristics may affect validity, reliability, or both.

Data may be collected by the researcher themselves, by the participant, or by a third party. In some forms of qualitative research such as participant-observation and interview-based research, the researcher may be the primary data collector. In survey research, the interviewer is typically a hired agent of the researcher, and in many cases multiple inteviewers are hired. These interviewers may ask questions differently, leading to less reliable (more variable) answers and in some cases validity problems when they ask questions in a way that leads to biased measures of $Y^*$. Participants themselves are often asked to collect data on themselves, either through self-administered surveys, journaling, or taking measurements of themselves using thermometers or scales. A primary concern with self-reports is validity: do respondents report their measurements truthfully. A parallel concern is raised when participants do not collect their own data but are made aware of the fact that they are being measured by others. Finally, data may be collected by agents of government or other organizations, yielding so-called administrative data. The difference between administrative data and other forms of data is only in the identity of the data collector.

Most of the variety in measurement strategies is how those data collectors obtain their data. Humans can code data by observation through the five senses of sight, hearing, touch, smell, and taste, and by asking other humans for self-reports about themselves in surveys. Measurement instruments can also be used to record waves of light (e.g., photos), sound (e.g., audio and seismic recordings), electromagnetism (e.g., EKGs and x-rays), and combinations of more than one (e.g., video); characteristics of the atmosphere (e.g., temperature and pressure), the water (e.g., salinity and pollution), and the soil (i.e., mercury pollution); and human and animal health (e.g., blood tests). Considerable recent progress has been made in taking advantage of all of these measurement modes due to increasing computing power and machine learning techniques that can code streams of raw data from photos, videos, and these other sources and translate them into usable data. The translation of raw data into coded data that can be used for analysis is part of $Q$ in the measurement strategy.

When data are collected, and how often, can also affect validity and reliability. The inquiry should guide when data is collected in relation to other events such as an election or the holiday period or the time after a treatment is delivered to research participants. The inquiry defines whether the effect of interest is a month after treatment or in the case of longterm effects a year or more. However, data need not be collected at a single time period. The model encodes beliefs about the autocorrelation (correlation over time) of data that varies over time, and this can help guide whether to collect multiple measurements or just one and whether to measure data at baseline as well as endline. If data are expected to be highly variable (low autocorrelation), then taking multiple measurements and averaging them may provide efficiency gains. When they exhibit high autocorrelation, then multiple measures will waste resources --- approximately the same measure will be returned each time. However, under high autocorrelation there will be large gains from collecting a baseline measure before a treatment in an experiment, because controlling for baseline outcomes the only major change will be the response to treatment. Thus, the estimates of treatment effects will be much more efficient (more T cite). Beliefs about autocorrelation can guide decisions about the tradeoff between including more participants or more measures of a smaller set of participants.

Parallel considerations apply to the choice to measure the same latent $Y_i^*$ with multiple measurement tools in the same time period. When possible, taking multiple, different measures and averaging will typically yield efficiency improvements. (In some cases, there may be a tradeoff with a budget constraint in terms of survey length or the number of subjects.) The multiple measures can then be combined to construct a single measure of $Y_i^*$. When the tools produce answers that are highly correlated, taking multiple measures is unlikely to be worth the cost but when the correlation is low it will be worth taking multiple measurements and averaging to improve efficiency.

Beyond what and by whom, two important features of any data collection strategy are when and how often. In experiments, data is often collected after treatment is delivered, and sometimes before at baseline to enable block randomization and before-after comparisons. In difference-in-difference studies, data is always collected before and after, and to buttress claims of parallel trends between treated and control often for multiple periods before treatment. In process tracing studies, data is typically collected at many periods in between a change in the independent variable and a change in the dependent variable to demonstrate the connection between the two. How often data are collected after the treatment (or change in the independent variable) influences reliability: measuring highly variable outcomes multiple times and averaging can increase measurement reliability. The flip side of this is when variables are fast to change, taking measurements at baseline to compare to endline measurements is not likely to increase reliability. When outcomes change slowly, however, baseline measurements are likely to substantially improve the efficiency of estimates of treatment effects (for further discussion, see cite more T).

Perhaps the most common pitfall of measurement is the sensitivity bias that affects measures self-reported by participants in the presence of social pressure or the risk of sanction from authorities who prefer to hide or wish to detect sensitive characteristics in the population (BCM2020). Sensitivity bias represents an excludability violation: the latent outcome $Y_i^*$ depends on which measurement tool is used. More generally, excludability violations of measurement imply that the very act of measurement affects the latent outcome, often known as a Hawthorne effect. Typically, we want to avoid these excludability violations, because we are trying to measure a latent quantity that exists independent of how it is measured.

Selecting among measurement modes, data collectors, time periods, frequency, and the number of measurements reduces to tradeoffs between their validity and reliability. We want the largest set of valid, measures to combine into a measurement of $Y_i^*$ that meet our budget and logistical constraints. However, some measurement techniques will vary in both validity and reliability, so often we must decide on a weighting of the two concerns given our research goals. In some cases, validity will be important even if the measures are unreliable, but in others we may want to weight them approximately equally.

Learning which measurement tools are valid and reliable is ultimately guesswork, though it can be informed guesswork. We cannot measure the true $Y_i^*$, so we cannot truly "validate" any measurement technique. Often studies present themselves as validation studies by comparing a proposed measure to a "ground truth," measured from administrative data or a second technique to reduce measurement error. However, neither measurement is known to be exactly  $Y_i^*$, so ultimately these studies are comparisons of multiple techniques each with their own advantages and disadvantages. This does not make them useless, but rather should be used to understand the how measurements differ on average and in variability. They may also be useful for informing how to combine multiple measures.

inattention

differential measurement
parallel measurement across treatment conditions
measurement bias subtracted off from T-C comparison -- but what are you really measuring (identification is saved yes, but what does it identify)

<!-- - issues with rates - y^* / x^*, then rates are messed up -->

### Example: @weaver2019too

## Threats to implementation

```{r, echo = FALSE}
# design <-
#   declare_population(N = 100) +
#   declare_potential_outcomes(D ~ Z) +
#   declare_potential_outcomes(Ystar ~ D, assignment_variables = D) +
#   declare_sampling(n = 99) +
#   declare_assignment() +
#   declare_measurement(Y = Ystar)

dag <- dagify(Y ~ Ystar + Q + R,
              R ~ S + U + Z,
              Ystar ~ D + U + S + Z + Q,
              D ~ Z + U,
              Z ~ S)
nodes <-
  tibble(
    name = c("S", "R", "Z", "Q", "D", "Ystar", "Y", "U"),
    label = c("S", "R", "Z", "Q", "D", "Y<sup>*</sup>", "Y", "U"),
    annotation = c(
      "**Sampling**",
      "**Response**",
      "**Treatment assignment**",
      "**Measurement tool**",
      "**Treatment received**",
      "**Latent outcome**",
      "**Observed outcome**",
      "**Unobserved heterogeneity**"
    ),
    x = c(1, 3, 1, 5, 3, 4, 5, 2.5),
    y = c(4, 4, 1, 4, 1, 2.5, 2.5, 2.5),
    nudge_direction = c("N", "N", "S", "N", "S", "N", "S", "S"),
    answer_strategy = "uncontrolled"
  )

endnodes <-
  nodes %>%
  transmute(to = name, xend = x, yend = y)

ggdd_df <-
  dag %>%
  tidy_dagitty() %>%
  select(name, direction, to, circular) %>%
  as_tibble %>%
  mutate(
    data_strategy = case_when(
      name == "D" ~ "unmanipulated",
      name == "Q" ~ "measurement",
      name == "S" ~ "sampling",
      name == "Ystar" ~ "unmanipulated",
      name == "Z" ~ "assignment",
      name == "Y" ~ "unmanipulated",
      name == "U" ~ "unmanipulated",
      name == "R" ~ "unmanipulated"
    ),
    exclusion_restriction = case_when(
      name %in% c("S", "Z", "Q") & to == "Ystar" ~ "yes",
      name %in% c("Z") & to == "R" ~ "yes",
      TRUE ~ "no"
    )
  ) %>%
  # left_join(design_nodes, by = "name") %>%
  left_join(nodes, by = "name") %>%
  left_join(endnodes, by = "to") %>%
  left_join(nudges_df, by = c("x", "y", "nudge_direction")) %>%
  left_join(aes_df, by = c("data_strategy", "answer_strategy")) %>%
  mutate(shape_y = y + shape_nudge_y)

base_dag_plot %+% ggdd_df
```

Data strategies are plans for how we will sample units, assign treatments, and measure outcomes. But our studies do not always go according to plan. In this section, we explore the threats to our inferences that emerge while implementing data strategies. Anticipating and planning for these threats presents two benefits: we can assess the properties of the designs we will actually run, obtaining more realistic guesses of the power and bias of our study given problems like attrition and noncompliance; and we can incorporate procedures to mitigate the risk of these threats as part of our designs.

Above, we adapt the figure first present in the chapter's introduction to introduce threats that come from noncompliance (failure to treat), attrition (failure to be included in the sample or provide measures), and excludability violations (causal effects of random sampling, random assignment, or measurement on the latent outcome).

### Noncompliance

The first type of threat during implementation is noncompliance: when random assignment $Z$ imperfectly manipulates the treatment variable $D$. In the absence of noncompliance, $D_i = Z_i$. One-sided noncompliance is the setting in which some treated units fail to be treated (and receive the control condition instead). Two-sided noncompliance is when some units fail to be treated, but in addition some units assigned to the control group receive the treatment. 

In randomized experiments, noncompliance may happen due to administrative error, miscommunications between researcher and partners, shortages of materials or staff, transportation problems, participants refusing treatment, and the inability of researchers to find the participant to treat, among other problems. 

Noncompliance also affects observational designs for causal inference in which nature or a non-random administrative process affects treatment such as a threshold cut-off. In instrumental variables designs, noncompliance is at the core: the instrument $Z$ causally affects treatment $D$, but other factors also affect $D$. In a regression discontinuity design, a threshold determines treatment status such that above a certain value of a scale the unit is treated and below it is not. But noncompliance can also occur, leading to fuzzy regression discontinuity designs, in which the threshold is imperfect and some units below receive treatment and some units above do not.

Under two-sided noncompliance, there are four types of participants, determined by the potential outcomes $D$ as a function of which condition the unit is randomly assigned two. For a two-arm trial, there are two treatment potential outcomes: whether you would receive the treatment if you are assigned to control ($D_i(Z_i = 0)$) and whether you would receive the treatment if assigned to treatment ($D_i(Z_i = 1)$). The four types are enumerated and labeled in the table below.

| Type          | $D_i(Z_i = 0)$   | $D_i(Z_i = 1)$   |
|---------------|------------------|------------------|
| Never-taker   | 0                | 0                |
| Complier      | 0                | 1                |
| Defier        | 1                | 0                |
| Always-taker  | 1                | 1                |

Never-takers are those who never take the treatment, no matter what treatment they are assigned; compliers take exactly the treatment they are assigned; defiers take exactly the opposite condition they are assigned; and always-takers receive the treatment regardless of whether the are assigned to. In one-sided noncompliance, there are only two types of participants: never-takers and compliers. 

In the presence of noncompliance, a change in inquiry is inevitable. The average treatment effect cannot be estimated by comparing those assigned to treatment and those assigned to control, because the assignment differs from whether treatment was received. The average difference between those assigned to the two conditions can be relabeled the intent-to-treat effect. Comparing those who *received* treatment to those that did not is also not an option, without additional herculean assumptions, because unobserved heterogeneity now jointly affects $D$ with $Z$. The randomized experiment is broken: the treatment group is no longer comparable to the control group in expectation. Instead, a complier average treatment may be obtained using instrumental variables estimation, which implies switching to a local inquiry among complier types. This effect may differ from the average treatment effect if compliers differ systematically from other types. Estimating the complier average treatment effect requires the addition of assumptions on top of those for randomized experiments, including the ignorability of treatment assignment and, in the case of two-sided noncompliance, a monotonicity assumption that rules out defiers. 

In the case of randomized experiments, spending budget and time to carefully design the treatment delivery protocols to avoid noncompliance will help avoid or minimize the threat from noncompliance.

A parallel set of decisions faces the designer of an observational study with noncompliance in treatments. Instrumental variables designs imply there is noncompliance and the inquiry is the complier average treatment effect (in some cases, the intent-to-treat effect or reduced form effect is also of interest). Researchers who adopt regression discontinuity designs also focus on a local effect among units near the threshold, and in the case of the fuzzy regression discontinuity design with noncompliance must switch to a complier local average treatment effect. 

Compliance need not be binary: if you are assigned to treatment, you may receive partial treatment not all or none of it. However, it is typically difficult to measure partial compliance, and more difficult still to account for this possibility in analysis (we cannot separate the effects of partial compliance from full compliance without further treatments that attempt to manipulate treatment more and less strongly).

In multi-arm trials or with continuous rather than binary instruments, noncompliance becomes a more complex problem to define and address through the data strategy and answer strategy. We must define complier types according to all of the (potentially-infinite) possible treatment conditions. For multiarm trials, the complier types for the first treatment may not be the same for the second treatment; in other words, units will comply at different rates to different treatments. Apparent differences in complier average treatment effects and intent-to-treat effects, as a result, may reflect not differences in treatment effects but different rates of compliance.

### Attrition

Attrition is the special form of noncompliance to sampling and measurement, when we do not have measures for all units who are sampled. There are two types of missing data that may result: when a single measure is missing, commonly known as item nonresponse; and when all measures are missing for a participant, known as survey nonresponse. Though these terms were coined by survey researchers, the problems are identical when not collected by surveys but by passive measurement or by instrumentation. Item nonresponse may result from many causes, but often because respondents do not wish to answer a specific questions because they are sensitive or intrusive. Survey nonresponse in which no questions are answered may be instead due to lack of interest in the measurement as presented, inability to contact to a subject, or an inability of the data collector to build rapport and gain the trust of the participant.

Attrition occurs when at least one unit does not respond to at least one measurement item. Whether attrition is a problem depends on whether a participant response $R$ is causally affected by variables other than sampling or is completely random. Though missingness completely at random is rare, it is possible, often due to administrative or computer error. If attrition is completely at random, there is no effect of any variable on $R$, and there is a loss of sample size but no distortion to estimates.

For descriptive inquiries, if attrition is not completely at random, then the types of units who respond are different in some way from those that do not respond. Model-based inference is required to adjust estimates to match the characteristics of the original sample, upweighting types of respondents who were less likely to respond and downweighting those that were more likely to respond. However, if we guess the wrong set of variables to reweight in our model $M$, then we will miss the target.

For causal inquiries, we also need to know whether random assignment $Z$ affects $R$, as well as $U$. If $Z$ affects $R$ (there is a lower/higher rate of attrition in one treatment group than the other), but $U$ does not affect $R$, then there is just a loss of sample size but no distortion to estimates. We simply have a more (less) precise estimate of the potential outcomes from one group than we do from the other. However, when $Z$ and $U$ affect $R$, then we do not have a random sample of either the treated potential outcome or the control potential outcome from the treatment or control group. Therefore, we have to resort to model-based inference for both groups, reweighting each group based on our assumptions (in $M$) about how $U$ and $Z$ affect $R$. Alternatively, we can construct bounds of our estimates of the causal effect, which incorporate our uncertainty due to the non-random attrition.

In the case of descriptive and of causal inquiries, there is also a design-based way to estimate effects in the presence of uncertainty that does not rely on assumptions in $M$: double sampling. When there are missing outcomes, we can take a second sample of those who do not respond and conduct additional measurement to try to obtain those units' outcomes. If we spend more resources on making sure we can obtain measures for all of these randomly-sampled units, then we can use this random sample of non-responders in combination with the original sample to construct a full sample. There will still be a loss of sample size, because we take a subsample in the second round, and do not attempt to interview all nonresponders. We can also use double sampling for causal inquiries, conducting a random sample of nonresponders in the treatment group to stand in for those missing observations and a second in the control group.

### Interference

We have four endogenous outcomes in our DAG of a research design above: $R$, whether a participant responds to data collection; $D$, whether a respondent receives treatment; $Y^*$, the latent outcome; and $Y$, the observed outcome. Setting aside attrition and noncompliance for the moment, $R$ is a function only of sampling; $D$ of treatment assignment; $Y^*$ of $D$; and $Y$ of measurement strategy $Q$. Interference is the problem in which these endogneous variables depend not only on whether and how *they* are sampled, assigned to treatment, and measured, but whether and how *other units* are sampled, assigned to treatment, and measured. In other to define descriptive quantities like the sample average effect and causal quantities like the average treatment effect, we need to assume away interference.

Interference is common, perhaps even ubiquitous, and is often referred to as spillovers. Though we focus on spillovers of treatments, interference in sampling and measurement is also common.

Interference may be due to contagion, communication between participants, social comparison, deterrance, displacement, persistance and memory (in the case of over-time designs), and resource allocation decisions when there is a fixed budget. 

- Contagion
- Communication
- Socialcomparison
- Deterrence
- Persistanceandmemory
- Resourceallocationdecisions


Interference can affect measurement, just as it affects sampling and treatment assignment. Measurement interference occurs when $Y_i^*$ depends on whether and how *other* units are measured. If participants in an experiment discuss the measurement technique itself and what researchers are asking about with other participants, this may lead to a violation. Psychology researchers on academic campuses worry about this possibility and encourage subjects not to discuss what they do in the lab with potential participants. They are worried about intereference in treatment assignment but also of measurement, because often implicit measurement techniques can be defeated (and measure something other than $Y^i^*$) when the concept is known before the session. Other researchers worry that when measuring sensitive questions that the sensitive topic will be revealed to participants either by earlier participants or local leaders who discover its purpose (Lyall Blair Imai).  

### Excludability

-- no effect of S on $Y^*$

act of being included in the sample changes your attitudes, e.g., you reflect more deeply on them
demand effects (guess the hypothesis of the researcher, want to be a good subject)

-- no effect of Z on $Y^*$

act of being included in the treatment group changes your attitudes
demands effects (being in the treatment group makes you think you should have one attitudes, being in control group makes you think you should have another)

"exclusion restriction violations" in observational studies
- alternative channels through which rainfall effects outcomes besides the hypothesized channel
- effect of being induced to vote in habit studies has effects besides being put onto voter rolls

-- no effect of Q on $Y^*$

question includes a component that changes latent attitudes, i.e., has a persuasive component
Hawthorne effects

-- no effect of Z on Q

blinding, different tools to measure treatment and control group





<!-- - ethics belongs in data strategy -->
<!-- - just downloading the data.  Did you offload the data strategy -->
<!-- - possibly ambiguous where the data strategy ends and the analysis strategy ends. -->
<!-- - SOMEone did parts of the datastrategy -->
<!-- - Answer strategy section distinguishes qual from quant; should the data strategy section do the same? Make the point that lots of kinds of activities are data strategies. Interviews are a complex interaction of data strategy and answer strategy make them hard to declare. -->
<!-- - Data strategy is a main locus of ethical problems. That's not to say that ethical challenges don't occur when we have bad models or inquiries, or when our answer strategies violate subject privacy. Data strategies change what outcomes are revealed (as in an experiment) or measured (as in a survey). The act of treating or measure can induce trauma or other bad outcomes.  -->
