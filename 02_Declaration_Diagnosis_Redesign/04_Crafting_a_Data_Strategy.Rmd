---
title: "Crafting a data strategy"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# Crafting a data strategy

<!-- make sure to rename the section title below -->

```{r crafting_a_data_strategy, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

The data strategy is what researchers do in the world in order to collect information about it. Depending on the design, it could include decisions about any or or all of the following: how to sample or select cases, how to assign treatments, or how to measure outcomes. 

These choices apply to all kinds of research. In experimental research, a large focus is given to the assignment of treatments. How many treatment conditions should there be? Should we use a simple coin flip to decide who receives treatment, or should we use a more complicated strategy like blocking? Experimenters are of course also very concerned with sampling and measurement procedures, but it is the random assignment to treatments that make experiments distinctive among research designs. 

Quantitative descriptive research, on the other hand, often has an inquiry like the population average of some outcome variable. Since the goal here is to draw inferences about a population on the basis of a sample, we need to pay special attention to the procedure by which units are selected into the sample. We might use a random sampling procedure in order to generate a design-based justification for generalizing from samples to population. Nonrandom sampling procedures are also possible: convenience sampling, respondent-driven sampling, and snowball sampling are all data strategies that do not include an explictly random component.

Once we have selected units into the sample, we need to measure them in some way. The tools we use to measure are a critical part of the data strategy. For many social scientific studies, a main way we collect information is through surveys. A huge methodological literature on survey administration has developed to help guide researchers who have to design questionnaires. Bad survey questions yield distorted or noisy responses. They can be distored if responses are systematically biased away from the true latent target the question is designed to measure, in which case the question has low *validity*. They can be high variance if (hypothetically) you would obtain different answers each time you asked the same person the same question, in which case the question has low *reliability*. 

Beyond surveys, we might use administrative data to collect outcomes. The concerns about validity and reliability do not disappear once we move out of the survey environment. The information that shows up in an administrative database is itself the result of many human decisions, each of which has the possibility of increasing or decreasing the distance between the measurement and the thing to be measured.

Researchers have to choose good sampling, assignment, and measurement techniques that, when combined and applied to the world, will produce information that is ready for analysis. We will discuss answer strategies -- the set of analysis choices about what to do with the data once it's collected -- in the next chapter. The data and answer strategies are of course intimately interconnected. How you analyze data depends deeply on how it was collected *and* how you collect data depends just as deeply on how you plan to analyze it. For the moment, we are thinking through the many choices we might make as part of the data strategy, but of course they will have to be considered in concert with the answer strategy in any applied research design setting.

The data strategy is a *set of procedures* that result in a dataset. It is important to keep these two concepts straight. If you apply data strategy $D$, it produces dataset $d$. The data $d$ is the *result* of the data strategy $D$. We say $d$ is "the" result of $D$, since when we apply the data strategy to the world, we only do so once and we obtain the data that we obtain. But when we are crafting a data strategy, we have to think about the many datasets that the data strategy *could have* produced. Some of the datasets might be really excellent. For example, in good datasets, we achieve good covariate balance across the treatment and control groups. Or we might draw a sample whose distribution of observable characteristics looks really similar to the population. But some of the datasets might be worse: because of the vagaries of randomization, the particular realizations of the random assignment or random sampling might more more or less balanced. We do not have to settle for data strategies that can produce worse datasets! We want to choose data strategy $D$ that is likely to result in a high-quality dataset $d$.

The data strategy has three elements: sampling, assignment, and measurement. Sampling is the procedure for selecting which units will be measured; assignment is the procedure for allocating treatments to sampled units; and measurement is the procedure for turning information about the sampled units into data. All empiricial studies have a data strategy and every data strategy involves sampling, assignment, and measurement. Even studies that measure the full universe of cases involve sampling, since *future* units are not included. Even studies in which the researchers do not apply treatments themselves involve assignment, since processes outside researcher control end up assigning units to conditions. Even studies in which researchers took no new measurements involve measurement, since a choice about which measurements made by others must nevertheless be made.

Sampling choices confront the fundamental problem of generalization: we want to make inferences about units *not* sampled. Assignment choices confront the fundamental problem of causal inference: we want to make inferences about the conditions to which units were *not* assigned. Measurement choices confront the fundamental problem of descriptive inference: we want to make inferences about latent values on the basis of measured values. These problems are all fundamental in the sense that they cannot be surmounted and researchers should be humble in face of them. Strong research design can help, but we can never be *sure* that our sample generalizes, or that we know what would have happened in a counterfactual state of the world, or what true latent value of the outcome is (or if it even exists).


<!-- All studies will have a measurement strategy (otherwise nothing is measured and no data results from the data strategy), but only some will have sampling and treatment assignment procedures. A survey to measure satisfaction in a class might not sample units and instead measure all students in the class and no treatment would be assigned (we could, of course, define the sampling strategy as sampling all units). An experiment conducted in a class might have measurement and a treatment assignment strategy but no sampling. A survey of voters in a city might have a measurement strategy and a sampling strategy (random digit dialing, for example), but no treatment assignment (of course, here, we could define the treatment assignment procedure as assigning all units to a no-intervention condition). -->

## Sampling

Sampling strategies concern how units are selected from the population to be studied. We sample units in order to assign them to treatments, measure their characteristics, or both. Every design has a sampling strategy, but some are simple, such as "take all the units from a population," and others are complex, such as "

The model defines the target population and the sampling strategy defines which units from that population will be sampled. Though the model defines the population of units about which we want to make inferences, it may be impossible to obtain a list of those units or even to obtain a convenience sample of them. Studies that use Mechanical Turk face this problem: often we are interested in studying the effect of a treatment on Americans, but we have access to a convenience sample of Mechanical Turkers. The Turkers that take our survey are the sample, and the population in the model is Americans. Why would we ever be content to study a sample and not the full population? Cost and logistical constraints conspire to make it essentially impossible to study the full population of units. 





There are three classes of sampling procedures: convenience sampling; random sampling; and purposive sampling. 

Convenience sampling means you obtain a set of units from the population, but how you do so is not governed by any rule. Instead, you find units in the cheapest way possible. Convenience sampling is a good choice in two circumstances: it is the only strategy that is affordable, or when sampling is ignorable. Ignorability refers to the independence of missingness and characteristics of units. For many purposes, convenience sampling is sufficient, such as conducting experiments where treatment effects are not expected to vary by characteristics of the units in the sample. In other circumstances, such as measuring a sample mean like the proportion of people who are unmarried, convenience sampling is likely to lead to badly biased estimates due to a lack of ignorability. The bias comes from the fact that the types of people you are able to sample may have different outcomes than the types of people you are unable to sample. Many types of qualitative and quantitative research involve convenience sampling. Archival research often involves a convenience sample of documents on a certain topic that exist in an archive. The question of how these documents differ from those that would be in a different archive, or how the documents available in archives differ from those that do not ever make it into the archive importantly shapes what we can learn from them (Aliza Luft paper cite). With the decline of telephone survey response rates (cite), researchers can no longer rely on random digit dialing to obtain a representative sample of people in many countries, and instead must rely on convenience samples from the internet or panels who agree to have their phone numbers in a list. Reweighting techniques in the answer strategy can, in some cases, help recover estimates for the population as a whole if sampling is ignorable conditional on variables you can measure and weight on.

<!--  When your inquiry is a causl inference, if how units respond to treatments is independent of unit characteristics (i.e., homogeneous treatment effects), then which units are selected will not affect inferences. For descriptive inference, this is less likely to be true,  -->

Random sampling addresses this problem, and breaks the dependence between whether you are sampled and unit characteristics. Individual outcomes are independent of whether a unit is sampled. [longer discussion of why]

The ideal sampling strategy is random sampling from a list of all units in the population. If there is a positive probability of sampling every unit, then we can make inferences about the entire population. The easiest way to guarantee this is the case is to draw up a list of all units and to take a random sample without replacement from the list. There are many ways to draw a random sample, but two are particularly common in the social sciences: simple and stratified. Simple random sampling is sampling without replacement with a fixed sample size. Stratified random sampling takes a simple random sample of a fixed size from two or more subgroups, or strata, in the data. There are two different reasons you might select stratified random sampling over simple: your inquiry might involve comparisons between two subgroups, and you want to ensure you have a sufficient size in each subgroup; and there may be very different amounts of variability in two groups, and you want the most efficient estimate of a summary of that data so you oversample from the group with the higher variance to get a similar amount of precision from each group. Stratified sampling is a simple case of a larger class of sampling strategies, in which the probability of inclusion in the sample varies across units. In stratified sampling with two strata, the probability of inclusion is set for the two groups and is fixed within them. However, the probability can vary within strata as well. If you wish to have you sample include all age groups but upweight older people, you could make the probability of inclusion a function of age: the probability of sampling an 18 year old could be 0.01 ranging up to a probability of 0.1 for 80 year olds.

When it is impossible to get a list of all units in the target population either because it does not exist or is too costly to obtain, clustered sampling is often a substitute. In cluster sampling, you obtain a list of clusters of units, randomly sample from the list of clusters, and either collect data from all units within selected clusters, or conduct a simple random sample of units within sampled cluster (this would represent a simple form of multistage sampling). The advantage is you can still obtain a random sample of units, but it is feasible to do so because you have a list of clusters (but not of all units). The disadvantage is that units within clusters often have similar outcomes, so you don't get as much benefit from sampling more clusters as you do from sampling units from other clusters that are more different. This efficiency tradeoff -- how many clusters to choose vs. how many units within clusters -- can be explored in design diagnosis by comparing the RMSE of various choices. There are many other forms of multistage sampling -- sample provinces then sample districts within provinces, then villages within districts, and people within villages -- that fit particular aims and budgets. 

Purposive sampling is a catch-all term for rule-based sampling strategies that do not involve random draws but also are not purely based on convenience and cost. A common example is quota sampling. In studies where there is treatment effect heterogeneity, getting samples highly imbalanced in the characteristics by which effects vary will lead to bias for the population average treatment effect. If effects vary by gender and the study is conducted on Mechanical Turk, which skews male, there may be a problem. Quota sampling addresses the problem by ensuring through a convenience sample that fixed proportions of types of people are selected. In Mechanical Turk, we might set a quota of 50% men and 50% women. We would continue to select people in the convenience sample until we reached this threshold, by rejecting men from the sample after we reached 50%. Another common form of purposive sampling is respondent-driven sampling, which is used to sample from hard-to-reach populations such as HIV-positive needle users. RDS methods often begin with a convenience sample and then systematically obtain contacts for other units who share the same characteristic in order the build a large sample.

Case selection is another term for a sampling strategy. In case study research, whether qualitative or quantitative, the way you select the (typically small) set of cases is of great importance, and considerable attention has been paid to developing purposive case selection methods. Mills (cite) method of difference is a common case selection procedure, in which cases are selected that have different outcomes and the cases are inspected to identify characteristics that differ that may have led to the divergence. Lieberman (2005) proposes a method for selecting cases for indepth study from an initial regression study: cases on the regression line, well-predicted by it, are selected to determine whether the hypothesized causal relationship took place in those cases. Off-the-line cases, that are not well-predicted, are also selected to determine what factors may be left out of the regression model that provide alternative mechanisms. In each method, the inferential goal determines how cases are selected. Research design diagnosis can also help compare the efficiency and bias of these methods to select cases.

Selection on the dependent variable is purposive sampling method that has recieved substantial attention. The procedure is to select positive outcomes --- units in which an event such as a revolution took place, for example --- and study what led to that outcome. Geddes (XXXX) lays out the problem in this method for case study research: we cannot know whether the factors identified from investigating these cases led to the event, because we do not know whether those factors are also present in units where the outcome did not happen. If the factor is present in both, it is unlikely to have caused the positive outcome. The same problem plagues observational quantitative research. Ramsay et al. (XXXX) discuss the bias that results from studying the factors predicting suicide terrorism by only looking at observations where suicide terrorism occurred. In order to avoid these problems accidentally creeping into our research, it is important to diagnose the bias of our designs and not just the efficiency. 

In short, the choice of sampling strategy depends on features of the model and the inquiry, and different sampling strategies can be compared in terms of power and RMSE in design diagnosis. The model defines the population of units we are interested in making inferences about, and the target population of the sampling strategy should match that as much as possible. The model also points us to important subgroups (defined by nodes or endogenous variables) that we may wish to stratify on, depending on the variability within those subgroups. Whether we select convenience, random, or purposive sampling depends on our budget and logistical constraints as well as the efficiency (power or RMSE) of the design. If there is little bias from convenience sampling, we will often want to select it for cost reasons; if we cannot obtain a convenience sample that has the right composition, we may choose a purposive method that ensures we do. The choice between simple and stratified sampling comes down to the inquiry and to a diagnosis of the RMSE: when the inquiry involves a comparison of subgroups, we will often select stratified sampling. In either, a diagnosis of alternative designs in terms of power or RMSE will guide selection.

Sampling, like all data strategies, is an intervention in the world by researchers. As a result, it may have independent causal effects on outcomes. As we will see in the next section, there is no deep difference between sampling and treatment assignment, because in each case we are randomly sampling from two potential outcomes. In treatment assignment, we are assigning units to a treatment group and a control group, and obtaining a sample of the treated potential outcome and a second random sample of the control potential outcome. In random sampling, we are assigning units to be sampled or to not be sampled. We obtain one sample of the sampled potential outcome. However, the non-sampled potential outcome exists conceptually, and may differ from the sampled potential outcome. The act of including units in the sample may change outcomes, for example if you are selected to participate in a medical trial you believe you are going to receive better care and a placebo effect changes your health condition. A research design in which you conduct an experiment on sampled units, but also unobtrusively measure outcomes in nonsampled units could be used to estimate the effect of inclusion in the sample. This design would provide a random sample of the nonsampled potential outcome.

<!-- you create strata and clusters from real things in the world -->

### Example: @wang2015forecasting

Xbox forecasting of elections

<!-- what is scraping? -->
<!-- what is going to the archive? -->
<!-- sampling on the DV -->

<!-- redefine your inquiry -->

<!-- ALSO is your inquiry really the right one - difference between target population and real population (Americans in 2016 vs Americans every year) -->

<!-- -- sample all of the population or part of it -->
<!-- -- random sampling, purposive sampling -->

<!-- - simple, complete, stratified, clustered, stratified and clustered -->
<!-- - weighted sampling (over/undersampling) -->
<!-- - finding missing populations (RDS) -->
<!-- - quota sampling -->
<!-- - hawthorne effects -->
<!-- - the inquiry and the model guide your choice of which units to sample and how to sample them -->

## Treatment assignment

Treatment assignment is closely related to sampling. A sampling strategy assigns units in a population either to be included in the sample or excluded. A simple treatment assignment strategy assigns units in the sample to one of two conditions, treatment or control. Both assign units from a group into one of two groups. The difference is only in the fact that we do not measure outcomes for nonsampled units (though we could, to learn about how nonsampled units differ from sampled units). The mechanics and statistics of sampling and simple treatment assignments are identical but for this difference. Assignment procedures may assign units into more than two treatment conditions. In a factorial design, two treatments are assigned to a set of units, and a unit can be assigned to neither treatment (this is a "pure control"), just one, the other, or both. In this case, there are four groups a unit could be assigned to.

- simple, complete, blocked, clustered, blocked and clustered
- point restricted randomization
- no assignment procedure at all
- multiple arms
- adaptive
- the inquiry and the model guide your choices of which treatments to assign and how to assign them

### Example: sociology natural experiment from government or official cutoff


## Measurement

- what variables to measure? similar to M, the ones that are used to execute D and A

- trying to measure a latent outcome $Y^*$, but we can only ever imperfectly observe it through a measurement function $Q(Y^*) = Y^{\mathrm obs}$. 
- there are two basic ways to assess the quality of $Q$: bias, which is given the special label validity; and variance, which is labeled reliability. both are measures of the difference $Y^{\mathrm obs} - Y^*$. 

- matrix of who collects measures of $Y^*$: the researcher, the participant, or a third party (e.g., administrative data collected by government)
- what mode they use to collect it: human coding of observations by sight, hearing, touch, smell, and taste; recordings of waves of light (e.g., photos), sound (e.g., audio and seismic recordings), electromagnetic waves (e.g., EKGs and x-rays), and combinations (e.g., video); thermometers and barometers; medical and biometric devices (e.g., blood tests)
- how often measurements are taken and when
- whether the participant knows they are being observed
- 


- Should this be where we do the first bit of distinction between latent and observed
- survey experiments to measure latent characteristics
- more T (david McKenzie).  How frequently to measure.  Andy - arguing against intermediate measurement?
- multiple measurements of Y.  make a scale
- the inquiry and model guide your choices of which endogenous outcomes to measure
- issues with rates - y^* / x^*, then rates are messed up

### Example: @weaver2019too


