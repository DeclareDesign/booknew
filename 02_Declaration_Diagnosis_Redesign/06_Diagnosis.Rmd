---
title: "Diagnosis"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# Diagnosis 

<!-- make sure to rename the section title below -->

```{r diagnosis, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- TRUE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
library(ggforce)
library(ggridges)
```

Research design diagnosis is the process of evaluating the properties of a research design. Since "property of a research design" is a cumbersome phrase, we made up the word "diagnosand" to refer to those properties of a research design we would like to diagnose. Many diagnosands are familiar. Power is the probability of obtaining a statistically significant result. Bias is the average deviation of estimates from the true value of the estimand. Other diagnosands are more exotic, like the Type-S error rate, which is the probability the estimate has the incorrect sign, conditional on being statistically significant (Gelman and Carlin 2014).

Not every diagnosand is relevant for every study. For example, in a descriptive study whose goal is to estimate the fraction of people in France who are left-handed, statistical power is irrelevant. A hypothesis test against the null hypothesis that 0 percent of the people in France are left-handed is preposterous. We know for sure that the fraction is not zero, we just don't know its precise value. A much more important diagnosand for this study would be RMSE (root-mean-squared-error), which is a measure of how well-measured the estimand that incorporates both bias and variance.

How should you choose your diagnosands? In our experience, writing out what would make the study either a success or a failure helps tremendously. Suppose your study would be a success if it produced an estimate that is higher than 4, had a standard error of 1 or less, and yielded statistically significant evidence of treatment effect heterogeneity -- write down a diagnosand that is the probability of all three of those conditions being true at the same time. If a study is a failure (in terms of not having been worth the money and effort *ex post*) when the confidence interval is 20 points wide or wider, then the research team should design to minimize the probability of that occurance.

A brief aside on the most common diagnosand of statistical power. This diagnosand is the probability of getting a statistically significant result, which of course depends on many things about your design including, crucially, the unknown magnitude of the parameter to be estimated. You can think of statistical power as the probability of a success, where success is defined as getting a significant results. The conventional "power target" is 80\% power. One could imagine redefining statistical power as "null risk," or the probability of obtaining a null result. In these terms, the conventional power target is a 20\% null risk, or a one in five chance of "failure." Those odds aren't great, so we recommend designing studies with lower null risk. We also think that statistical power is often over-emphasized relative to other more important diagnosands like bias and rmse. If your design is systematically biased away from zero, your statistical power will be high, but your design is nevertheless weak.




## Estimating diagnosands analytically

Diagnosis can be be done with analytic, pencil-and-paper methods. Indeed, research design textbooks often contain many formulas for calculating power under a variety of designs. For example, Gerber and Green include the following power formula:

They write: 

> "To illustrate a power analysis, consider a completely randomized experiment where $N>2$ of $N$ units are selected into a binary treatment. The researcher must now make assumptions about the distributions of outcomes for treatment and for control units. In this example, the researcher assumes that the control group has a normally distributed outcome with mean $\mu_c$, the treatment group has a normally distributed outcome with mean $\mu_t$, and both group's outcomes have a standard deviation $\sigma$. The researcher must also choose $\alpha$, the desired level of statistical significance (typically 0.05).
Under this scenario, there exists a simple asymptotic approximation for the power of the experiment (assuming that the significance test is two-tailed):
> $$
\beta = \Phi \bigg(\frac{|\mu_t - \mu_c| \sqrt{N}}{2\sigma} - \Phi^{-1} (1 - \frac{\alpha}{2}) \bigg)
> $$
where $\beta$ is the statistical power of the experiment, $\Phi(\cdot)$ is the normal cumulative distribution function (CDF), and $\Phi^{-1}(\cdot)$ is the inverse of the normal CDF."

This power formula makes **detailed** assumptions about $M$, $D$, and $A$? Under $M$, it assumes that both potential outcomes are normally distributed with group specific means and a common variance. Under $D$, it assumes a particular randomization strategy (simple random assignment). Under $A$, it assumes a particular hypothesis testing approach (equal variance $t$-test with $N - 2$ degrees of freedom). This set of assumptions may be "close enough" in many research settings, but it can be difficult to understand the specific impacts of different beliefs about $M$, $D$ and $A$ on the value of the diagnosand. What if instead of being normally distributed, the potential outcomes are measured in 1 - 5 Likert scales? What if the randomization procedure includes blocking? What if we include covariates in our treatment effect estimation approach? Formulas for some large sources of design variation have been derived (such as clustering), but certainly not for every design variant.

Very quickly, hope for analytic design diagnosis fades. The analytic formulas are abstractions -- they abstract away from design details and sometimes those design details are important. This problem is not confined to the "power" diagnosand. In randomized experiments, claims about the bias diagnosand are quite general. Many randomized designs are unbiased for the ATE, but not all. Designs that encounter noncompliance, attrition, or some forms of spillover may not be unbiased for the ATE. Even without any of those complications, cluster randomized trials with heteogeneous cluster sizes are not unbiased (joel, imai). 

Diagnosands depend on design details, because how you conduct your study matters for its properties. That means design diagnosis must be design-aware. Since designs are so heteogenous and can vary on so many dimensions, computer simulation is only feasible way to diagnose anything beyond the simplest ideal-type designs.

## Estimating diagnosands via simulation

Research design diagnosis usually occurs in a two-step, simulation-based procedure. First we simulate research designs over and cover, collecting "diagnostic statistics" from each run of the simulation. Second, we summarise the distribution of the diganostic statistics in order to estimate the diagnosands.

So we estimate diagnosands by summarizing the distribution of diagnostic statistics -- of course this raises the question: what is a diagnostic statistic? We take a draw from the model ($m$) and calculate the value of the inquiry $I(m) = a^m$. We take one draw from the data strategy ($D(m) = d$), and calculates the value of the answer strategy $A(d) = a^d$. A diagnostic statistic is some function of $a^m$ and $a^d$.

A simple diagnostic statistic is "error," or the difference between the estimate and the estimand: $error = a^d - a^m$. The bias diagnosand is the expectation of the error statistic $E[error]$ over all possible ways the study could have come out.

Usually, we consider many diagnostic statistics at the same time. Here's a design declaration for a two-arm trial with a balanced (50/50) design. We have 100 subjects and their responses to treatment are drawn from a normal distribution with mean 0.1 and sd 0.1.   

```{r, echo=FALSE}
design <-
  declare_population(N = 100, 
                     Tau = rnorm(N, mean = 0.1, sd = 0.1),
                     U = rnorm(N, 0, 0.2)) +
  declare_potential_outcomes(Y ~  Tau * Z + U) + 
  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +
  declare_assignment(prob = 0.5) +
  declare_estimator(Y ~ Z, estimand = "ATE")
```

One draw of this simulation returns the following:

```{r, eval = FALSE, echo = FALSE}
run_design(design)
```

One draw from the simulation returns the following:

| estimand| estimate| std.error| conf.low| conf.high| p.value|
|--------:|--------:|---------:|--------:|---------:|-------:|
|     0.10|     0.08|      0.04|    0.003|     0.156|    0.04|

Figure XXX shows the information we might obtain from a single run of the simulation. The filled point is the estimate $a^d$. The open triangle is the estimand $a^m$. The bell-shaped curve is our normal-approximation based estimate of the sampling distribution. The standard deviation of this estimated distribution is our estimated standard error, which expresses our uncertainty. The confidence interval around the estimate is another expression of our uncertainty: We're not sure where $a^d$ is, but if things are going according to plan, confidence intervals constructed this way will bracket $a^d$ 95\% of the time. 

```{r, echo=FALSE}
design <-
  declare_population(N = 100, U = rnorm(N, 0, 0.2)) +
  declare_potential_outcomes(Y ~ rnorm(N, mean = 0.1, sd = 0.1) * Z + U) + 
  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +
  declare_assignment() +
  declare_estimator(Y ~ Z, estimand = "ATE", model = lm_robust)

# diagnose_design(design, sims = 250)

set.seed(343)
simulations <- 
  simulate_design(design, sims = 10)   %>%
  mutate(significant = p.value <= 0.05)


gg_df <-
  simulations %>%
  group_by(sim_ID) %>%
  nest %>%
  mutate(densities = lapply(data, function(df) {
    with(df, tibble(
      x = seq(-2, 2, 0.0001),
      density = dnorm(x, mean = estimate, sd = std.error)
    ))
  })) %>%
  unnest(cols = c(data, densities)) %>%
  group_by(sim_ID)


gg_df <- 
  gg_df %>%
  mutate(
    
    low_cut = case_when(
      estimate < 0 ~ x < -2*abs(estimate),
      estimate > 0 ~ x <= 0),
    
    high_cut = case_when(estimate < 0 ~ x >= 0,
                         estimate > 0 ~ x >= 2*abs(estimate) ),
    
    area_under = case_when(low_cut ~ "low",
                           high_cut ~ "high",
                           TRUE ~ "middle")
  ) 
```


```{r, fig.height = 1.5, fig.width = 5, echo = FALSE}
simulation_1 <- simulations %>% filter(sim_ID == 1)

simulation_1 %>% 
  select(estimand, estimate, std.error, conf.low, conf.high, p.value) %>%
  knitr::kable(digits = 3)

gg_df_1 <- gg_df %>% filter(sim_ID == 1)

ggplot(simulation_1, aes(x = estimate, y = as.factor(sim_ID)), color = dd_light_blue) +
  # geom_rect(xmin = -10, xmax = 0, ymin = -100, ymax = 100, fill = gray(0.96), size = 0) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high),
                 height = 0.075,
                 size = 0.25,
                 color = gray(0.5)) +
  geom_point(size = 1.5, color = gray(0.5)) +
  geom_point(
    aes(x = estimand),
    size = 1.5,
    fill = NA,
    color = gray(0.5),
    pch = 24,
    position = position_nudge(y = -0.1)
  ) +
  # first plot the density for p-values
  geom_ridgeline(
    data = gg_df_1,
    size = 0,
    aes(x = x, height = density, fill = area_under),
    scale = 0.05,
    min_height = 0.01
  ) +
  # then the density lines (to make it plot correctly and not be cut off)
  geom_ridgeline(
    data = gg_df_1,
    size = 0.1,
    aes(x = x, height = density, fill = NA),
    scale = 0.05,
    min_height = 0.01,
    color = dd_light_blue,
  ) +
  theme_void() +
  theme(
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    panel.grid = element_blank(),
    legend.position = "none"
  ) +
  coord_cartesian(xlim = c(-0.1, 0.25)) +
  geom_vline(
    xintercept = 0,
    color = gray(0.5),
    linetype = "dashed",
    alpha = 0.5
  ) +
  # geom_vline(
  #   xintercept = 0.1,
  #   color = dd_light_blue,
  #   linetype = "dotted",
  #   alpha = 0.5
  # ) +
  scale_fill_manual(name = "Probability",
                    values = c(dd_light_blue_alpha, dd_light_blue_alpha, NA)) +
  labs(x = "Estimated sampling distribution (from point estimate and standard error)")
```

<!-- We're interested in a few diagnosands. Bias (the expectation of the error statistics) is one, but we also want to know about RMSE (the square root of the expectation of the squared errors), power (the probability the $p$-value is below a threshold), and coverage (the probability the 95% confidence interval brackets the estimand).  -->

From this single draw, we can't yet estimate diagnosands, but we can estimate diagnostic statistics. The estimate was higher than the estimand in this draw, so the error is 0.10 - 0.08 = 0.02. Likewise, the squared error is `(0.10 - 0.08)^2 = 0.0004`. The $p$-value is 0.04, which is just barely lower than the threshold of 0.05, so "statistical significance" diagnostic statistic is equal to `TRUE`. The confidence interval stretches from 0.003 to 0.156, and the value of the estimand (0.10) is between those bounds, so the "covers" diagnostic statistic is equal to `TRUE` as well.

Learning the **distribution** of diagnostic statistics is the main barrier to design diagnosis. If we could simply write down the distribution of diagnostic statistics, it would be a straightforward matter to summarize them in order to calculate diagnosands. But the distributions of diagnositic statistics depend on the complex of information in all four parts of a research design: M, I, D, and A. For example, the error statistic depends on both $a^d$ and $a^m$, so the details of each matter greatly. 

To calculate the distributions of the diagnostic statistics, we have to simulate designs not just once, but many many times over. The bias diagnosand is the average error over many runs of the simulation. The statistical power diagnosand is the fraction of runs in which the estimate is significant. The coverage diagnosand is the fraction of runs in which the confidence interval covers the estimand.

This figure visualizes just 10 runs of the simulation (obtained with `simulate_design(design)`). We can see that in each run, $a^m$ is a little different. This might seem counterintuitive -- isn't the estimand supposed to be a fixed number? Some estimands are fixed, others are stochastic, depending on the specifics of the model. Notice how in the design declaration, we drew the potential outcomes from a distribution rather then having them be fixed numbers. This choice incorportates some of our modeling uncertainty. The treatment effects for each unit are close to 0.1, but we're not sure how close for each particulat unit. We can also see that some of the draws produce statistical significant estimates (the shaded areas are small and the confidence intervals don't overlap zero), but not all. We get a sense of the *true* standard error by seeing how the point estimates bounce around. We get a feel for the difference between the estimates of the standard error and true standard error. Design diagnosis is the process of learning about all the ways the study might come out, not just the one way that it will.



```{r, fig.height = 3.5, fig.width = 5, echo = FALSE}
ggplot(simulations, aes(x = estimate, y = as.factor(sim_ID)), color = dd_light_blue) +
  # geom_rect(xmin = -10, xmax = 0, ymin = -100, ymax = 100, fill = gray(0.96), size = 0) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high),
                 height = 0.075,
                 size = 0.25,
                 color = gray(0.5)) +
  geom_point(size = 1.5, color = gray(0.5)) +
  geom_point(
    aes(x = estimand),
    size = 1.5,
    fill = NA,
    color = gray(0.5),
    pch = 24,
    position = position_nudge(y = -0.1)
  ) +
  # first plot the density for p-values
  geom_ridgeline(
    data = gg_df,
    size = 0,
    aes(x = x, height = density, fill = area_under),
    scale = 0.05,
    min_height = 0.01
  ) +
  # then the density lines (to make it plot correctly and not be cut off)
  geom_ridgeline(
    data = gg_df,
    size = 0.1,
    aes(x = x, height = density, fill = NA),
    scale = 0.05,
    min_height = 0.01,
    color = dd_light_blue,
  ) +
  theme_void() +
  theme(
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    panel.grid = element_blank(),
    legend.position = "none"
  ) +
  coord_cartesian(xlim = c(-0.1, 0.25)) +
  geom_vline(
    xintercept = 0,
    color = gray(0.5),
    linetype = "dashed",
    alpha = 0.5
  ) +
  geom_vline(
    xintercept = 0.1,
    color = dd_light_blue,
    linetype = "dotted",
    alpha = 0.5
  ) +
  scale_fill_manual(name = "Probability",
                    values = c(dd_light_blue_alpha, dd_light_blue_alpha, NA)) +
  labs(x = "Estimated sampling distribution (from point estimate and standard error)")
```


This line of code does it all in one.  We simulate the design 1000 times to calculate the diagnositic satistics, then we summarise them in terms of bias, the true standard error (the standard deviation of the sampling distribution), RMSE, power, and coverage.

```{r, eval=FALSE}
diagnosis <- diagnose_design(design, sims = 1000, 
                             diagnosands = declare_diagnosands(
                               select = c("bias", "sd_estimate", "rmse", "power", "coverage")))
```



```{r, eval = do_diagnosis & !exists("do_bookdown"), echo=FALSE}
diagnosis <- diagnose_design(design, sims = sims, 
                             diagnosands = declare_diagnosands(
                               select = c("bias", "sd_estimate", "rmse", "power", "coverage")))
```


```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("diagnosis"), "/diagnosis.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnosis, path = rds_file_path)
}
diagnosis <- read_rds(rds_file_path)
```

```{r}
diagnosis
```





## Interactive element

You can have fun building up a design diagnosis run-by-run with this shiny app. Each time you run the design, it adds a new estimate to the sampling distribution so you can learn about the relationship of a single run of the design to the overall diagnosis.

```{r, echo = FALSE}
knitr::include_app('https://gblair.shinyapps.io/diagnosis/', height = '400px')
```

## Bias, Variance, and Mean-squared Error

This section walks through the interrelationship between three important diagnosands: bias, variance, and a combination of the two: mean-squared error. Many research design decisions involve trading off bias and variance. In trade-off settings,we may need to accept higher variance in order to decrease bias. Likewise, we may need to accept a bit of bias in order to achieve lower variance. The tradeoff is captured by mean-squared error -- which is the average squared distance between $a^d$ and $a^m$. Of course we would ideally like to have as low a mean-squared error as possible. We would like to achieve low variance and low bias simultaneously.

To illustrate, consider the following three designs as represeted by three targets. The inquiry is the bullseye of the target. The data and answer strategies combine to generate a process by which arrows are shot towards the target. On the left, we have a very bad archer: even though the estimates are unbiased in the sense that they hit the bullseye "on average", very few of the arrows are on target. In the middle, we have the Katniss Everdeen (the heroine of the Hunger Games novels who is good with a bow) of data and answer strategies: they are both on target and low variance. On the right, we have an archer who is very consistent (low variance) but biased. The mean squared error is highest on the left and lowest in the middle. 

```{r, echo = FALSE, fig.width = 6.5, fig.height = 3}
n <- 50

points_df <- tibble(rho = sqrt(runif(n)),
                    theta = runif(n, 0, 2*pi),
                    x = rho * cos(theta),
                    y = rho * sin(theta)
)

summary_df <- points_df %>% 
  summarize(
    bias = round(mean(sqrt(x^2 + y^2)), 2),
    mse = round(mean(x^2 + y^2), 2),
    var = round(mean((x - mean(x))^2 + (y - mean(y))^2), 3)
  )

unbiased_lowprecision <- ggplot() + 
  geom_circle(
    data = tibble(
      x0 = rep(0, 5),
      y0 = rep(0, 5),
      r = rev(seq(0.1, 1, length.out = 5))
    ), 
    aes(x0 = x0, y0 = y0, r = r, fill = fct_rev(as.factor(r))),
    col = gray(0.25), lwd = 0.25, alpha = 0.5) +
  geom_point(data = points_df, aes(x, y), size = 0.5) + 
  scale_fill_manual(values = rev(c("yellow", "red", "cyan", "black", "white"))) + 
  coord_fixed() + 
  dd_theme() + 
  xlab("") + ylab("") + 
  ggtitle("Unbiased, imprecise", subtitle = glue::glue("Variance = {summary_df$var}\nBias = {summary_df$bias}\nRMSE = {summary_df$mse}")) + 
  theme(
    legend.position = "none",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank()
  )

n <- 50

points_df <- tibble(
  rho = sqrt(runif(n, min = 0, max = 0.015)),
  theta = runif(n, 0, 2*pi),
  x = rho * cos(theta),
  y = rho * sin(theta)
)

summary_df <- points_df %>% 
  summarize(
    bias = round(mean(sqrt(x^2 + y^2)), 2),
    mse = round(mean(x^2 + y^2), 2),
    var = round(mean((x - mean(x))^2 + (y - mean(y))^2), 3)
  )

unbiased_highprecision <- ggplot() + 
  geom_circle(
    data = tibble(
      x0 = rep(0, 5),
      y0 = rep(0, 5),
      r = rev(seq(0.1, 1, length.out = 5))
    ), 
    aes(x0 = x0, y0 = y0, r = r, fill = fct_rev(as.factor(r))),
    col = gray(0.25), lwd = 0.25, alpha = 0.5) +
  geom_point(data = points_df, aes(x, y), size = 0.5) + 
  scale_fill_manual(values = rev(c("yellow", "red", "cyan", "black", "white"))) + 
  coord_fixed() + 
  dd_theme() + 
  xlab("") + ylab("") + 
  ggtitle("Unbiased, imprecise", subtitle = glue::glue("Variance = {summary_df$var}\nBias = {summary_df$bias}\nRMSE = {summary_df$mse}")) + 
  theme(
    legend.position = "none",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank()
  )

n <- 50

points_df <- tibble(
  rho = sqrt(runif(n, min = 0, max = 0.015)),
  theta = runif(n, 0, 2*pi),
  x = rho * cos(theta) + 0.5,
  y = rho * sin(theta) - 0.2
)

summary_df <- points_df %>% 
  summarize(
    bias = round(mean(sqrt(x^2 + y^2)), 2),
    mse = round(mean(x^2 + y^2), 2),
    var = round(mean((x - mean(x))^2 + (y - mean(y))^2), 3)
  )

biased_highprecision <- ggplot() + 
  geom_circle(
    data = tibble(
      x0 = rep(0, 5),
      y0 = rep(0, 5),
      r = rev(seq(0.1, 1, length.out = 5))
    ), 
    aes(x0 = x0, y0 = y0, r = r, fill = fct_rev(as.factor(r))),
    col = gray(0.25), lwd = 0.25, alpha = 0.5) +
  geom_point(data = points_df, aes(x, y), size = 0.5) + 
  scale_fill_manual(values = rev(c("yellow", "red", "cyan", "black", "white"))) + 
  coord_fixed() + 
  dd_theme() + 
  xlab("") + ylab("") + 
  ggtitle("Unbiased, imprecise", subtitle = glue::glue("Variance = {summary_df$var}\nBias = {summary_df$bias}\nRMSE = {summary_df$mse}")) + 
  theme(
    legend.position = "none",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank()
  )

library(patchwork)

unbiased_lowprecision + unbiased_highprecision + biased_highprecision
```

The archery metaphor is common in research design textbooks because it effectively conveys the difference between variance and bias, but it does elide an important point. It really matters **which target** your archer is shooting at. Figure XXX shows a bizarre double-target representing two inquiries. The empirical strategy is unbiased and precise for the left inquiry, but it is clearly biased for the right inquiry. When we are describing the properties of $a^d$, we have to be clear about which $a^a$ they are associated with.


```{r, echo=FALSE}
n <- 50

points_df <- tibble(
  rho = sqrt(runif(n, min = 0, max = 0.015)),
  theta = runif(n, 0, 2*pi),
  x = rho * cos(theta),
  y = rho * sin(theta)
)

summary_df <- points_df %>% 
  summarize(
    bias_1 = round(mean(sqrt(x^2 + y^2)), 2),
    mse_1 = round(mean(x^2 + y^2), 2),
    var_1 = round(mean((x - mean(x))^2 + (y - mean(y))^2), 3),
    
    bias_2 = round(mean(sqrt((x-0.25)^2 + y^2)), 2),
    mse_2 = round(mean((x-0.25)^2 + y^2), 2),
    var_2 = round(mean(((x-0.25) - mean(x-0.25))^2 + (y - mean(y))^2), 3)
  )

circle_df <- 
  bind_rows(
    "left" = tibble(
      x0 = rep(0, 5),
      y0 = rep(0, 5),
      r = rev(seq(0.1, 1, length.out = 5))
    ),
    "right" = tibble(
      x0 = rep(0.35, 5),
      y0 = rep(0, 5),
      r = rev(seq(0.1, 1, length.out = 5))
    ),
    .id = "side"
  ) %>% 
  arrange(desc(r), side)

dual_estimands <- ggplot() + 
  geom_circle(
    data = circle_df, 
    aes(x0 = x0, y0 = y0, r = r, fill = fct_rev(as.factor(r))),
    col = gray(0.25), lwd = 0.25) +
  geom_point(data = points_df, aes(x, y), size = 0.5) + 
  scale_fill_manual(values = rev(c("yellow", "red", "cyan", "black", "white"))) + 
  coord_fixed() + 
  dd_theme() + 
  xlab("") + ylab("") + 
  ggtitle("Diagnosands depend on the estimand", subtitle = glue::glue("Variance = {summary_df$var_1} (left), {summary_df$var_2} (right)\nBias = {summary_df$bias_1} (left), {summary_df$bias_2} (right)\nRMSE = {summary_df$mse_1} (left), {summary_df$mse_2} (right)")) + 
  theme(
    legend.position = "none",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank()
  )
dual_estimands
```



## Diagnosands {#diagnosands}

| Diagnostic-statistic                                    | Definition                                                                   |
|---------------------------------------------------------|------------------------------------------------------------------------------|
| Estimate                                                | $a^d$                                                                        |
| Estimand under the model                                | $a^m$                                                                        |
| True estimand                                           | $a^w$                                                                        |
| $p$-value                                               | $p \equiv \Pr_{M_0} \{\mid A - a^{m_0}\mid~\geq~\mid a^d - a^{m_0}\mid \}$   |
| $p$-value is no greater than $\alpha$                   | $\mathbf{I}(p \leq \alpha)$                                                  |
| Confidence interval                                     | $\mathrm{CI}_{1-\alpha}$                                                     |
| Confidence interval covers the estimand under the model | $\mathrm{Covers} \equiv \mathbb{I}\{ a^m \in \mathrm{CI}_{1-\alpha} \}$      |
| Estimated standard error                                | $\widehat\sigma(A)$                                                          |
| Cost                                                    | $\mathrm{cost}$                                                              |

| Diagnosand                         | Description                                                                                                                          | Definition                                                            |
|------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------|------------------------------                                         |
| Power                              | Probability of rejecting null hypothesis of no effect                                                                                | $\E(I(p \leq \alpha))$                                                |
| Bias                               | Expected difference between estimate and estimand                                                                                    | $\E(a^d - a^m)$                                                       |
| Variance                           |                                                                                                                                      | $\Var(a^d)$                                                           |
| True standard error                |                                                                                                                                      | $\sqrt{\Var(a^d)}$                                                    |
| RMSE                               | Root mean-squared-error                                                                                                              | $\sqrt{\E(a^d - a^m)}$                                                |
| Coverage                           |                                                                                                                                      | $\Pr(\mathrm{covers})$                                                |
| Type S error rate                  | Probability estimate has incorrect sign, if statistically significant (Gelman and Carlin 2014)                                       | $\Pr(\mathrm{sgn}(a^d) \neq \mathrm{sgn}(a^m) \mid p \leq \alpha)$    |
| Exaggeration ratio                 | Expected ratio of absolute value of estimate to estimand, if statistically significant (Gelman and Carlin 2014)                      | $\E( a^d / a_m \mid p \leq \alpha)$                                   |
| Type I error                       |                                                                                                                                      | $\Pr(p \leq \alpha \mid a^m = a^0)$                                   |
| Type II error                      |                                                                                                                                      | $\Pr(p \geq \alpha \mid a^m \neq a^0)$                                |
| Sampling bias                      | Expected difference between population average treatment effect and sample average treatment effect \(Imai, King, and Stuart 2008\)  | $\E(a^m_{\mathrm sample} - a^m_{\mathrm population})$                 |
| Maximum possible cost              |                                                                                                                                      | $\max{cost}$                                                          |
| Bayesian learning                  |                                                                                                                                      | $a^m_{\mathrm post} - a^m_{\mathrm pre}$                              |
| Value for money                    | Probability that a decision based on estimated effect yields net benefits                                                            |                                                                       |
| Success                            |                                                                                                                                      |                                                                       |
| Minimum detectable effect (MDE)    |                                                                                                                                      | $\argmin_{a^m_*} \Pr(p \leq \alpha) = 0.8$                            |
| Robustness                         | Joint probability of rejecting the null hypothesis across multiple tests                                                             |                                                                       |
| Proportion of subjects harmed      |                                                                                                                                      |                                                                       |
| Proportion of subjects who consent |                                                                                                                                      |                                                                       |

## Diagnosing with respect to variations in M

### Uncertainty about M

### Adjudicating betweetn competing Ms


<!-- ## Ethics -->
<!-- - There are ethical diagnosands, e.g., what is the distribution of ethical costs such as loss of autonomy. -->

<!-- - Cite Tara's paper. Cite Lauren's paper.  -->

<!-- - Do a diagnosis of how many minutes of subject time are wasted by audit experiments, weigh that against the cost of demonstrating outright racial bias. (cite an audit study that does this calculation) -->

## grab bag

<!-- example of when there is a mismatch between Am and Ad -->

```{r, echo = FALSE}
ATE <- 0.0

design <- 
  declare_population(N = 1000,
                     binary_covariate = rbinom(N, 1, 0.5),
                     normal_error = rnorm(N)) +
  # crucial step in POs: effects are not heterogeneous
  declare_potential_outcomes(Y ~ ATE * Z + normal_error) +
  declare_assignment(prob = 0.5) +
  declare_estimator(Y ~ Z, subset = (binary_covariate == 0), label = "CATE(0)") + 
  declare_estimator(Y ~ Z, subset = (binary_covariate == 1), label = "CATE(1)") +
  declare_estimator(Y ~ Z * binary_covariate, 
                    model = lm_robust, term = "Z:binary_covariate", label = "Interaction")
```

```{r, echo = FALSE, purl = FALSE}
# note this was rerun a bunch of times to get the right example (one is non sig the other is sig diff and diff-in-CATE is not diff from zero)
# estimates <- draw_estimates(design)
rds_file_path <- paste0(get_dropbox_path("answer_strategy"), "/estimates_diff_in_significance_plot.RDS")
# write_rds(estimates, path = rds_file_path)
estimates <- read_rds(rds_file_path)
```

```{r, echo = FALSE, fig.height = 3}

g1 <- ggplot(data = estimates %>% filter(term == "Z"), aes(estimator_label, estimate)) + 
  geom_point() + 
  geom_errorbar(aes(x = estimator_label, ymin = conf.low, ymax = conf.high), width = 0.2) + 
  ylab("Estimate (95% confidence interval)") +
  geom_hline(yintercept = 0, lty = "dashed") +
  ggtitle("Visualization A") +
  dd_theme() + 
  theme(axis.title.x = element_blank())

g2 <- ggplot(data = estimates, aes(estimator_label, estimate)) + 
  geom_point() + 
  geom_errorbar(aes(x = estimator_label, ymin = conf.low, ymax = conf.high), width = 0.2) + 
  ylab("Estimate (95% confidence interval)") +
  geom_hline(yintercept = 0, lty = "dashed") +
  ggtitle("Visualization B") +
  dd_theme() + 
  theme(axis.title.x = element_blank())

g1 + g2
```

We now demonstrate that the answer strategy on the left is flawed. XXYY describe sims.

```{r, echo = FALSE}
# sweep across all ATEs from 0 to 0.5
designs <- redesign(design, ATE = seq(0, 0.5, 0.05))
```

```{r, echo = FALSE, eval = do_diagnosis & !exists("do_bookdown")}
simulations_one_significant_not_other <- simulate_design(designs, sims = sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("answer_strategy"), "/simulations_one_significant_not_other.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(simulations_one_significant_not_other, path = rds_file_path)
}
simulations_one_significant_not_other <- read_rds(rds_file_path)
```

```{r, eval = FALSE, echo = FALSE, fig.height = 3.5}
# Summarize simulations ---------------------------------------------------

reshaped_simulations <-
  simulations_one_significant_not_other %>%
  transmute(ATE,
            sim_ID,
            estimator_label,
            estimate,
            conf.high,
            conf.low,
            significant = p.value < 0.05) %>%
  pivot_wider(id_cols = c("ATE", "sim_ID"), names_from = "estimator_label", values_from = c("estimate", "conf.high", "conf.low", "significant"))


# Plot 1 ------------------------------------------------------------------

gg_df <- 
  reshaped_simulations %>%
  group_by(ATE) %>%
  summarize(`Significant for one group but not the other` = mean(xor(significant_CATE_0, significant_CATE_1)),
            `Difference in subgroup effects is significant` = mean(significant_interaction)) %>%
  gather(condition, power, -ATE)

ggplot(gg_df, aes(ATE, power, color = condition)) +
  geom_point() +
  geom_line() +
  geom_label(data = (. %>% filter(ATE == 0.2)),
             aes(label = condition),
             nudge_y = 0.02,
             family = "Palatino") +
  dd_theme() +
  scale_color_manual(values = c("red", "blue")) +
  theme(legend.position = "none") +
  labs(
    x = "True constant effect size",
    y = "Probability of result (akin to statistical power)"
  )
```

```{r}
report_lower_p_value <- function(data){
  fit_nocov <- lm_robust(Y ~ Z, data)
  fit_cov <- lm_robust(Y ~ Z + X, data)
  
  # select fit with lower p.value on Z
  if(fit_cov$p.value[2] < fit_nocov$p.value[2]){
    fit_selected <- fit_cov
  } else {
    fit_selected <- fit_nocov
  }
  fit_selected %>% tidy %>% filter(term == "Z")
}

design <-
  declare_population(    
    N = 100, X = rbinom(N, 1, 0.5), u = rnorm(N)
  ) + 
  declare_potential_outcomes(Y ~ 0.25 * Z + 10 * X + u) + 
  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + 
  declare_assignment(prob = 0.5) + 
  declare_reveal(Y, Z) + 
  declare_estimator(Y ~ Z, model = lm_robust, label = "nocov", estimand = "ATE") + 
  declare_estimator(Y ~ Z, model = lm_robust, label = "cov", estimand = "ATE") + 
  declare_estimator(
    handler = label_estimator(report_lower_p_value),
    label = "select-lower-p-value",
    estimand = "ATE") 

diags <- diagnose_design(design, sims = sims)
```

```{r, echo = FALSE}
kable(get_diagnosands(diags))
```

```{r}
bivariate_correlation_decision <- function(data) {
  fit <- lm_robust(y2 ~ y1, data) %>% tidy %>% filter(term == "y1")
  tibble(decision = fit$p.value <= 0.05)
}

interacted_correlation_decision <- function(data) {
  fit <- lm_robust(y2 ~ y1 + x, data) %>% tidy %>% filter(term == "y1")
  tibble(decision = fit$p.value <= 0.05)
}

robustness_check_decision <- function(data) {
  main_analysis <- bivariate_correlation_decision(data)
  robustness_check <- interacted_correlation_decision(data)
  tibble(decision = main_analysis$decision == TRUE & robustness_check$decision == TRUE)
}

robustness_checks_design <- 
  declare_population(
    N = 100,
    x = rnorm(N),
    y1 = rnorm(N),
    y2 = 0.15 * y1 + 0.01 * x + rnorm(N)
  ) +
  declare_estimand(y1_y2_are_related = TRUE) + 
  declare_estimator(handler = label_estimator(bivariate_correlation_decision), label = "bivariate") + 
  declare_estimator(handler = label_estimator(robustness_check_decision), label = "robustness-check")

decision_diagnosis <- declare_diagnosands(correct = mean(decision == estimand), keep_defaults = FALSE)

diag <- diagnose_design(robustness_checks_design, sims = sims, diagnosands = decision_diagnosis)
```

We evaluate the two answer strategies in terms of the rate of correctly deciding there is a correlation between `y2` and `y1`. In the main analysis, this means we judge there is a correlation when the p-value is below $0.05$. In our robustness check answer strategy, we decide there is a correlation when both the main analysis and the robustness check return p-values below $0.05$ on the coefficient on `y1`. We see that we are more likely to correctly judge there is a correlation in the simpler analysis strategy. This is because we added an additional criterion to our decision; both criteria, due to random noise, sometimes fail to reject the null of no correlation. Our second answer strategy is more robust in the sense that we have stronger evidence of a correlation when we run the two analyses together. But we are also less likely to decide (correctly) that there is a relationship. The robustness check is conservative. This exercise highlights that the properties of an answer strategy with secondary analyses will be different than the properties of the main analysis alone. If we planned (or conducted) robustness checks, we may wish to know how good the pair of strategies is together.

<!--
```{r}
robustness_checks_design <-
  robustness_checks_design +
  declare_estimator(handler = label_estimator(interacted_correlation_decision), label = "interacted")

robustness_checks_design_dgp2 <- replace_step(
  robustness_checks_design,
  step = 1,
  new_step = 
    declare_population(
      N = 100,
      x = rnorm(N),
      y1 = rnorm(N),
      y2 = 0.15 * y1 + 0.01 * x + 0.05 * y1 * x + rnorm(N)
    )
)

robustness_checks_design_dgp3 <- replace_step(
  robustness_checks_design,
  step = 1,
  new_step = 
    declare_population(
      N = 100,
      x = rnorm(N),
      y1 = 0.15 * x + rnorm(N),
      y2 = 0.15 * x + rnorm(N)
    )
)

robustness_checks_design_dgp3 <- replace_step(
  robustness_checks_design_dgp3, 
  step = 2,
  new_step = declare_estimand(y1_y2_are_related = FALSE)
)

decision_diagnosis <- declare_diagnosands(correct = mean(decision == estimand), keep_defaults = FALSE)

diag <- diagnose_design(
  robustness_checks_design, robustness_checks_design_dgp2, robustness_checks_design_dgp3, 
  sims = sims, diagnosands = decision_diagnosis)
```
-->
