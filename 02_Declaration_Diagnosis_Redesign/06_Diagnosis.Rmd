---
title: "Diagnosis"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# Diagnosis {#p2diagnosis}

<!-- make sure to rename the section title below -->

```{r diagnosis, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- TRUE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
library(ggforce)
library(ggridges)
```

Research design diagnosis is the process of evaluating the properties of a research design. Since "property of a research design" is a cumbersome phrase, we made up the word "diagnosand" to refer to those properties of a research design we would like to diagnose. Many diagnosands are familiar. Power is the probability of obtaining a statistically significant result. Bias is the average deviation of estimates from the true value of the estimand. Other diagnosands are more exotic, like the Type-S error rate, which is the probability the estimate has the incorrect sign, conditional on being statistically significant (Gelman and Carlin 2014).

<!-- snip -->

Research designs are strong when the empirical answer $a^d$ generated by the design is close to the true answer $a^w$. Since we can never know $a^w$, we have to assess whether the distribution of $a^d$ over possible realizations of the research design is close to the distribution of $a^m$, the answer under the model. How can we assess how close the distributions are? The distribution of $a^m$ is easy to simulate. Given a theoretical model, we can easily ask a computer to generate the distribution of the estimand. More often than not, the distribution of $a^M$ is degenerate -- in a fixed population, for example, most theoretical models posit that estimands like the Average Treatment Effect have just one value.  

The distribution of $a^d$ is trickier to estimate. The *actual* research design will only be implemented once, so we don't get to see the distribution of actual answers that *could have* eventuated from the application of D and A to the world. To solve this problem, we make a small -- but extremely consequential -- substitution of $m$ for $w$ in the DAG of a research design. Swapping $m$ in for $w$, we can ask the computer to simulate the distribution of $a^d$ *conditional on the model*. If the model is wrong, the simulated distribution of $a^d$ will be wrong too -- as ever, garbage in, garbage out. Figure YY shows the DAG we use to simulate research designs. When we simulate designs, $d$ is affected by $m$ (a realization of a theoretical model), rather than by $w$. This makes sense, since the computer simulations can be entirely untethered from reality.

Design diagnosis is the process of simulating both $I(m) = a^m$ and $A(d) = a^d$ over many draws from $M()$ and $D(m)$, and comparing them. The specific comparisons we make are called "diagnostic statistics."

A "diagnostic statistic" is a function $g$ of $a^d$, the answer given the data, of $a^m$, the answer given the model, or of both answers. This function might be really simple, like the identity function, $g(a^d)=a^d$, or the difference between the two answers: $g(a^m, a^d) = a^d - a^m$. Because $a^m$ and $a^d$ are random variables, and any function of random variables is also a random variable, any diagnostic statisitic is also a random variable. A diagnosand is a summary of that random variable. 
<!-- If $a^m$ and $a^d$ are vector-valued (e.g., when answers provide a prediction for each observation), one might have a vector of diagnostic statistics, such as the vector of differences between each element of  $a^m$ and $a^d$, or a scalar valued summary, such as a multidimensional distance measure.  In some cases $a^m$ and $a^d$ might have different dimensions; for instance $a^d$ may be the lower and upper bounds on a confidence interval and the diagnostic statistic is "Is $a^m$ within $a^d$?" -->

$$
\phi = f(g(a^m, a^d))
$$

The $f()$ is a statistical functional that summarizes the random variable. For example, the expectation function $E[X]$ summarizes the random variable $X$ with its expectation, the mean, while the variance function summarizes the expectation of the squared deviation of a random variable from its mean.<!-- Diagnosands are summaries of diagnostic statics.--> We'll use the Greek letter $\phi$ to describe the idea of a diagnosand in general.

Let's back up a moment to work through some concrete examples of common diagnosands (see section \@ref(diagnosands) for a more exhaustive list). Consider the diagnosand "bias." Bias is the average difference between the estimand and the estimate. Under a model that has two potential outcomes, a treated potential outcome and an untreated potential outcome, the inquiry might be the average treatment effect (the difference in the two potential outcomes averaged over units in the population or sample, abbreviated as ATE). Under a single realization $m$ of the model $M$, the value of the ATE will be a particular number, which we call $a^m$. If our data strategy is simply to collect data on those who come to be treated versus those who don't (i.e., we do not use random assignment), and our answer strategy is difference-in-means, our answer $a^d$ could be systematically different from $a^m$. The diagnostic statistic is the error $a^d - a^m$; this error is a random variable because each draw of $m$ from $M$ is slightly different. The expectation of this random variable is $E[a^d - a^m]$, or the value of the bias diagnosand.

Answer strategies commonly rely on measures of uncertainty like $p$-values, standard errors, and confidence intervals in order to make decisions about how to interpret $a^d$. Any measure estimated from the data to make a decision about $a^d$ can be used as a diagnostic statistic and summarized as a diagnosand. Like bias, statistical power is an expectation, this time of the diagnostic statistic $\mathbb{1}(p \leq 0.05)$, an indicator function that equals 1 if the $p$-value is no greater than 0.05 and 0 otherwise. Power describes how frequently (under beliefs about the model) a research design would return a statistically significant result. A standard error provides another diagnostic statistic whose expectation provides the expected standard error diagnosand, for example. This can be especially informative in comparison with another diagnosand, $\sqrt{\V(a^d)}$, the actual standard deviation of the estimates generated by the model.

Some diagnosands can be calculated analytically. It can be straightforward to calculate $\sqrt{\V(a^d)}$ if you have a two-arm experiment and are willing to make a lot of simplifying assumptions.^[Insert formula 3.4 and cite to FEDAI.] Most diagnosands for even moderately complex designs, however, require Monte Carlo computer simulation. A main purpose of the `DeclareDesign` software package is making the simulation step easier.

In practice, it is important to diagnose a design under multiple possible $M$'s, given our fundamental uncertainty about the world $w$. We do not know the precise distributions of exogenous variables or the exact functional forms of potential outcomes (e.g., we do not know the true effect size). Diagnosis, therefore, should typically involve simulating the properties of a fixed set of inquiries, data strategies, and answer strategies under *multiple* likely models. A $D$ and $A$ for a given $I$ that provide good diagnosand values under multiple $M$'s can be said to be robust to multiple models. 

```{r, echo=FALSE, fig.cap="MIDA as a DAG", fig.width = 6.5, fig.height = 3}
dag <-
  dagify(aw ~ w + I,
         m ~ M,
         am ~ m + I,
         d ~ D + m,
         ad ~ A + d,
         w ~ W)

dag_base <- tidy_dagitty(dag) %>%
  select(name, direction, to, circular) %>%
  as_tibble

nodes_df <-
  tibble(
    name = c("M", "I", "D", "A", "m", "am", "aw", "d", "ad", "w", "W"),
    label = c("M", "I", "D", "A", "m", "a<sup>m</sup>", "a<sup>w</sup>", "d", "a<sup>d</sup>", "w", "W"),
    long_label = c("Theoretical<br>causal model", "Inquiry", "Data<br>strategy", "Answer<br>strategy", "Model<br>draw", "Theoretical<br>answer", "True answer", "Simulated<br>data", "Simulated<br>answer", "Real<br>world", "True<br>causal model"),
    lbl_direction = c("N", "N", "N", "N", "S", "S", "S", "S", "S", "S", "N"),
    x = c(1, 2, 5.5, 6.5, 1, 2, 4.25, 5.5, 6.5, 3.25, 3.25),
    y = c(3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3)
  )

endnodes_df <-
  nodes_df %>%
  transmute(to = name, xend = x, yend = y)

gg_df <-
  dag_base %>%
  left_join(nodes_df, by = "name") %>%
  left_join(endnodes_df, by = "to")

gg_df <-
  gg_df %>%
  mutate(arced1 = (name == "m" & to == "d"),
         arced2 = (name == "I" & to == "aw")) %>%
  arrange(name)

rect_df <-
  tibble(
    xmin = c(.4, 4.9),
    xmax = c(2.6, 7.1),
    ymin = c(1.15, 1.15),
    ymax = c(3.85, 3.85)
  )

g <-
  ggplot(data = filter(gg_df, !arced1 & !arced2), aes(
    x = x,
    y = y,
    xend = xend,
    yend = yend
  )) +
  geom_point(color = gray(.1), fill = NA, size = 15, stroke = 0.5, pch = 1) +
  geom_dag_edges(edge_width = 0.35) +
  geom_dag_edges_arc(data = filter(gg_df, arced1), curvature = -0.575, edge_width = 0.35) +
  geom_dag_edges_arc(data = filter(gg_df, arced2), curvature = .7, edge_width = 0.35) +
  geom_richtext(color = "black",
                parse = TRUE,
                aes(label = label),
                fill = NA,
                label.color = NA,
                label.padding = grid::unit(rep(0, 4), "pt"),
                size = 4) +
  geom_richtext(
    aes(y = y + if_else(lbl_direction == "N", 0.4, -0.4),
        vjust = if_else(lbl_direction == "N", "bottom", "top"),
        label = long_label),
    color = gray(0.5),
    parse = TRUE,
    fill = NA,
    label.color = NA,
    label.padding = grid::unit(rep(0, 4), "pt"),
    size = 4) +
  coord_fixed(ylim = c(0.5, 4)) + 
  geom_rect(data = rect_df, aes(x = NULL, y = NULL, 
                                xend = NULL, yend = NULL,
                                xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax),
            alpha = 0.15) +
  annotate("text", x = 1.5, y = 4.05, label = "Theory") +
  annotate("text", x = 6, y = 4.05, label = "Simulation") +
  annotate("text", x = 3.75, y = 4.05, label = "Reality") +
  # annotate("text", x = 3, y = 1.6, label = "Truth") +
  theme_dag()
g
```

<!-- snip -->


## Estimating diagnosands analytically

Diagnosis can be be done with analytic, pencil-and-paper methods. Indeed, research design textbooks often contain many formulas for calculating power under a variety of designs. For example, Gerber and Green include the following power formula:

They write: 

> "To illustrate a power analysis, consider a completely randomized experiment where $N>2$ of $N$ units are selected into a binary treatment. The researcher must now make assumptions about the distributions of outcomes for treatment and for control units. In this example, the researcher assumes that the control group has a normally distributed outcome with mean $\mu_c$, the treatment group has a normally distributed outcome with mean $\mu_t$, and both group's outcomes have a standard deviation $\sigma$. The researcher must also choose $\alpha$, the desired level of statistical significance (typically 0.05).
Under this scenario, there exists a simple asymptotic approximation for the power of the experiment (assuming that the significance test is two-tailed):
> $$
\beta = \Phi \bigg(\frac{|\mu_t - \mu_c| \sqrt{N}}{2\sigma} - \Phi^{-1} (1 - \frac{\alpha}{2}) \bigg)
> $$
where $\beta$ is the statistical power of the experiment, $\Phi(\cdot)$ is the normal cumulative distribution function (CDF), and $\Phi^{-1}(\cdot)$ is the inverse of the normal CDF."

This power formula makes **detailed** assumptions about $M$, $D$, and $A$? Under $M$, it assumes that both potential outcomes are normally distributed with group specific means and a common variance. Under $D$, it assumes a particular randomization strategy (simple random assignment). Under $A$, it assumes a particular hypothesis testing approach (equal variance $t$-test with $N - 2$ degrees of freedom). This set of assumptions may be "close enough" in many research settings, but it can be difficult to understand the specific impacts of different beliefs about $M$, $D$ and $A$ on the value of the diagnosand. What if instead of being normally distributed, the potential outcomes are measured in 1 - 5 Likert scales? What if the randomization procedure includes blocking? What if we include covariates in our treatment effect estimation approach? Formulas for some large sources of design variation have been derived (such as clustering), but certainly not for every design variant.

Very quickly, hope for analytic design diagnosis fades. The analytic formulas are abstractions -- they abstract away from design details and sometimes those design details are important. This problem is not confined to the "power" diagnosand. In randomized experiments, claims about the bias diagnosand are quite general. Many randomized designs are unbiased for the ATE, but not all. Designs that encounter noncompliance, attrition, or some forms of spillover may not be unbiased for the ATE. Even without any of those complications, cluster randomized trials with heteogeneous cluster sizes are not unbiased (joel, imai). 

Diagnosands depend on design details, because how you conduct your study matters for its properties. That means design diagnosis must be design-aware. Since designs are so heteogenous and can vary on so many dimensions, computer simulation is only feasible way to diagnose anything beyond the simplest ideal-type designs.

## Estimating diagnosands via simulation

Research design diagnosis usually occurs in a two-step, simulation-based procedure. First we simulate research designs over and cover, collecting "diagnostic statistics" from each run of the simulation. Second, we summarise the distribution of the diganostic statistics in order to estimate the diagnosands.

So we estimate diagnosands by summarizing the distribution of diagnostic statistics -- of course this raises the question: what is a diagnostic statistic? We take a draw from the model ($m$) and calculate the value of the inquiry $I(m) = a^m$. We take one draw from the data strategy ($D(m) = d$), and calculates the value of the answer strategy $A(d) = a^d$. A diagnostic statistic is some function of $a^m$ and $a^d$.

A simple diagnostic statistic is "error," or the difference between the estimate and the estimand: $error = a^d - a^m$. The bias diagnosand is the expectation of the error statistic $E[error]$ over all possible ways the study could have come out.

Usually, we consider many diagnostic statistics at the same time. Here's a design declaration for a two-arm trial with a balanced (50/50) design. We have 100 subjects and their responses to treatment are drawn from a normal distribution with mean 0.1 and sd 0.1.   

```{r, echo=FALSE}
design <-
  declare_population(N = 100, 
                     Tau = rnorm(N, mean = 0.1, sd = 0.1),
                     U = rnorm(N, 0, 0.2)) +
  declare_potential_outcomes(Y ~  Tau * Z + U) + 
  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +
  declare_assignment(prob = 0.5) +
  declare_estimator(Y ~ Z, estimand = "ATE")
```

One draw of this simulation returns the following:

```{r, eval = FALSE, echo = FALSE}
run_design(design)
```

One draw from the simulation returns the following:

| estimand| estimate| std.error| conf.low| conf.high| p.value|
|--------:|--------:|---------:|--------:|---------:|-------:|
|     0.10|     0.08|      0.04|    0.003|     0.156|    0.04|

Figure \@ref(simdrawsone) shows the information we might obtain from a single run of the simulation. The filled point is the estimate $a^d$. The open triangle is the estimand $a^m$. The bell-shaped curve is our normal-approximation based estimate of the sampling distribution. The standard deviation of this estimated distribution is our estimated standard error, which expresses our uncertainty. The confidence interval around the estimate is another expression of our uncertainty: We're not sure where $a^d$ is, but if things are going according to plan, confidence intervals constructed this way will bracket $a^d$ 95\% of the time. 

```{r, echo=FALSE}
design <-
  declare_population(N = 100, U = rnorm(N, 0, 0.2)) +
  declare_potential_outcomes(Y ~ rnorm(N, mean = 0.1, sd = 0.1) * Z + U) + 
  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +
  declare_assignment() +
  declare_estimator(Y ~ Z, estimand = "ATE", model = lm_robust)

# diagnose_design(design, sims = 250)

set.seed(343)
simulations <- 
  simulate_design(design, sims = 10)   %>%
  mutate(significant = p.value <= 0.05)


gg_df <-
  simulations %>%
  group_by(sim_ID) %>%
  nest %>%
  mutate(densities = lapply(data, function(df) {
    with(df, tibble(
      x = seq(-2, 2, 0.0001),
      density = dnorm(x, mean = estimate, sd = std.error)
    ))
  })) %>%
  unnest(cols = c(data, densities)) %>%
  group_by(sim_ID)


gg_df <- 
  gg_df %>%
  mutate(
    
    low_cut = case_when(
      estimate < 0 ~ x < -2*abs(estimate),
      estimate > 0 ~ x <= 0),
    
    high_cut = case_when(estimate < 0 ~ x >= 0,
                         estimate > 0 ~ x >= 2*abs(estimate) ),
    
    area_under = case_when(low_cut ~ "low",
                           high_cut ~ "high",
                           TRUE ~ "middle")
  ) 
```


```{r simdrawsone, fig.cap = "Visualization of one draw from a design diagnosis.", fig.height = 1.5, fig.width = 5, echo = FALSE}
simulation_1 <- simulations %>% filter(sim_ID == 1)

simulation_1 %>% 
  select(estimand, estimate, std.error, conf.low, conf.high, p.value) %>%
  knitr::kable(digits = 3)

gg_df_1 <- gg_df %>% filter(sim_ID == 1)

ggplot(simulation_1, aes(x = estimate, y = as.factor(sim_ID)), color = dd_light_blue) +
  # geom_rect(xmin = -10, xmax = 0, ymin = -100, ymax = 100, fill = gray(0.96), size = 0) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high),
                 height = 0.075,
                 size = 0.25,
                 color = gray(0.5)) +
  geom_point(size = 1.5, color = gray(0.5)) +
  geom_point(
    aes(x = estimand),
    size = 1.5,
    fill = NA,
    color = gray(0.5),
    pch = 24,
    position = position_nudge(y = -0.1)
  ) +
  # first plot the density for p-values
  geom_ridgeline(
    data = gg_df_1,
    size = 0,
    aes(x = x, height = density, fill = area_under),
    scale = 0.05,
    min_height = 0.01
  ) +
  # then the density lines (to make it plot correctly and not be cut off)
  geom_ridgeline(
    data = gg_df_1,
    size = 0.1,
    aes(x = x, height = density, fill = NA),
    scale = 0.05,
    min_height = 0.01,
    color = dd_light_blue,
  ) +
  theme_void() +
  theme(
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    panel.grid = element_blank(),
    legend.position = "none"
  ) +
  coord_cartesian(xlim = c(-0.1, 0.25)) +
  geom_vline(
    xintercept = 0,
    color = gray(0.5),
    linetype = "dashed",
    alpha = 0.5
  ) +
  # geom_vline(
  #   xintercept = 0.1,
  #   color = dd_light_blue,
  #   linetype = "dotted",
  #   alpha = 0.5
  # ) +
  scale_fill_manual(name = "Probability",
                    values = c(dd_light_blue_alpha, dd_light_blue_alpha, NA)) +
  labs(x = "Estimated sampling distribution (from point estimate and standard error)")
```

<!-- We're interested in a few diagnosands. Bias (the expectation of the error statistics) is one, but we also want to know about RMSE (the square root of the expectation of the squared errors), power (the probability the $p$-value is below a threshold), and coverage (the probability the 95% confidence interval brackets the estimand).  -->

From this single draw, we can't yet estimate diagnosands, but we can estimate diagnostic statistics. The estimate was higher than the estimand in this draw, so the error is 0.10 - 0.08 = 0.02. Likewise, the squared error is `(0.10 - 0.08)^2 = 0.0004`. The $p$-value is 0.04, which is just barely lower than the threshold of 0.05, so "statistical significance" diagnostic statistic is equal to `TRUE`. The confidence interval stretches from 0.003 to 0.156, and the value of the estimand (0.10) is between those bounds, so the "covers" diagnostic statistic is equal to `TRUE` as well.

Learning the **distribution** of diagnostic statistics is the main barrier to design diagnosis. If we could simply write down the distribution of diagnostic statistics, it would be a straightforward matter to summarize them in order to calculate diagnosands. But the distributions of diagnositic statistics depend on the complex of information in all four parts of a research design: M, I, D, and A. For example, the error statistic depends on both $a^d$ and $a^m$, so the details of each matter greatly. 

To calculate the distributions of the diagnostic statistics, we have to simulate designs not just once, but many many times over. The bias diagnosand is the average error over many runs of the simulation. The statistical power diagnosand is the fraction of runs in which the estimate is significant. The coverage diagnosand is the fraction of runs in which the confidence interval covers the estimand.

This figure visualizes just 10 runs of the simulation (obtained with `simulate_design(design)`). We can see that in each run, $a^m$ is a little different. This might seem counterintuitive -- isn't the estimand supposed to be a fixed number? Some estimands are fixed, others are stochastic, depending on the specifics of the model. Notice how in the design declaration, we drew the potential outcomes from a distribution rather then having them be fixed numbers. This choice incorportates some of our modeling uncertainty. The treatment effects for each unit are close to 0.1, but we're not sure how close for each particulat unit. We can also see that some of the draws produce statistical significant estimates (the shaded areas are small and the confidence intervals don't overlap zero), but not all. We get a sense of the *true* standard error by seeing how the point estimates bounce around. We get a feel for the difference between the estimates of the standard error and true standard error. Design diagnosis is the process of learning about all the ways the study might come out, not just the one way that it will.

```{r simdrawsten, fig.cap = "Visualization of ten draws from a design diagnosis.", fig.height = 3.5, fig.width = 5, echo = FALSE}
ggplot(simulations, aes(x = estimate, y = as.factor(sim_ID)), color = dd_light_blue) +
  # geom_rect(xmin = -10, xmax = 0, ymin = -100, ymax = 100, fill = gray(0.96), size = 0) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high),
                 height = 0.075,
                 size = 0.25,
                 color = gray(0.5)) +
  geom_point(size = 1.5, color = gray(0.5)) +
  geom_point(
    aes(x = estimand),
    size = 1.5,
    fill = NA,
    color = gray(0.5),
    pch = 24,
    position = position_nudge(y = -0.1)
  ) +
  # first plot the density for p-values
  geom_ridgeline(
    data = gg_df,
    size = 0,
    aes(x = x, height = density, fill = area_under),
    scale = 0.05,
    min_height = 0.01
  ) +
  # then the density lines (to make it plot correctly and not be cut off)
  geom_ridgeline(
    data = gg_df,
    size = 0.1,
    aes(x = x, height = density, fill = NA),
    scale = 0.05,
    min_height = 0.01,
    color = dd_light_blue,
  ) +
  theme_void() +
  theme(
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    panel.grid = element_blank(),
    legend.position = "none"
  ) +
  coord_cartesian(xlim = c(-0.1, 0.25)) +
  geom_vline(
    xintercept = 0,
    color = gray(0.5),
    linetype = "dashed",
    alpha = 0.5
  ) +
  geom_vline(
    xintercept = 0.1,
    color = dd_light_blue,
    linetype = "dotted",
    alpha = 0.5
  ) +
  scale_fill_manual(name = "Probability",
                    values = c(dd_light_blue_alpha, dd_light_blue_alpha, NA)) +
  labs(x = "Estimated sampling distribution (from point estimate and standard error)")
```


This line of code does it all in one.  We simulate the design 1000 times to calculate the diagnositic satistics, then we summarise them in terms of bias, the true standard error (the standard deviation of the sampling distribution), RMSE, power, and coverage.

```{r, eval=FALSE}
diagnosis <- diagnose_design(design, sims = 1000, 
                             diagnosands = declare_diagnosands(
                               select = c("bias", "sd_estimate", "rmse", "power", "coverage")))
```



```{r, eval = do_diagnosis & !exists("do_bookdown"), echo=FALSE}
diagnosis <- diagnose_design(design, sims = sims, 
                             diagnosands = declare_diagnosands(
                               select = c("bias", "sd_estimate", "rmse", "power", "coverage")))
```


```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("diagnosis"), "/diagnosis.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnosis, path = rds_file_path)
}
diagnosis <- read_rds(rds_file_path)
```

```{r}
diagnosis
```

<!-- ## Interactive element -->

<!-- You can have fun building up a design diagnosis run-by-run with this shiny app. Each time you run the design, it adds a new estimate to the sampling distribution so you can learn about the relationship of a single run of the design to the overall diagnosis. -->

<!-- ```{r, echo = FALSE} -->
<!-- knitr::include_app('https://gblair.shinyapps.io/diagnosis/', height = '400px') -->
<!-- ``` -->


### Interpreting diagnosands

Say you decide to simulate the design 1000 times to see if it's biased. After a few seconds, the computer spits out bias of .02. Unsure whether to be concerned, you simulate the design another 1000 times. Now the computer says the bias is .01. Since each simulation was randomly generated, you should expect to get estimates of bias that vary. Given the random variation in the simulations and how large the estimate and estimand are, maybe it was even quite likely to get .01 or .02 bias even if the true bias is zero. Or maybe your design is biased.

As with frequentist statistics, we have to separate signal from noise. We have an estimate (here, of the bias diagnosand) that varies depending on how each simulation turns out. We want to quantify that variation in order to make a decision about whether the estimated bias is large enough to be significant. The simulation standard error (or, Monte Carlo standard error) is an estimate of the variation created by the simulation procedure. If that estimated variation is high relative to the estimated diagnosand, then we need to increase the number of simulations. 
<!-- Following the example from the previous paragraph, if the simulation standard error was .015, then the simulation we ran was consistent with negative, positive, and no bias given the point estimate of .01. However, if we estimate a simulation standard error of .0001, then the .01 or .02 bias estimates give real cause for concern, since they were so unlikely to arise if the true bias were zero.  -->

@morrisetal2019 provide a range of helpful formulas to estimate simulation standard errors of diagnosands. Define $a^d_i$ as the point estimate in the $i$'th simulation, $n_{sim}$ as the total number of simulations, and the average of the point estimates across simulations as $\bar{a^d} = \frac{1}{n_{sim}}\sum a^d_i$. Then, if the estimand is a constant and $a^d_i$ is independently and normally distributed, the simulation standard error for bias can be calculated as $\sqrt{1/(n_{sim}(n_{sim} - 1)) \sum^{n_{sim}}_{i = 1} (a^d_i - \bar{a^d})^2}$. Here is how you can do it in code:


```{r}
simulated_estimates <- with(diagnosis$simulations_df, estimate)
mean_estimate <- mean(simulated_estimates)
n_sim <- length(simulated_estimates)
SE_bias <- sqrt(1/(n_sim * (n_sim - 1)) * (sum((simulated_estimates - mean_estimate)^2)))
SE_bias
```

Of course, this estimate of the variance relies on assumptions and won't work as soon as the estimand is not a constant or the estimates are not normally distributed. Rather than using analytic formulas, we recommend using an approach called nonparametric bootstrapping to estimate the simulation standard errors. Nonparametric bootstrapping can be used to estimate the standard error of any diagnosand whose diagnostic statistic is independently and identically distributed, so you don't have to limit yourself to the classic diagnosands for which we have a formula. Nonparametric bootstrapping is quite simple to do: you randomly resample your $n_{sims}$ diagnostic statistics with replacement a large number of times and re-estimate the diagnosand on each resampled collection of diagnostic statistics. The standard deviation of the resulting distribution of diagnosand estimates gives you an estimate of the simulation standard error. `DeclareDesign` does nonparametric bootstrapping by default whenever the `diagnose_design()` function is called. The numbers are reported in the parentheses under the diagnosand estimates in the table above. Here's the simulation standard error for bias calculated by bootstrap resampling 100 times:

```{r}
diagnosis$diagnosands_df %>% pull("se(bias)")
```

As you can see, the nonparametric bootstrap gets us very close to the correct answer given by the analytic formula, without making any assumptions about the shape of the distribution of diagnostic statistics.

### How many simulations to run

Unlike increasing sample size in the real world, increasing the precision of your diagnosand estimates is as simple as waiting a little longer while your computer runs a higher number of simulations. However, while running 100 simulations may take a matter of seconds, running 10,000,000 simulations may take a few days. So how do you know when you've done enough?

As a rule of thumb, you should be multiplying the simulation standard errors by two in your head in order to make decisions about whether to do more simulations, since this gets you close to the upper and lower bounds of a 95\% confidence interval (properly calculated by muliplying the standard error by 1.96). So, if you change your randomization strategy and the power increases from .78 to .82 with a simulation standard error of .015, you should increase the number of simulations: the upper bound on the first estimate $(.78 + 0.015*1.96 = .81)$ overlaps with the lower bound on the second $(.82 - 0.015*1.96)$, so the apparent improvement in power may be an artefact of the simulation.

## How to choose among diagnosands {#diagnosands}

A diagnostic statistic is a summary function of $a^m$ and $a^d$, and a diagnosand is a summary of diagnostic-statistics. As a result, there are a great many to choose from. In Table \@ref(tab:diagnosticstatistics), we introduce a set of diagnostic statistics (far from complete!), and in Table \@ref(tab:diagnosands) a set of diagnosands including those commonly and less commonly considered.

| Diagnostic-statistic                                    | Definition                                                                   |
|---------------------------------------------------------|------------------------------------------------------------------------------|
| Estimate                                                | $a^d$                                                                        |
| Estimand under the model                                | $a^m$                                                                        |
| True estimand                                           | $a^w$                                                                        |
| $p$-value                                               | $p \equiv \Pr_{M_0} \{\mid A - a^{m_0}\mid~\geq~\mid a^d - a^{m_0}\mid \}$   |
| $p$-value is no greater than $\alpha$                   | $\mathbf{I}(p \leq \alpha)$                                                  |
| Confidence interval                                     | $\mathrm{CI}_{1-\alpha}$                                                     |
| Confidence interval covers the estimand under the model | $\mathrm{covers}^{a^m} \equiv \mathbb{I}\{ a^m \in \mathrm{CI}_{1-\alpha} \}$|
| Estimated standard error                                | $\widehat\sigma(A)$                                                          |
| Cost                                                    | $\mathrm{cost}$                                                              |
| Proportion of subjects harmed                           | $\Pr(\mathrm{harm}) \equiv \frac{1}{n} \sum_i \mathrm{harm_i}$

Table: (\#tab:diagnosticstatistics) Diagnostic statistics.

| Diagnosand                             | Description                                                                                                                         | Definition                                                            |
|----------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------|
| Average estimate                       |                                                                                                                                     | $\E(a^d)$                                                             |
| Average estimand                       |                                                                                                                                     | $\E(a^m)$                                                             |
| Power                                  | Probability of rejecting null hypothesis of no effect                                                                               | $\E(I(p \leq \alpha))$                                                |
| Bias                                   | Expected difference between estimate and estimand                                                                                   | $\E(a^d - a^m)$                                                       |
| Variance                               |                                                                                                                                     | $\V(a^d)$                                                             |
| True standard error                    |                                                                                                                                     | $\sqrt{\V(a^d)}$                                                      |
| Average estimated standard error       |                                                                                                                                     | $\widehat\sigma(A)$                                                   |
| RMSE                                   | Root mean-squared-error                                                                                                             | $\sqrt{\E(a^d - a^m)}$                                                |
| Coverage                               | Probability confidence interval overlaps estimand                                                                                   | $\Pr(\mathrm{covers}^{a^m})$                                          |
| Bias‐eliminated coverage               | Probability confidence interval overlaps average estimate (@morrisetal2019)                                                         | $\Pr(\mathrm{covers}^{a^d})$                                          |
| Type S error rate                      | Probability estimate has incorrect sign, if statistically significant (Gelman and Carlin 2014)                                      | $\Pr(\mathrm{sgn}(a^d) \neq \mathrm{sgn}(a^m) \mid p \leq \alpha)$    |
| Exaggeration ratio                     | Expected ratio of absolute value of estimate to estimand, if statistically significant (Gelman and Carlin 2014)                     | $\E( a^d / a_m \mid p \leq \alpha)$                                   |
| Type I error                           |                                                                                                                                     | $\Pr(p \leq \alpha \mid a^m = a^0)$                                   |
| Type II error                          |                                                                                                                                     | $\Pr(p \geq \alpha \mid a^m \neq a^0)$                                |
| Sampling bias                          | Expected difference between population average treatment effect and sample average treatment effect \(Imai, King, and Stuart 2008\) | $\E(a^m_{\mathrm sample} - a^m_{\mathrm population})$                 |
| Maximum possible cost                  |                                                                                                                                     | $\max{cost}$                                                          |
| Bayesian learning                      |                                                                                                                                     | $a^m_{\mathrm post} - a^m_{\mathrm pre}$                              |
| Value for money                        | Probability that a decision based on estimated effect yields net benefits                                                           |                                                                       |
| Success                                |                                                                                                                                     |                                                                       |
| Minimum detectable effect (MDE)        |                                                                                                                                     | $\mathrm{argmin}_{a^m_*} \Pr(p \leq \alpha) = 0.8$                    |
| Robustness                             | Joint probability of rejecting the null hypothesis across multiple tests                                                            |                                                                       |
| Maximum proportion of subjects harmed  |                                                                                                                                     | $\min{\Pr(\mathrm{harm})}$                                            |

Table: (\#tab:diagnosands) Diagnosands.

Not all of these diagnosands are relevant for every study. For example, in a descriptive study whose goal is to estimate the fraction of people in France who are left-handed, statistical power is irrelevant. A hypothesis test against the null hypothesis that 0 percent of the people in France are left-handed is preposterous. We know for sure that the fraction is not zero, we just don't know its precise value. A much more important diagnosand for this study would be RMSE (root-mean-squared-error), which is a measure of how well-estimated the estimand is that incorporates both bias and variance.

Often, we need to look at several diagnosands in order to understand what might be going wrong. If your design exhibits "undercoverage" (e.g., your coverage is less than $1 - \alpha$), that might be because your standard errors are too small (under-estimation of the variance in the sampling distribution) or because your point estimate is biased, or some combination of the two. In really perverse instances, you might have a biased point estimate which, thanks to overly-wide confidence intervals, just happens to get covered 95\% of the time. So, when assessing coverage it's important to look not only at point estimate bias, but you should also check that the average estimated standard error lines up with the true standard error. Alternatively, you could look at the "bias-eliminated coverage," which assesses coverage purely in terms of confidence interval width, ignoring bias.

Many research design decisions involve trading off bias and variance. In trade-off settings, we may need to accept higher variance in order to decrease bias. Likewise, we may need to accept a bit of bias in order to achieve lower variance. The tradeoff is captured by mean-squared error -- which is the average squared distance between $a^d$ and $a^m$. Of course we would ideally like to have as low a mean-squared error as possible. We would like to achieve low variance and low bias simultaneously.

To illustrate, consider the following three designs as represented by three targets. The inquiry is the bullseye of the target. The data and answer strategies combine to generate a process by which arrows are shot towards the target. On the left, we have a very bad archer: even though the estimates are unbiased in the sense that they hit the bullseye "on average", very few of the arrows are on target. In the middle, we have the Katniss Everdeen (the heroine of the Hunger Games novels who is good with a bow) of data and answer strategies: they are both on target and low variance. On the right, we have an archer who is very consistent (low variance) but biased. The mean squared error is highest on the left and lowest in the middle. 

```{r, echo = FALSE, fig.width = 6.5, fig.height = 3}
n <- 50

points_df <- tibble(rho = sqrt(runif(n)),
                    theta = runif(n, 0, 2*pi),
                    x = rho * cos(theta),
                    y = rho * sin(theta)
)

summary_df <- points_df %>% 
  summarize(
    bias = round(mean(sqrt(x^2 + y^2)), 2),
    mse = round(mean(x^2 + y^2), 2),
    var = round(mean((x - mean(x))^2 + (y - mean(y))^2), 3)
  )

unbiased_lowprecision <- ggplot() + 
  geom_circle(
    data = tibble(
      x0 = rep(0, 5),
      y0 = rep(0, 5),
      r = rev(seq(0.1, 1, length.out = 5))
    ), 
    aes(x0 = x0, y0 = y0, r = r, fill = fct_rev(as.factor(r))),
    col = gray(0.25), lwd = 0.25, alpha = 0.5) +
  geom_point(data = points_df, aes(x, y), size = 0.5) + 
  scale_fill_manual(values = rev(c("yellow", "red", "cyan", "black", "white"))) + 
  coord_fixed() + 
  dd_theme() + 
  xlab("") + ylab("") + 
  ggtitle("Unbiased, imprecise", subtitle = glue::glue("Variance = {summary_df$var}\nBias = {summary_df$bias}\nRMSE = {summary_df$mse}")) + 
  theme(
    legend.position = "none",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank()
  )

n <- 50

points_df <- tibble(
  rho = sqrt(runif(n, min = 0, max = 0.015)),
  theta = runif(n, 0, 2*pi),
  x = rho * cos(theta),
  y = rho * sin(theta)
)

summary_df <- points_df %>% 
  summarize(
    bias = round(mean(sqrt(x^2 + y^2)), 2),
    mse = round(mean(x^2 + y^2), 2),
    var = round(mean((x - mean(x))^2 + (y - mean(y))^2), 3)
  )

unbiased_highprecision <- ggplot() + 
  geom_circle(
    data = tibble(
      x0 = rep(0, 5),
      y0 = rep(0, 5),
      r = rev(seq(0.1, 1, length.out = 5))
    ), 
    aes(x0 = x0, y0 = y0, r = r, fill = fct_rev(as.factor(r))),
    col = gray(0.25), lwd = 0.25, alpha = 0.5) +
  geom_point(data = points_df, aes(x, y), size = 0.5) + 
  scale_fill_manual(values = rev(c("yellow", "red", "cyan", "black", "white"))) + 
  coord_fixed() + 
  dd_theme() + 
  xlab("") + ylab("") + 
  ggtitle("Unbiased, imprecise", subtitle = glue::glue("Variance = {summary_df$var}\nBias = {summary_df$bias}\nRMSE = {summary_df$mse}")) + 
  theme(
    legend.position = "none",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank()
  )

n <- 50

points_df <- tibble(
  rho = sqrt(runif(n, min = 0, max = 0.015)),
  theta = runif(n, 0, 2*pi),
  x = rho * cos(theta) + 0.5,
  y = rho * sin(theta) - 0.2
)

summary_df <- points_df %>% 
  summarize(
    bias = round(mean(sqrt(x^2 + y^2)), 2),
    mse = round(mean(x^2 + y^2), 2),
    var = round(mean((x - mean(x))^2 + (y - mean(y))^2), 3)
  )

biased_highprecision <- ggplot() + 
  geom_circle(
    data = tibble(
      x0 = rep(0, 5),
      y0 = rep(0, 5),
      r = rev(seq(0.1, 1, length.out = 5))
    ), 
    aes(x0 = x0, y0 = y0, r = r, fill = fct_rev(as.factor(r))),
    col = gray(0.25), lwd = 0.25, alpha = 0.5) +
  geom_point(data = points_df, aes(x, y), size = 0.5) + 
  scale_fill_manual(values = rev(c("yellow", "red", "cyan", "black", "white"))) + 
  coord_fixed() + 
  dd_theme() + 
  xlab("") + ylab("") + 
  ggtitle("Unbiased, imprecise", subtitle = glue::glue("Variance = {summary_df$var}\nBias = {summary_df$bias}\nRMSE = {summary_df$mse}")) + 
  theme(
    legend.position = "none",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank()
  )

library(patchwork)

unbiased_lowprecision + unbiased_highprecision + biased_highprecision
```

The archery metaphor is common in research design textbooks because it effectively conveys the difference between variance and bias, but it does elide an important point. It really matters **which target** your archer is shooting at. Figure XXX shows a bizarre double-target representing two inquiries. The empirical strategy is unbiased and precise for the left inquiry, but it is clearly biased for the right inquiry. When we are describing the properties of $a^d$, we have to be clear about which $a^a$ they are associated with.


```{r, echo=FALSE}
n <- 50

points_df <- tibble(
  rho = sqrt(runif(n, min = 0, max = 0.015)),
  theta = runif(n, 0, 2*pi),
  x = rho * cos(theta),
  y = rho * sin(theta)
)

summary_df <- points_df %>% 
  summarize(
    bias_1 = round(mean(sqrt(x^2 + y^2)), 2),
    mse_1 = round(mean(x^2 + y^2), 2),
    var_1 = round(mean((x - mean(x))^2 + (y - mean(y))^2), 3),
    
    bias_2 = round(mean(sqrt((x-0.25)^2 + y^2)), 2),
    mse_2 = round(mean((x-0.25)^2 + y^2), 2),
    var_2 = round(mean(((x-0.25) - mean(x-0.25))^2 + (y - mean(y))^2), 3)
  )

circle_df <- 
  bind_rows(
    "left" = tibble(
      x0 = rep(0, 5),
      y0 = rep(0, 5),
      r = rev(seq(0.1, 1, length.out = 5))
    ),
    "right" = tibble(
      x0 = rep(0.35, 5),
      y0 = rep(0, 5),
      r = rev(seq(0.1, 1, length.out = 5))
    ),
    .id = "side"
  ) %>% 
  arrange(desc(r), side)

dual_estimands <- ggplot() + 
  geom_circle(
    data = circle_df, 
    aes(x0 = x0, y0 = y0, r = r, fill = fct_rev(as.factor(r))),
    col = gray(0.25), lwd = 0.25) +
  geom_point(data = points_df, aes(x, y), size = 0.5) + 
  scale_fill_manual(values = rev(c("yellow", "red", "cyan", "black", "white"))) + 
  coord_fixed() + 
  dd_theme() + 
  xlab("") + ylab("") + 
  ggtitle("Diagnosands depend on the estimand", subtitle = glue::glue("Variance = {summary_df$var_1} (left), {summary_df$var_2} (right)\nBias = {summary_df$bias_1} (left), {summary_df$bias_2} (right)\nRMSE = {summary_df$mse_1} (left), {summary_df$mse_2} (right)")) + 
  theme(
    legend.position = "none",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank()
  )
dual_estimands
```

RMSE is an exactly equal weighting of variance and bias. Yet many other weightings of these two diagnosands are possible, and different researchers will vary in their weightings. Those weights may also depend on the research question a researcher is studying, their career stage, the strength of priors, the size of the effects, and other features. 

In evaluating a research design diagnosis, what we need to know is a researcher's weighting of all relevant diagnosands. You can think of this as your utility unction. Your utility function includes how important it is to you to study big questions, to shift beliefs in a research field, to overturn established findings, to obtain unbiased answers, and to get the sign right. Your utility function evaluated for a given design will yield a utility and these can be compared across designs (this is the process of redesign, described in detail in the next section).

We often consider the diagnosand power on its own. This diagnosand is the probability of getting a statistically significant result, which of course depends on many things about your design including, crucially, the unknown magnitude of the parameter to be estimated. You can think of statistical power as the probability of a success, where success is defined as getting a significant results. The conventional power target is 80\% power. One could imagine redefining statistical power as "null risk," or the probability of obtaining a null result. In these terms, the conventional power target is a 20\% null risk, or a one in five chance of "failure." Those odds aren't great, so we recommend designing studies with lower null risk. But considering power alone is also misleading: no researcher wants to design a study that is 80\% powered but which returns highly biased estimates that are 2-3x the true estimate. Another way of saying this is that researchers always carry about power and bias. How much they care about each feature determines the weight of power and bias in their utility function.

Diagnosands need not be about hypothesis testing or even statistical analysis of the data at all. We often tradeoff how much we learn from a research design with its cost in terms of money and our time. We have financial and time budgets that provide hard constraints to our designs, but we also at the margin many researchers wish to select cheaper (or shorter) designs in order to carry out more studies or finish their degree sooner. Time and cost are also diagnostic statistics! We may wish to explore the maximum cost of a study or the maximum amount of time it would take.

Ethical considerations also often enter the process of assessing research designs, if implicitly. We can explicitly incorporate them into our utility function by valuing minimizing harm and maximimizing the degree of informed consent requested of subjects. When collecting, researchers often believe that they face a tradeoff between informing subjects about the subject of the data collection (an ethical consideration, or a requirement of the IRB) on the one hand and the bias that comes from Hawthorne or demand effects. We can incorporate these considerations in a research design diagnosis by specifying diagnostic statistics related to the amount of disclosure about the purposeses of research or the number of subjects harmed in the research.

<!-- Suppose your study would be a success if it produced an estimate that is higher than 4, had a standard error of 1 or less, and yielded statistically significant evidence of treatment effect heterogeneity -- write down a diagnosand that is the probability of all three of those conditions being true at the same time. If a study is a failure (in terms of not having been worth the money and effort *ex post*) when the confidence interval is 20 points wide or wider, then the research team should design to minimize the probability of that occurance. -->

## Diagnosing with respect to variations in M

We are always uncertain about M -- if we were certain of M (or there was no real dispute about it), there would be no need to conduct new empirical research about it. Research design diagnosis can account for this uncertainty by evaluating the performace of the design under alternative models. We are unsure of the exact value of the intra-cluster correlation of outcomes we will encounter, so we simulate the variance of the estimator under a range of plausible ICC values. We are unsure of the true average treatment effect, so we simulate the power of the study over a range of plausible effect sizes. Uncertainty over model inputs like the means, variances, and covariances in data that will eventually be collect is a major reason to simulate under a range of plausible values. 

### Estimating the minimum detectable effect size




### Adjudicating between competing models

We can apply the same principle to *competing* models. Imagine that you believe $M_1$ is true but that your scholarly rival believes $M_2$. In the spirit of scientific progress, you design a study together. The design should (A) demonstrate $M_1$ is true if it is true and (B) demonstrate $M_2$ is true if *it* is true. In order to come agreement about the properties of the design, you will need to simulate the design under both models. 


## Further reading

- @Gelman2014b on Type M and Type S errors
- @herron2016careful on case selection / sampling bias
- @Baumgartner2019 and @rohlfing2018power on diagnosands in qualitative research
- @Rubin1984 on diagnosands in Bayesian research


<!-- ## Ethics -->
<!-- - There are ethical diagnosands, e.g., what is the distribution of ethical costs such as loss of autonomy. -->

<!-- - Cite Tara's paper. Cite Lauren's paper.  -->

<!-- - Do a diagnosis of how many minutes of subject time are wasted by audit experiments, weigh that against the cost of demonstrating outright racial bias. (cite an audit study that does this calculation) -->

<!-- ## grab bag -->

<!-- <!-- example of when there is a mismatch between Am and Ad --> -->

<!-- ```{r, echo = FALSE} -->
<!-- ATE <- 0.0 -->

<!-- design <-  -->
<!--   declare_population(N = 1000, -->
<!--                      binary_covariate = rbinom(N, 1, 0.5), -->
<!--                      normal_error = rnorm(N)) + -->
<!--   # crucial step in POs: effects are not heterogeneous -->
<!--   declare_potential_outcomes(Y ~ ATE * Z + normal_error) + -->
<!--   declare_assignment(prob = 0.5) + -->
<!--   declare_estimator(Y ~ Z, subset = (binary_covariate == 0), label = "CATE(0)") +  -->
<!--   declare_estimator(Y ~ Z, subset = (binary_covariate == 1), label = "CATE(1)") + -->
<!--   declare_estimator(Y ~ Z * binary_covariate,  -->
<!--                     model = lm_robust, term = "Z:binary_covariate", label = "Interaction") -->
<!-- ``` -->

<!-- ```{r, echo = FALSE, purl = FALSE} -->
<!-- # note this was rerun a bunch of times to get the right example (one is non sig the other is sig diff and diff-in-CATE is not diff from zero) -->
<!-- # estimates <- draw_estimates(design) -->
<!-- rds_file_path <- paste0(get_dropbox_path("answer_strategy"), "/estimates_diff_in_significance_plot.RDS") -->
<!-- # write_rds(estimates, path = rds_file_path) -->
<!-- estimates <- read_rds(rds_file_path) -->
<!-- ``` -->

<!-- ```{r, echo = FALSE, fig.height = 3} -->

<!-- g1 <- ggplot(data = estimates %>% filter(term == "Z"), aes(estimator_label, estimate)) +  -->
<!--   geom_point() +  -->
<!--   geom_errorbar(aes(x = estimator_label, ymin = conf.low, ymax = conf.high), width = 0.2) +  -->
<!--   ylab("Estimate (95% confidence interval)") + -->
<!--   geom_hline(yintercept = 0, lty = "dashed") + -->
<!--   ggtitle("Visualization A") + -->
<!--   dd_theme() +  -->
<!--   theme(axis.title.x = element_blank()) -->

<!-- g2 <- ggplot(data = estimates, aes(estimator_label, estimate)) +  -->
<!--   geom_point() +  -->
<!--   geom_errorbar(aes(x = estimator_label, ymin = conf.low, ymax = conf.high), width = 0.2) +  -->
<!--   ylab("Estimate (95% confidence interval)") + -->
<!--   geom_hline(yintercept = 0, lty = "dashed") + -->
<!--   ggtitle("Visualization B") + -->
<!--   dd_theme() +  -->
<!--   theme(axis.title.x = element_blank()) -->

<!-- g1 + g2 -->
<!-- ``` -->

<!-- We now demonstrate that the answer strategy on the left is flawed. XXYY describe sims. -->

<!-- ```{r, echo = FALSE} -->
<!-- # sweep across all ATEs from 0 to 0.5 -->
<!-- designs <- redesign(design, ATE = seq(0, 0.5, 0.05)) -->
<!-- ``` -->

<!-- ```{r, echo = FALSE, eval = do_diagnosis & !exists("do_bookdown")} -->
<!-- simulations_one_significant_not_other <- simulate_design(designs, sims = sims) -->
<!-- ``` -->

<!-- ```{r, echo = FALSE, purl = FALSE} -->
<!-- # figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file -->
<!-- rds_file_path <- paste0(get_dropbox_path("answer_strategy"), "/simulations_one_significant_not_other.RDS") -->
<!-- if (do_diagnosis & !exists("do_bookdown")) { -->
<!--   write_rds(simulations_one_significant_not_other, path = rds_file_path) -->
<!-- } -->
<!-- simulations_one_significant_not_other <- read_rds(rds_file_path) -->
<!-- ``` -->

<!-- ```{r, eval = FALSE, echo = FALSE, fig.height = 3.5} -->
<!-- # Summarize simulations --------------------------------------------------- -->

<!-- reshaped_simulations <- -->
<!--   simulations_one_significant_not_other %>% -->
<!--   transmute(ATE, -->
<!--             sim_ID, -->
<!--             estimator_label, -->
<!--             estimate, -->
<!--             conf.high, -->
<!--             conf.low, -->
<!--             significant = p.value < 0.05) %>% -->
<!--   pivot_wider(id_cols = c("ATE", "sim_ID"), names_from = "estimator_label", values_from = c("estimate", "conf.high", "conf.low", "significant")) -->


<!-- # Plot 1 ------------------------------------------------------------------ -->

<!-- gg_df <-  -->
<!--   reshaped_simulations %>% -->
<!--   group_by(ATE) %>% -->
<!--   summarize(`Significant for one group but not the other` = mean(xor(significant_CATE_0, significant_CATE_1)), -->
<!--             `Difference in subgroup effects is significant` = mean(significant_interaction)) %>% -->
<!--   gather(condition, power, -ATE) -->

<!-- ggplot(gg_df, aes(ATE, power, color = condition)) + -->
<!--   geom_point() + -->
<!--   geom_line() + -->
<!--   geom_label(data = (. %>% filter(ATE == 0.2)), -->
<!--              aes(label = condition), -->
<!--              nudge_y = 0.02, -->
<!--              family = "Palatino") + -->
<!--   dd_theme() + -->
<!--   scale_color_manual(values = c("red", "blue")) + -->
<!--   theme(legend.position = "none") + -->
<!--   labs( -->
<!--     x = "True constant effect size", -->
<!--     y = "Probability of result (akin to statistical power)" -->
<!--   ) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- report_lower_p_value <- function(data){ -->
<!--   fit_nocov <- lm_robust(Y ~ Z, data) -->
<!--   fit_cov <- lm_robust(Y ~ Z + X, data) -->

<!--   # select fit with lower p.value on Z -->
<!--   if(fit_cov$p.value[2] < fit_nocov$p.value[2]){ -->
<!--     fit_selected <- fit_cov -->
<!--   } else { -->
<!--     fit_selected <- fit_nocov -->
<!--   } -->
<!--   fit_selected %>% tidy %>% filter(term == "Z") -->
<!-- } -->

<!-- design <- -->
<!--   declare_population(     -->
<!--     N = 100, X = rbinom(N, 1, 0.5), u = rnorm(N) -->
<!--   ) +  -->
<!--   declare_potential_outcomes(Y ~ 0.25 * Z + 10 * X + u) +  -->
<!--   declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +  -->
<!--   declare_assignment(prob = 0.5) +  -->
<!--   declare_reveal(Y, Z) +  -->
<!--   declare_estimator(Y ~ Z, model = lm_robust, label = "nocov", estimand = "ATE") +  -->
<!--   declare_estimator(Y ~ Z, model = lm_robust, label = "cov", estimand = "ATE") +  -->
<!--   declare_estimator( -->
<!--     handler = label_estimator(report_lower_p_value), -->
<!--     label = "select-lower-p-value", -->
<!--     estimand = "ATE")  -->

<!-- diags <- diagnose_design(design, sims = sims) -->
<!-- ``` -->

<!-- ```{r, echo = FALSE} -->
<!-- kable(get_diagnosands(diags)) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- bivariate_correlation_decision <- function(data) { -->
<!--   fit <- lm_robust(y2 ~ y1, data) %>% tidy %>% filter(term == "y1") -->
<!--   tibble(decision = fit$p.value <= 0.05) -->
<!-- } -->

<!-- interacted_correlation_decision <- function(data) { -->
<!--   fit <- lm_robust(y2 ~ y1 + x, data) %>% tidy %>% filter(term == "y1") -->
<!--   tibble(decision = fit$p.value <= 0.05) -->
<!-- } -->

<!-- robustness_check_decision <- function(data) { -->
<!--   main_analysis <- bivariate_correlation_decision(data) -->
<!--   robustness_check <- interacted_correlation_decision(data) -->
<!--   tibble(decision = main_analysis$decision == TRUE & robustness_check$decision == TRUE) -->
<!-- } -->

<!-- robustness_checks_design <-  -->
<!--   declare_population( -->
<!--     N = 100, -->
<!--     x = rnorm(N), -->
<!--     y1 = rnorm(N), -->
<!--     y2 = 0.15 * y1 + 0.01 * x + rnorm(N) -->
<!--   ) + -->
<!--   declare_estimand(y1_y2_are_related = TRUE) +  -->
<!--   declare_estimator(handler = label_estimator(bivariate_correlation_decision), label = "bivariate") +  -->
<!--   declare_estimator(handler = label_estimator(robustness_check_decision), label = "robustness-check") -->

<!-- decision_diagnosis <- declare_diagnosands(correct = mean(decision == estimand), keep_defaults = FALSE) -->

<!-- diag <- diagnose_design(robustness_checks_design, sims = sims, diagnosands = decision_diagnosis) -->
<!-- ``` -->

<!-- We evaluate the two answer strategies in terms of the rate of correctly deciding there is a correlation between `y2` and `y1`. In the main analysis, this means we judge there is a correlation when the p-value is below $0.05$. In our robustness check answer strategy, we decide there is a correlation when both the main analysis and the robustness check return p-values below $0.05$ on the coefficient on `y1`. We see that we are more likely to correctly judge there is a correlation in the simpler analysis strategy. This is because we added an additional criterion to our decision; both criteria, due to random noise, sometimes fail to reject the null of no correlation. Our second answer strategy is more robust in the sense that we have stronger evidence of a correlation when we run the two analyses together. But we are also less likely to decide (correctly) that there is a relationship. The robustness check is conservative. This exercise highlights that the properties of an answer strategy with secondary analyses will be different than the properties of the main analysis alone. If we planned (or conducted) robustness checks, we may wish to know how good the pair of strategies is together. -->

<!--
```{r, eval = FALSE}
robustness_checks_design <-
  robustness_checks_design +
  declare_estimator(handler = label_estimator(interacted_correlation_decision), label = "interacted")

robustness_checks_design_dgp2 <- replace_step(
  robustness_checks_design,
  step = 1,
  new_step = 
    declare_population(
      N = 100,
      x = rnorm(N),
      y1 = rnorm(N),
      y2 = 0.15 * y1 + 0.01 * x + 0.05 * y1 * x + rnorm(N)
    )
)

robustness_checks_design_dgp3 <- replace_step(
  robustness_checks_design,
  step = 1,
  new_step = 
    declare_population(
      N = 100,
      x = rnorm(N),
      y1 = 0.15 * x + rnorm(N),
      y2 = 0.15 * x + rnorm(N)
    )
)

robustness_checks_design_dgp3 <- replace_step(
  robustness_checks_design_dgp3, 
  step = 2,
  new_step = declare_estimand(y1_y2_are_related = FALSE)
)

decision_diagnosis <- declare_diagnosands(correct = mean(decision == estimand), keep_defaults = FALSE)

diag <- diagnose_design(
  robustness_checks_design, robustness_checks_design_dgp2, robustness_checks_design_dgp3, 
  sims = sims, diagnosands = decision_diagnosis)
```
-->
