---
title: "Choosing an answer strategy"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
  html_document:
    toc: yes
    toc_depth: 2
bibliography: ../bib/book.bib
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# Choosing an answer strategy

<!-- make sure to rename the section title below -->

```{r choosing_an_answer_strategy, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 10
b_sims <- 10
```

Answer strategies $A$ are the full set of procedures you use to map the data generated by the data strategy ($d$) to the eventual estimate $a^d$. Just like we can write $I(m) = a^m$, we can write $A(d) = a^a$. The mapping of data to estimates through the answer strategy is to all empirical research, regardless of whether the inquiry is causal or descriptive, whether the data strategy is observational or experimental, and whether the answer strategy itself is fundamentally qualitative or quantitative. A research design needs at least one answer strategy for each "atomic" inquiry, though in principle, one answer strategy could target multiple inquiries and one inquiry can be targeted by multiple answer strategies. 

A simple answer strategy for a randomized trial is the difference-in-means estimator. The difference-in-means answer strategy aims at the average treatment effect inquiry. Another answer strategy is the average answer to a survey question about support for a politician among 500 respondents from a random sample of the country, which answers the inquiry ``what is the average support for the politician in the country.''

An answer strategy is a function that produces that an estimate from data. For quantitative research designs, this includes the literal function executed by the computer to analyze the data (like `lm_robust` in R or `reg` in Stata), but also includes other choices made along the way from raw data to the estimate reported in the paper. The choices include data cleaning procedures, inclusion and exclusion criteria, and the routes through the garden of forking paths researchers follow depending on how the study turns out. For some qualitative research designs, the answer strategy might not include a computer function but is nevertheless the function that maps the information gathered by the researcher to the analysis of it they give in words. It is a function in the sense that if the data were different, the analysis would be different as well. 

The idea that answer strategies are \textit{procedures} has implications both for how we design research but also for how we evaluate it. The full procedure includes the processes used to select final estimators, the ancillary analyses used to build confidence in the main estimates (i.e., robustness checks), and the visual and prose descriptions of the study results. When the data strategy does not go according to plan (for example, when participants do not comply with assigned treatments or respond to survey questions), the set of compensating adjustments to the data analysis are also part of the answer strategy. If the answer strategy depends on how the data turn out, we need to assess the if-then analysis procedure under different possible realizations of the data to know if the procedure is a good one.

Answer strategies commonly include measures of uncertainty. When answer strategies are decisions, we can express our uncertainty about the decision as a probability it was the correct one. When answer strategies produce point estimates of parameters, we can express our uncertainty as our understanding of the sampling distribution (usually summarized by its standard deviation, which goes by the special name "standard error."). The uncertainty measures for point estimates are often used as the basis for significance tests. Significance tests and point estimators usually refer to different inquiries. For example, if my inquiry is the ATE, my difference-in-means estimator gives me a point estimate and a standard error to characterize my uncertainty about that estiamte. The significance test regards a different inquiry from the ATE itself. The inquiry for the (null hypothesis) significance test is: is the ATE equal to zero? The possible values of $a^m$ are `NO` and `YES` and the possible values of $a^d$ are `NO` and `I don't know`, which leads to quite a bit of confusion about hypothesis testing.

<!-- I think this may be an orphan from changes from above? -->
<!-- The answer strategy provides an answer to the inquiry about the model using data that result from the data strategy. But as we have seen, the choice of answer strategy is also intimately connected to our causal model of the world, to the inquiry about the model, and to our chosen data strategy. Our model helps guide our choices about what variables to control for --- and which controls would lead to bias. Our inquiry can shape our choices about which subsets of the data to analyze and how to weight them. And our data strategy can shape the estimators and standard errors we use in our answer strategy depending on how we stratified samples and treatment assignments. -->

How should you choose an answer strategy? A good answer strategy is one that generates an answer $a^d$ that is close to $a^m$ -- ideally identical. This standard is only so useful, because now we need to ask how we can keep $a^d$ close to $a^m$? This question has no answer in general, because the specifics of the answer strategy depend strongly on the details of the model, the inquiry, and the data strategy. 

That said, inspiration for an all-purpose approach to finding a good answer stratgey comes from the "plug-in principle" (See XXX, also Aronow and Miller for a good introduction). In estimation theory, the plug-in principle reflects how you can often construct estimators from the mathematical definitions of estimands simply by replacing expectations and varaiances with sample analogues. That is, the function that defines the estimand (the inquiry) is analogous to the function that defines the estimator (the answer strategy). The plug-in principle is behind the difference-in-means estimator of the ATE. The ATE estimand is written:

$$
ATE = E[Y_i(1) - Y_i(0)] = E[Y_i(1)] - E[Y_i(0)]
$$

The difference-in-means estimator just replaces the expectations with sums, where the first 1 through $m$ subjects are assigned to treatment and the remainder to control. As it happens, this estimator is unbiased for the ATE estimand when the data strategy samples from $Y_i(1)$ and $Y_i(0)$ at random.

$$
\mathrm{DIM} = \frac{\sum_1^m Y_i(1)}{m} - \frac{\sum_{m + 1}^N Y_i(0)}{N-m}
$$

The symmetry between the definition of the ATE and the definition of the difference-in-means operator is not an accident -- it's an instance of the plug in princple at work. The plug-in-principle points the way to how we should choose answer strategies. In order to keep $a^d$ close to $a^m$, we should keep $A(d)$ as close to $I(m)$ as is possible. 







<!-- The model influences the answer strategy through inquiry of course, but also guiding our choices about what variables to control for and which to leave uncontrolled. The model shapes our choices about which subsets of the data to analyze and how to weight them. The model also conditions the data strategy can shape the estimators and standard errors we use in our answer strategy depending on how we stratified samples and treatment assignments. -->



<!-- Consider a qualtitive research design (cite and gloss VESLA's PORTALS PROJECT). The answer strategy is not just "describe common themes in the interview transcripts." It involves manual coding of many discovered features of the transcripts, then counting the frequency of those features, then describing those features within an analytic framework with the best examples from the corpus. If the interviews had come out differently (i.e. if $d$ had been different), then every step in the process would be different as well. -->

## Example: psych three papers study with mediation

- discuss the descriptive, mediation, etc. along with meta-mida

## Large Classes of Answer Strategies

<!-- MH: Are there distinct classes of answer strategies we might point to, that might all be used for the same I e.g. -->
<!-- * point valued estimates -->
<!-- * bounds -->
<!-- * posterior distributions -->
<!-- * test results -->

In this section, we describe four large classes of answer strategies.

### Testing
Tests are an elemental kind of answer strategy. Tests yield yes / no answers to specific tests. In some qualitative traditions, hoop tests, straw-in-the-wind tests, smoking-gun tests, and doubly-decisive tests are common. These tests are procedures for making analysis decisions in a structured way. In frequentist statistics, significance tests are used to make a decision whether to reject or fail to reject a particular null hypothesis. Null hypothesis significance testing has developed a (rightfully) sketchy reputation in recent years as too much weight has been put on the reject / fail to reject decision. That said, sometimes that decision is rightfully important and in such cases, null hypothesis significance testing is a great tool.



[cite erin and naoki on sign tests]
[cite erin on equivalence testinf]



### Point estimation

Point estimation is possibly the most common class of answer strategy in quantitative social science. Point estimators are things like the difference-in-means, ordinary least squares regression, instrumental variables regression, random forests, the LASSO, ridge regression, BART... the list goes on and on. These are the data analysis tools taught in many graduate methods courses. What's incredible is that even though the population of estimators proliferates with each passing year, the number of kinds of inquiries they are useful for estimating stays relatively flat. When we do point estimatation, we are mainly interested in estimating averages, differences, variances, and conditional expectation functions of increasing dimensionality. There are of course many other kinds of estimands (like ratios and quantiles), but the main point is, most of the variation in how scholars conduct point estimation is in the answer strategy, not in the inquiry.

(point people to Hastie and Tibshirani)

### Bounds

- EV bounds
- trimming bounds
- is CI a bounding estimator?

### Posterior distributions


### Testing

Tests are answer strategies that return TRUE or FALSE as an answer. 

- significance testing (incl. sign test)
- equivalence testing
- straw in the wind / hoop test
- doubly decisive
- 1997 book on tests




## Answer Strategies as Procedures


Consider a randomized experiment that seeks to estimate the causal effect of a treatment. The answer strategy is not just "Logistic Regression with Covariate adjustment". It includes every step in the process that takes the raw data, cleans and recodes it, considers 5 alternative estimators (DIM, OLS with covariate adjustment, a fancy thing your colleague suggested but you couldn't get to converge), before finally settling on logit.

**Multiple estimates.** Answer strategies can account how many statistical tests you are conducting. Often, when generating an answer to a single inquiry, we may construct multiple estimates that provide different types of answers of varying quality. When you present the results from many null hypothesis tests, the rate of falsely rejecting at least one of those tests even when all are true goes up, due to the multiple comparisons problem. If you plan to adjust for this problem, those adjustments are part of your answer strategy, because they will typically adjust the p-values you report and the decisions readers make with them. We may have three survey items that imperfectly estimate a latent quantity. In presenting the results, we could present three estimates from three regressions, we could adjust the three estimates using a procedure such as a family-wise error rate correction, or we could average the three items together into an index and present one estimate from one regression. Which of these three methods we select will change the properties of our answer strategy. 

**Analysis procedures**. The final estimator that goes into a paper is neither the beginning nor the end of the answer strategy. Procedures, if any, by which you explore the data and determine a final set of estimates are part of the answer strategy. Procedures for summarizing multiple estimates are one example of many.

Commonly, the final estimator that is selected depended on a exploratory procedure in which multiple models are assessed, for example by comparing model fit statistics. The answer strategy of our research design is not to fit the final model --- is it this multiple step if-then procedure. These procedures may be part of a prespecified analysis plan or they may be informal, so it may sometimes only be possible to declare the full design after the data is obtained. (We may find that a different analysis procedure that was not data dependent would have been preferable, if we diagnose the design after the fact.) The reason to declare the procedure rather than the final estimator is that the diagnosis of the design may differ. The procedure may be more powerful, if for example we assessed multiple sets of covariate controls and selecting the specification with the lowest standard error of the estimate. But the procedure may also exhibit poor coverage, accounting for these multiple bites at the apple.

We also sometimes find that the model we planned to run to analyze the data cannot be estimated. In these cases, there is an iterative estimation procedure in which a first model is run, changes to the specification are made, and a second or third model is presented as the result. The full set of steps --- a decision tree, depending on what is estimable --- is the answer strategy and we can evaluate whether it is a good one not only under the realized data but under other possible realizations where the decision *tree* would be the same but the decisions different.

In fact, there are examples of analysis procedures in most types of research, quantitative or qualitative. Many strategies for causal inference with observational data involve not only an estimation strategy but a set of falsification or placebo tests. The answer provided by these research designs depends in a crucial way on the results of these tests: if the tests fail, the design provides no definitive answer. In qualitative research, process tracing involves a set of steps, the results of which depend on information gathered in earlier steps. Many mixed methods strategies are also multi-step procedures. Nested designs involve running a quantitative analysis and then selecting cases on the basis of predictions from the regression. These designs cannot be assessed by considering a single step of the procedure in isolation.
<!-- need cites -->


**When things do not go according to plan.** To compare answer strategies, you can imagine the estimators that are possible *if things go well* as well as *if things go wrong*, when there is missing data or there are outliers in variables. A good answer strategy, which might be a single estimator, or a procedure if-this-then-that, can handle both states of the world. Procedures for addressing deviations from expected analyses are part of the answer strategy. Even in the absence of a preanalysis plan, we often have a way we expect to analyze the data if things go well. When they do not --- because data are missing, there is noncompliance with an intervention, or the study is suspended for example --- the answers will change. These procedures determine the answer the study provides (or in some cases does not), so are part of the answer strategy. *Standard operating procedures* (lin and green) are documents that systematize these procedures in advance.



We demonstrate the fact that the properties of procedures differ from the properties of a design with the final estimator in a simple example. We compare two possible estimation specifications, with and without covariates, to a procedure in which we run both models and report the model in our paper that has the lower p-value. The models are exactly the same, but the properties of the *procedure* differ from the properties of either of the two possible models. In particular, the procedure has higher power than either of the two models, but it exhibits poor coverage, which means we have a bias in our measure of uncertainty. 

```{r}
report_lower_p_value <- function(data){
  fit_nocov <- lm_robust(Y ~ Z, data)
  fit_cov <- lm_robust(Y ~ Z + X, data)
  
  # select fit with lower p.value on Z
  if(fit_cov$p.value[2] < fit_nocov$p.value[2]){
    fit_selected <- fit_cov
  } else {
    fit_selected <- fit_nocov
  }
  fit_selected %>% tidy %>% filter(term == "Z")
}

design <-
  declare_population(    
    N = 100, X = rbinom(N, 1, 0.5), u = rnorm(N)
  ) + 
  declare_potential_outcomes(Y ~ 0.25 * Z + 10 * X + u) + 
  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) + 
  declare_assignment(prob = 0.5) + 
  declare_reveal(Y, Z) + 
  declare_estimator(Y ~ Z, model = lm_robust, label = "nocov", estimand = "ATE") + 
  declare_estimator(Y ~ Z, model = lm_robust, label = "cov", estimand = "ATE") + 
  declare_estimator(
    handler = label_estimator(report_lower_p_value),
    label = "select-lower-p-value",
    estimand = "ATE") 

diags <- diagnose_design(design, sims = sims)
```

```{r, echo = FALSE}
kable(get_diagnosands(diags))
```

## Robustness

**Robustness checks** are part of the answer strategy. Often, a single estimator is presented as the main analysis but then a series of alternative specifications are displayed in an appendix (such as including or excluding covariates and their interactions, different subsets of the data, or alternative statistical models). These differ from multiple estimates of a latent quantity in that the goal is not a primary analysis, but rather to support the main analysis. The purpose is to provide readers with evidence about how dependent the main results are on the specification, data subset, and statistical model used. The decision a reader makes from a paper depends not only on the main estimate but also the robustness checks. As a result, we want to assess the properties of the two together. 

[NOTE TO ADD HERE]: robustness checks are often more about $M$ than about $A$ -- we see how $a^d$ changes when we change $A$ in order see how our inferences change under alternative $M$s.

We illustrate with a simple analysis of the correlation between two variables `y1` and `y2`, who have a true positive correlation. `y2` is also a function of an observed covariate `x` and measurement error. Our main analysis is a bivariate regression predicting `y2` with `y1`. We compare this answer strategy to one in which we run that analysis, but also run a robustness check controlling for `x`. We do this because as the analyst we are unsure of the true DGP and wish to demonstrate to reviewer's that our results are not dependent on the functional form we choose. 

```{r}
bivariate_correlation_decision <- function(data) {
  fit <- lm_robust(y2 ~ y1, data) %>% tidy %>% filter(term == "y1")
  tibble(decision = fit$p.value <= 0.05)
}

interacted_correlation_decision <- function(data) {
  fit <- lm_robust(y2 ~ y1 + x, data) %>% tidy %>% filter(term == "y1")
  tibble(decision = fit$p.value <= 0.05)
}

robustness_check_decision <- function(data) {
  main_analysis <- bivariate_correlation_decision(data)
  robustness_check <- interacted_correlation_decision(data)
  tibble(decision = main_analysis$decision == TRUE & robustness_check$decision == TRUE)
}

robustness_checks_design <- 
  declare_population(
    N = 100,
    x = rnorm(N),
    y1 = rnorm(N),
    y2 = 0.15 * y1 + 0.01 * x + rnorm(N)
  ) +
  declare_estimand(y1_y2_are_related = TRUE) + 
  declare_estimator(handler = label_estimator(bivariate_correlation_decision), label = "bivariate") + 
  declare_estimator(handler = label_estimator(robustness_check_decision), label = "robustness-check")

decision_diagnosis <- declare_diagnosands(correct = mean(decision == estimand), keep_defaults = FALSE)

diag <- diagnose_design(robustness_checks_design, sims = sims, diagnosands = decision_diagnosis)
```

We evaluate the two answer strategies in terms of the rate of correctly deciding there is a correlation between `y2` and `y1`. In the main analysis, this means we judge there is a correlation when the p-value is below $0.05$. In our robustness check answer strategy, we decide there is a correlation when both the main analysis and the robustness check return p-values below $0.05$ on the coefficient on `y1`. We see that we are more likely to correctly judge there is a correlation in the simpler analysis strategy. This is because we added an additional criterion to our decision; both criteria, due to random noise, sometimes fail to reject the null of no correlation. Our second answer strategy is more robust in the sense that we have stronger evidence of a correlation when we run the two analyses together. But we are also less likely to decide (correctly) that there is a relationship. The robustness check is conservative. This exercise highlights that the properties of an answer strategy with secondary analyses will be different than the properties of the main analysis alone. If we planned (or conducted) robustness checks, we may wish to know how good the pair of strategies is together.

<!-- - distinguish this from changes to the model where we do robustnesss vis a vis a fixed answer and data strategy. i.e. two notions of "robustness". one is fix I D A and change M, is this "design" robust to changes in M. the other is, within a given run, is the estimate "robust" to changing the estimation procedure, so this is a diagnostic statistic. note I must be defined across these changes in M. -->

Using the MIDA way of thinking about designs, we discuss in the diagnosis section another notion of the "robustness" of a design. The typical way we think of robustness checks is multiple secondary analyses *conditional on the observed data* to build confidence in an analysis of that fixed data. However, the motivation for these robustness checks is uncertainty about the true data generating process. By declaring a design in terms of MIDA, we can think about the robustness of a *single* estimator to multiple possible true data generating processes. An estimator that is robust in this sense is one that is unbiased with low uncertainty regardless of, say, the true functional form between `y1` and `y2`. To determine whether an estimator is robust, we can redefine a set of designs with different functional forms and assess the rate of correct decisions of our robustness checks strategy under each different model. 

<!--
```{r}
robustness_checks_design <-
  robustness_checks_design +
  declare_estimator(handler = label_estimator(interacted_correlation_decision), label = "interacted")

robustness_checks_design_dgp2 <- replace_step(
  robustness_checks_design,
  step = 1,
  new_step = 
    declare_population(
      N = 100,
      x = rnorm(N),
      y1 = rnorm(N),
      y2 = 0.15 * y1 + 0.01 * x + 0.05 * y1 * x + rnorm(N)
    )
)

robustness_checks_design_dgp3 <- replace_step(
  robustness_checks_design,
  step = 1,
  new_step = 
    declare_population(
      N = 100,
      x = rnorm(N),
      y1 = 0.15 * x + rnorm(N),
      y2 = 0.15 * x + rnorm(N)
    )
)

robustness_checks_design_dgp3 <- replace_step(
  robustness_checks_design_dgp3, 
  step = 2,
  new_step = declare_estimand(y1_y2_are_related = FALSE)
)

decision_diagnosis <- declare_diagnosands(correct = mean(decision == estimand), keep_defaults = FALSE)

diag <- diagnose_design(
  robustness_checks_design, robustness_checks_design_dgp2, robustness_checks_design_dgp3, 
  sims = sims, diagnosands = decision_diagnosis)
```
-->

## Presentation of answer the answer strategy

How results are presented in tables and figures is an important part of the answer strategy. Considerable attention has been paid to how to display data, including arguments for switching from tables to graphs as the primary way to present statistical models (Kastellec) and for visually present raw data and models together (Coppock). The reason for this attention is that what inferences readers make when reading a paper depends not only on the statistical procedures used for estimation but the medium in which they are displayed. When numerical estimates are not provided at all, and only visualizations of results, clearly aesthetic choices about which estimates are displayed and even the width of axes will determine what the reader takes away from the answer strategy. This principle applies not just to tables and visualizations; how we describe results in the text of a paper also shapes readers' inferences from the data. Registered reports are a format for preparing scientific papers that involves prespecifying not only the form of graphs and tables but the text the author plans to write depending on the results. In short, decisions made from your results by readers are not just a function of numerical estimates but how they are presented.

We explore this claim by comparing two possible graphical displays of conditional avareage treatment effects in an experiment. A common presentational format is to present the average treatment effect in one group and then the other along with confidence intervals. Inferences are made --- either by the author, or by readers --- as a function of whether one is significant and not the other. If that is true, the inference is that there is a difference in CATEs. An alternative is to present the estimated difference along with the two effects. The inferences can then directly be based on whether the confidence interval of the difference crosses zero. We illustrate these two visual answer strategies below:

```{r, echo = FALSE}
ATE <- 0.0

design <- 
  declare_population(N = 1000,
                     binary_covariate = rbinom(N, 1, 0.5),
                     normal_error = rnorm(N)) +
  # crucial step in POs: effects are not heterogeneous
  declare_potential_outcomes(Y ~ ATE * Z + normal_error) +
  declare_assignment(prob = 0.5) +
  declare_estimator(Y ~ Z, subset = (binary_covariate == 0), label = "CATE(0)") + 
  declare_estimator(Y ~ Z, subset = (binary_covariate == 1), label = "CATE(1)") +
  declare_estimator(Y ~ Z * binary_covariate, 
                    model = lm_robust, term = "Z:binary_covariate", label = "Interaction")
```

```{r, echo = FALSE, purl = FALSE}
# note this was rerun a bunch of times to get the right example (one is non sig the other is sig diff and diff-in-CATE is not diff from zero)
# estimates <- draw_estimates(design)
rds_file_path <- paste0(get_dropbox_path("answer_strategy"), "/estimates_diff_in_significance_plot.RDS")
# write_rds(estimates, path = rds_file_path)
estimates <- read_rds(rds_file_path)
```

```{r, echo = FALSE, fig.height = 3}

g1 <- ggplot(data = estimates %>% filter(term == "Z"), aes(estimator_label, estimate)) + 
  geom_point() + 
  geom_errorbar(aes(x = estimator_label, ymin = conf.low, ymax = conf.high), width = 0.2) + 
  ylab("Estimate (95% confidence interval)") +
  geom_hline(yintercept = 0, lty = "dashed") +
  ggtitle("Visualization A") +
  dd_theme() + 
  theme(axis.title.x = element_blank())

g2 <- ggplot(data = estimates, aes(estimator_label, estimate)) + 
  geom_point() + 
  geom_errorbar(aes(x = estimator_label, ymin = conf.low, ymax = conf.high), width = 0.2) + 
  ylab("Estimate (95% confidence interval)") +
  geom_hline(yintercept = 0, lty = "dashed") +
  ggtitle("Visualization B") +
  dd_theme() + 
  theme(axis.title.x = element_blank())

g1 + g2
```

We now demonstrate that the answer strategy on the left is flawed. XXYY describe sims.

```{r, echo = FALSE}
# sweep across all ATEs from 0 to 0.5
designs <- redesign(design, ATE = seq(0, 0.5, 0.05))
```

```{r, echo = FALSE, eval = do_diagnosis & !exists("do_bookdown")}
simulations_one_significant_not_other <- simulate_design(designs, sims = sims)
```

```{r, echo = FALSE, purl = FALSE}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("answer_strategy"), "/simulations_one_significant_not_other.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(simulations_one_significant_not_other, path = rds_file_path)
}
simulations_one_significant_not_other <- read_rds(rds_file_path)
```

```{r, echo = FALSE, fig.height = 3.5}
# Summarize simulations ---------------------------------------------------

reshaped_simulations <-
  simulations_one_significant_not_other %>%
  transmute(ATE,
            sim_ID,
            estimator_label,
            estimate,
            conf.high,
            conf.low,
            significant = p.value < 0.05) %>%
  pivot_wider(id_cols = c("ATE", "sim_ID"), names_from = "estimator_label", values_from = c("estimate", "conf.high", "conf.low", "significant"))


# Plot 1 ------------------------------------------------------------------

gg_df <- 
  reshaped_simulations %>%
  group_by(ATE) %>%
  summarize(`Significant for one group but not the other` = mean(xor(significant_CATE_0, significant_CATE_1)),
            `Difference in subgroup effects is significant` = mean(significant_interaction)) %>%
  gather(condition, power, -ATE)

ggplot(gg_df, aes(ATE, power, color = condition)) +
  geom_point() +
  geom_line() +
  geom_label(data = (. %>% filter(ATE == 0.2)),
             aes(label = condition),
             nudge_y = 0.02,
             family = "Palatino") +
  dd_theme() +
  scale_color_manual(values = c("red", "blue")) +
  theme(legend.position = "none") +
  labs(
    x = "True constant effect size",
    y = "Probability of result (akin to statistical power)"
  )
```



<!-- - you can *select an answer strategy in advance*, by simulating data. when estimators are selected with the data in hand, choices are often made in response to the realized data through examining model fit statistics that appear ideal in the context of this data, but are not ideal from the perspective of other data that could have been collected. we want answer strategies that perform well no matter how the data turn out.  -->
<!-- - issues are: overfitting, selecting a suboptimal estimator from design perspective that passes a model fit statistic (show this is worse in a simple example) -->

## Grab Bag of things we don't know where they go

- How you already choose answer strategies in current practice
- talk about issue of model-based vs design based, as separated from the model you assume in M. in model based you run a procedure that assumes a dgp, which may or may not be connected to the M.
- your data strategy should shape your answer strategy (analyse as you randomize)
- Flexibility / overfitting? relationship to nonparametrics claim in 2.1?
- If you are a bayesian, you can diagnose w.r.t. the location and stregth of your priors.

Mediation Inquires:

Consider $X -> B <- A$ and $X -> A$. In this DAG $A$ is a mediator. There are three causal inquiries:

$X -> B$
$X -> A$
$A -> B$.

If we randomize $X$, we can learn about $X -> A$ but not $X -> B$, because $X -> B$ is confounded.  We obviously don't learn anything about $A -> B$ because for that we'd need to randomize $A$. Mediation analysis tries to get all three for the price of one random assignment. At best we get $X -> A$ and the total effect of X on B, which is not just $X -> B$, but instead $X -> B$ + $(X -> A) * (A -> B)$. Mediation analysis tries to get something for nothing.








<!-- ### Qualtitative Answer Strategies -->

<!-- [This section should focus on really bringing qual into the fold -- not sure how] -->

<!-- ### Quantitative Answer Strategies -->

<!-- The first choice in an answer strategy is to choose how you will construct your estimate for the inquiry. There are two established principles for selecting among possible estimation procedures, which apply both for qualitative and quantitative designs. The first is to draw on your model to learn what variables to select a conditioning strategy that avoids blocks back-door paths and does not condition on colliders. The second is to analyze your data according to your data strategy, accounting for how you stratified your data and the probabilities of sampling and assignment to avoid bias. A third principle highlighted by the MIDA structure is that your inquiry will also necessarily guide answer strategy choices. With an identical model and data strategy, the analysis strategy to target the sample average and the population average as inquiries would implies two different analysis strategies. -->

<!-- With an estimator in mind, the second task in choosing an answer strategy is describing uncertainty. In many cases, this will mean selecting an estimator for the standard error of the estimate and a method for constructing a confidence interval. The choice of standard error estimator is parallel to the choice of estimator: there is a true standard error of any estimator-estimand pair, and we want a standard error estimator with low bias and high precision. However, in trading off bias and variance in standard error estimators, we often tend to choose estimators that are conservative. Choosing a conservative estimator implies a scientific or moral preference for avoiding overconfidence in our estimates. The data strategy will often shape the choice of standard error estimator: how likely a given estimate is will be a function of features of the model, but also the probability of a given treatment assignment or sample.  -->

<!-- In addition to an estimate and measure of uncertainty in it, we often need to construct a hypothesis test about the estimate in order to provide an answer to an inquiry. If the inquiry is of the form, "is the proportion of voters who support the candidate greater than half" or "is the treatment effect positive", this implies a hypothesis test that provides a "yes" or "no" answer based on the data. The inquiry guides the choice of null hypothesis. Then a hypothesis testing framework, such as null-hypothesis testing or equivalence testing. The data strategy may also shape the choice of hypothesis testing method. Permutation inference a.k.a. randomization inference is a method for testing sharp null hypotheses implied by a particular sampling and/or treatment assignment procedure without relying on asymptotic inference.  -->

<!-- We declare an answer strategy with an estimator for the average treatment effect and its standard error and a method for constructing confidence intervals and a null hypothesis as follows, using linear regressions: -->


<!-- ```{r, eval = FALSE} -->
<!-- declare_estimator(Y ~ Z + covariate, model = lm, term = "Z", estimand = "ATE") -->
<!-- ``` -->

<!-- This declaration asserts a choice about the model (linear regression using the built-in `lm` function), the functional form (linear terms for the treatment variable and a covariate), selects the coefficient of interest that forms our estimate (`Z`), and links that estimate to an estimate ("ATE"). The result is: -->

<!-- ```{r, echo = FALSE} -->
<!-- declare_estimator(Y ~ Z + covariate, model = lm, term = "Z", estimand = "ATE")(simple_design_data) %>% kable(digits = 3) -->
<!-- ``` -->
