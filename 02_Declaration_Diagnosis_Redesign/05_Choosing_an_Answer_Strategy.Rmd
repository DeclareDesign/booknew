---
title: "Choosing an answer strategy"
output:
  html_document:
    toc: yes
    toc_depth: 4
  pdf_document:
    toc: yes
    toc_depth: '4'
bibliography: ../bib/book.bib
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# Choosing an answer strategy

<!-- make sure to rename the section title below -->

```{r choosing_an_answer_strategy, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 10
b_sims <- 10
```


Your answer strategy is your plan for what you will do with the information gathered from the world in order to generate an answer to the inquiry. Qualtitative and quantitative methods courses overflow with advice about which answer strategies to choose. Under what conditions should you use Ordinary Least Squares, when should you use logit? When is a machine learning algorithm the appropriate choice and when would a comparative case study be more informative? When is *no* answer strategy worth persuing because of the fundamental limitations of the data strategy? 

Our perspective on answer strategies is that they should be informed by the other three elements of research design: the model, the inquiry, and the data strategy. Sometimes answer strategy advice is offered on the on the basis of the realized data $d$ only, without any particular attention paid to important features of the question to be answered or the manner in which the data were collected and generated. For example, a bit of recieved methodological wisdom holds that whenever a dependent variable is binary, a binary choice statistical model like logit or probit must be used. This advice is based only the knowledge that $d$ contains a binary outcome; whether it is appropriate depends on features of $I$ and features of $D$. For example, if $I$ is the average treatment effect and $D$ includes randomization of the treatment, answer strategies beyond binary choice models may serve just as well or better. If a student asks, "which estimator should I use?" the response is always, "it depends." What does it depend on? The model, the inquiry, and the answer strategy.

Choosing "design-aware" answer strategies sounds straightforward enough, but the precise way in which this approach is applied in any real empricial setting will of course differ from case to case. Our highest-level advice is that we want to pick an answer strategy $A$ such that the answer it provides $A(d) = a^d$ is close to the answer under our model of the world, $I(m) = a^m$. The implication is that we should strive for parallelism across $A$ and $I$ -- this is the "plug-in principle." When the function $A$ is very close to the function $I$, then we can "plug in" $d$ for $m$. 

Because all answer strategies rely on assumptions. For descriptive inference, we have to assume that the sample represents the population well. For causal inference, we have to assume that treated and untreated units are similar in all other respects beyond treatment status. For all kinds of inference, we have to assume that our measurements are close to the latent construsts we wish to measure. Answer strategies are "model-based" when the most consequential of these assumptions are part of $M$. Answer strategies are "design-based" to the extent that we can pull the assumptions out of $M$ and assure them by design, i.e., by choosing $D$ in such a way that we can be confident that the assumption is true. Observational causal inference often relies on an assumption of "selection on observables," or the claim that within groups of units that have the same observed characteristics, treatments are as-if randomly assigned.  Observational causal inference is model-based in the sense that the selection on observables assumption is grounded in researchers' theoretical model of the world, which of course might be wrong. Experimental causal inference is "design-based" in the sense that we can be sure that treatments were *actually* randomly assigned because random assignment was part of the data. In both observational and experimental settings we need to make the same assumption, but in one case, we rely on a theoretical model and in the other, we rely on a data strategy. To the extent possible, we prefer assuring assumptions by design to asserting them on the basis of a theoretical model.

Answer strategies come in a number of varieties. The most familiar of these are point-estimators that produce estimates of parameters. Ordinary least squares, difference-in-means, logit, random forests, and a very long list of others are in the point-estimator answer strategy class. A second class is comprised by tests. Tests return a binary decision (True or False). Null hypothesis significance tests are a common form of test in quantitative research. Qualitative researchers also employ tests; these go by names like "hoop test" and "straw-in-the-wind" test. Bayesian answer strategies return full posterior distributions. In contrast to point-estimators some answer strategies are interval-estimators. 


```{r make-all-models, echo = FALSE}
possible_models <-
  expand.grid(
    XU = c("X<-U", "none"),
    YU = c("Y<-U", "none"),
    YX = c("Y->X", "Y<-X", "none"),
    DU = c("D<-U", "none"),
    DX = c("D->X", "D<-X", "none"),
    DY = c("D->Y", "D<-Y", "none"),
    stringsAsFactors = FALSE
  )

possible_models <-
  possible_models %>%
  rowwise() %>%
  mutate(
    var = str_c(XU, YU, YX, DU, DX, DY, sep = ";"),
    var = str_remove_all(var, "none;"),
    var = str_remove_all(var, "none"),
    dag = list(dagitty(paste0(
      "dag{", var, "; X; D; Y; U}"
    ))),
    acyclic = isAcyclic(dag),
    consistent_with_ignorability = !(DU == "D<-U" & YU == "Y<-U"),
    id_if_adjusted = isAdjustmentSet(dag, "X", exposure = "D", outcome = "Y"),
    id_if_unadjusted = isAdjustmentSet(dag, NULL, exposure = "D", outcome = "Y"),
    id_adjustment_fac = as.factor(
      case_when(
        id_if_unadjusted &
          id_if_adjusted ~ "Yes, regardless of conditioning",
        id_if_unadjusted &
          !id_if_adjusted ~ "Only when NOT conditioning on X",!id_if_unadjusted &
          id_if_adjusted ~ "Only when conditioning on X",!id_if_unadjusted &
          !id_if_adjusted ~ "No, regardless of conditioning"
      )
    ),
    consistent_with_RA = DU != "D<-U" &
      DX != "D<-X" & DY != "D<-Y",
    consistent_with_X_pretreatment =
      DX != "D->X" &
      YX != "Y->X" &
      !(DU == "D->U" & XU == "X<-U") &
      !(YU == "Y->U" & XU == "X<-U")
  )

nested_data <-
  possible_models %>%
  filter(var != "") %>%
  mutate(dag_data = list(as_tibble(tidy_dagitty(dag))))

points_df <-
  tibble(name = as.factor(c("D", "X", "Y", "U")),
         x = c(1, 2, 2, 1),
         y = c(1, 2, 1, 2))
ends <-
  points_df %>%
  rename(to = name,
         xend = x,
         yend = y)

fix_no_edges <-
  tibble(
    var = "no edges",
    acyclic = TRUE,
    consistent_with_RA = TRUE,
    consistent_with_X_pretreatment = TRUE,
    XU = "none",
    YU = "none",
    YX = "none",
    DU = "none",
    DX = "none",
    DY = "none",
    name = "X",
    id_adjustment_fac = "Yes, regardless of conditioning"
  )

gg_df <-
  nested_data %>%
  unnest(cols = dag_data) %>%
  select(-x, -y, -xend, -yend) %>%
  left_join(points_df) %>%
  left_join(ends) %>%
  bind_rows(fix_no_edges) %>%
  mutate(
    XU_fac = factor(
      XU,
      levels =  c("X<-U", "X->U", "none"),
      labels =  c("X %<-% U", "X %->% U", "X~~~U")
    ),
    YU_fac = factor(
      YU,
      levels =  c("Y<-U", "Y->U", "none"),
      labels =  c("Y %<-% U", "Y %->% U", "Y~~~U")
    ),
    YX_fac = factor(
      YX,
      levels =  c("Y<-X", "Y->X", "none"),
      labels =  c("Y %<-% X", "Y %->% X", "Y~~~X")
    ),
    DU_fac = factor(
      DU,
      levels =  c("D<-U", "D->U", "none"),
      labels =  c("D %<-% U", "D %->% U", "D~~~U")
    ),
    DX_fac = factor(
      DX,
      levels =  c("D<-X", "D->X", "none"),
      labels =  c("D %<-% X", "D %->% X", "D~~~X")
    ),
    DY_fac = factor(
      DY,
      levels =  c("D<-Y", "D->Y", "none"),
      labels =  c("D %<-% Y", "D %->% Y", "D~~~Y")
    )
  ) %>%
  arrange(XU_fac, YU_fac, YX_fac, DU_fac, DX_fac, DY_fac)

gg_df <-
  gg_df %>%
  mutate(
    var_fac = factor(var, levels = c(possible_models$var, "no edges")),
    DY_DX = as.factor(paste0(DY, " ", DX)),
    U_relationship = paste0(XU, " ", YU, " ", DU),
    U_relationship_fac =
      factor(
        U_relationship,
        levels = c(
          'X<-U Y<-U D<-U',
          'none Y<-U D<-U',
          'X<-U none D<-U',
          'X<-U Y<-U none',
          'none none D<-U',
          'none Y<-U none',
          'X<-U none none',
          'none none none'
        ),
        labels = c(
          "U affects: D, X, Y",
          "U affects: D, Y",
          "U affects: D, X",
          "U affects: X, Y",
          "U affects: D",
          "U affects: Y",
          "U affects: X",
          "U affects: none"
        )
      ),
    ruled_out_by = as.factor(
      case_when(
        !acyclic ~ "Acyclicity",!consistent_with_RA ~ "Random assignment",!consistent_with_X_pretreatment ~ "Measuring X before treatment",
        TRUE ~ NA_character_
      )
    )
  )
```


## Using the Model to inform your Answer strategy

As we described in chapter 6, we can (non-parametrically) express our model of the world as a directed acyclic graph, or DAG. DAGs express *some* but not all parts of our model, precisely because they are nonparametric. They don't encode how variables cause each other, just whether they do. Even so, writing down a theoretical model in as parsimonious a form as a nonparametric structual causal model can guide answer strategies in an enormously powerful way. Given a DAG, we can learn whether *any* answer strategy would be sufficient for estimating a causal effect. Further, we can learn which variable our answer strategy must condition on, and which must be left alone. 

Table XXX shows the three elemental building blocks of DAGs: chains, collider paths, and forks. All three kinds paths involve three variables. A chain describes a mediation path. The causal effect of D on Y is mediated through X, so we can describe X as a "mediator." A collider path is a chain of three variables in which the middle variable X is caused by both D and Y. Here X is a collider variable. Finally, forks describe a causal setting in which the middle variable X causes both D and Y. Here X is a "confounder", since it confounds the relationship between X and Y.


| Path         | X is a ... | Path is called a... |
| ------------ | ---------- | ------------------- |
| D -> X -> Y  | mediator   | chain               |
| D -> X <- Y  | collider   | collider path       |
| D <- X -> Y  | confounder | fork                |


```{r, echo=FALSE, fig.width=6.5, fig.height=2}
dag1 <- dagitty("dag{
                D<-X->Y; D->Y
                }")

dag2 <- dagitty("dag{
                D->X<-Y; D->Y
                }")

dag3 <- dagitty("dag{
                D->X->Y; D->Y
                }")


points_df <-
  tibble(name = c("D", "X", "Y"),
         x = c(1, 2, 3),
         y = c(0, 1, 0))


gg_df <-
  list(dag1, dag2, dag3) %>%
  map_df( ~ as_tibble(tidy_dagitty(., layout = points)), .id = "dag") %>%
  mutate(dag = factor(
    dag,
    levels = 1:3,
    labels = c("X is a confounder", "X is a collider", "X is a mediator")
  ))



ggplot(gg_df, aes(x, y)) +
  geom_text(data = points_df, aes(label = name)) +
  geom_dag_edges(aes(xend = xend, yend = yend)) +
  coord_fixed() +
  facet_wrap(~dag) +
  theme_void()

```




Definition: The backdoor criterion. Given an ordered pair of variables D, Y in a DAG G, a set of variables X, satifies the backdoor criterion relative to D, Y if no node in X is a descendant of D, and X blocks every path between D and Y that contains an arrow into D. (pg. 61 Pearl primer)

$$\Pr(Y = y \mid do(D=d)) = \sum_x \Pr(Y = y \mid D = d, X = x) \Pr(X = x)$$
$$\Pr(Y_i(D_i = 1) = y) = \sum_x \Pr(Y_i(D_i = 1) = y \mid X_i = x) \Pr(X_i = x)$$

Definition: The frontdoor criterion. A set of variables X is said to satisfy the frontdoor criterion relative to an ordered pair of variables D, Y if X intercepts all directed paths from D to Y, there is no unblocked path from D to X, all backdoor paths from X to Y are blocked by D. (pg. 69 Pearl primer)

$$\Pr(Y = y \mid do(D = d)) = \sum_x \Pr(X = x \mid D = d) \sum_{d^{\prime}} \Pr(Y = y \mid D = d^{\prime}, X = x) \Pr(D = d^{\prime})$$

### Descriptive analysis

Answering descriptive inquiries involves studying the existence of nodes and the values they take on. We might study whether a behavior exists in the world, and measure its frequency. We might also study how the frequency of the behavior varies over time and across people. Our answer strategy, following the plug-in principle, will often involve the average value of the node in the sample as an estimator of the average value in the population. 

### Causal analysis

By contrast, when we target a causal inquiry with our answer strategy, we are interested in the existence (or non-existence) of an edge between two nodes. We can label the two $D$ (treatment) and $Y$ (outcome).

We can only learn the answer to the inquiry when we can identify the causal effect, meaning if we had infinite data we could estimate without bias the causal effect. Identification can be obtained if the backdoor criterion is met, if the frontdoor criterion is met, or if there are no other causal relationships into both $D$ and $Y$. These conditions will be violated in several circumstances, including when an observed confounder $X$ confounds the relationship between $D$ and $Y$ and is left unadjusted or when there is an unobserved confounder $U$. 

To reason about causal identification, we turn back to DAGs. We start with a simple DAG with four variables: a treatment $D$, an outcome $Y$, a observed variable $X$ (which may or may not confound the relationship between $D$ and $Y$), and an unobserved variable $U$. There are six edges between these variables, each with three possible relations: each variable may cause (e.g., $D \rightarrow Y$), be caused by (e.g., $D \leftarrow Y$), or not be causally related to every other variable. With three possible relationships and six edges, there are $3^6 = 729$ conceptually possible DAGs. We rule out DAGs in which other variables cause $U$, because we defined $U$ as an unobserved confounder. This leaves 216 possible combinations.

```{r}
# four variable dag with undirected edges between all sides
```

In the presence of possible confounding from $X$ and/or $U$, we have several options for our research design: we can estimate the causal effect of $D$ on $Y$ by controlling for the observed confounder $X$ (or not), by invoking additional conditional independence assumptions, and by randomizing the treatment. In each case, our aim is to rule out DAGs in which the causal effect is not identified. 

#### Causal identification through controls 

The first strategy is that we can control for the observed variable $X$ or not. Whether controlling for $X$ enables, prevents, or does not affect the causal identification of the effect of $D$ on $Y$ depends on our beliefs about which DAG is the true DAG. 

We illustrate possible beliefs about these edges by visualizing all of the 216 DAGs in which $U$ is an unobserved confounder (see Figure XX). The large groups of three squares of nine group DAGs based on how $U$ affects other variables in the model: the top left grouping of 27 DAGs represents those in which $U$ affects all of $D$, $X$ and $Y$. Within that grouping, there are three squares of nine squares. Each group of nine squares represents a set of DAGs with a single relationship between $D$ and $Y$. The top left group of nine are those DAGs in which $D\rightarrow Y$. 

We can rule out several of these 216 through acyclicity: the gray squares are graphs that are cyclical. (We operate under the view that cyclicity is not possible in the world, which would imply that these variables simulataneously cause each other.) 

We color the rest of the squares in terms of whether controlling for $X$ will identify the causal effect of $D$ on $Y$.

In some cases, the effect of $D$ on $Y$ will be identified regardless of whether we control for $X$ (white squares). For example, in the left DAG, $X$ affects $D$ and $U$ affects $Y$ but neither affect both. The causal effect is identified because there is no path between $D$ and $Y$ except for the direct effect $D\rightarrow Y$. In other words, neither $X$ nor $U$ confound the relationship, and the potential outcomes of $Y$ are independent of $D$.

In other cases, the effect of $D$ on $Y$ is identified if and only if we control for $X$ (blue squares). These are situations in which $X$ confounds the relationship and there is no additional confounding from unmeasured confounders. These are cases in which the potential outcomes of $Y$ are independent of $D$ *conditional* on $X$. 

However, conditioning on $X$ is risky, because if we are not in one of the blue DAGs we might be in the purple DAGs in which the effect of $D$ on $Y$ is identified only if we do *not* condition on $X$ (purple squares). These are DAGs in which $X$ is a collider, opening a backdoor path between $D$ and $Y$ aside from the direct effect when $X$ is conditioned on. If we knew we were in one of these DAGs, we would not control for $X$. 

There are also many situations --- a majority in fact, the pink squares --- in which the relationship between $D$ and $Y$ is *never* identified, regardless of whether you control for $X$. Most of these result from confounding from unobservable confounders $U$. The upper left quadrant contains the cases where $U$ affects all of the other variables, and the middle left those where $U$ affects $D$ and $Y$. In both cases, identification would require at a minimum the ability to control for these unmeasured confounders. The other situations in which causal identification fails are those in which the causal order between $D$ and $Y$ is reversed, i.e. $Y$ causes $D$. Without additional assumptions or control of the order in which variables are collected from the data strategy, we cannot rule out this possibility, the first and fourth columns of subgraphs labeled $D\leftarrow Y$.

Without additional assumptions or manipulation in our data strategy, we cannot know where we are among the 200 acyclic DAGs represented in the plot. As a result, we are in danger either of having the effect of $D$ on $Y$ unidentified regardless of what we do, or of making the wrong choice to control or not to control. 

```{r fig-dags-adjustment, echo = FALSE, fig.width = 30, fig.height = 21}
gg_df <-
  gg_df %>%
  mutate(tile_fac = as.factor(if_else(
    !acyclic,
    "Ruled out by acyclicity",
    as.character(id_adjustment_fac)
  ))
  ) 

fill_scale <- c(
  `Ruled out by acyclicity` = dd_light_gray,
  `Yes, regardless of conditioning` = "transparent",  
  `Only when NOT conditioning on X` = dd_purple,
  `Only when conditioning on X` = dd_light_blue,
  `No, regardless of conditioning` = dd_pink
)

subplot_function <- function(data) {
  dag_df <-
    data %>%
    group_by(var, DX_fac, YX_fac, tile_fac) %>%
    summarize(x = 1.5, y = 1.5, n = n())
  
  g <- 
  ggplot(data, aes(x, y)) +
    geom_tile(data = dag_df, aes(fill = tile_fac), height = 1.75, width = 1.75, alpha = 0.5) +
    geom_text(data = points_df, aes(label = name), size = 5) +
    geom_dag_edges(aes(xend = xend, yend = yend), 
                   edge_width = 0.4, 
                   arrow_directed = grid::arrow(length = grid::unit(4, "pt"), type = "closed")) +
    scale_fill_manual("Effect of D on Y identified?", values = fill_scale, drop = FALSE) + 
    facet_grid(YX_fac ~ DX_fac, switch = "both", labeller = label_parsed) +
    coord_fixed() + 
    theme_void() +
    theme(plot.title = element_text(hjust = 0.5), 
          plot.subtitle = element_text(hjust = 0.5)) + 
    labs(subtitle = parse(text = as.character(unique(data$DY_fac))))
  if(as.character(unique(data$DY_fac)) == "D %->% Y"){
    g <- g + labs(title = as.character(unique(data$U_relationship_fac)))
  }
  g
}

my_fun <- function(data){
  data %>%
    split(.$DY_fac) %>% 
    map(~subplot_function(.)) %>% 
    wrap_plots(nrow = 1) 
}

gg <- gg_df %>% 
  split(.$U_relationship_fac) %>% 
  map(my_fun) 

wrap_plots(gg, ncol = 2, byrow = FALSE) + plot_layout(guides = "collect") & theme(legend.position = "bottom") 
```

#### Causal identification by assumption

We can address the problem of unmeasured confounding by invoking conditional independence assumptions. The ``selection-on-observables'' answer strategy invokes the assumption that $D$ is statistically independent of the potential outcomes of $Y$ *given* X, i.e. after adjusting for $X$. In other words, controlling for $X$ blocks all backdoor paths between $D$ and $Y$. This assumption is also known as a conditional independence or ignorability assumption.

In Figure XX, we display the 216 possible causal graphs again, ruling out in gray those that are cyclical. Among the 200 that remain, we color in the same way as under controlling for $X$ but add a fourth color: lavender for DAGs that we rule out based on our conditional independence assumption. Those that are ruled out, in the upper left quadrants, have unobserved confounding from $U$ that cannot be addressed by adjusting for $X$.

Analysts who invoke this conditional independence assumption assure that there are many fewer circumstances in which identification is not possible either through controlling for $X$ or not controlling for $X$. However, this is a strong assumption that is not possible to test directly. Instead, the analyst must justify the assumption based on circumstantial qualitative or quantitative evidence. The task is to rule out through this evidence either a relationship between $U$ and $D$ or a relationship between $U$ and $Y$ or both. In the first case, this evidence might take the form of background knowledge about how values of $D$ are determined. The treatment might be assigned using a cut-off rule, in which case all those above the cut-off are assigned to treatment (e.g., are admitted to a college) and those below are not. In this case, there is no relationship between unobserved variables $U$ and treatment $D$, there is only a relationship between $X$ (score) and $D$. Controlling for $X$ will enable causal identification even if $Y$ is affected by $U$. However, the assumption of conditional independence between $U$ and $D$ given $X$ is a strong assumption: the analyst must be sure that there is no unobserved variable $U$ that directly affects $D$, such as legacy applicants who may be "pushed" over the threshold of the cut-off if they are close enough to it. 

Under selection-on-observables, there are still many DAGs that raise problems for us. There are still many DAGs in which $D\leftarrow Y$, $D$ is caused by $Y$ instead of the other way around. If we cannot rule those out by assumption, we will never identify the causal effect of $D$ on $Y$ regardless of whether we control for $X$. The DAGs in blue (the effect is identified only when we condition on $X$ and not otherwise) and purple (the opposite, that we only achieve identification when we do not condition on $X$) still remain. Those in blue involve $X$ confounding the relationship between $D$ and $Y$ and those in purple are when $X$ is a collider so conditioning on it opens up a backdoor path to $U$. In other words, the conditional independence of $D$ and $U$ given $X$ is insufficient to identify the effect, without ruling out these other scenarios. 

```{r, echo = FALSE, fig.width = 30, fig.height = 21}
gg_df <-
  gg_df %>%
  mutate(tile_fac = as.factor(
    case_when(
      !acyclic ~ "Ruled out by acyclicity",
      consistent_with_ignorability == 0 ~ "Ruled out by ignorability",
      TRUE ~ as.character(id_adjustment_fac) 
    )
  ))

fill_scale <- c(
  `Ruled out by acyclicity` = dd_light_gray,
  `Yes, regardless of conditioning` = "transparent",  
  `Only when NOT conditioning on X` = dd_purple,
  `Only when conditioning on X` = dd_light_blue,
  `No, regardless of conditioning` = dd_pink,
  `Ruled out by ignorability` = dd_dark_blue_alpha
)

subplot_function <- function(data) {
  dag_df <-
    data %>%
    group_by(var, DX_fac, YX_fac, tile_fac) %>%
    summarize(x = 1.5, y = 1.5, n = n())
  
  g <- 
  ggplot(data, aes(x, y)) +
    geom_tile(data = dag_df, aes(fill = tile_fac), height = 1.75, width = 1.75, alpha = 0.5) +
    geom_text(data = points_df, aes(label = name), size = 5) +
    geom_dag_edges(aes(xend = xend, yend = yend), 
                   edge_width = 0.4, 
                   arrow_directed = grid::arrow(length = grid::unit(4, "pt"), type = "closed")) +
    scale_fill_manual("Effect of D on Y identified?", values = fill_scale, drop = FALSE, guide = guide_legend(nrow = 1)) + 
    facet_grid(YX_fac ~ DX_fac, switch = "both", labeller = label_parsed) +
    coord_fixed() + 
    theme_void() +
    theme(plot.title = element_text(hjust = 0.5), 
          plot.subtitle = element_text(hjust = 0.5)) + 
    labs(subtitle = parse(text = as.character(unique(data$DY_fac))))
  if(as.character(unique(data$DY_fac)) == "D %->% Y"){
    g <- g + labs(title = as.character(unique(data$U_relationship_fac)))
  }
  g
}

my_fun <- function(data){
  data %>%
    split(.$DY_fac) %>% 
    map(~subplot_function(.)) %>% 
    wrap_plots(nrow = 1) 
}

gg <- gg_df %>% 
  split(.$U_relationship_fac) %>% 
  map(my_fun) 

wrap_plots(gg, ncol = 2, byrow = FALSE) + plot_layout(guides = "collect") & theme(legend.position = "bottom") 
```

#### Causal identification through random assignment

When we are unable to rule out confounding by assumption and adjustment, we can randomly assign $D$ to sever connections between unobserved variables $U$ and $D$ by design. Ruling out confounders by assumption or adjustment requires the *model* to be correct, so is often known as model-based inference, whereas ruling out confounders by design is labeled design-based inference.^[In truth, there is a continuum between the two. Design-based inferences that rely on non-parametric estimators of the average treatment effect using data from a randomized experiment are a classic design-based estimator, yet they also rely on a modeling assumption: the stable unit treatment-value assumption.] In addition, we can measure $X$ before treatment to rule out situations in which $D$ causes $X$, which can lead to collider bias or opening backdoor paths from $D$ to $Y$. 

In doing so, we dramatically reduce the set of possible DAGs, because we set the causal order between $X$ and $D$, and dramatically expand the number of settings under which the effect of $D$ on $Y$ is identified both due to this restriction on causal order and the randomization of $D$ which guarantees ignorability of $U$.

In Figure XX, we swap the colors to now indicate DAGs ruled out by measuring $X$ before treatment (salmon squares), and those ruled out by random assignment (blue squares). In all of the remaining white squares, the effect of $D$ on $Y$ is causally identified, regardless of whether we adjust for $X$ or not. We remove the conditionality of inference depending on whether we control. This is good, because ultimately even in the presence of strong ignorability assumptions outlined in the last section there are many possible DAGs in which controlling or failing to control lead to bias. Now, our inferences do not depend on guessing the correct DAG. 

```{r, echo = FALSE, fig.width = 30, fig.height = 21}
gg_df <-
  gg_df %>%
  mutate(tile_fac = as.factor(if_else(
    !acyclic,
    "Ruled out by acyclicity",
    as.character(id_adjustment_fac)
  )))

gg_df <-
  gg_df %>%
  mutate(
    tile_fac2 = as.factor(case_when(
      ruled_out_by == "Acyclicity" ~ "Ruled out by acyclicity",
      ruled_out_by == "Measuring X before treatment" ~ "Ruled out by pretreatment measurement",
      ruled_out_by == "Random assignment" ~ "Ruled out by random assignment",
      is.na(ruled_out_by) ~ "Effect of D on Y identified"
    ))
  )

fill_scale <- c(
  `Ruled out by acyclicity` = dd_light_gray,
  `Ruled out by pretreatment measurement` = dd_orange,  
  `Ruled out by random assignment` = dd_dark_blue,
  `Effect of D on Y identified` = "transparent" 
)

subplot_function <- function(data) {
  dag_df <-
    data %>%
    group_by(var, DX_fac, YX_fac, tile_fac2) %>%
    summarize(x = 1.5, y = 1.5, n = n())
  
  g <- 
    ggplot(data, aes(x, y)) +
    geom_tile(data = dag_df, aes(fill = tile_fac2), height = 1.75, width = 1.75, alpha = 0.5) +
    geom_text(data = points_df, aes(label = name), size = 5) +
    geom_dag_edges(aes(xend = xend, yend = yend), 
                   edge_width = 0.4, 
                   arrow_directed = grid::arrow(length = grid::unit(4, "pt"), type = "closed")) +
    scale_fill_manual("Situation", values = fill_scale, drop = FALSE) + 
    facet_grid(YX_fac ~ DX_fac, switch = "both", labeller = label_parsed) +
    coord_fixed() + 
    theme_void() +
    theme(plot.title = element_text(hjust = 0.5), 
          plot.subtitle = element_text(hjust = 0.5)) + 
    labs(subtitle = parse(text = as.character(unique(data$DY_fac))))
  if(as.character(unique(data$DY_fac)) == "D %->% Y"){
    g <- g + labs(title = as.character(unique(data$U_relationship_fac)))
  }
  g
}

my_fun <- function(data){
  data %>%
    split(.$DY_fac) %>% 
    map(~subplot_function(.)) %>% 
    wrap_plots(nrow = 1) 
}

gg <- gg_df %>% 
  split(.$U_relationship_fac) %>% 
  map(my_fun) 

wrap_plots(gg, ncol = 2, byrow = FALSE) + plot_layout(guides = "collect") & theme(legend.position = "bottom") 
```

In short, what we do by controlling the timing of measurement of $X$ and randomizing $D$ is to move our assumptions about conditional independence from $M$ our assumed (but possible incorrect!) model of the world into our data strategy, which we control and so can guarantee by design.

However, in order to benefit from these two controlled decisions in our data strategy, we must follow the dictum due to R.A. Fisher to "analyze as you randomize." Your answer strategy should follow your data strategy. There are four components: make comparisons only across randomly-assigned conditions; analyze data at the level of random assignment; make comparisons only in groups within which random assignment was conducted (e.g., strata); and adjust for differences in probabilities of random assignment.

Making comparisons across randomly-assigned conditions within groups in which random assignment was conducted directly follow from our comparison of causal identification by assignment vs. by design. If we make comparisons that rely on differences both in $D$ and $X$, for example a difference in treatment effects between two subgroups, we are susceptible to confounding from $U$, because $X$ is not randomly-assigned. Similarly, analysis of data from block-randomized experiments can be broken by failing to account for blocking, because blocks are not randomly assigned and in fact are typically constructed so that the outcomes of units in the block are similar within blocks and very different across blocks. If there are differential probabilities of assignment across blocks and we pool our data ignoring the blocking structure, our unweighted comparisons may be contrasting groups that differ systematically.

more on analyze as you randomize

#### Effect estimation and distinguishing among DAGs

The identification of the causal effect of $D$ on $Y$ either by assumption or design enables us to undertake two tasks: estimate the average treatment effect, estimate the sign of the effect, or estimate whether there is an affect or not. 

The first task, estimating the magnitude of the effect of $D$ on $Y$, can be accomplished using model-based inference under the selection-on-observables design or under a randomized experiment. In both cases, we apply the plug-in principle, replacing the true but unknown average potential outcomes under treatment (control) with the sample analogues, the average outcomes in the treatment (control) group. With data from randomized experiments, $X$ is ignorable, so we can either adjust for it (which may reduce variability in estimates) or not without bias. But under selection-on-observables, we have to rule out many DAGs by assumption that include $X$ as a collider or in which $X$ opens a backdoor path between $D$ and $Y$ in order to safely select an answer strategy.

For estimating the sign of the effect, we can calculate the sign of the effect magnitude, and then conduct a statistical test of the null hypothesis of zero effect to distinguish among zero, positive, and negative effects.

The third task, determining whether there is an effect or not, similarly involves a statistical test of the null hypothesis of zero effect. If we fail to reject the null, then our posterior belief is that there is no effect, but if we reject in a two-sided test then we leave believing there is an effect. This zero average effect null hypothesis test can help us take the final step in distinguishing among the 216 possible DAGs representing the relationships between $D$ and $Y$ and the confounders $X$ and $U$. In Figure XX, we display the 16 DAGs that are identified under random assignment and pretreatment measurement of $X$. We need to use data and the two-sided null hypothesis test to distinguish between the top row (there is an effect $D\rightarrow Y$) and the bottom row (there is no effect). Our design got us most of the way there, but then we need to use the data to narrow further to one of the two rows of eight DAGs. Since our inquiry is about the causal relationship between $D$ and $Y$, we may not be concerned about distinguishing among the eight. If we are, we need to develop an alternative research design, to learn about the causal effects of $X$.

```{r, echo = FALSE, fig.width = 30, fig.height = 21}
gg_df <-
  gg_df %>%
  filter(is.na(ruled_out_by)) %>% 
  mutate(
    ruled_out_by_sig_test = as.factor(if_else(DY_fac == "D~~~Y", "Ruled out by rejection of null hypothesis of no effect", "Cannot rule out")),
    fct_rows = paste0(YX_fac, YU_fac, XU_fac)
  )

fill_scale <- c(
  `Ruled out by rejection of null hypothesis of no effect` = dd_dark_blue,
  `Cannot rule out` = "transparent"
)

dag_df <-
  gg_df %>%
  group_by(var, DX_fac, DY_fac, YX_fac, ruled_out_by_sig_test, fct_rows) %>%
  summarize(x = 1.5, y = 1.5, n = n()) 

g <- 
  ggplot(gg_df, aes(x, y)) +
  geom_tile(data = dag_df, aes(fill = ruled_out_by_sig_test), height = 1.75, width = 1.75, alpha = 0.5) +
  geom_text(data = points_df, aes(label = name), size = 5) +
  geom_dag_edges(aes(xend = xend, yend = yend), 
                 edge_width = 0.4, 
                 arrow_directed = grid::arrow(length = grid::unit(4, "pt"), type = "closed")) +
  scale_fill_manual("Situation", values = fill_scale, drop = FALSE) + 
  facet_grid(DY_fac ~ fct_rows) +
  coord_fixed() + 
  theme_void() +
  theme(plot.title = element_text(hjust = 0.5), 
        plot.subtitle = element_text(hjust = 0.5),
        legend.position = "bottom",
        strip.text.y = element_blank()) 

g
```

## Point estimation

Point estimation is possibly the most common class of answer strategy in quantitative social science. Point estimators are things like the difference-in-means, ordinary least squares regression, instrumental variables regression, random forests, the LASSO, ridge regression, BART... the list goes on and on. These are the data analysis tools taught in many graduate methods courses. What's incredible is that even though the population of estimators proliferates with each passing year, the number of kinds of inquiries they are useful for estimating stays relatively flat. When we do point estimatation, we are mainly interested in estimating averages, differences, variances, and conditional expectation functions of increasing dimensionality. There are of course many other kinds of estimands (like ratios and quantiles), but the main point is, most of the variation in how scholars conduct point estimation is in the answer strategy, not in the inquiry.

(point people to Hastie and Tibshirani)

## When parameters are not point identified

- confounding
- EV bounds
- trimming bounds

## Characterizing uncertainty 

<!-- (s.e., CIs, posterior probs) -->

## Tests

Tests are an elemental kind of answer strategy. Tests yield binary yes/no answers to an inquiry. In some qualitative traditions, hoop tests, straw-in-the-wind tests, smoking-gun tests, and doubly-decisive tests are common. These tests are procedures for making analysis decisions in a structured way. In frequentist statistics, significance tests are used to make a decision whether to reject or fail to reject a particular null hypothesis. 

<!-- Null hypothesis significance testing has developed a (rightfully) sketchy reputation in recent years as too much weight has been put on the reject / fail to reject decision. That said, sometimes that decision is rightfully important and in such cases, null hypothesis significance testing is a great tool. -->

<!-- - significance testing (incl. sign test) -->
<!-- - equivalence testing -->
<!-- - straw in the wind / hoop test -->
<!-- - doubly decisive -->
<!-- - 1997 book on tests -->

<!-- [cite erin and naoki on sign tests] -->
<!-- [cite erin on equivalence testinf] -->

## Answer strategies as procedures

Consider a randomized experiment that seeks to estimate the causal effect of a treatment. The answer strategy is not just "Logistic Regression with Covariate adjustment". It includes every step in the process that takes the raw data, cleans and recodes it, considers 5 alternative estimators (DIM, OLS with covariate adjustment, a fancy thing your colleague suggested but you couldn't get to converge), before finally settling on logit.

**Multiple estimates.** Answer strategies can account how many statistical tests you are conducting. Often, when generating an answer to a single inquiry, we may construct multiple estimates that provide different types of answers of varying quality. When you present the results from many null hypothesis tests, the rate of falsely rejecting at least one of those tests even when all are true goes up, due to the multiple comparisons problem. If you plan to adjust for this problem, those adjustments are part of your answer strategy, because they will typically adjust the p-values you report and the decisions readers make with them. We may have three survey items that imperfectly estimate a latent quantity. In presenting the results, we could present three estimates from three regressions, we could adjust the three estimates using a procedure such as a family-wise error rate correction, or we could average the three items together into an index and present one estimate from one regression. Which of these three methods we select will change the properties of our answer strategy. 

**Analysis procedures**. The final estimator that goes into a paper is neither the beginning nor the end of the answer strategy. Procedures, if any, by which you explore the data and determine a final set of estimates are part of the answer strategy. Procedures for summarizing multiple estimates are one example of many.

Commonly, the final estimator that is selected depended on a exploratory procedure in which multiple models are assessed, for example by comparing model fit statistics. The answer strategy of our research design is not to fit the final model --- is it this multiple step if-then procedure. These procedures may be part of a prespecified analysis plan or they may be informal, so it may sometimes only be possible to declare the full design after the data is obtained. (We may find that a different analysis procedure that was not data dependent would have been preferable, if we diagnose the design after the fact.) The reason to declare the procedure rather than the final estimator is that the diagnosis of the design may differ. The procedure may be more powerful, if for example we assessed multiple sets of covariate controls and selecting the specification with the lowest standard error of the estimate. But the procedure may also exhibit poor coverage, accounting for these multiple bites at the apple.

We also sometimes find that the model we planned to run to analyze the data cannot be estimated. In these cases, there is an iterative estimation procedure in which a first model is run, changes to the specification are made, and a second or third model is presented as the result. The full set of steps --- a decision tree, depending on what is estimable --- is the answer strategy and we can evaluate whether it is a good one not only under the realized data but under other possible realizations where the decision *tree* would be the same but the decisions different.

In fact, there are examples of analysis procedures in most types of research, quantitative or qualitative. Many strategies for causal inference with observational data involve not only an estimation strategy but a set of falsification or placebo tests. The answer provided by these research designs depends in a crucial way on the results of these tests: if the tests fail, the design provides no definitive answer. In qualitative research, process tracing involves a set of steps, the results of which depend on information gathered in earlier steps. Many mixed methods strategies are also multi-step procedures. Nested designs involve running a quantitative analysis and then selecting cases on the basis of predictions from the regression. These designs cannot be assessed by considering a single step of the procedure in isolation.
<!-- need cites -->


**When things do not go according to plan.** To compare answer strategies, you can imagine the estimators that are possible *if things go well* as well as *if things go wrong*, when there is missing data or there are outliers in variables. A good answer strategy, which might be a single estimator, or a procedure if-this-then-that, can handle both states of the world. Procedures for addressing deviations from expected analyses are part of the answer strategy. Even in the absence of a preanalysis plan, we often have a way we expect to analyze the data if things go well. When they do not --- because data are missing, there is noncompliance with an intervention, or the study is suspended for example --- the answers will change. These procedures determine the answer the study provides (or in some cases does not), so are part of the answer strategy. *Standard operating procedures* (lin and green) are documents that systematize these procedures in advance.



We demonstrate the fact that the properties of procedures differ from the properties of a design with the final estimator in a simple example. We compare two possible estimation specifications, with and without covariates, to a procedure in which we run both models and report the model in our paper that has the lower p-value. The models are exactly the same, but the properties of the *procedure* differ from the properties of either of the two possible models. In particular, the procedure has higher power than either of the two models, but it exhibits poor coverage, which means we have a bias in our measure of uncertainty. 

## Robustness

**Robustness checks** are part of the answer strategy. Often, a single estimator is presented as the main analysis but then a series of alternative specifications are displayed in an appendix (such as including or excluding covariates and their interactions, different subsets of the data, or alternative statistical models). These differ from multiple estimates of a latent quantity in that the goal is not a primary analysis, but rather to support the main analysis. The purpose is to provide readers with evidence about how dependent the main results are on the specification, data subset, and statistical model used. The decision a reader makes from a paper depends not only on the main estimate but also the robustness checks. As a result, we want to assess the properties of the two together. 

[NOTE TO ADD HERE]: robustness checks are often more about $M$ than about $A$ -- we see how $a^d$ changes when we change $A$ in order see how our inferences change under alternative $M$s.

We illustrate with a simple analysis of the correlation between two variables `y1` and `y2`, who have a true positive correlation. `y2` is also a function of an observed covariate `x` and measurement error. Our main analysis is a bivariate regression predicting `y2` with `y1`. We compare this answer strategy to one in which we run that analysis, but also run a robustness check controlling for `x`. We do this because as the analyst we are unsure of the true DGP and wish to demonstrate to reviewer's that our results are not dependent on the functional form we choose. 

<!-- - distinguish this from changes to the model where we do robustnesss vis a vis a fixed answer and data strategy. i.e. two notions of "robustness". one is fix I D A and change M, is this "design" robust to changes in M. the other is, within a given run, is the estimate "robust" to changing the estimation procedure, so this is a diagnostic statistic. note I must be defined across these changes in M. -->

Using the MIDA way of thinking about designs, we discuss in the diagnosis section another notion of the "robustness" of a design. The typical way we think of robustness checks is multiple secondary analyses *conditional on the observed data* to build confidence in an analysis of that fixed data. However, the motivation for these robustness checks is uncertainty about the true data generating process. By declaring a design in terms of MIDA, we can think about the robustness of a *single* estimator to multiple possible true data generating processes. An estimator that is robust in this sense is one that is unbiased with low uncertainty regardless of, say, the true functional form between `y1` and `y2`. To determine whether an estimator is robust, we can redefine a set of designs with different functional forms and assess the rate of correct decisions of our robustness checks strategy under each different model. 


## Concluding thoughts on answer strategies

How results are presented in tables and figures is an important part of the answer strategy. Considerable attention has been paid to how to display data, including arguments for switching from tables to graphs as the primary way to present statistical models (Kastellec) and for visually present raw data and models together (Coppock). The reason for this attention is that what inferences readers make when reading a paper depends not only on the statistical procedures used for estimation but the medium in which they are displayed. When numerical estimates are not provided at all, and only visualizations of results, clearly aesthetic choices about which estimates are displayed and even the width of axes will determine what the reader takes away from the answer strategy. This principle applies not just to tables and visualizations; how we describe results in the text of a paper also shapes readers' inferences from the data. Registered reports are a format for preparing scientific papers that involves prespecifying not only the form of graphs and tables but the text the author plans to write depending on the results. In short, decisions made from your results by readers are not just a function of numerical estimates but how they are presented.