---
title: "Choosing an answer strategy"
output:
  html_document:
    toc: yes
    toc_depth: 4
bibliography: ../bib/book.bib
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE, dev = "svg")
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# Choosing an answer strategy

<!-- make sure to rename the section title below -->

```{r choosing_an_answer_strategy, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 10
b_sims <- 10
```


Your answer strategy is your plan for what you will do with the information gathered from the world in order to generate an answer to the inquiry. Qualitative and quantitative methods courses overflow with advice about which answer strategies to choose. Under what conditions should you use Ordinary Least Squares, when should you use logit? When is a machine learning algorithm the appropriate choice and when would a comparative case study be more informative? When is *no* answer strategy worth pursuing because of the fundamental limitations of the data strategy?

Our perspective on answer strategies is that they should be informed by the other three elements of research design: the model, the inquiry, and the data strategy. Sometimes answer strategy advice is offered on the basis of the realized data $d$ only, without any particular attention paid to important features of the question to be answered or the manner in which the data were collected and generated. For example, a bit of received methodological wisdom holds that whenever a dependent variable is binary, a binary choice statistical model like logit or probit must be used. This advice is based only the knowledge that $d$ contains a binary outcome; whether it is appropriate depends on features of $I$ and features of $D$. For example, if $I$ is the average treatment effect and $D$ includes randomization of the treatment, answer strategies beyond binary choice models may serve just as well or better. If a student asks, "which estimator should I use?" the response is always, "it depends." What does it depend on? The model, the inquiry, and the data strategy.

Choosing "design-aware" answer strategies sounds straightforward enough, but the precise way in which this approach is applied in any real empricial setting will of course differ from case to case. Our highest-level advice is that we want to pick an answer strategy $A$ such that the answer it provides $A(d) = a^d$ is close to the answer under our model of the world, $I(m) = a^m$. The implication is that we should strive for parallelism across $A$ and $I$. This idea sometimes described as the "plug-in principle." When the function $A$ is very close to the function $I$, then we can "plug in" $d$ for $m$. Following the plug-in principle leads to very straightforward estimation procedures. If the inquiry is a population mean, we can use the sample mean estimator to estimate it, so long as the data strategy produces data $d$ that are sufficiently representative of $m$. To the extent possible, we want to choose data strategies that enable plug-in estimator answer strategies.

At some level, all answer strategies rely on unverifiable but hopefully plausible assumptions. For descriptive inference, we have to assume that the sample represents the population well. For causal inference, we have to assume that treated and untreated units are similar in all other respects beyond treatment status. For all kinds of inference, we have to assume that our measurements are close to the latent construsts we wish to measure. Answer strategies are *model-based* when the most consequential of these assumptions are part of $M$. Answer strategies are *design-based* to the extent that we can pull the assumptions out of $M$ and assure them by design, i.e., by choosing $D$ in such a way that we can be confident that the assumption is true. Observational causal inference often relies on an assumption of "selection on observables," or the claim that within groups of units that have the same observed characteristics, treatments are as-if randomly assigned. Observational causal inference is model-based in the sense that the selection on observables assumption is grounded in researchers' theoretical model of the world, which of course might be wrong. Experimental causal inference is design-base in the sense that we can be sure that treatments were indeed randomly assigned because random assignment was part of the data strategy. In both observational and experimental settings we need to make the same assumption, but in one case, we rely on a theoretical model and in the other, we rely on a data strategy. Where possible, we prefer assuring assumptions by design to asserting them on the basis of a theoretical model.

Answer strategies come in a number of varieties. The most familiar of these are point-estimators that produce estimates of parameters. Ordinary least squares, difference-in-means, logit, random forests, and a very long list of others are in the point-estimator answer strategy class. A second class is comprised of tests. Tests return a binary decision (True or False). Null hypothesis significance tests are a common form of test in quantitative research. Qualitative researchers also employ tests; these go by names like "hoop test" and "straw-in-the-wind" test. Bayesian answer strategies return full posterior distributions. In contrast to point-estimators some answer strategies are interval-estimators. Which class of estimator to choose -- and which particular estimator within the class you select -- will depend on features of your model, inquiry, and data strategy. The sections to follow walk through some of the most important ways these design features influence the answer strategy, though they will by no means offer a full accounting. The range of estimation of approaches is vast and this book is not the place to tally each of their strengths and weaknesses. Instead, our goal is to provide a framework for thinking about how you would go about choosing one answer strategy over another.

```{r make-all-models, echo = FALSE}
possible_models <-
  expand.grid(
    XU = c("X<-U", "none"),
    YU = c("Y<-U", "none"),
    YX = c("Y->X", "Y<-X", "none"),
    DU = c("D<-U", "none"),
    DX = c("D->X", "D<-X", "none"),
    DY = c("D->Y", "D<-Y", "none"),
    stringsAsFactors = FALSE
  )

possible_models <-
  possible_models %>%
  rowwise() %>%
  mutate(
    var = str_c(XU, YU, YX, DU, DX, DY, sep = ";"),
    var = str_remove_all(var, "none;"),
    var = str_remove_all(var, "none"),
    dag = list(dagitty(paste0(
      "dag{", var, "; X; D; Y; U}"
    ))),
    acyclic = isAcyclic(dag),
    consistent_with_ignorability = !(DU == "D<-U" & YU == "Y<-U"),
    id_if_adjusted = isAdjustmentSet(dag, "X", exposure = "D", outcome = "Y"),
    id_if_unadjusted = isAdjustmentSet(dag, NULL, exposure = "D", outcome = "Y"),
    id_adjustment_fac = as.factor(
      case_when(
        id_if_unadjusted &
          id_if_adjusted ~ "Yes, regardless of conditioning",
        id_if_unadjusted &
          !id_if_adjusted ~ "Only when NOT conditioning on X",!id_if_unadjusted &
          id_if_adjusted ~ "Only when conditioning on X",!id_if_unadjusted &
          !id_if_adjusted ~ "No, regardless of conditioning"
      )
    ),
    consistent_with_RA = DU != "D<-U" &
      DX != "D<-X" & DY != "D<-Y",
    consistent_with_X_pretreatment =
      DX != "D->X" &
      YX != "Y->X" &
      !(DU == "D->U" & XU == "X<-U") &
      !(YU == "Y->U" & XU == "X<-U")
  )

nested_data <-
  possible_models %>%
  filter(var != "") %>%
  mutate(dag_data = list(as_tibble(tidy_dagitty(dag))))

points_df <-
  tibble(name = as.factor(c("D", "X", "Y", "U")),
         x = c(1, 2, 2, 1),
         y = c(1, 2, 1, 2))
ends <-
  points_df %>%
  rename(to = name,
         xend = x,
         yend = y)

fix_no_edges <-
  tibble(
    var = "no edges",
    acyclic = TRUE,
    consistent_with_RA = TRUE,
    consistent_with_X_pretreatment = TRUE,
    XU = "none",
    YU = "none",
    YX = "none",
    DU = "none",
    DX = "none",
    DY = "none",
    name = "X",
    id_adjustment_fac = "Yes, regardless of conditioning"
  )

gg_df <-
  nested_data %>%
  unnest(cols = dag_data) %>%
  select(-x, -y, -xend, -yend) %>%
  left_join(points_df) %>%
  left_join(ends) %>%
  bind_rows(fix_no_edges) %>%
  mutate(
    XU_fac = factor(
      XU,
      levels =  c("X<-U", "X->U", "none"),
      labels =  c("X %<-% U", "X %->% U", "X~~~U")
    ),
    YU_fac = factor(
      YU,
      levels =  c("Y<-U", "Y->U", "none"),
      labels =  c("Y %<-% U", "Y %->% U", "Y~~~U")
    ),
    YX_fac = factor(
      YX,
      levels =  c("Y<-X", "Y->X", "none"),
      labels =  c("Y %<-% X", "Y %->% X", "Y~~~X")
    ),
    DU_fac = factor(
      DU,
      levels =  c("D<-U", "D->U", "none"),
      labels =  c("D %<-% U", "D %->% U", "D~~~U")
    ),
    DX_fac = factor(
      DX,
      levels =  c("D<-X", "D->X", "none"),
      labels =  c("D %<-% X", "D %->% X", "D~~~X")
    ),
    DY_fac = factor(
      DY,
      levels =  c("D<-Y", "D->Y", "none"),
      labels =  c("D %<-% Y", "D %->% Y", "D~~~Y")
    )
  ) %>%
  arrange(XU_fac, YU_fac, YX_fac, DU_fac, DX_fac, DY_fac)

gg_df <-
  gg_df %>%
  mutate(
    var_fac = factor(var, levels = c(possible_models$var, "no edges")),
    DY_DX = as.factor(paste0(DY, " ", DX)),
    U_relationship = paste0(XU, " ", YU, " ", DU),
    U_relationship_fac =
      factor(
        U_relationship,
        levels = c(
          'X<-U Y<-U D<-U',
          'none Y<-U D<-U',
          'X<-U none D<-U',
          'X<-U Y<-U none',
          'none none D<-U',
          'none Y<-U none',
          'X<-U none none',
          'none none none'
        ),
        labels = c(
          "U affects: D, X, Y",
          "U affects: D, Y",
          "U affects: D, X",
          "U affects: X, Y",
          "U affects: D",
          "U affects: Y",
          "U affects: X",
          "U affects: none"
        )
      ),
    ruled_out_by = as.factor(
      case_when(
        !acyclic ~ "Acyclicity",!consistent_with_RA ~ "Random assignment",!consistent_with_X_pretreatment ~ "Measuring X before treatment",
        TRUE ~ NA_character_
      )
    )
  )
```

## Following the model

As we described in chapter 6, we can (non-parametrically) express our model of the world as a directed acyclic graph, or DAG. DAGs express *some* but not all parts of our model, precisely because they are nonparametric. They don't encode how variables cause each other, just whether they do. Even so, writing down a theoretical model in as parsimonious a form as a nonparametric structual causal model can guide answer strategies in an enormously powerful way. Given a DAG, we can learn whether *any* answer strategy would be sufficient for estimating a causal effect. Further, we can learn which variables our answer strategy must condition on, and which must be left alone.

When we want to estimate a particular causal relationship, such as the average causal effect of D on Y, we can read off the DAG whether that relationship is *identified* or not. The most common (and useful) criterion for identification is the "backdoor criterion." If there exists an unblocked "backdoor path" from D to Y, then the relationship is not identified.^[Footnote here about other criteria.] A backdoor path is a causal path that begins with an arrow in to D and ends with an arrow in to Y. Backdoor paths can be blocked in two ways: conditioning the analysis on a variable along the path or *not* conditioning on a "collider" along the path.

Pearl (p. 61) defines the backdoor criterion:

Given an ordered pair of variables D, Y in a DAG G, a set of variables X, satifies the backdoor criterion relative to D, Y if no node in X is a descendant of D, and X blocks every path between D and Y that contains an arrow into D. (pg. 61 Pearl primer)

With this definition in hand, we can inspect DAGs to find "adjustment sets." An adjustment set is a set of variables that may be conditioned upon in the answer strategy. "Conditioned upon" is a sufficiently vague phrase to include conditioning procedures such as controlling for a variable in a regression setting or stratifying the analysis according to those variables.

In general, if $X$ is an adjustment set that satisfies the backdoor criterion, then we can estimate the conditional probability distributions of $Y$ for each level of $D$ using this expression.

$$\Pr(Y = y \mid do(D=d)) = \sum_x \Pr(Y = y \mid D = d, X = x) \Pr(X = x)$$

We can write the same expression using potential outcomes notation:

$$\Pr(Y_i(D_i = 1) = y) = \sum_x \Pr(Y_i(D_i = 1) = y \mid X_i = x) \Pr(X_i = x)$$

Figure \@ref(fig:threedags) shows three DAGs with these three variables D, X, and Y, as well as an additional unmeasured variable $U$ (we will consider $U$ in later sections). In all three cases, our inquiry is the average treatment effect of D on Y, and in all three cases, X, D, and Y are correlated. In the first case, $X$ confounds the causal relationship between D and Y, which is to say if we simply compared units with different levels of D, our estimates of the causal effect would be prone to bias. However, if we conduct the analysis separately *within* levels of X (that is to say, we condition on X), then combine the separate analyses, our overall estimate will be unbiased. This first DAG is the setting that analysts have in mind when controlling for observables in order to estimate causal effects.

A dangerous possibility, however, is represented by the second DAG. In this causal graph, X doesn't confound the relationship between D and Y -- instead, it is a downstream consequence of both variables. If an analyst mistakenly conditions on X, a noncausal confounding path opens up between D and Y, biasing estimates of average effect on D on Y. The contrast between the first and second graphs is an illustration of the general principle that our theoretical models guide analytic choices. In the first case, our estimates are unbiased if and only if we *do* control for $X$, in the second case if and only if we *do not* control for X.

The third case describes a setting in which D has both a direct effect on Y and an indirect effect that travels through X -- in this case, X is a mediator. If we condition on X, our estimate of the effect D on Y is biased (X is a descendant of D). Like in the collider case, the effect is identified if we *do not* control for X, but is not identified if we *do* condition. Some intuition for the problems associated with controlling for mediators: controlling for X "controls away" some portion of the effect.

This example illustrates in a small way how your model of the world guides your answer strategy. In all three cases, we could in principle have the same dataset of D, X, and Y. If we followed some common regression advice, we would control for X in all three cases because it is correlated with both D and Y -- this approach works only if X is a confounder. If X is a collider or a mediator, then this control strategy would induce bias. Without further changes to the design, no empirical tests can distinguish between these three DAGs, so in a very real way, theoretical assumptions in the Model must be relied upon to correctly choose a control strategy.

```{r threedags, fig.cap = "Three roles for a variable X.", echo=FALSE, fig.width=6.5, fig.height=2}
dag1 <- dagitty("dag{
                D <- X -> Y; D -> Y
                }")

dag2 <- dagitty("dag{
                D -> X <- Y ; D -> Y
                }")

dag3 <- dagitty("dag{
                D -> X -> Y; D -> Y
                }")

points_df_small <-
  tibble(name = c("D", "X", "Y", "U"),
         x = c(0, 1, 1, 0),
         y = c(0, 1, 0, 1))
ends_df_small <-
  tibble(to = c("D", "X", "Y", "U"),
         xend = c(0, 1, 1, 0),
         yend = c(0, 1, 0, 1))

gg_df_small <-
  list(dag1, dag2, dag3) %>%
  map_df( ~ as_tibble(tidy_dagitty(.)), .id = "dag") %>%
  mutate(dag = factor(
    dag,
    levels = 1:3,
    labels = c("X is a confounder", "X is a collider", "X is a mediator")
  )) %>%
  select(-x, -y, -xend, -yend) %>%
  left_join(points_df_small) %>%
  left_join(ends_df_small)

ggplot(gg_df_small, aes(x, y)) +
  geom_text(data = points_df_small, aes(label = name)) +
  geom_dag_edges(aes(xend = xend, yend = yend)) +
  coord_fixed(ylim = c(0, 1.1), xlim = c(-.1, 1.1)) +
  facet_wrap(~dag) +
  theme_void()
```

<!-- Definition: The frontdoor criterion. A set of variables X is said to satisfy the frontdoor criterion relative to an ordered pair of variables D, Y if X intercepts all directed paths from D to Y, there is no unblocked path from D to X, all backdoor paths from X to Y are blocked by D. (pg. 69 Pearl primer) -->

<!-- $$\Pr(Y = y \mid do(D = d)) = \sum_x \Pr(X = x \mid D = d) \sum_{d^{\prime}} \Pr(Y = y \mid D = d^{\prime}, X = x) \Pr(D = d^{\prime})$$ -->

We consider three possible paths to estimating the effect of the treatment $D$ on $Y$. First, we consider conditioning on $X$ without making additional assumptions about the DAG; second, we consider invoking an ignorability assumption, often known as selection-on-observables; third, we consider randomizing the treatment and measuring $X$ before treatment. Finally, we consider the role of estimation in narrowing down the set of possible DAGs that describe the world.

### Identification

We can only learn the answer to the inquiry when we can *identify* the causal effect, meaning if we had infinite data we could estimate without bias the causal effect. Identification can be obtained if the backdoor criterion is met, if the frontdoor criterion is met, or if there are no other causal relationships into both $D$ and $Y$. These conditions will be violated in several circumstances, including when an observed confounder $X$ confounds the relationship between $D$ and $Y$ and is left unadjusted or when there is an unobserved confounder $U$.

To reason about causal identification, we turn back to DAGs. We start with a simple DAG with four variables: a treatment $D$, an outcome $Y$, a observed variable $X$ (which may or may not confound the relationship between $D$ and $Y$), and an unobserved variable $U$. There are six edges between these variables, each with three possible relations: each variable may cause (e.g., $D \rightarrow Y$), be caused by (e.g., $D \leftarrow Y$), or not be causally related to every other variable. With three possible relationships and six edges, there are $3^6 = 729$ conceptually possible DAGs. We rule out DAGs in which other variables cause $U$, because we defined $U$ as an unobserved confounder. This leaves 216 possible combinations. We do not know which one! Our goal as researchers is to narrow down the possible DAGs.

To do so, we collect data on the three observable variables, as in Table \@ref(fig:identificationsimdf). We are faced with the choice of answer strategy. In the presence of possible confounding from $X$ and/or $U$, we have several options for our research design: we can estimate the causal effect of $D$ on $Y$ by controlling for the observed confounder $X$ (or not), by invoking additional conditional independence assumptions, and by randomizing the treatment. In each case, our aim is to rule out DAGs in which the causal effect is not identified.

```{r identificationsimdf, echo = FALSE}
design <-
  declare_population(
    N = 100,
    X = rnorm(N),
    U = rnorm(N)
  ) +
  declare_potential_outcomes(
    Y ~ 0.1 * D + X + U, assignment_variables = D
  ) +
  declare_assignment(D = if_else(U > 0.5, 1, 0), handler = mutate) +
  declare_reveal(outcome_variables = Y, assignment_variables = D)

simulated_df <- draw_data(design)

simulated_df %>% select(ID, X, D, Y) %>% head %>% kable(caption = "Simulated data from a DAG with varaibles $X$, $D$, $Y$, and $U$.", digits = 3)
```

#### Consequences of conditioning

The first strategy is that we can control for the observed variable $X$ or not. Whether controlling for $X$ enables, prevents, or does not affect the causal identification of the effect of $D$ on $Y$ depends on our beliefs about which DAG is the true DAG.

We illustrate possible beliefs about these edges by visualizing all of the 216 DAGs in which $U$ is an unobserved confounder (see Figure \@ref(fig:dagsadjustment)). The large groups of three squares of nine group DAGs based on how $U$ affects other variables in the model: the top left grouping of 27 DAGs represents those in which $U$ affects all of $D$, $X$ and $Y$. Within that grouping, there are three squares of nine squares. Each group of nine squares represents a set of DAGs with a single relationship between $D$ and $Y$. The top left group of nine are those DAGs in which $D\rightarrow Y$.

We can rule out several of these 216 through acyclicity: the gray squares are graphs that are cyclical. (We operate under the view that cycling is not possible in the world, which would imply that these variables simultaneously cause each other.)

We color the rest of the squares in terms of whether controlling for $X$ will identify the causal effect of $D$ on $Y$.

In some cases, the effect of $D$ on $Y$ will be identified regardless of whether we control for $X$ (white squares). For example, in the left DAG, $X$ affects $D$ and $U$ affects $Y$ but neither affect both. The causal effect is identified because there is no path between $D$ and $Y$ except for the direct effect $D\rightarrow Y$. In other words, neither $X$ nor $U$ confound the relationship, and the potential outcomes of $Y$ are independent of $D$.

In other cases, the effect of $D$ on $Y$ is identified if and only if we control for $X$ (blue squares). These are situations in which $X$ confounds the relationship and there is no additional confounding from unmeasured confounders. These are cases in which the potential outcomes of $Y$ are independent of $D$ *conditional* on $X$.

However, conditioning on $X$ is risky, because if we are not in one of the blue DAGs we might be in the purple DAGs in which the effect of $D$ on $Y$ is identified only if we do *not* condition on $X$ (purple squares). These are DAGs in which $X$ is a collider, opening a backdoor path between $D$ and $Y$ aside from the direct effect when $X$ is conditioned on. If we knew we were in one of these DAGs, we would not control for $X$.

There are also many situations --- a majority in fact, the pink squares --- in which the relationship between $D$ and $Y$ is *never* identified, regardless of whether you control for $X$. Most of these result from confounding from unobservable confounders $U$. The upper left quadrant contains the cases where $U$ affects all of the other variables, and the middle left those where $U$ affects $D$ and $Y$. In both cases, identification would require at a minimum the ability to control for these unmeasured confounders. The other situations in which causal identification fails are those in which the causal order between $D$ and $Y$ is reversed, i.e. $Y$ causes $D$. Without additional assumptions or control of the order in which variables are collected from the data strategy, we cannot rule out this possibility, the first and fourth columns of subgraphs labeled $D\leftarrow Y$.

Without additional assumptions or manipulation in our data strategy, we cannot know where we are among the 200 acyclic DAGs represented in the plot. As a result, we are in danger either of having the effect of $D$ on $Y$ unidentified regardless of what we do, or of making the wrong choice to control or not to control.

```{r dagsadjustment, fig.cap = "Consequences of conditioning on variable X for all 216 possible DAGs.", echo = FALSE, fig.width = 40, fig.height = 28}
gg_df <-
  gg_df %>%
  mutate(tile_fac = as.factor(if_else(
    !acyclic,
    "Ruled out by acyclicity",
    as.character(id_adjustment_fac)
  ))
  )

fill_scale <- c(
  `Ruled out by acyclicity` = dd_light_gray,
  `Yes, regardless of conditioning` = "transparent",  
  `Only when NOT conditioning on X` = dd_purple,
  `Only when conditioning on X` = dd_light_blue,
  `No, regardless of conditioning` = dd_pink
)

subplot_function <- function(data) {
  dag_df <-
    data %>%
    group_by(var, DX_fac, YX_fac, tile_fac) %>%
    summarize(x = 1.5, y = 1.5, n = n(), .groups = "drop")

  g <-
  ggplot(data, aes(x, y)) +
    geom_tile(data = dag_df, aes(fill = tile_fac), height = 1.75, width = 1.75, alpha = 0.5) +
    geom_text(data = points_df, aes(label = name), size = 5) +
    geom_dag_edges(aes(xend = xend, yend = yend),
                   edge_width = 0.4,
                   arrow_directed = grid::arrow(length = grid::unit(4, "pt"), type = "closed")) +
    scale_fill_manual("Effect of D on Y identified?", values = fill_scale, drop = FALSE, guide = guide_legend(nrow = 2)) +
    facet_grid(YX_fac ~ DX_fac, switch = "both", labeller = label_parsed) +
    coord_fixed() +
    theme_void() +
    theme(plot.title = element_text(size = 25, hjust = 0.5),
          plot.subtitle = element_text(size = 25, hjust = 0.5),
          plot.margin = margin(10, 10, 100, 10, unit = "pt")) +
    labs(subtitle = parse(text = as.character(unique(data$DY_fac))))
  if(as.character(unique(data$DY_fac)) == "D %->% Y"){
    g <- g + labs(title = as.character(unique(data$U_relationship_fac)))
  }
  g
}

my_fun <- function(data){
  data %>%
    split(.$DY_fac) %>%
    map(~subplot_function(.)) %>%
    wrap_plots(nrow = 1)
}

gg <- gg_df %>%
  split(.$U_relationship_fac) %>%
  map(my_fun)

wrap_plots(gg, ncol = 2, byrow = FALSE) + plot_layout(guides = "collect") &
  theme(
    legend.position = "bottom",
    legend.title = element_text(size = 35),
    legend.text = element_text(size = 35),
    legend.key.size = unit(35, "mm"),
    legend.margin = margin(t = 20, r = 0, b = 0, l = 0, unit = "mm"),
    strip.text = element_text(size = 25)
    )
```

#### Adding model-based assumptions

We can address the problem of unmeasured confounding by invoking conditional independence assumptions. The ``selection-on-observables'' answer strategy invokes the assumption that $D$ is statistically independent of the potential outcomes of $Y$ *given* X, i.e. after adjusting for $X$. In other words, controlling for $X$ blocks all backdoor paths between $D$ and $Y$. This assumption is also known as a conditional independence or ignorability assumption.

In Figure \@ref(fig:dagsignorability), we display the 216 possible causal graphs again, ruling out in gray those that are cyclical. Among the 200 that remain, we color in the same way as under controlling for $X$ but add a fourth color: lavender for DAGs that we rule out based on our conditional independence assumption. Those that are ruled out, in the upper left quadrants, have unobserved confounding from $U$ that cannot be addressed by adjusting for $X$.

Analysts who invoke this conditional independence assumption assure that there are many fewer circumstances in which identification is not possible either through controlling for $X$ or not controlling for $X$. However, this is a strong assumption that is not possible to test directly. Instead, the analyst must justify the assumption based on circumstantial qualitative or quantitative evidence. The task is to rule out through this evidence either a relationship between $U$ and $D$ or a relationship between $U$ and $Y$ or both. In the first case, this evidence might take the form of background knowledge about how values of $D$ are determined. The treatment might be assigned using a cut-off rule, in which case all those above the cut-off are assigned to treatment (e.g., are admitted to a college) and those below are not. In this case, there is no relationship between unobserved variables $U$ and treatment $D$, there is only a relationship between $X$ (score) and $D$. Controlling for $X$ will enable causal identification even if $Y$ is affected by $U$. However, the assumption of conditional independence between $U$ and $D$ given $X$ is a strong assumption: the analyst must be sure that there is no unobserved variable $U$ that directly affects $D$, such as legacy applicants who may be "pushed" over the threshold of the cut-off if they are close enough to it.

Under selection-on-observables, there are still many DAGs that raise problems for us. There are still many DAGs in which $D\leftarrow Y$, $D$ is caused by $Y$ instead of the other way around. If we cannot rule those out by assumption, we will never identify the causal effect of $D$ on $Y$ regardless of whether we control for $X$. The DAGs in blue (the effect is identified only when we condition on $X$ and not otherwise) and purple (the opposite, that we only achieve identification when we do not condition on $X$) still remain. Those in blue involve $X$ confounding the relationship between $D$ and $Y$ and those in purple are when $X$ is a collider so conditioning on it opens up a backdoor path to $U$. In other words, the conditional independence of $D$ and $U$ given $X$ is insufficient to identify the effect, without ruling out these other scenarios.

```{r dagsignorability, echo = FALSE, fig.cap = "Consequences of conditional ignorability assumption for all 216 possible DAGs.", fig.width = 30, fig.height = 21}
gg_df <-
  gg_df %>%
  mutate(tile_fac = as.factor(
    case_when(
      !acyclic ~ "Ruled out by acyclicity",
      consistent_with_ignorability == 0 ~ "Ruled out by ignorability",
      TRUE ~ as.character(id_adjustment_fac)
    )
  ))

fill_scale <- c(
  `Ruled out by acyclicity` = dd_light_gray,
  `Yes, regardless of conditioning` = "transparent",  
  `Only when NOT conditioning on X` = dd_purple,
  `Only when conditioning on X` = dd_light_blue,
  `No, regardless of conditioning` = dd_pink,
  `Ruled out by ignorability` = dd_dark_blue_alpha
)

subplot_function <- function(data) {
  dag_df <-
    data %>%
    group_by(var, DX_fac, YX_fac, tile_fac) %>%
    summarize(x = 1.5, y = 1.5, n = n(), .groups = "drop")

  g <-
  ggplot(data, aes(x, y)) +
    geom_tile(data = dag_df, aes(fill = tile_fac), height = 1.75, width = 1.75, alpha = 0.5) +
    geom_text(data = points_df, aes(label = name), size = 5) +
    geom_dag_edges(aes(xend = xend, yend = yend),
                   edge_width = 0.4,
                   arrow_directed = grid::arrow(length = grid::unit(4, "pt"), type = "closed")) +
    scale_fill_manual("Effect of D on Y identified?", values = fill_scale, drop = FALSE, guide = guide_legend(nrow = 2)) +
    facet_grid(YX_fac ~ DX_fac, switch = "both", labeller = label_parsed) +
    coord_fixed() +
    theme_void() +
    theme(plot.title = element_text(hjust = 0.5),
          plot.subtitle = element_text(hjust = 0.5)) +
    labs(subtitle = parse(text = as.character(unique(data$DY_fac))))
  if(as.character(unique(data$DY_fac)) == "D %->% Y"){
    g <- g + labs(title = as.character(unique(data$U_relationship_fac)))
  }
  g
}

my_fun <- function(data){
  data %>%
    split(.$DY_fac) %>%
    map(~subplot_function(.)) %>%
    wrap_plots(nrow = 1)
}

gg <- gg_df %>%
  split(.$U_relationship_fac) %>%
  map(my_fun)

wrap_plots(gg, ncol = 2, byrow = FALSE) + plot_layout(guides = "collect") &
  theme(
    legend.position = "bottom",
    legend.title = element_text(size = 28),
    legend.text = element_text(size = 28),
    legend.key.size = unit(25, "mm")
  )
```

#### Design-based identification

When we are unable to rule out confounding by assumption and adjustment, we can randomly assign $D$ to sever connections between unobserved variables $U$ and $D$ by design. Ruling out confounders by assumption or adjustment requires the *model* to be correct, so is often known as model-based inference, whereas ruling out confounders by design is labeled design-based inference.^[In truth, there is a continuum between the two. Design-based inferences that rely on non-parametric estimators of the average treatment effect using data from a randomized experiment are a classic design-based estimator, yet they also rely on a modeling assumption: the stable unit treatment-value assumption.] In addition, we can measure $X$ before treatment to rule out situations in which $D$ causes $X$, which can lead to collider bias or opening backdoor paths from $D$ to $Y$.

In doing so, we dramatically reduce the set of possible DAGs, because we set the causal order between $X$ and $D$, and dramatically expand the number of settings under which the effect of $D$ on $Y$ is identified both due to this restriction on causal order and the randomization of $D$ which guarantees ignorability of $U$.

In Figure \@ref(fig:dagsrandomization), we swap the colors to now indicate DAGs ruled out by measuring $X$ before treatment (salmon squares), and those ruled out by random assignment (blue squares). In all of the remaining white squares, the effect of $D$ on $Y$ is causally identified, regardless of whether we adjust for $X$ or not. We remove the conditionality of inference depending on whether we control. This is good, because ultimately even in the presence of strong ignorability assumptions outlined in the last section there are many possible DAGs in which controlling or failing to control lead to bias. Now, our inferences do not depend on guessing the correct DAG.

```{r dagsrandomization, echo = FALSE, fig.cap = "Consequences of randomizing Z and measuring X before treatment for all 216 possible DAGs.", fig.width = 30, fig.height = 21}
gg_df <-
  gg_df %>%
  mutate(tile_fac = as.factor(if_else(
    !acyclic,
    "Ruled out by acyclicity",
    as.character(id_adjustment_fac)
  )))

gg_df <-
  gg_df %>%
  mutate(
    tile_fac2 = as.factor(case_when(
      ruled_out_by == "Acyclicity" ~ "Ruled out by acyclicity",
      ruled_out_by == "Measuring X before treatment" ~ "Ruled out by pretreatment measurement",
      ruled_out_by == "Random assignment" ~ "Ruled out by random assignment",
      is.na(ruled_out_by) ~ "Effect of D on Y identified"
    ))
  )

fill_scale <- c(
  `Ruled out by acyclicity` = dd_light_gray,
  `Ruled out by pretreatment measurement` = dd_orange,  
  `Ruled out by random assignment` = dd_dark_blue,
  `Effect of D on Y identified` = "transparent"
)

subplot_function <- function(data) {
  dag_df <-
    data %>%
    group_by(var, DX_fac, YX_fac, tile_fac2) %>%
    summarize(x = 1.5, y = 1.5, n = n(), .groups = "drop")

  g <-
    ggplot(data, aes(x, y)) +
    geom_tile(data = dag_df, aes(fill = tile_fac2), height = 1.75, width = 1.75, alpha = 0.5) +
    geom_text(data = points_df, aes(label = name), size = 5) +
    geom_dag_edges(aes(xend = xend, yend = yend),
                   edge_width = 0.4,
                   arrow_directed = grid::arrow(length = grid::unit(4, "pt"), type = "closed")) +
    scale_fill_manual("Situation", values = fill_scale, drop = FALSE, guide = guide_legend(nrow = 2)) +
    facet_grid(YX_fac ~ DX_fac, switch = "both", labeller = label_parsed) +
    coord_fixed() +
    theme_void() +
    theme(plot.title = element_text(hjust = 0.5),
          plot.subtitle = element_text(hjust = 0.5)) +
    labs(subtitle = parse(text = as.character(unique(data$DY_fac))))
  if(as.character(unique(data$DY_fac)) == "D %->% Y"){
    g <- g + labs(title = as.character(unique(data$U_relationship_fac)))
  }
  g
}

my_fun <- function(data){
  data %>%
    split(.$DY_fac) %>%
    map(~subplot_function(.)) %>%
    wrap_plots(nrow = 1)
}

gg <- gg_df %>%
  split(.$U_relationship_fac) %>%
  map(my_fun)

wrap_plots(gg, ncol = 2, byrow = FALSE) + plot_layout(guides = "collect") &
  theme(
    legend.position = "bottom",
    legend.title = element_text(size = 28),
    legend.text = element_text(size = 28),
    legend.key.size = unit(25, "mm")
  )
```

In short, what we do by controlling the timing of measurement of $X$ and randomizing $D$ is to move our assumptions about conditional independence from $M$ our assumed (but possibly incorrect!) model of the world into our data strategy, which we control and so can guarantee by design.

### Estimation

The identification of the causal effect of $D$ on $Y$ either by assumption or design enables us to undertake two tasks: estimate the average treatment effect, estimate the sign of the effect, or estimate whether there is an affect or not.

The first task, estimating the magnitude of the effect of $D$ on $Y$, can be accomplished using model-based inference under the selection-on-observables design or under a randomized experiment. In both cases, we apply the plug-in principle, replacing the true but unknown average potential outcomes under treatment (control) with the sample analogues, the average outcomes in the treatment (control) group. With data from randomized experiments, $X$ is ignorable, so we can either adjust for it (which may reduce variability in estimates) or not without bias. But under selection-on-observables, we have to rule out many DAGs by assumption that include $X$ as a collider or in which $X$ opens a backdoor path between $D$ and $Y$ in order to safely select an answer strategy.

For estimating the sign of the effect, we can calculate the sign of the effect magnitude, and then conduct a statistical test of the null hypothesis of zero effect to distinguish among zero, positive, and negative effects.

The third task, determining whether there is an effect or not, similarly involves a statistical test of the null hypothesis of zero effect. If we fail to reject the null, then our posterior belief is that there is no effect, but if we reject in a two-sided test then we leave believing there is an effect. This zero average effect null hypothesis test can help us take the final step in distinguishing among the 216 possible DAGs representing the relationships between $D$ and $Y$ and the confounders $X$ and $U$. In Figure XX, we display the 16 DAGs that are identified under random assignment and pretreatment measurement of $X$. We need to use data and the two-sided null hypothesis test to distinguish between the top row (there is an effect $D\rightarrow Y$) and the bottom row (there is no effect). Our design got us most of the way there, but then we need to use the data to narrow further to one of the two rows of eight DAGs. Since our inquiry is about the causal relationship between $D$ and $Y$, we may not be concerned about distinguishing among the eight. If we are, we need to develop an alternative research design in order to learn about the causal effects of $X$.

```{r, echo = FALSE, fig.width = 30, fig.height = 21}
gg_df <-
  gg_df %>%
  filter(is.na(ruled_out_by)) %>%
  mutate(
    ruled_out_by_sig_test = as.factor(if_else(DY_fac == "D~~~Y", "Ruled out by rejection of null hypothesis of no effect", "Cannot rule out")),
    fct_rows = paste0(YX_fac, YU_fac, XU_fac)
  )

fill_scale <- c(
  `Ruled out by rejection of null hypothesis of no effect` = dd_dark_blue,
  `Cannot rule out` = "transparent"
)

dag_df <-
  gg_df %>%
  group_by(var, DX_fac, DY_fac, YX_fac, ruled_out_by_sig_test, fct_rows) %>%
  summarize(x = 1.5, y = 1.5, n = n(), .groups = "drop")

g <-
  ggplot(gg_df, aes(x, y)) +
  geom_tile(
    data = dag_df,
    aes(fill = ruled_out_by_sig_test),
    height = 1.75,
    width = 1.75,
    alpha = 0.5
  ) +
  geom_text(data = points_df, aes(label = name), size = 5) +
  geom_dag_edges(
    aes(xend = xend, yend = yend),
    edge_width = 0.4,
    arrow_directed = grid::arrow(length = grid::unit(4, "pt"), type = "closed")
  ) +
  scale_fill_manual("Situation", values = fill_scale, drop = FALSE) +
  facet_grid(DY_fac ~ fct_rows) +
  coord_fixed() +
  theme_void() +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5),
    legend.position = "bottom",
    legend.title = element_text(size = 28),
    legend.text = element_text(size = 28),
    legend.key.size = unit(25, "mm"),
    strip.text.y = element_blank()
  )

g
```

### Robustness

Robustness checks are modified answer strategies that aim to demonstrate the "robustness" or "sensitivity" to answer strategy choices. Often a primary analysis about a quantity of interest such as the effect of treatment $D$ on outcome $Y$ is presented and then in an appendix a set of modified analyses are presented. The author discusses whether or not there is variation in the magnitude and statistical significance of the estimates across these specifications. 

Robustness checks are implicitly a response to the fundamental uncertainty we identified in the last several sections over which is the true DAG. We do not know! As a result, each robustness check should be motivated by a particular DAG. When we complete a set of robustness checks, we can then make claims about how "robust" our estimates of the effect of $D$ on $Y$ are to alternative DAGs. Each robustness analyses should aim to falsify a DAG, as in the case of controlling for $X$. If we have three variables $X_1$, $X_2$, and $X_3$ that either confound the relationship between $D$ and $Y$ or play no role in the causal system, then we may want to present robustness checks including and excluding each combination of the three variables. In doing so, we rule out each variable as a collider or a mediator by assumption. These analyses can help distinguish among DAGs and give us evidence about the effect of $D$ and $Y$ across *possible* DAGs that involve confounding by some combination of $X_1$, $X_2$, and $X_3$. However, it is important to highlight the assumptions we make in selecting the DAGs that are ruling in or out these relationships. In order for controlling for $X_2$ to be a sensible strategy, we must invoke the assumption that $X_2$ is not a collider. If it is possible that it is, it is not clear what we can learn from adjusting for $X_2$. If we find its inclusion wipes out the effect of $D$ on $Y$ this may not be because there is no causal relationship but because $X_2$ is a collider.

<!-- two other kinds of robustness checks: functional form and subsetting. subsetting is about the units in which our assumptions about causal relations are met. functional form is about rejecting parametric structural causal models. -->


<!-- Robustness checks are part of the answer strategy. Often, a single estimator is presented as the main analysis but then a series of alternative specifications are displayed in an appendix (such as including or excluding covariates and their interactions, different subsets of the data, or alternative statistical models). These differ from multiple estimates of a latent quantity in that the goal is not a primary analysis, but rather to support the main analysis. The purpose is to provide readers with evidence about how dependent the main results are on the specification, data subset, and statistical model used. The decision a reader makes from a paper depends not only on the main estimate but also the robustness checks. As a result, we want to assess the properties of the two together. -->

<!-- [NOTE TO ADD HERE]: robustness checks are often more about $M$ than about $A$ -- we see how $a^d$ changes when we change $A$ in order see how our inferences change under alternative $M$s. -->

We illustrate with a simple analysis of the correlation between two variables `y1` and `y2`, who have a true positive correlation. `y2` is also a function of an observed covariate `x` and measurement error. Our main analysis is a bivariate regression predicting `y2` with `y1`. We compare this answer strategy to one in which we run that analysis, but also run a robustness check controlling for `x`. We do this because as the analyst we are unsure of the true DGP and wish to demonstrate to reviewer's that our results are not dependent on the functional form we choose.

<!-- - distinguish this from changes to the model where we do robustnesss vis a vis a fixed answer and data strategy. i.e. two notions of "robustness". one is fix I D A and change M, is this "design" robust to changes in M. the other is, within a given run, is the estimate "robust" to changing the estimation procedure, so this is a diagnostic statistic. note I must be defined across these changes in M. -->

Using the MIDA way of thinking about designs, we discuss in the diagnosis section another notion of the "robustness" of a design. The typical way we think of robustness checks is multiple secondary analyses *conditional on the observed data* to build confidence in an analysis of that fixed data. However, the motivation for these robustness checks is uncertainty about the true data generating process. By declaring a design in terms of MIDA, we can think about the robustness of a *single* estimator to multiple possible true data generating processes. An estimator that is robust in this sense is one that is unbiased with low uncertainty regardless of, say, the true functional form between `y1` and `y2`. To determine whether an estimator is robust, we can redefine a set of designs with different functional forms and assess the rate of correct decisions of our robustness checks strategy under each different model.


## Following the inquiry

Answering descriptive inquiries involves studying the existence of nodes and the values they take on. We might study whether a behavior exists in the world, and measure its frequency. We might also study how the frequency of the behavior varies over time and across people. Our answer strategy, following the plug-in principle, will often involve the average value of the node in the sample as an estimator of the average value in the population.

By contrast, when we target a causal inquiry with our answer strategy, we are interested in the existence (or non-existence) of an edge between two nodes. We can label the two $D$ (treatment) and $Y$ (outcome).


### Plug-in principle

The plug-in principle refers to the idea that good estiamtes of population quantities $I(m) = a^m$ can often be generated by choosing an $A$ that is very similar to $I$ and then "plugging-in" $d$ for $m$. Suppose that our inquiry is the average treatment effect among the $N$ units in the population.

$I(m) = \frac{1}{N}\sum_1^N[Y_i(1) - Y_i(0)] = \frac{1}{N}\sum_1^NY_i(1) - \frac{1}{N}\sum_1^NY_i(0) = ATE$

We can develop a plug-in estimator of the average treatment effect by replacing the population means ($\frac{1}{N}\sum_1^NY_i(1)$ and $\frac{1}{N}\sum_1^NY_i(0)$) with sample analogues:

$A(d) = \frac{1}{m}\sum_1^m{Y_i} - \frac{1}{N - m}\sum_{m+1}^N{Y_i}$,

where units 1 though $m$ reveal their treated potential outcomes and the remainder reveal their untreated potential outcomes. This difference-in-means estimator of course depends on the strength of the analogy between the population mean and the sample mean, which in turn depends on features of the Data strategy. Under many (but not all) forms of random assignment, the sample mean estimators of the treated and untreated outcomes will be unbiased estimators. As discussed in Section \@ref(p2followdatastrategy), the specifics of the random assignment strategy such as probabilities of assignment that vary by unit must be taken into account. 

More formally, Aronow and Miller describe a plug-in estimator as:

> For i.i.d. random variables $X_1, X_2, \ldots, X_n$ with common CDF $F$, the plug-in estimator of $\theta = T(F)$ is: $\widehat\theta = T(\widehat F)$.



### Point estimation

Point estimation is possibly the most common class of answer strategy in quantitative social science. Point estimators are things like the difference-in-means, ordinary least squares regression, instrumental variables regression, random forests, the LASSO, ridge regression, BART... the list goes on and on. These are the data analysis tools taught in many graduate methods courses. What's incredible is that even though the population of estimators proliferates with each passing year, the number of kinds of inquiries they are useful for estimating stays relatively flat. When we do point estimatation, we are mainly interested in estimating averages, differences, variances, and conditional expectation functions of increasing dimensionality. There are of course many other kinds of estimands (like ratios and quantiles), but the main point is, most of the variation in how scholars conduct point estimation is in the answer strategy, not in the inquiry.

- Bias variance tradeoff
 - overfitting
 - add modeling assumptions that might decrease variance a lot at the cost of a little bias.
 - interacts with testing: could win on RMSE, but have a higher false positive rate.

(point people to Hastie and Tibshirani)

### Estimating the variance

Variance estimation is often more difficult than point estimation.

We can often do the bootstrap, which plugs in (re-) sampling from the sample for sampling from the population.

<!-- (s.e., CIs, posterior probs) -->

### Tests

Tests are an elemental kind of answer strategy. Tests yield binary yes/no answers to an inquiry. In some qualitative traditions, hoop tests, straw-in-the-wind tests, smoking-gun tests, and doubly-decisive tests are common. These tests are procedures for making analysis decisions in a structured way. In frequentist statistics, significance tests are used to make a decision whether to reject or fail to reject a particular null hypothesis.

<!-- Null hypothesis significance testing has developed a (rightfully) sketchy reputation in recent years as too much weight has been put on the reject / fail to reject decision. That said, sometimes that decision is rightfully important and in such cases, null hypothesis significance testing is a great tool. -->

<!-- - significance testing (incl. sign test) -->
<!-- - equivalence testing -->
<!-- - straw in the wind / hoop test -->
<!-- - doubly decisive -->
<!-- - 1997 book on tests -->

<!-- [cite erin and naoki on sign tests] -->
<!-- [cite erin on equivalence testinf] -->

### Partial identification

- confounding
- EV bounds
- trimming bounds


## Following the data strategy {#p2followdatastrategy}

However, in order to benefit from these two controlled decisions in our data strategy, we must follow the dictum due to R.A. Fisher to "analyze as you randomize." Your answer strategy should follow your data strategy. There are four components: make comparisons only across randomly-assigned conditions; analyze data at the level of random assignment; make comparisons only in groups within which random assignment was conducted (e.g., strata); and adjust for differences in probabilities of random assignment. A parallel set of rules govern answer strategies for descriptive inference: draw only on random samples for describing a population; analyze data at the level of random sampling (the primary sampling unit); and adjust for differences in probabilities of sampling.

Making comparisons across randomly-assigned conditions within groups in which random assignment was conducted directly follow from our comparison of causal identification by assignment vs. by design. If we make comparisons that rely on differences both in $D$ and $X$, for example a difference in treatment effects between two subgroups, we are susceptible to confounding from $U$, because $X$ is not randomly-assigned. Similarly, analysis of data from block-randomized experiments can be broken by failing to account for blocking, because blocks are not randomly assigned and in fact are typically constructed so that the outcomes of units in the block are similar within blocks and very different across blocks. If there are differential probabilities of assignment across blocks and we pool our data ignoring the blocking structure, our unweighted comparisons may be contrasting groups that differ systematically. For descriptive inferences, we must always adjust for different sampling inclusion probabilities. 

In random sampling, the sampling probability quantifies each person's likelihood of being in the sample. By taking the inverse of this probability, we can generate weights that allow us to make a really skewed sample representative of the target population. This basic insight can be extended from random sampling to many other contexts. It applies to random treatment assignment, where we use inverse probability weighting to make unbiased inferences in the presence of spillovers and heterogeneous assignment probabilities. It even extends to the use of poststratification in nonrandom sampling and inverse propensity weighting for unbiased causal inference in the presence of confounding. For this reason, understanding the connection between sampling probabilities and inverse probability weights is one of the most important concepts in descriptive and causal inference. However, where these weights come from and what they mean can often seem confusing. In fact, the basic intuition is very simple and the random sampling analogy extends directly to these other contexts.

When we sample, we select a subset of units to stand in for the unselected units. That's what we mean when we say the sample is "representative"---the units in it literally represent the units that are not in it (but could have been). We can even quantify the exact number of units in the population that each unit in the sample represents. If I sample $n = 10$ people from a group of $N = 1000$ people, then when I make an inference about the larger group from the smaller group, each of the 10 people in the sample is standing in for $N/n = 100$ people in the population. If I sample all people, then each person just represents one person, themselves: $N/n = 1$. Let's call $w$ the number of people in the population each person in the sample represents:  $w = N / n$. And let's call $p$ the probability of being in the sample: $p = n / N$. Notice that $p$ and $w$ are the reciprocal of one another. Since the number 1 divided by a fraction gives its reciprocal, we can use the inverse of $p$ to find $w$: $\frac{1}{p} = \frac{1}{1} \times \frac{N}{n} = w$. Now you know what an inverse probability weight is: it is a count of the number of people in the population that a given unit in the sample represents.

Now suppose you have two samples, each of size $n_1 = n_2 = 10$. The first sample is drawn from a population of 10 people, each of whom has $X = 1$. The second sample is drawn from a population of 20 people, each of whom has $X = 0$. We want to know the true mean of $X$ across both populations: $\bar{X} = \frac{10 + 0}{10 + 20} = \frac{1}{3}$. If we pool the data and take the average of $X$, we get the wrong answer: $\hat{\bar{X}} = \frac{1}{2}$. We get the wrong answer because people from the first group are overrepresented: while $1/2$ of the sample is from group 1, only $1/3$ of the combined population is from group 1. In terms of sampling probabilities and weights, every person in the first sample only represents one person: $\frac{1}{p_1} = \frac{1}{1} \frac{N_1}{n_1} = \frac{1}{1} \frac{1}{1} = 1 = w_1$, whereas every person in the second sample stands in for two people, $\frac{1}{p_2} = \frac{1}{1} \frac{20}{10} = 2 = w_2$. By weighting our sample estimate of the average by the number of people each person in the pooled sample represents, we recover the right answer:

$$\hat{\bar{X}}_{IPW} = \frac{\sum X_i w_i}{\sum w_i} = \frac{n_1 \times 1 \times w_1 + n_2 \times 0 \times w_2}{n_1 \times w_1 + n_2 \times w_2} = \frac{10 \times 1\times 1 + 10 \times 0 \times 2}{10 \times 1 + 10 \times 2} = \frac{1}{3} = \bar{X}.$$

Notice that this example is equivalent to a stratified random sampling strategy, in which units from group 1 are sampled with $p_1 = 1$ and units from group 2 are sampled with probability $p_2 = 1/2$. Any time we use stratified random sampling and the probabilities vary among strata, we should weight estimates so that each observation's contribution is equivalent to the number of units it represents, which corrects for any over- and underrepresentation introduced by the sampling procedure.

This way of thinking provides a framework for poststratification on nonrandom samples. Suppose a researcher comes across our dataset of 20 people. They don't know anything about the sampling strategy---to them, it's a non-random sample. However, they do know that there are 10 people from group 1 and 20 people from group 2 in the population, so they are able to construct estimates of the sampling probabilities using $\hat{p_1} = n_1/N_1$ and $\hat{p_2} = n_2/N_2$. From there, they can construct weights using $1/p_1$ and $1/p_2$ that will reweight their data so that it is representative. That's the basic intuition behind \textit{post}stratification: you use known population counts to reverse engineer the probability weights that a hypothetical stratified random sampling strategy would have given you. See entry X in the design library for more on this.

The analogy extends even further when we consider that all random treatment assignment procedures are also sampling procedures. Rather than sampling entire units, however, treatment assignments randomly sample from different potential outcomes. If you assign 5/15 people to treatment and the rest to control, for example, then the five treated potential outcomes need to "represent" $1/(5/15) = 3$ people's treated potential outcomes and the ten people assigned to control represent $1/(10/15) = 1.5$ people's control potential outcomes. If you tried to pool different experiments to get the average treatment effect across all people in all experiments, you would need to take account of the fact that different experiments might over or underrepresent treatment or control potential outcomes, and weight accordingly. This issue arises in block randomized designs where treatment assignment probabilities vary by block (see entry X in the design library), in designs with spillovers where spatial networks induce differential assignment probabilities for spillovers (see entry X in the design library), and in stepped wedge designs where earlier waves overrepresent control potential outcomes and later waves overrepresent treatment potential outcomes (see entry X in the design library). Finally, in the same way that poststratification uses covariates to construct sample weights corresponding to a hypothetical random \textit{sampling} procedure, matching and selection observable designs use covariates to reverse engineer probabilities of sampling treatment and control potential outcomes in order to reweight the data to account for the systematic selection of certain kinds of units into or out of treatment (confounding).

## Formalizing qualitative modes of inference

Most researchers don't implement qualitative answer strategies by writing and executing code on a computer. Instead, they use logic and common sense to arrive at an answer, weighing very different sources of evidence that might include arguments made by other researchers, transcripts or impressions from interviews and participant observation, and various kinds of archival records. Since so much of the inferential work resides in the mind of the researcher, formalizing qualitative modes of inference for design diagnosis presents a more formidable task than formalizing quantitative methods that are already available in code.

Yet it is possible and often simpler than you might think to represent qualitative modes of inference using code. The task is also becoming easier due to the increasing use of formal notation and software to represent qualitative inference that we describe below. Of course, any representation of an answer strategy in code will involve unrealistic simplifications. But unrealistically simplistic representations of the researcher's process still shed light on what is at stake in the often bewildering set of design choices that there are to make. 

### Boolean formalizations

One of the simplest ways to formalize a qualitative strategy is by constructing "if-then" statements, which computers do very well. For example, in @bccmapsr, we simulate a researcher employing a nested mixed methods analysis [@lieberman2005nested] to understand an average causal effect. Formalizing this procedure in even the simplest syntax revealed the many different choices facing the simulated researcher: what share of qualitative cases need to validate the large-N theory, for example, for the latter to be determined valid? How much effort to allocate to put into building versus testing theories? Moreover, it revealed that these choices mattered a lot to the likelihood of getting a good answer.   

### Bayesian formalizations

Recent developments in qualitative methods have sought to take Bayes' rule "from metaphor to analytic tool" [@BennettAppendix]. This approach characterizes qualitative inference as one in which prior beliefs about the world can be specified numerically and then are updated on the basis of evidence observed. At minimum, writing down such an answer strategy on a computer requires specifying beliefs, expressed as probabilities, about the likelihood of seeing certain kinds of evidence under different hypotheses. We provide a simple example of such a strategy in the design library. @HerronQuinn provide one approach to formalizing a qualitative answer strategy that focuses on understanding an average treatment effect. 
@humphreys2015mixing provide an approach that can be used to formalize answer strategies targeting both causal effect and causal attribution inquiries, while @fairfield2017explicit formalize a Bayesian approach that approaches causal attribution as a problem of attaching posterior probability to competing alternative hypotheses. 
@abell2019subjective suggest the use of "Supra-Bayesian" methods to aggregate multiple participant-provided narratives in ethnographic studies targeting causal attribution estimands. 

### Set-theoretic formalizations

Set-theoretic approaches to formalize qualitative reasoning employ deterministic concepts of causal necessity and sufficiency. Variants of qualitative comparative analysis (QCA), which typically seeks to understand the minimal set of combinations of factors sufficient to produce an outcome, have been implemented on computers, and are therefore straightforward to declare and diagnose.  QCA has been the subject of numerous simulation studies: (ADD HERE). We provide an example in @bccmapsr.

### Games as formal narratives

In counterfactual accounts of causality [see @dawid2000causal for an alternative], causal inferences often rely on speculations about a counterfactual state of affairs. The average outcome of the control group in an experiment, for example, represents the counterfactual average outcome of the treatment group had they not been assigned to the treatment. In comparative historical analyses, these conjectures often focus on the case level. Speaking of the "balance rule" introduced into the United States senate under the Missouri Compromise of 1820, and which provided a veto power to coalitions of Northern and Southern states over policy, @weingast1998political (p.176) speculates that 
>  Had the veto not been engineered to prevent Northern assaults on slavery, there would have been more assaults then actually observed, and some might have succeeded. The Southern veto in the senate made assaults on slavery relatively rare.

He reaches this conclusion through the analysis of a formal model in extensive form. 

<!-- Say more here about simulation of counterfactuals provided by formalized narratives as method for diagnosing different case comparison / critical juncture strategies.  -->


<!-- ### randomization inference -->

## Answer strategies as procedures

Consider a randomized experiment that seeks to estimate the causal effect of a treatment. The answer strategy is not just "Logistic Regression with Covariate adjustment". It includes every step in the process that takes the raw data, cleans and recodes it, considers 5 alternative estimators (DIM, OLS with covariate adjustment, a fancy thing your colleague suggested but you couldn't get to converge), before finally settling on logit.

**Multiple estimates.** Answer strategies can account how many statistical tests you are conducting. Often, when generating an answer to a single inquiry, we may construct multiple estimates that provide different types of answers of varying quality. When you present the results from many null hypothesis tests, the rate of falsely rejecting at least one of those tests even when all are true goes up, due to the multiple comparisons problem. If you plan to adjust for this problem, those adjustments are part of your answer strategy, because they will typically adjust the p-values you report and the decisions readers make with them. We may have three survey items that imperfectly estimate a latent quantity. In presenting the results, we could present three estimates from three regressions, we could adjust the three estimates using a procedure such as a family-wise error rate correction, or we could average the three items together into an index and present one estimate from one regression. Which of these three methods we select will change the properties of our answer strategy.

**Analysis procedures**. The final estimator that goes into a paper is neither the beginning nor the end of the answer strategy. Procedures, if any, by which you explore the data and determine a final set of estimates are part of the answer strategy. Procedures for summarizing multiple estimates are one example of many.

Commonly, the final estimator that is selected depended on a exploratory procedure in which multiple models are assessed, for example by comparing model fit statistics. The answer strategy of our research design is not to fit the final model --- is it this multiple step if-then procedure. These procedures may be part of a prespecified analysis plan or they may be informal, so it may sometimes only be possible to declare the full design after the data is obtained. (We may find that a different analysis procedure that was not data dependent would have been preferable, if we diagnose the design after the fact.) The reason to declare the procedure rather than the final estimator is that the diagnosis of the design may differ. The procedure may be more powerful, if for example we assessed multiple sets of covariate controls and selecting the specification with the lowest standard error of the estimate. But the procedure may also exhibit poor coverage, accounting for these multiple bites at the apple.

We also sometimes find that the model we planned to run to analyze the data cannot be estimated. In these cases, there is an iterative estimation procedure in which a first model is run, changes to the specification are made, and a second or third model is presented as the result. The full set of steps --- a decision tree, depending on what is estimable --- is the answer strategy and we can evaluate whether it is a good one not only under the realized data but under other possible realizations where the decision *tree* would be the same but the decisions different.

In fact, there are examples of analysis procedures in most types of research, quantitative or qualitative. Many strategies for causal inference with observational data involve not only an estimation strategy but a set of falsification or placebo tests. The answer provided by these research designs depends in a crucial way on the results of these tests: if the tests fail, the design provides no definitive answer. In qualitative research, process tracing involves a set of steps, the results of which depend on information gathered in earlier steps. Many mixed methods strategies are also multi-step procedures. Nested designs involve running a quantitative analysis and then selecting cases on the basis of predictions from the regression. These designs cannot be assessed by considering a single step of the procedure in isolation.
<!-- need cites -->

**When things do not go according to plan.** To compare answer strategies, you can imagine the estimators that are possible *if things go well* as well as *if things go wrong*, when there is missing data or there are outliers in variables. A good answer strategy, which might be a single estimator, or a procedure if-this-then-that, can handle both states of the world. Procedures for addressing deviations from expected analyses are part of the answer strategy. Even in the absence of a preanalysis plan, we often have a way we expect to analyze the data if things go well. When they do not --- because data are missing, there is noncompliance with an intervention, or the study is suspended for example --- the answers will change. These procedures determine the answer the study provides (or in some cases does not), so are part of the answer strategy. *Standard operating procedures* (lin and green) are documents that systematize these procedures in advance.

We demonstrate the fact that the properties of procedures differ from the properties of a design with the final estimator in a simple example. We compare two possible estimation specifications, with and without covariates, to a procedure in which we run both models and report the model in our paper that has the lower p-value. The models are exactly the same, but the properties of the *procedure* differ from the properties of either of the two possible models. In particular, the procedure has higher power than either of the two models, but it exhibits poor coverage, which means we have a bias in our measure of uncertainty.

## Interpretation

The answer strategies we have described thus far also yield a single answer, whether it is a point estimate, a set of bounds, or a p-value. Yet these take up only a tiny proportion of the writeup of a study's results. Much more ink is taken up by the tables that report on them, the figures that visualize them, and the results and discussion sections that describe them. Importantly, how we report, visualize, and describe results will change depending on the realization of the data. These choices often depend on the results both in simple ways, such as the scale of the outcomes changing, and in more complex ways, such as whether a result is even reported on in the paper if it is not statistically significant. 

How we report, visualize, and describe results is part of the answer strategy. What answer we provide to readers depends on these components, and not only the numbers that pop out of our regression. Considering how these aspects of our answer strategy change depending on how the data strategy turns out 

<!-- How results are presented in tables and figures is an important part of the answer strategy. Considerable attention has been paid to how to display data, including arguments for switching from tables to graphs as the primary way to present statistical models (Kastellec) and for visually present raw data and models together (Coppock). The reason for this attention is that what inferences readers make when reading a paper depends not only on the statistical procedures used for estimation but the medium in which they are displayed. When numerical estimates are not provided at all, and only visualizations of results, clearly aesthetic choices about which estimates are displayed and even the width of axes will determine what the reader takes away from the answer strategy. This principle applies not just to tables and visualizations; how we describe results in the text of a paper also shapes readers' inferences from the data. Registered reports are a format for preparing scientific papers that involves prespecifying not only the form of graphs and tables but the text the author plans to write depending on the results. In short, decisions made from your results by readers are not just a function of numerical estimates but how they are presented. -->

## Further reading

- @gelman2006data and [other stories] on multilevel modeling.
- @aronow2016does on the generalizability of regression estimators in observational settings.
- @VanEvera1997 on hoop tests.
