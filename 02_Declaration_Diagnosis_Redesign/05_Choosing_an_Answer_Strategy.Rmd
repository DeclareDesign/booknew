---
title: "Choosing an answer strategy"
output:
  html_document:
    toc: yes
    toc_depth: 4
  pdf_document:
    toc: yes
    toc_depth: '4'
bibliography: ../bib/book.bib
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# Choosing an answer strategy

<!-- make sure to rename the section title below -->

```{r choosing_an_answer_strategy, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 10
b_sims <- 10
```

Making good on M:I::D:A

We want to pick an A() s.t. A(d) = a^d is close to a^m. We have to design w. r. t. a^m. 

The distance between a^d and a^m depends on assumptions. When we can assure those assumptions with design, we can engage in design-based inference. That is, when we can ensure that treatment assignment is independent of potential outcomes by conducting a literal random assigment, we can rely on known features of the randomization process to conduct inference. In the absence of literal random assignment, we have to assume (perhaps after statistical adjustment) that the treatment is "as-if" randomly assigned. When the required assumption are part of M() rather than part of D(), we engage in model-based inference.

- use a DAG (M!), informed by your inquiry (plug in principle), analyze as you randomize

- put elemental kinds of answer strategies (point, uncertainty, tests, intervals, visualization,  written narratives)

How results are presented in tables and figures is an important part of the answer strategy. Considerable attention has been paid to how to display data, including arguments for switching from tables to graphs as the primary way to present statistical models (Kastellec) and for visually present raw data and models together (Coppock). The reason for this attention is that what inferences readers make when reading a paper depends not only on the statistical procedures used for estimation but the medium in which they are displayed. When numerical estimates are not provided at all, and only visualizations of results, clearly aesthetic choices about which estimates are displayed and even the width of axes will determine what the reader takes away from the answer strategy. This principle applies not just to tables and visualizations; how we describe results in the text of a paper also shapes readers' inferences from the data. Registered reports are a format for preparing scientific papers that involves prespecifying not only the form of graphs and tables but the text the author plans to write depending on the results. In short, decisions made from your results by readers are not just a function of numerical estimates but how they are presented.

```{r make-all-models, echo = FALSE}
possible_models <-
  expand.grid(
    XU = c("X<-U", "none"),
    YU = c("Y<-U", "none"),
    YX = c("Y->X", "Y<-X", "none"),
    DU = c("D<-U", "none"),
    DX = c("D->X", "D<-X", "none"),
    DY = c("D->Y", "D<-Y", "none"),
    stringsAsFactors = FALSE
  )

possible_models <-
  possible_models %>%
  rowwise() %>%
  mutate(
    var = str_c(XU, YU, YX, DU, DX, DY, sep = ";"),
    var = str_remove_all(var, "none;"),
    var = str_remove_all(var, "none"),
    dag = list(dagitty(paste0(
      "dag{", var, "; X; D; Y; U}"
    ))),
    acyclic = isAcyclic(dag),
    consistent_with_ignorability = !(DU == "D<-U" & YU == "Y<-U"),
    id_if_adjusted = isAdjustmentSet(dag, "X", exposure = "D", outcome = "Y"),
    id_if_unadjusted = isAdjustmentSet(dag, NULL, exposure = "D", outcome = "Y"),
    id_adjustment_fac = as.factor(
      case_when(
        id_if_unadjusted &
          id_if_adjusted ~ "Yes, regardless of conditioning",
        id_if_unadjusted &
          !id_if_adjusted ~ "Only when NOT conditioning on X",!id_if_unadjusted &
          id_if_adjusted ~ "Only when conditioning on X",!id_if_unadjusted &
          !id_if_adjusted ~ "No, regardless of conditioning"
      )
    ),
    consistent_with_RA = DU != "D<-U" &
      DX != "D<-X" & DY != "D<-Y",
    consistent_with_X_pretreatment =
      DX != "D->X" &
      YX != "Y->X" &
      !(DU == "D->U" & XU == "X<-U") &
      !(YU == "Y->U" & XU == "X<-U")
  )

nested_data <-
  possible_models %>%
  filter(var != "") %>%
  mutate(dag_data = list(as_tibble(tidy_dagitty(dag))))

points_df <-
  tibble(name = as.factor(c("D", "X", "Y", "U")),
         x = c(1, 2, 2, 1),
         y = c(1, 2, 1, 2))
ends <-
  points_df %>%
  rename(to = name,
         xend = x,
         yend = y)

fix_no_edges <-
  tibble(
    var = "no edges",
    acyclic = TRUE,
    consistent_with_RA = TRUE,
    consistent_with_X_pretreatment = TRUE,
    XU = "none",
    YU = "none",
    YX = "none",
    DU = "none",
    DX = "none",
    DY = "none",
    name = "X",
    id_adjustment_fac = "Yes, regardless of conditioning"
  )

gg_df <-
  nested_data %>%
  unnest(cols = dag_data) %>%
  select(-x, -y, -xend, -yend) %>%
  left_join(points_df) %>%
  left_join(ends) %>%
  bind_rows(fix_no_edges) %>%
  mutate(
    XU_fac = factor(
      XU,
      levels =  c("X<-U", "X->U", "none"),
      labels =  c("X %<-% U", "X %->% U", "X~~~U")
    ),
    YU_fac = factor(
      YU,
      levels =  c("Y<-U", "Y->U", "none"),
      labels =  c("Y %<-% U", "Y %->% U", "Y~~~U")
    ),
    YX_fac = factor(
      YX,
      levels =  c("Y<-X", "Y->X", "none"),
      labels =  c("Y %<-% X", "Y %->% X", "Y~~~X")
    ),
    DU_fac = factor(
      DU,
      levels =  c("D<-U", "D->U", "none"),
      labels =  c("D %<-% U", "D %->% U", "D~~~U")
    ),
    DX_fac = factor(
      DX,
      levels =  c("D<-X", "D->X", "none"),
      labels =  c("D %<-% X", "D %->% X", "D~~~X")
    ),
    DY_fac = factor(
      DY,
      levels =  c("D<-Y", "D->Y", "none"),
      labels =  c("D %<-% Y", "D %->% Y", "D~~~Y")
    )
  ) %>%
  arrange(XU_fac, YU_fac, YX_fac, DU_fac, DX_fac, DY_fac)

gg_df <-
  gg_df %>%
  mutate(
    var_fac = factor(var, levels = c(possible_models$var, "no edges")),
    DY_DX = as.factor(paste0(DY, " ", DX)),
    U_relationship = paste0(XU, " ", YU, " ", DU),
    U_relationship_fac =
      factor(
        U_relationship,
        levels = c(
          'X<-U Y<-U D<-U',
          'none Y<-U D<-U',
          'X<-U none D<-U',
          'X<-U Y<-U none',
          'none none D<-U',
          'none Y<-U none',
          'X<-U none none',
          'none none none'
        ),
        labels = c(
          "U affects: D, X, Y",
          "U affects: D, Y",
          "U affects: D, X",
          "U affects: X, Y",
          "U affects: D",
          "U affects: Y",
          "U affects: X",
          "U affects: none"
        )
      ),
    ruled_out_by = as.factor(
      case_when(
        !acyclic ~ "Acyclicity",!consistent_with_RA ~ "Random assignment",!consistent_with_X_pretreatment ~ "Measuring X before treatment",
        TRUE ~ NA_character_
      )
    )
  )
```


## using model to inform answer strategy

- your inquiry defines was you are shooting at, now question is how to use model to choose an answer stategy to shoot at it

- types of paths table

- descriptive
- causal (obsefvational, experimental)

Definition: The backdoor criterion. Given an ordered pair of variables D, Y in a DAG G, a set of variables X, satifies the backdoor criterion relative to D, Y if no node in X is a descendant of D, and X blocks every path between D and Y that contains an arrow into D. (pg. 61 Pearl primer)

$$\Pr(Y = y \mid do(D=d)) = \sum_x \Pr(Y = y \mid D = d, X = x) \Pr(X = x)$$
$$\Pr(Y_i(D_i = 1) = y) = \sum_x \Pr(Y_i(D_i = 1) = y \mid X_i = x) \Pr(X_i = x)$$

Definition: The frontdoor criterion. A set of variables X is said to satisfy the frontdoor criterion relative to an ordered pair of variables D, Y if X intercepts all directed paths from D to Y, there is no unblocked path from D to X, all backdoor paths from X to Y are blocked by D. (pg. 69 Pearl primer)

$$\Pr(Y = y \mid do(D = d)) = \sum_x \Pr(X = x \mid D = d) \sum_{d^{\prime}} \Pr(Y = y \mid D = d^{\prime}, X = x) \Pr(D = d^{\prime})$$


### Descriptive analysis

Answering descriptive inquiries involves studying the existence of nodes and the values they take on. We might study whether a behavior exists in the world, and measure its frequency. We might also study how the frequency of the behavior varies over time and across people. Our answer strategy, following the plug-in principle, will often involve the average value of the node in the sample as an estimator of the average value in the population. 

### Causal analysis

By contrast, when we target a causal inquiry with our answer strategy, we are interested in the existence (or non-existence) of an edge between two nodes. 

Our aim is to distinguish between the 

#### Ruling out confounding by adjustment

```{r, echo = FALSE, fig.width = 30, fig.height = 21}
gg_df <-
  gg_df %>%
  mutate(tile_fac = as.factor(if_else(
    !acyclic,
    "Ruled out by acyclicity",
    as.character(id_adjustment_fac)
  ))
  ) 

fill_scale <- c(
  `Ruled out by acyclicity` = dd_light_gray,
  `Yes, regardless of conditioning` = "transparent",  
  `Only when NOT conditioning on X` = dd_purple,
  `Only when conditioning on X` = dd_light_blue,
  `No, regardless of conditioning` = dd_pink
)

subplot_function <- function(data) {
  dag_df <-
    data %>%
    group_by(var, DX_fac, YX_fac, tile_fac) %>%
    summarize(x = 1.5, y = 1.5, n = n())
  
  g <- 
  ggplot(data, aes(x, y)) +
    geom_tile(data = dag_df, aes(fill = tile_fac), height = 1.75, width = 1.75, alpha = 0.5) +
    geom_text(data = points_df, aes(label = name), size = 5) +
    geom_dag_edges(aes(xend = xend, yend = yend), 
                   edge_width = 0.4, 
                   arrow_directed = grid::arrow(length = grid::unit(4, "pt"), type = "closed")) +
    scale_fill_manual("Effect of D on Y identified?", values = fill_scale, drop = FALSE) + 
    facet_grid(YX_fac ~ DX_fac, switch = "both", labeller = label_parsed) +
    coord_fixed() + 
    theme_void() +
    theme(plot.title = element_text(hjust = 0.5), 
          plot.subtitle = element_text(hjust = 0.5)) + 
    labs(subtitle = parse(text = as.character(unique(data$DY_fac))))
  if(as.character(unique(data$DY_fac)) == "D %->% Y"){
    g <- g + labs(title = as.character(unique(data$U_relationship_fac)))
  }
  g
}

my_fun <- function(data){
  data %>%
    split(.$DY_fac) %>% 
    map(~subplot_function(.)) %>% 
    wrap_plots(nrow = 1) 
}

gg <- gg_df %>% 
  split(.$U_relationship_fac) %>% 
  map(my_fun) 

wrap_plots(gg, ncol = 2, byrow = FALSE) + plot_layout(guides = "collect") & theme(legend.position = "bottom") 
```

#### Ruling out confounding by assumption

We can address the problem of unmeasured confounding by invoking conditional independence assumptions. The ``selection-on-observables'' answer strategy invokes the assumption that D is statistically independent of the potential outcomes of $Y$ *given* X, i.e. after adjusting for $X$. This means that there are no unobserved confounders not in $X$. 

```{r, echo = FALSE, fig.width = 30, fig.height = 21}
gg_df <-
  gg_df %>%
  mutate(tile_fac = as.factor(
    case_when(
      !acyclic ~ "Ruled out by acyclicity",
      consistent_with_ignorability == 0 ~ "Ruled out by ignorability",
      TRUE ~ as.character(id_adjustment_fac) 
    )
  ))

fill_scale <- c(
  `Ruled out by acyclicity` = dd_light_gray,
  `Yes, regardless of conditioning` = "transparent",  
  `Only when NOT conditioning on X` = dd_purple,
  `Only when conditioning on X` = dd_light_blue,
  `No, regardless of conditioning` = dd_pink,
  `Ruled out by ignorability` = dd_dark_blue_alpha
)

subplot_function <- function(data) {
  dag_df <-
    data %>%
    group_by(var, DX_fac, YX_fac, tile_fac) %>%
    summarize(x = 1.5, y = 1.5, n = n())
  
  g <- 
  ggplot(data, aes(x, y)) +
    geom_tile(data = dag_df, aes(fill = tile_fac), height = 1.75, width = 1.75, alpha = 0.5) +
    geom_text(data = points_df, aes(label = name), size = 5) +
    geom_dag_edges(aes(xend = xend, yend = yend), 
                   edge_width = 0.4, 
                   arrow_directed = grid::arrow(length = grid::unit(4, "pt"), type = "closed")) +
    scale_fill_manual("Effect of D on Y identified?", values = fill_scale, drop = FALSE, guide = guide_legend(nrow = 1)) + 
    facet_grid(YX_fac ~ DX_fac, switch = "both", labeller = label_parsed) +
    coord_fixed() + 
    theme_void() +
    theme(plot.title = element_text(hjust = 0.5), 
          plot.subtitle = element_text(hjust = 0.5)) + 
    labs(subtitle = parse(text = as.character(unique(data$DY_fac))))
  if(as.character(unique(data$DY_fac)) == "D %->% Y"){
    g <- g + labs(title = as.character(unique(data$U_relationship_fac)))
  }
  g
}

my_fun <- function(data){
  data %>%
    split(.$DY_fac) %>% 
    map(~subplot_function(.)) %>% 
    wrap_plots(nrow = 1) 
}

gg <- gg_df %>% 
  split(.$U_relationship_fac) %>% 
  map(my_fun) 

wrap_plots(gg, ncol = 2, byrow = FALSE) + plot_layout(guides = "collect") & theme(legend.position = "bottom") 
```

#### Ruling out confounding by design

- same graph but slice the connectoins to unobserved confounding into D
- then block RA where we slice connections to unobserved confounding but not the X into D
- analyze as you randomize

```{r, echo = FALSE, fig.width = 30, fig.height = 21}
gg_df <-
  gg_df %>%
  mutate(tile_fac = as.factor(if_else(
    !acyclic,
    "Ruled out by acyclicity",
    as.character(id_adjustment_fac)
  )))

gg_df <-
  gg_df %>%
  mutate(
    tile_fac2 = as.factor(case_when(
      ruled_out_by == "Acyclicity" ~ "Ruled out by acyclicity",
      ruled_out_by == "Measuring X before treatment" ~ "Ruled out by pretreatment measurement",
      ruled_out_by == "Random assignment" ~ "Ruled out by random assignment",
      is.na(ruled_out_by) ~ "Effect of D on Y identified"
    ))
  )

fill_scale <- c(
  `Ruled out by acyclicity` = dd_light_gray,
  `Ruled out by pretreatment measurement` = dd_orange,  
  `Ruled out by random assignment` = dd_dark_blue,
  `Effect of D on Y identified` = "transparent" 
)

subplot_function <- function(data) {
  dag_df <-
    data %>%
    group_by(var, DX_fac, YX_fac, tile_fac2) %>%
    summarize(x = 1.5, y = 1.5, n = n())
  
  g <- 
    ggplot(data, aes(x, y)) +
    geom_tile(data = dag_df, aes(fill = tile_fac2), height = 1.75, width = 1.75, alpha = 0.5) +
    geom_text(data = points_df, aes(label = name), size = 5) +
    geom_dag_edges(aes(xend = xend, yend = yend), 
                   edge_width = 0.4, 
                   arrow_directed = grid::arrow(length = grid::unit(4, "pt"), type = "closed")) +
    scale_fill_manual("Situation", values = fill_scale, drop = FALSE) + 
    facet_grid(YX_fac ~ DX_fac, switch = "both", labeller = label_parsed) +
    coord_fixed() + 
    theme_void() +
    theme(plot.title = element_text(hjust = 0.5), 
          plot.subtitle = element_text(hjust = 0.5)) + 
    labs(subtitle = parse(text = as.character(unique(data$DY_fac))))
  if(as.character(unique(data$DY_fac)) == "D %->% Y"){
    g <- g + labs(title = as.character(unique(data$U_relationship_fac)))
  }
  g
}

my_fun <- function(data){
  data %>%
    split(.$DY_fac) %>% 
    map(~subplot_function(.)) %>% 
    wrap_plots(nrow = 1) 
}

gg <- gg_df %>% 
  split(.$U_relationship_fac) %>% 
  map(my_fun) 

wrap_plots(gg, ncol = 2, byrow = FALSE) + plot_layout(guides = "collect") & theme(legend.position = "bottom") 
```

## choosing an estimator

Point estimation is possibly the most common class of answer strategy in quantitative social science. Point estimators are things like the difference-in-means, ordinary least squares regression, instrumental variables regression, random forests, the LASSO, ridge regression, BART... the list goes on and on. These are the data analysis tools taught in many graduate methods courses. What's incredible is that even though the population of estimators proliferates with each passing year, the number of kinds of inquiries they are useful for estimating stays relatively flat. When we do point estimatation, we are mainly interested in estimating averages, differences, variances, and conditional expectation functions of increasing dimensionality. There are of course many other kinds of estimands (like ratios and quantiles), but the main point is, most of the variation in how scholars conduct point estimation is in the answer strategy, not in the inquiry.

(point people to Hastie and Tibshirani)

```{r, echo = FALSE, fig.width = 30, fig.height = 21}
gg_df <-
  gg_df %>%
  filter(is.na(ruled_out_by)) %>% 
  mutate(
    ruled_out_by_sig_test = as.factor(if_else(DY_fac == "D~~~Y", "Ruled out by rejection of null hypothesis of no effect", "Cannot rule out")),
    fct_rows = paste0(YX_fac, YU_fac, XU_fac)
  )

fill_scale <- c(
  `Ruled out by rejection of null hypothesis of no effect` = dd_dark_blue,
  `Cannot rule out` = "transparent"
)

dag_df <-
  gg_df %>%
  group_by(var, DX_fac, DY_fac, YX_fac, ruled_out_by_sig_test, fct_rows) %>%
  summarize(x = 1.5, y = 1.5, n = n()) 

g <- 
  ggplot(gg_df, aes(x, y)) +
  geom_tile(data = dag_df, aes(fill = ruled_out_by_sig_test), height = 1.75, width = 1.75, alpha = 0.5) +
  geom_text(data = points_df, aes(label = name), size = 5) +
  geom_dag_edges(aes(xend = xend, yend = yend), 
                 edge_width = 0.4, 
                 arrow_directed = grid::arrow(length = grid::unit(4, "pt"), type = "closed")) +
  scale_fill_manual("Situation", values = fill_scale, drop = FALSE) + 
  facet_grid(DY_fac ~ fct_rows) +
  coord_fixed() + 
  theme_void() +
  theme(plot.title = element_text(hjust = 0.5), 
        plot.subtitle = element_text(hjust = 0.5),
        legend.position = "bottom",
        strip.text.y = element_blank()) 

g
```


## when parameters are not point identified

- confounding
- EV bounds
- trimming bounds

## characterizing uncertainty (s.e., CIs, posterior probs)

## constructing tests (test value, p-value)

Tests are an elemental kind of answer strategy. Tests yield yes / no answers to specific tests. In some qualitative traditions, hoop tests, straw-in-the-wind tests, smoking-gun tests, and doubly-decisive tests are common. These tests are procedures for making analysis decisions in a structured way. In frequentist statistics, significance tests are used to make a decision whether to reject or fail to reject a particular null hypothesis. Null hypothesis significance testing has developed a (rightfully) sketchy reputation in recent years as too much weight has been put on the reject / fail to reject decision. That said, sometimes that decision is rightfully important and in such cases, null hypothesis significance testing is a great tool.

Tests are answer strategies that return TRUE or FALSE as an answer. 

- significance testing (incl. sign test)
- equivalence testing
- straw in the wind / hoop test
- doubly decisive
- 1997 book on tests

[cite erin and naoki on sign tests]
[cite erin on equivalence testinf]

## Answer strategies as procedures

Consider a randomized experiment that seeks to estimate the causal effect of a treatment. The answer strategy is not just "Logistic Regression with Covariate adjustment". It includes every step in the process that takes the raw data, cleans and recodes it, considers 5 alternative estimators (DIM, OLS with covariate adjustment, a fancy thing your colleague suggested but you couldn't get to converge), before finally settling on logit.

**Multiple estimates.** Answer strategies can account how many statistical tests you are conducting. Often, when generating an answer to a single inquiry, we may construct multiple estimates that provide different types of answers of varying quality. When you present the results from many null hypothesis tests, the rate of falsely rejecting at least one of those tests even when all are true goes up, due to the multiple comparisons problem. If you plan to adjust for this problem, those adjustments are part of your answer strategy, because they will typically adjust the p-values you report and the decisions readers make with them. We may have three survey items that imperfectly estimate a latent quantity. In presenting the results, we could present three estimates from three regressions, we could adjust the three estimates using a procedure such as a family-wise error rate correction, or we could average the three items together into an index and present one estimate from one regression. Which of these three methods we select will change the properties of our answer strategy. 

**Analysis procedures**. The final estimator that goes into a paper is neither the beginning nor the end of the answer strategy. Procedures, if any, by which you explore the data and determine a final set of estimates are part of the answer strategy. Procedures for summarizing multiple estimates are one example of many.

Commonly, the final estimator that is selected depended on a exploratory procedure in which multiple models are assessed, for example by comparing model fit statistics. The answer strategy of our research design is not to fit the final model --- is it this multiple step if-then procedure. These procedures may be part of a prespecified analysis plan or they may be informal, so it may sometimes only be possible to declare the full design after the data is obtained. (We may find that a different analysis procedure that was not data dependent would have been preferable, if we diagnose the design after the fact.) The reason to declare the procedure rather than the final estimator is that the diagnosis of the design may differ. The procedure may be more powerful, if for example we assessed multiple sets of covariate controls and selecting the specification with the lowest standard error of the estimate. But the procedure may also exhibit poor coverage, accounting for these multiple bites at the apple.

We also sometimes find that the model we planned to run to analyze the data cannot be estimated. In these cases, there is an iterative estimation procedure in which a first model is run, changes to the specification are made, and a second or third model is presented as the result. The full set of steps --- a decision tree, depending on what is estimable --- is the answer strategy and we can evaluate whether it is a good one not only under the realized data but under other possible realizations where the decision *tree* would be the same but the decisions different.

In fact, there are examples of analysis procedures in most types of research, quantitative or qualitative. Many strategies for causal inference with observational data involve not only an estimation strategy but a set of falsification or placebo tests. The answer provided by these research designs depends in a crucial way on the results of these tests: if the tests fail, the design provides no definitive answer. In qualitative research, process tracing involves a set of steps, the results of which depend on information gathered in earlier steps. Many mixed methods strategies are also multi-step procedures. Nested designs involve running a quantitative analysis and then selecting cases on the basis of predictions from the regression. These designs cannot be assessed by considering a single step of the procedure in isolation.
<!-- need cites -->


**When things do not go according to plan.** To compare answer strategies, you can imagine the estimators that are possible *if things go well* as well as *if things go wrong*, when there is missing data or there are outliers in variables. A good answer strategy, which might be a single estimator, or a procedure if-this-then-that, can handle both states of the world. Procedures for addressing deviations from expected analyses are part of the answer strategy. Even in the absence of a preanalysis plan, we often have a way we expect to analyze the data if things go well. When they do not --- because data are missing, there is noncompliance with an intervention, or the study is suspended for example --- the answers will change. These procedures determine the answer the study provides (or in some cases does not), so are part of the answer strategy. *Standard operating procedures* (lin and green) are documents that systematize these procedures in advance.



We demonstrate the fact that the properties of procedures differ from the properties of a design with the final estimator in a simple example. We compare two possible estimation specifications, with and without covariates, to a procedure in which we run both models and report the model in our paper that has the lower p-value. The models are exactly the same, but the properties of the *procedure* differ from the properties of either of the two possible models. In particular, the procedure has higher power than either of the two models, but it exhibits poor coverage, which means we have a bias in our measure of uncertainty. 

## Robustness

**Robustness checks** are part of the answer strategy. Often, a single estimator is presented as the main analysis but then a series of alternative specifications are displayed in an appendix (such as including or excluding covariates and their interactions, different subsets of the data, or alternative statistical models). These differ from multiple estimates of a latent quantity in that the goal is not a primary analysis, but rather to support the main analysis. The purpose is to provide readers with evidence about how dependent the main results are on the specification, data subset, and statistical model used. The decision a reader makes from a paper depends not only on the main estimate but also the robustness checks. As a result, we want to assess the properties of the two together. 

[NOTE TO ADD HERE]: robustness checks are often more about $M$ than about $A$ -- we see how $a^d$ changes when we change $A$ in order see how our inferences change under alternative $M$s.

We illustrate with a simple analysis of the correlation between two variables `y1` and `y2`, who have a true positive correlation. `y2` is also a function of an observed covariate `x` and measurement error. Our main analysis is a bivariate regression predicting `y2` with `y1`. We compare this answer strategy to one in which we run that analysis, but also run a robustness check controlling for `x`. We do this because as the analyst we are unsure of the true DGP and wish to demonstrate to reviewer's that our results are not dependent on the functional form we choose. 

<!-- - distinguish this from changes to the model where we do robustnesss vis a vis a fixed answer and data strategy. i.e. two notions of "robustness". one is fix I D A and change M, is this "design" robust to changes in M. the other is, within a given run, is the estimate "robust" to changing the estimation procedure, so this is a diagnostic statistic. note I must be defined across these changes in M. -->

Using the MIDA way of thinking about designs, we discuss in the diagnosis section another notion of the "robustness" of a design. The typical way we think of robustness checks is multiple secondary analyses *conditional on the observed data* to build confidence in an analysis of that fixed data. However, the motivation for these robustness checks is uncertainty about the true data generating process. By declaring a design in terms of MIDA, we can think about the robustness of a *single* estimator to multiple possible true data generating processes. An estimator that is robust in this sense is one that is unbiased with low uncertainty regardless of, say, the true functional form between `y1` and `y2`. To determine whether an estimator is robust, we can redefine a set of designs with different functional forms and assess the rate of correct decisions of our robustness checks strategy under each different model. 


