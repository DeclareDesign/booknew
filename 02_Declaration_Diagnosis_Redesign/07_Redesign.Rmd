---
title: "Redesign"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
```

<!-- start post here, do not edit above -->

# Redesign

<!-- make sure to rename the section title below -->

```{r redesign, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

Design diagnosis is about estimating the diagnosands for a single design. Redesign is about changing design parameters to understand how diagnosands change in reponse. The purpose of redesign is to choose a strong design among the set of feasible designs -- or to learn than no feasible design is worth implementing. 

We usually redesign with respect to the data and answer strategies. We tweak the data strategy by carying $N$ to see how RMSE changes. We change the answer strategy to see how the inclusion of covariates decreases bias. While we usually redesign the empirical side, redesign can take place with respect to the theoretical side (model and inquiry) as well.^[There are some subtleties here. Sometimes a single design includes ncertainty about the model -- for example when we draw parameter values from a random distribution. We have a parameter be drawn from a normal distribution centered at 0.3 with an SD of 0.1 because we think a resonable value for the parameter is "about three, plus or minus". We \textit{could} imagine that we are "redesigning" with respect to each value of parameter, but only if the goal is to see how the changes in the model change the diagnosands. Otherwise, it's not redesign, it's just plain diagnosis, averaging over multiple models.]

You may have encountered some figures that implicitly do redesign. Power curves are an example of the redesign process. A power curve has sample size on the horizontal axis and statistical power on the vertical axis. From a power curve, you can learn how big a study needs to be in order to achieve a desired level of statistical power. A "minimum detectable effect" figure is similar. It has sample size on the horizontal axis too, but plots the smallest effect size for which 80% power can be achieved. These plots are useful for learning something like, "given my budget, I can only sample 400 units. At 400 units, the MDE is a 0.5 standard deviation effect. My theory says that that the effect should be smaller than that, something closer to 0.1 SDs. I should apply for more funding or study something else."

The highest level advice we have about redesign is, at the beginning of the processes at least, change one design parameter at a time. Vary the data strategy, holding the model, inquiry, and answer strategy constant. Change the answer strategy, while holding the other aspects of the design constant. 

## Redesign Mountain

This figure shows a schematic of the redesign process. We vary design parameter 1 and design parameter 2 independently to learn the value of the diagnosand for each combination of design parameters. The redesign process can be in more than 2 dimensions of course. We suggest varying the design parameters you have control over first.

```{r, echo=FALSE}
expand.grid(param_1 = seq(0, 1, length.out = 100),
            param_2 = seq(0, 1, length.out = 100)) %>%
  mutate(diagnosand = 1 - sqrt((param_1 - 0.3) ^ 2 + (param_2 - 0.7) ^ 2)) %>%
  ggplot(aes(param_1, param_2)) + 
  geom_tile(aes(fill = diagnosand)) +
  geom_point(x = 0.3, y = 0.7, size = 2) +
  geom_contour(aes(z = diagnosand)) +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(x = "Design Parameter 1",
       y = "Design Parameter 2",
       title = "Diagnosands change as design parameters change")
```









## Redesign to assess the robustness of designs to models [GB]

- Hold inquiry constant! (read Richard Crump “Moving the Goalposts”)
- Hold three constant, vary one of MIDA at a time
- M: ICC, null model, alternative DAGs, heterogeneity
- I:






<!-- Diagnosis is like this: -->

<!-- (we need a math glyph for left relation composition notation) -->

<!-- $$ -->
<!-- MIDA |> \\ -->
<!--   simulate |> \\ -->
<!--   summarize -->
<!-- $$ -->

<!-- Redesign is like this -->

<!-- $$ -->
<!-- map(parameters) |> \\ -->
<!--   MIDA |>  \\ -->
<!--   simulate |> \\  -->
<!--   group\_by(parameters) |> \\ -->
<!--   summarize -->
<!-- $$ -->
