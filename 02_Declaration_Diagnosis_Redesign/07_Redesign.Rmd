---
title: "Redesign"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")

```

<!-- start post here, do not edit above -->

# Redesign

<!-- make sure to rename the section title below -->

```{r redesign, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

Diagnosis is the process of learning the value of a diagnosand and the utility of a research design. Often diagnosis is undertaken for a range of  specifications of  *M* to assess robustness to alternative models. 

Redesign is the process of *changing* parts of *D* and *A* (and, perhaps, *I*) in order to learn how diagnosand values and researcher utility change. The redesign process is complete when a researcher settles on a choice of *D* and *A* among the feasible set.

For example, we can compare how the distribution of errors changes if we use a different data strategy $D'$: $P_M(a^d - a^m|D')$ or a different answer strategy $A'$: $P_M(a^{d'} - a^m|D)$. In each case, as we examine variation in *D* and *A*, our diagnoses can assess how the distribution of errors differs for alternative model $M'$: $P_M(a^{d} - a^{m'}|D)$.

Sometimes redesign is about fixing problems. You diagnose a design and find that your estimator is shooting at an altogether different quantity than what you have in mind. In such cases, redesign is about fixing errors. 

Often however researchers wish to find the optimal design for their question subject to financial and logistical constraints. Even simple designs have many variations, so the search space must be limited. For example, a researcher might want to find the sample size $N$ and the number of treated units $N_t$ that minimize a design's error subject to a fixed budget for the experiment in which data collection for each unit costs \$25 and treating one unit costs \$5. They solve the optimization problem:

\begin{equation*}
\begin{aligned}
& \underset{N, N_t}{\text{argmin}}
& & E_M(L(a^{d} - a^{m}|D_{N, N_t})) \\
& \text{s.t.}
& & 25  N + 5  N_t \leq 5000
\end{aligned}
\end{equation*}

where $L$ is a loss function, increasing in the difference between $a^{d}$ and $a^{m}$.

There is no reason however to optimize with respect to a single diagnosand only. If we redesign by optimizing for the power diagnosand, we are likely to find a design that is highly powered but highly biased --- it is targeting the wrong estimand. We can solve this by switching to the root mean-squared error as the diagnosand, which reflects both the bias and efficiency of the design. More generally, the choice of an optimal design requires the researcher to select a weighting of diagnosands. They must balance bias, efficiency, the risk of imprecise null results, the risk of getting the sign of an effect wrong, and other diagnosands given their research goals.

The full evaluation of a design --- declaration, diagnosis, and redesign --- depends on the assessment of one or more diagnosands, and comparing the diagnoses to what could be achieved under alternative designs.

<!-- We usually redesign with respect to the data and answer strategies. We tweak the data strategy by changing $N$ to see how RMSE changes. We change the answer strategy to see how the inclusion of covariates decreases bias. While we usually redesign the empirical side, redesign can take place with respect to the theoretical side (model and inquiry) as well.^[There are some subtleties here. Sometimes a single design includes uncertainty about the model -- for example when we draw parameter values from a random distribution. We have a parameter be drawn from a normal distribution centered at 0.3 with an SD of 0.1 because we think a reasonable value for the parameter is "about three, plus or minus". We *could* imagine that we are "redesigning" with respect to each value of parameter, but only if the goal is to see how the changes in the model change the diagnosands. Otherwise, it's not redesign, it's just plain diagnosis, averaging over multiple models.] -->

## Graphical inspection 

One approach is to examine performance along multiple diagnosands visually. You may have encountered some figures that implicitly do redesign. Power curves are an example of the redesign process. A power curve has sample size on the horizontal axis and statistical power on the vertical axis. From a power curve, you can learn how big a study needs to be in order to achieve a desired level of statistical power. A "minimum detectable effect" figure is similar. It has sample size on the horizontal axis too, but plots the smallest effect size for which 80% power can be achieved. These plots are useful for learning something like, "given my budget, I can only sample 400 units. At 400 units, the MDE is a 0.5 standard deviation effect. My theory says that that the effect should be smaller than that, something closer to 0.1 SDs. I should apply for more funding or study something else."

The highest level advice for redesign decisions using this approach is, at the beginning of the processes at least, to change one design parameter at a time. Vary the data strategy, holding the model, inquiry, and answer strategy constant. Change the answer strategy, while holding the other aspects of the design constant. Graphical representation of how diagnosands change as designs change is an intuitive way to do this.

## Formal optimization

A somewhat more integrated approach is to generate a diagnosand that reflects your objective function (or loss function)---which may be a combination of performance and cost---and choose the design the performs best for this diagnosand. The steps under this approach involve:

* Declaring a model that reflects your *prior* beliefs over underlying processes: expectations over diagnosands will be taken with respect to these priors.
* Varying *D* and *A* over viable alternatives, recording the costs of different alternatives.
* Declaring a diagnosand that reflects ultimate objectives including performance and cost.

Consider for example a design in which we place a prior distribution over a treatment effect, $b$. We consider designs that vary (a) the number of units studied and (b) the investment in careful measurement. Each comes with a cost. We imagine that results will be used as follows: if a *significant* treatment effect of $b>c$ is estimated then an intervention is implemented which has value $b$ and cost $c$. Our diagnosand is the expected utility from  whatever policy is implemented after the study: $b-c$ if the policy is adopted, 0 otherwise. Thus it is responsive to different types of errors: implementing a weak intervention with $b<c$ and failing to implement a strong intervention with $b>c$.  Note of course that this policy decision rule is not *itself* optimal insofar as  it is based on significance not on posteriors, yet it may still be the rule used and we may still need to know how to undertake optimal design given this rule.

We illustrate with a simple design in which we explicitly provide a prior distribution over the  estimand and build in decision relevant parameters as arguments. 

```{r}

# Decision parameters

N <- 1000  # Number of cases
r <- 1     # Measurement error
c <- .2   # Cost of policy

# Design
design <- 
  declare_model(
    b = add_level(N = 1, b = rnorm(1,0,1)),
    i = add_level(N = N, X = rnorm(N), Y = 1+b*X + rnorm(N))) +
  declare_measurement(Y_seen = Y + rnorm(N, 0, r)) +
  declare_inquiry(b = b[1]) +
  declare_estimator(Y_seen ~ X, model = lm_robust) 
```

The key step then is to define a diagnosand that reflects choices, benefits, and costs and then assess performance over a range of designs.

```{r}
# Utility as a diagnosand

diagnosands <- declare_diagnosands(
  utility = mean((p.value <= 0.05)*(estimate > c)*(estimand - c) - N/100000+ r/250))
                                  
design <- set_diagnosands(design, diagnosands)

designs <- redesign(design, N = 100*2^(0:6), r = (0:8)/2)
```


Payoffs across two decision dimensions are shown in Figure \@ref(fig:optimaldesign).

<!-- utility <- declare_diagnosands( -->
<!--   utility_1 = mean((statistic > 1.96)*(estimate > c)*(estimand - c)), -->
<!--   utility_2 = mean((statistic > 1.96)*(estimate > c)*(estimand - c) - N/50000+ r/40), -->
<!--   power = mean(statistic > 1.96), -->
<!--   benefit_given_significant = mean(estimand[statistic > 1.96]) -->
<!-- ) -->

```{r, eval = do_diagnosis & !exists("do_bookdown")}
diagnosis <- diagnose_design(designs, sims = 20000)
```

```{r optimaldesign, echo = FALSE, purl = FALSE, eval = TRUE, fig.cap= "Utility over two design parameters."}
# figure out where the dropbox path is, create the directory if it doesn't exist, and name the RDS file
rds_file_path <- paste0(get_dropbox_path("optimal_design"), "/optimal_design.RDS")
if (do_diagnosis & !exists("do_bookdown")) {
  write_rds(diagnosis, path = rds_file_path)
}
optimal_design_diagnosis <- read_rds(rds_file_path)

# optimal_design_diagnosis$diagnosands_df %>% 
#   ggplot(aes(N, 2 - r, z = utility_2))  + geom_contour() + 
#   ylab("Investment in precision") + geom_contour_filled(show.legend = FALSE) + scale_x_continuous(trans = 'log10')

c = .2
optimal_design_diagnosis$simulations_df %>% group_by(N, r) %>% summarize(
  utility = mean((statistic > 1.96)*(estimate > c)*(estimand - c) - N/100000  + r/250)) %>% 
  ggplot(aes(N, 4 - r, z = utility)) + 
  ylab("Investment in precision") + geom_contour_filled(show.legend = FALSE) + scale_x_continuous(trans = 'log10')

# check <- optimal_design_diagnosis$simulations_df %>% 
#   group_by(N, r) %>% summarize(sig = mean(statistic>1.96),
#                                included = mean((statistic > 1.96)*(estimate > c)),
#                                est_inc = mean(estimate[statistic > 1.96]),
#                                utility_1 = mean((statistic > 1.96)*(estimate > c)*(estimand - c))) 
# 
# plot( check$N, check$sig, type = "n")
# text( check$N, check$sig, check$r)
# plot( check$r, check$sig, type = "n")
# text( check$r, check$sig, check$N)

# plot as a contour plot showing optimal design ; should be an ellipse
# df <- d$simulations_df %>% filter(N == 10000, r== 0)

```



<!-- ## Redesign Mountain -->

<!-- This figure shows a schematic of the redesign process. We vary design parameter 1 and design parameter 2 independently to learn the value of the diagnosand for each combination of design parameters. The redesign process can be in more than 2 dimensions of course. We suggest varying the design parameters you have control over first. -->

<!-- ```{r redesignmountain, fig.cap = "Redesign mountain.", echo=FALSE} -->
<!-- expand.grid(param_1 = seq(0, 1, length.out = 100), -->
<!--             param_2 = seq(0, 1, length.out = 100)) %>% -->
<!--   mutate(diagnosand = 1 - sqrt((param_1 - 0.3) ^ 2 + (param_2 - 0.7) ^ 2)) %>% -->
<!--   ggplot(aes(param_1, param_2)) +  -->
<!--   geom_tile(aes(fill = diagnosand)) + -->
<!--   geom_point(x = 0.3, y = 0.7, size = 2) + -->
<!--   geom_contour(aes(z = diagnosand)) + -->
<!--   scale_fill_gradient(low = "blue", high = "red") + -->
<!--   labs(x = "Design Parameter 1", -->
<!--        y = "Design Parameter 2", -->
<!--        title = "Diagnosands change as design parameters change") -->
<!-- ``` -->









<!-- ## Redesign to assess the robustness of designs to models  -->

<!-- - Hold inquiry constant! (read Richard Crump “Moving the Goalposts”) -->
<!-- - Hold three constant, vary one of MIDA at a time -->
<!-- - M: ICC, null model, alternative DAGs, heterogeneity -->
<!-- - I: -->






<!-- Diagnosis is like this: -->

<!-- (we need a math glyph for left relation composition notation) -->

<!-- $$ -->
<!-- MIDA |> \\ -->
<!--   simulate |> \\ -->
<!--   summarize -->
<!-- $$ -->

<!-- Redesign is like this -->

<!-- $$ -->
<!-- map(parameters) |> \\ -->
<!--   MIDA |>  \\ -->
<!--   simulate |> \\  -->
<!--   group\_by(parameters) |> \\ -->
<!--   summarize -->
<!-- $$ -->


