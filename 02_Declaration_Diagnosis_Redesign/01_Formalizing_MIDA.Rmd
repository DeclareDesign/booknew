---
title: "Formalizing MIDA"
output: html_document
bibliography: ../bib/book.bib 
---

<!-- note do_bookdown is set in index.rmd, so we know if you're running just this .Rmd or compiling the book-->
```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) # files are all relative to RStudio project home
```

```{r, eval = !exists("do_bookdown"), echo = FALSE, include = FALSE, purl = FALSE}
# load common packages, set ggplot ddtheme, etc.
source("scripts/before_chapter_script.R")
source("scripts/vayr.R")
```

<!-- start post here, do not edit above -->

# Formalizing MIDA

<!-- make sure to rename the section title below -->

```{r research_questions, echo = FALSE, output = FALSE, purl = FALSE}
# run the diagnosis (set to TRUE) on your computer for this section only before pushing to Github. no diagnosis will ever take place on github.
do_diagnosis <- FALSE
sims <- 100
b_sims <- 20
```

```{r, echo = FALSE}
# load packages for this section here. note many (DD, tidyverse) are already available, see scripts/package-list.R
```

In Chapter 2, we gave a high-level overview of our framework for describing research designs in terms of their models, inquiries, data strategies, and answer strategies, our process for diagnosing their properties, and a general purpose approach for improving them to better fit research tasks. Now in this chapter, we place our approach on firmal formal footing. To do so, we employ elements from Pearl's [-@Pearl2009] approach to causal modeling (directed acyclic graphs, or DAGs for short), which provides a syntax for mapping design inputs to design outputs. We also use the potential outcomes framework as presented, for example, in @Imbens2015, which many social scientists use to clarify their inferential targets. 

Our goal here is not formalization for formalization's sake. Describing a research design as a DAG helps us to see the fundamental symmetries across the theoretical (M and I) and empirical (D and A) halves of a research design. A recurring theme of our book is that research designs tend to be stronger when the relationship of M to I is mirrored by the relationship of D to A; the aim of this chapter is to make this somewhat mystical claim more concrete.

## Declaration

We define a research design as $\Delta$, with four elements $<M,I,D,A>$. Describing a research design entails "declaring" each of these four elements. 

$M$ is a theoretical **model** of how the world works. Following Pearl's definition of a probabilistic causal model, $M$ contains three core elements. The first is a specification of the variables $X$ about which research is being conducted. This includes endogenous and exogenous variables ($V$ and $U$ respectively) and the ranges of these variables. In the formal literature this is sometimes called the *signature* of a model [@halpern2000]. The second element ($F$) a specification of how each endogenous variable depends on other variables. These can be considered functional relations or, as in @Imbens2015, potential outcomes. The third and final element is a probability distribution over exogenous variables, written $P(U)$. Since the model is probabalistic, we can think of $m$ as a *draw* from the model $M$. 

The **inquiry** $I$ is a summary of the variables $X$, perhaps given interventions on some variables. An inquiry might be the average value of an outcome $Y$: $\E[Y] = \int{Y*Pr(Y)}$, or the average value of the outcome conditional on the value of a treatment $Z$: $\E[Y|Z=1] = \int{Y*Pr(Y|Z=1)}$. Using Pearl's notation we can distinguish between descriptive inquiries and causal inquiries. Causal inquiries are those that summarise distributions that would arise under interventions, as indicated by the $do()$ operator, e.g., $\Pr(Y | do(Z = 1))$. Descriptive inquiries summarise distributions that arise without intervention, such as $\Pr(Y | Z =1)$.

We let $a^m$ denote the answer to $I$ \textit{under the model}. Conditional on the model, $a^m$ is the value of the estimand, the quantity that the researcher wants to learn about. The connection of $a^m$ to the model can be seen in the following relationship: $a^m = I(m)$. 

As the saying goes, models are wrong but some may perhaps be useful. We denote the *true* causal model of the world as $W$ and the realized world $w$ as a draw from this true causal model. The true answer, then, is $a^w = I(w)$. The answer under the model $a^m$ may be close or far from the true value $a^w$, which is to say it could be wrong. If the model $M$ is far from the $W$, then of course $a^m$ need not be correct. We note that $a^w$ might be undefined, since inquiries can only be stated in terms of theoretical models. If the theoretical model is wrong enough, then the inquiry might be nonsensical when applied to the real world.

A **data** strategy, $D$, generates data $d$. Data $d$ arises, under model $M$ with probability $P_M(d|D)$. The data strategy includes sampling strategies and assignment strategies, which we denote with $P_S$ and $P_Z$ respectively. Measurement techniques are also a part of data strategies and can be thought of as a selection of observable variables that carry information about unobservable variables. The data strategy operates on $w$ to produce the observed data: $D(w) = d$.

While $M$ reflects our beliefs about true underlying causal processes at work where $D$ is the procedure that results in the collection or creation of data. In this light, the phrase "data generating process" is imprecise. Scholars usually use the phrase "true DGP" to refer to $M$, even though $M$ doesn't create data, $D$ does when it is applied to the real world.

The **answer** strategy, $A$ generates answer $a^d$ using data $d$. We encode this relationship as $A(d) = a^d$. 

The full set of causal relationships between $M$, $I$, $D$, and $A$ with respect to $m$, $a^m$, $d$, $a^d$, $w$, and $a^w$ can be seen in the DAG schematic representation of a research design.

```{r worldandtheory, echo=FALSE, fig.cap="MIDA as a DAG", fig.width = 6.5, fig.height = 3}
dag <-
  dagify(aw ~ w + I,
         m ~ M,
         am ~ m + I,
         d ~ D + w,
         ad ~ A + d,
         w ~ W)

dag_base <- tidy_dagitty(dag) %>%
  select(name, direction, to, circular) %>%
  as_tibble

nodes_df <-
  tibble(
    name = c("M", "I", "D", "A", "m", "am", "aw", "d", "ad", "w", "W"),
    label = c("M", "I", "D", "A", "m", "a<sup>m</sup>", "a<sup>w</sup>", "d", "a<sup>d</sup>", "w", "W"),
    long_label = c("Theoretical<br>causal model", "Inquiry", "Data<br>strategy", "Answer<br>strategy", "Model<br>draw", "Theoretical<br>answer", "True answer", "Realized<br>data", "Empirical<br>answer", "Real<br>world", "True<br>causal model"),
    lbl_direction = c("N", "N", "N", "N", "S", "S", "S", "S", "S", "S", "N"),
    x = c(1, 2, 5.5, 6.5, 1, 2, 4.25, 5.5, 6.5, 3.25, 3.25),
    y = c(3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3)
  )

endnodes_df <-
  nodes_df %>%
  transmute(to = name, xend = x, yend = y)

gg_df <-
  dag_base %>%
  left_join(nodes_df, by = "name") %>%
  left_join(endnodes_df, by = "to")

gg_df <-
  gg_df %>%
  mutate(arced1 = (name == "w" & to == "d"),
         arced2 = (name == "I" & to == "aw")) %>%
  arrange(name)

rect_df <-
  tibble(
    xmin = c(.4, 4.9),
    xmax = c(2.6, 7.1),
    ymin = c(1.15, 1.15),
    ymax = c(3.85, 3.85)
  )

g <-
  ggplot(data = filter(gg_df, !arced1 & !arced2), aes(
    x = x,
    y = y,
    xend = xend,
    yend = yend
  )) +
  geom_point(color = gray(.1), fill = NA, size = 15, stroke = 0.5, pch = 1) +
  geom_dag_edges(edge_width = 0.35) +
  geom_dag_edges_arc(data = filter(gg_df, arced1), curvature = -0.28, edge_width = 0.35) +
  geom_dag_edges_arc(data = filter(gg_df, arced2), curvature = .7, edge_width = 0.35) +
  geom_richtext(color = "black",
                parse = TRUE,
                aes(label = label),
                fill = NA,
                label.color = NA,
                label.padding = grid::unit(rep(0, 4), "pt"),
                size = 4) +
  geom_richtext(
    aes(y = y + if_else(lbl_direction == "N", 0.4, -0.4),
        vjust = if_else(lbl_direction == "N", "bottom", "top"),
        label = long_label),
    color = gray(0.5),
    parse = TRUE,
    fill = NA,
    label.color = NA,
    label.padding = grid::unit(rep(0, 4), "pt"),
    size = 4) +
  coord_fixed(ylim = c(1.15, 4)) + 
  geom_rect(data = rect_df, aes(x = NULL, y = NULL, 
                                xend = NULL, yend = NULL,
                                xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax),
            alpha = 0.15) +
  annotate("text", x = 1.5, y = 4.05, label = "Theory") +
  annotate("text", x = 6, y = 4.05, label = "Empirics") +
  annotate("text", x = 3.75, y = 4.05, label = "Reality") +
  # annotate("text", x = 3, y = 1.6, label = "Truth") +
  theme_dag()
g
```

Figure \@ref(fig:worldandtheory) illustrates  how a research design is a correspondence $I(m) = a^m$ and $A(d) = a^d$. The theoretical half of a research design produces an answer to the inquiry **in theory**. The emprical half of a research design produces an *empirical estimate* of the answer to the inquiry. 

Neither answer is necessarily close to the truth $a^w$, of course. And, as shown in the figure, the truth is not directly accessible either to us in theory or in empirics. Our gamble in empirical research however is that our theoretical models are close enough to the truth: that the truth is like the set of models we imagine in our model set. If indeed the truth can be thought of as one of the possible worlds in our model  set then we have a simpler set of relations between theory and empirics, as shown in Figure X.  

<!-- MH: Having some trouble understanding the distrinction between W and w. If w includes potential outcome functions, isn't it the true causal model? If W is the set of possible worlds, is that a subject of inquiry. For instance say our estimand is sd(ate) would that be a(W) rather than a(w). -->

<!-- Think it worth having a second figure which shows just two panels:if indeed w \in M, then: -->



```{r worldandtheory2, echo=FALSE, fig.cap="MIDA as a DAG", fig.width = 6.5, fig.height = 3}
dag <-
  dagify(aw ~ w + I,
         w ~ M,
         d ~ D + w,
         ad ~ A + d)

dag_base <- tidy_dagitty(dag) %>%
  select(name, direction, to, circular) %>%
  as_tibble

nodes_df <- nodes_df[-c(7, 10, 11),]
nodes_df[1, 3] <- c("Possible worlds") 
nodes_df[5, 1] <- c("w") 
nodes_df[5, 2] <- c("w") 
nodes_df[5, 3] <- c("World") 
nodes_df[6, 1] <- c("aw") 
nodes_df[6, 2] <- c("a<sup>w</sup>") 
nodes_df[6, 3] <- c("True answer") 

endnodes_df <-
  nodes_df %>%
  transmute(to = name, xend = x, yend = y)

gg_df <-
  dag_base %>%
  left_join(nodes_df, by = "name") %>%
  left_join(endnodes_df, by = "to")

gg_df <-
  gg_df %>%
  mutate(arced1 = (name == "w" & to == "d"),
         arced2 = (name == "I" & to == "aw")) %>%
  arrange(name)

rect_df <-
  tibble(
    xmin = c(.4, 4.9),
    xmax = c(2.6, 7.1),
    ymin = c(1.15, 1.15),
    ymax = c(3.85, 3.85)
  )

g <-
  ggplot(data = filter(gg_df, !arced1), aes(
    x = x,
    y = y,
    xend = xend,
    yend = yend
  )) +
  geom_point(color = gray(.1), fill = NA, size = 15, stroke = 0.5, pch = 1) +
  geom_dag_edges(edge_width = 0.35) +
  geom_dag_edges_arc(data = filter(gg_df, arced1), curvature = -0.28, edge_width = 0.35) +
#  geom_dag_edges_arc(data = filter(gg_df, arced2), curvature = .7, edge_width = 0.35) +
  geom_richtext(color = "black",
                parse = TRUE,
                aes(label = label),
                fill = NA,
                label.color = NA,
                label.padding = grid::unit(rep(0, 4), "pt"),
                size = 4) +
  geom_richtext(
    aes(y = y + if_else(lbl_direction == "N", 0.4, -0.4),
        vjust = if_else(lbl_direction == "N", "bottom", "top"),
        label = long_label),
    color = gray(0.5),
    parse = TRUE,
    fill = NA,
    label.color = NA,
    label.padding = grid::unit(rep(0, 4), "pt"),
    size = 4) +
  coord_fixed(ylim = c(1.15, 4)) + 
  geom_rect(data = rect_df, aes(x = NULL, y = NULL, 
                                xend = NULL, yend = NULL,
                                xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax),
            alpha = 0.15) +
  annotate("text", x = 1.5, y = 4.05, label = "Theory") +
  annotate("text", x = 6, y = 4.05, label = "Empirics") +
  # annotate("text", x = 3, y = 1.6, label = "Truth") +
  theme_dag()
g
```


| Relationship  | Description                                                                           |
| ------------- | ------------------------------------------------------------------------------------  |
| $m = M()$     | $m$ is a draw from the theoretical causal model $M$                                   |
| $a^m = I(m)$  | the answer under the model $a^m$ is the inquiry applied to a draw from the model $m$  |
| $w = W()$     | the real world $w$ is a draw from the true causal model $W$                           |
| $a^w = I(w)$  | the true answer $a^w$ is the inquiry applied to the real world $w$                    |
| $d = D(w)$    | the realized dataset $d$ results from the data strategy applied to the real world $w$ |
| $a^d = A(d)$  | the empirical answer $a^d$ is the answer strategy applied to the dataset $d$          |



The central crux of good research design is generated by the deep analogy of Models and Inquiries to Data and Answer Strategies. This analogy can guide many design decisions. The guidance that measurement procedures should be theoretically informed is an instance of the general point that data strategies should parallel theoretical models. The general preference for nonparametric estimation approaches over parametric procedures is an instance of the general point that answer strategies should parallel inquiries. 

$$
M : I : : D : A
$$

The DAG also clarifies the main difference between the theoretical and emprical halves of research design is the introduction of reality. When we draw from $M$ to produce $m$, we write $M() = m$ -- the function has no inputs from outside the model because it is purely theoretical. When we draw from the $D$ to produce $d$, we write $D(w) = d$ -- the dataset $d$ that results from the data strategy $D$ depends on the actual state of the world $w$. Empirical researchers must be theorists too: our goal is to choose our theoretical model $M$ such that $m$ is as close to $w$ as possible. 

Idea we want to express here: M:I::D:A, but M and D are different. How are they different? The first way is that M is theoretical and D is empirical -- D() has w as an argument. The second way is that there are two "fundamental" problems facing empirical research. The first is the fundamental problem of causal inference -- we can't observe units in counterfactual states. The second is the fundmental problem of descriptive inference -- we can't observe the latent Y*, only Y as measured.

## Diagnosis

Research designs are strong when the empirical answer $a^d$ generated by the design is close to the true answer $a^w$. Since we can never know $a^w$, we have to assess whether the distribution of $a^d$ over possible realizations of the research design is close to the distribution of $a^m$, the answer under the model. How can we assess how close the distributions are? The distribution of $a^m$ is easy to simulate. Given a theoretical model, we can easily ask a computer to generate the distribution of the estimand. More often than not, the distribution of $a^M$ is degenerate -- in a fixed population, for example, most theoretical models posit that estimands like the Average Treatment Effect have just one value.  

The distribution of $a^d$ is trickier to estimate. The *actual* research design will only be implemented once, so we don't get to see the distribution of actual answers that *could have* eventuated from the application of D and A to the world. To solve this problem, we make a small -- but extremely consequential -- substitution of $m$ for $w$ in the DAG of a research design. Swapping $m$ in for $w$, we can ask the computer to simulate the distribution of $a^d$ *conditional on the model*. If the model is wrong, the simulated distribution of $a^d$ will be wrong too -- as ever, garbage in, garbage out. Figure YY shows the DAG we use to simulate research designs. When we simulate designs, $d$ is affected by $m$ (a realization of a theoretical model), rather than by $w$. This makes sense, since the computer simulations can be entirely untethered from reality.

Design diagnosis is the process of simulating both $I(m) = a^m$ and $A(d) = a^d$ over many draws from $M()$ and $D(m)$, and comparing them. The specific comparisons we make are called "diagnostic statistics."

A "diagnostic statistic" is a function $g$ of $a^m$ and $a^d$. This function could be simple, like the difference between them: $g(a^m, a^d) = a^d - a^m$, or far more complex. If $a^m$ and $a^d$ are vector-valued (e.g., when answers provide a prediction for each observation), one might have a vector of diagnostic statistics, such as the vector of differences between each element of  $a^m$ and $a^d$, or a scalar valued summary, such as a multidimensional distance measure. In some cases $a^m$ and $a^d$ might have different dimensions; for instance $a^d$ may be a a pair of bounds and the diagnostic statistic is "Is $a^m$ within $a^d$?"

Because $a^m$ and $a^d$ are random variables, and any function of random variables is also a random variable, any diagnostic statisitic is also a random variable. A diagnosand is a summary of that random variable. Just like we summarize random variables with statistical functionals like the expectation and variance operators, a diagnosand is a statistical functional of the diagnostic statistic.

$$
\phi = f(g(a^m, a^d))
$$

The $f()$ is a statistical functional; statistical functionals summarize random variables. For example the expectation function $E[X]$ summarizes the random variable $X$ with its expectation, the mean. Diagnosands are summaries of diagnostic statics. We'll use the Greek letter $\phi$ to describe the idea of a diagnosand in general.

Let's back up a moment to work through a concrete example of a diagnosand. Consider the diagnosand "bias." Bias is the average difference between the estimand and the estimate. Under a model that has two potential outcomes, a treated potential outcome and an untreated potential outcome, the inquiry might be the average treatment effect (the difference in the two potential outcomes averaged over units in the population or sample, abbreviated as ATE). Under a single realization $m$ of the model $M$, the value of the ATE will be a particular number, which we call $a^m$. If our data strategy is simply to collect data on those who come to be treated versus those who don't (i.e., we do not use random assignment), and our answer strategy is difference-in-means, our answer $a^d$ could be systematically different from $a^m$. The diagnostic statistic is the error $a^d - a^m$; this error is a random variable because each draw of $m$ from $M$ is slightly different. The expectation of this random variable is $E[a^d - a^m]$, or the value of the bias diagnosand.

Statistical power is a common diagnosand. Like bias, it is also an expectation, this time of the diagnostic statistic $\mathbb{1}(p \leq 0.05)$, an indicator function that equals 1 if the $p$-value is below 0.05 and 0 otherwise. Power describes how frequently (under beliefs about the model) a research design would return a statistically significant result.

Some diagnosands can be calculated analytically, though most require Monte Carlo computer simulation. A main purpose of the `DeclareDesign` software package is making the simulation step easier.

In practice, it is important to diagnose a design under multiple possible $M$'s, given our fundamental uncertainty about the world $w$. We do not know the precise distributions of exogenous variables or the exact functional forms of potential outcomes (e.g., we do not know the true effect size). Diagnosis, therefore, should typically involve simulating the properties of a fixed set of inquiries, data strategies, and answer strategies under *multiple* likely models. A $D$ and $A$ for a given $I$ that provide good diagnosand values under multiple $M$'s can be said to be robust to multiple models. 

```{r, echo=FALSE, fig.cap="MIDA as a DAG", fig.width = 6.5, fig.height = 3}
dag <-
  dagify(aw ~ w + I,
         m ~ M,
         am ~ m + I,
         d ~ D + m,
         ad ~ A + d,
         w ~ W)

dag_base <- tidy_dagitty(dag) %>%
  select(name, direction, to, circular) %>%
  as_tibble

nodes_df <-
  tibble(
    name = c("M", "I", "D", "A", "m", "am", "aw", "d", "ad", "w", "W"),
    label = c("M", "I", "D", "A", "m", "a<sup>m</sup>", "a<sup>w</sup>", "d", "a<sup>d</sup>", "w", "W"),
    long_label = c("Theoretical<br>causal model", "Inquiry", "Data<br>strategy", "Answer<br>strategy", "Model<br>draw", "Theoretical<br>answer", "True answer", "Simulated<br>data", "Simulated<br>answer", "Real<br>world", "True<br>causal model"),
    lbl_direction = c("N", "N", "N", "N", "S", "S", "S", "S", "S", "S", "N"),
    x = c(1, 2, 5.5, 6.5, 1, 2, 4.25, 5.5, 6.5, 3.25, 3.25),
    y = c(3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3)
  )

endnodes_df <-
  nodes_df %>%
  transmute(to = name, xend = x, yend = y)

gg_df <-
  dag_base %>%
  left_join(nodes_df, by = "name") %>%
  left_join(endnodes_df, by = "to")

gg_df <-
  gg_df %>%
  mutate(arced1 = (name == "m" & to == "d"),
         arced2 = (name == "I" & to == "aw")) %>%
  arrange(name)

rect_df <-
  tibble(
    xmin = c(.4, 4.9),
    xmax = c(2.6, 7.1),
    ymin = c(1.15, 1.15),
    ymax = c(3.85, 3.85)
  )

g <-
  ggplot(data = filter(gg_df, !arced1 & !arced2), aes(
    x = x,
    y = y,
    xend = xend,
    yend = yend
  )) +
  geom_point(color = gray(.1), fill = NA, size = 15, stroke = 0.5, pch = 1) +
  geom_dag_edges(edge_width = 0.35) +
  geom_dag_edges_arc(data = filter(gg_df, arced1), curvature = -0.575, edge_width = 0.35) +
  geom_dag_edges_arc(data = filter(gg_df, arced2), curvature = .7, edge_width = 0.35) +
  geom_richtext(color = "black",
                parse = TRUE,
                aes(label = label),
                fill = NA,
                label.color = NA,
                label.padding = grid::unit(rep(0, 4), "pt"),
                size = 4) +
  geom_richtext(
    aes(y = y + if_else(lbl_direction == "N", 0.4, -0.4),
        vjust = if_else(lbl_direction == "N", "bottom", "top"),
        label = long_label),
    color = gray(0.5),
    parse = TRUE,
    fill = NA,
    label.color = NA,
    label.padding = grid::unit(rep(0, 4), "pt"),
    size = 4) +
  coord_fixed(ylim = c(0.5, 4)) + 
  geom_rect(data = rect_df, aes(x = NULL, y = NULL, 
                                xend = NULL, yend = NULL,
                                xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax),
            alpha = 0.15) +
  annotate("text", x = 1.5, y = 4.05, label = "Theory") +
  annotate("text", x = 6, y = 4.05, label = "Simulation") +
  annotate("text", x = 3.75, y = 4.05, label = "Reality") +
  # annotate("text", x = 3, y = 1.6, label = "Truth") +
  theme_dag()
g
```

## Redesign

Diagnosis is the process of learning the value of a diagnosand for a single design, often under multiple specifications of M to assess robustness to alternative models. Redesign is the process of *changing* parts of D and A in order to learn how the diagnosands change. The redesign process is complete when a research selects the best (or one of the best) D and A among the feasible set, as measured by the changing values of the diagnosand.

For example, we can compare how the distribution of errors changes if we use a different data strategy $D'$: $P_M(a^d - a^m|D')$ or a different answer strategy $A'$: $P_M(a^{d'} - a^m|D)$. We can also hold the data and answer strategies fixed and consider the distribution of errors under an alternative model $M'$: $P_M(a^{d} - a^{m'}|D)$.

Researchers typically wish to find the optimal design for their question subject to financial and logistical constraints. Even simple designs have infinite variations, so the search space must be limited. Typically, researchers will want to find the optimal values of a small set of easily-controlled design parameters. For example, a researcher might want to find the sample size $N$ and the number of treated units $N_t$ that minimize a design's bias subject to a fixed budget for the experiment in which data collection for each unit costs \$25 and treating one unit costs \$5. They solve the optimization problem:

\begin{equation*}
\begin{aligned}
& \underset{N, N_t}{\text{argmin}}
& & E_M(a^{d} - a^{m}|D_{N, N_t}) \\
& \text{s.t.}
& & 25 * N + 5 * N_t \leq 5000
\end{aligned}
\end{equation*}

However, optimizing a design for a single diagnosand may lead to poor design choices. If we redesign by optimizing for the power diagnosand, we are likely to find a design that is highly powered but highly biased --- it is targeting the wrong estimand. We can solve this by switching to the root mean-squared error as the diagnosand, which is a weighted combination of the bias and efficiency of the design. More generally, the choice of an optimal design requires the researcher to select a weighting of diagnosands. They must balance bias, efficiency, the risk of imprecise null results, the risk of getting the sign of an effect wrong, and other diagnosands given their research goals.

The full evaluation of a design --- declaration, diagnosis, and redesign --- depends on the assessment of one or more diagnosands, and comparing the diagnoses to what could be achieved under alternative designs.

## Example

In this section, we declare the @Gulzar2020 example in finer detail. The aim is to capture all of the analytically-relevant features of the design. In Chapter 2, we declared in words and in code a simplified version of the design, in order to communicate its key features. We will make this move again throughout the book: we will present simplified versions of canonical designs in the text, and also provide worked examples with the details of specific studies. Moving back and forth between the two will, we hope, enable you to both learn about the general principles and how to declare your own designs which always involve the fine details of research implementation.

We first describe the design in words and then declare in code:

The *model* for the study describes the set of units: citizens living in the two Pakistani districts the study took place in, Haripur and Abbottabad. The endogenous outcomes of interest are whether the citizen filed papers to run for office; whether the citizen was elected; and the Euclidean distance of the citizen's preferences from average citizen preferences. The outcomes are a function in their causal model of three treatments, which emphasize either the social or personal benefits to holding public office or do not encourage anyone to run for office at all. Their model would include an expected treatment effect magnitude for each treatment and their suppositions about how correlated outcomes are within villages that they study. 

The *inquiry* is the effect of an encouragement to run for office focused on prosocial motivations for officeholding (rather than encouragements that emphasized  personal benefits) on the rate of filing papers to run for office between people living in villages randomly assigned to receive.

The *data strategy* entailed three steps: (1) randomly sampling 192 villages from among all villages in Haripur and Abbottabad districts, and using a random walk procedure to select 48 citizens in each village to participate in the experiment; (2) randomly-assigning each village with equal probability to one of three conditions (neutral, social benefits, or personal benefits); and (3) collecting administrative data on who filed papers to run and matching that back to pretreatment survey data on the 9,216 citizens. Sampling, treatment assignment, and measurement are the three common data strategy steps in an experiment; some experiments, instead, do not include a sampling step and instead assign treatments within a convenience sample. 

@Gulzar2020 have a two-step *answer strategy*, fitting a linear model predicting whether a citizen ran for office (the outcome) with indicators for the social benefits and the personal benefits treatments, and then calculating the difference between the two coefficients as an estimate of whether social benefits are more or less effective than the personal benefits. Their answer strategy includes presenting a table with estimated difference, a standard error clustered on village to account for village-level random assignment, and a *p*-value calculated using permutation inference.

```{r}
gulzar_khan_design <- 
  
  declare_population(
    # study is conducted in two districts in Pakistan, with 311 and 359 villages in them
    districts = add_level(N = 2, name = c("Haripur", "Abbottabad"), N_villages = c(311, 359)),
    
    # villages nested within districts
    villages = add_level(N = N_villages),
    
    # avg. 6500 citizens per village
    citizens = add_level(N = 6500)
  ) +
  
  # main outcome is whether a citizen filed papers to run for office
  # we define potential outcomes in response to being assigned to a social, personal, or neutral appeal to run
  declare_potential_outcomes(
    filed_papers ~ rbinom(N, 1, prob = 0.05 + 0.05 * (Z_appeal == "social") + 0.01 * (Z_appeal == "personal")),
    assignment_variable = Z_appeal, conditions = c("neutral", "social", "personal")
  ) + 
  
  # inquiry is the difference in rates of filing papers between the social and personal appeal conditions
  declare_estimand(ATE = mean(filed_papers_Z_appeal_social - filed_papers_Z_appeal_personal)) + 
  
  # sample 192 villages
  declare_sampling(clusters = villages, n = 192, sampling_variable = "S_villages") + 
  
  # sample 48 citizens in each village via random walk
  declare_sampling(strata = villages, n = 48, sampling_variable = "S_citizens") + 
  
  # assign villages to three arms with equal probabilities for three types of appeals to run for office
  declare_assignment(
    m_each = c(48, 72, 72),
    clusters = villages,
    conditions = c("neutral", "social", "personal"),
    assignment_variable = Z_appeal
  ) + 
  
  # recode treatment assignment for analysis into indicators for the two conditions of interest
  declare_step(
    Z_social_village = if_else(Z_appeal == "social", 1, 0),
    Z_personal_village = if_else(Z_appeal == "personal", 1, 0),
    handler = mutate
  ) +
  
  # 1. run a linear regression with condition indicators
  # 2. calculate the difference in effects between people in villages assigned to social appeals compared
  #    to those assigned to personal appeals
  # 3. calculate robust standard errors clustered on village
  declare_estimator(
    filed_papers ~ Z_social_village + Z_personal_village, 
    linear_hypothesis = "Z_social_village - Z_personal_village = 0",
    term = "Z_social_village - Z_personal_village = 0",
    clusters = villages,
    model = lh_robust
  )
```

```{r,echo=FALSE}
design <-
  declare_population(N = 100,
                     X = rbinom(N, 1, 0.3),
                     U = rnorm(N)) +
  declare_potential_outcomes(Y ~ Z + X + U) +
  declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0)) +
  declare_sampling(strata = X, prob = 1) + 
  declare_assignment(blocks = X, block_prob = c(0.1, 0.5)) +
  declare_estimator(Y ~ Z, estimand = "ATE", label = "Naive DIM") +
  declare_estimator(Y ~ Z,
                    blocks = X,
                    estimand = "ATE",
                    label = "Blocked DIM")

dag <- dagify(
  Y ~ Z + X + U,
  Z ~ X + S,
  S ~ X
)

nodes <-
  tibble(
    name = c("Y", "S", "Z", "U", "X"),
    label = c("Y", "S", "Z", "U", "X"),
    annotation = c(
      "**Outcome**<br>",
      "**Sampling**",
      "**Random assignment**<br>",
      "**Unknown heterogeneity**",
      "**villages**<br>Stratification, cluster assignment"),
    x = c(5, 1, 3, 5, 1),
    y = c(2.5, 2.5, 2.5, 4, 4), 
    nudge_direction = c("S", "S", "S", "N", "N"),
    answer_strategy = "uncontrolled"
  )
ggdd_df <- make_dag_df(dag, nodes, design)

base_dag_plot %+% ggdd_df
```


<!-- ## Brief illustration -->

<!-- To illustrate, consider the following design.  -->

<!-- * A model *M* specifies three variables $X$, $Y$ and $Z$ (all defined on the reals). These form the signature. In additional we assume functional relationships between them that allow for the possibility of confounding (for example, $Y =  bX + Z + \epsilon_Y; X = Z+ \epsilon_X$, with $Z, \epsilon_X, \epsilon_Z$ distributed standard normal).  -->

<!-- * The inquiry $I$ is ``what would be the average effect of a unit increase in $X$ on $Y$ in the population?'' Note that this question depends on the signature of the model, but not the functional equations of the model (the answer provided by the model does of course depend on the functional equations).  -->

<!-- * Under data strategy, $D$,  data is gathered on $X$ and $Y$ for $n$ randomly selected units.  -->

<!-- * Answer $a^d$, is then generated using ordinary least squares as the answer strategy, $A$. -->

<!-- We have specified all the components of MIDA. We now ask: How strong is this research design? One way to answer this question is with respect to the diagnosand "expected error." Here the model's functional equations provide an answer, $a^m$ to the inquiry (for any draw of $\beta$), and so the distribution of the expected error, *given the model*, $a^d-a^m$, can be calculated. In this example the expected performance of the design may be poor, as measured by this diagnosand, because the data and analysis strategy do not handle the confounding described by the model. -->

<!-- In comparison, better performance may be achieved through an alternative data strategy (e.g., where $D'$ randomly assigned $X$ to $n$ units before recording $X$ and $Y$) or an alternative analysis strategy (e.g., $A'$  conditions on $Z$). These design evaluations depend on the model, and so one might reasonably ask how performance would look were the model different (for example if the underlying process involved nonlinearities). -->













