---
title: "Research Design: Declare, Diagnose, Redesign"
output: 
  html_document:
    number_sections: true
---

Exercises in every chapter.

Second reader assigned to all of them.

# Introduction to Design Declaration and Diagnosis

This section is only in words, not in code (until the software primer)

## Preamble (high level overview)
- How to use this book
- Using code snippets (copy paste, download, etc.)

## MIDA 
- Explain MIDA [AC]
- Diagnosands and diagnosis [AC]
- Explain declaration -> diagnosis -> redesign process (graphic?) [GB]

## Putting designs to use

- Why should you adopt MIDA
- Publication bias (journal of null results)

### Before 

- PAPs, Registered Reports, SOPs
- Funders: Evaluating and supporting research [MH]

### After 

- Reconciliation

- Replication
  - Peers: Better scholarly critique [MH]

- Combining designs 
  - Job market papers with multiple studies / three paper paradigm in psych (is it one design      targeting same inquiry?) [JC 1p]
  - Multi-site studies -- take a design from another study and use it for another one
  - Knowledge accumulation
  
- A library of designs-as-objects [JC 1/2p]


## Software primer [GB]

- How to read declarations (if you don't know)
- Declare functions are function factories (with handlers)
  - Map of data in and data out (how the three data frames are passed along, Neal's graph; graphic of one run)
- Diagnosis is chaining functions together

# A Guide to Declaration and Diagnosis in Practice

NOTE: an R script for each subsection. For code snippets, two buttons: copy/paste and download full code for that subsection

## Research questions

### What is your research question [AC] 

- What is your research question? 
- What is the relationship between theory and estimands?
- Topics vs. questions; how to come up with a question - lit review, hunch, data
- research questions often begin with a bivariate "idea" (an x and a y for causal qs, or how does y vary according to x for descriptive)
- Do you want to answer a causal or descriptive question? [MH]
- Typology of questions: COE vs EOC for causal; questions about one of the modes of the distribution of a variable; [MH]
- What is your inquiry if you donâ€™t know the question in advance? (Discovery)
- The fact that these steps go in different orders in different projects / are sometimes missing. One research "project" often has many designs (by our definition). E.g. your JMP might have a series of inquiries and get at them through field exp, observational, qual, and survey exp. How to approach those situations from MIDA_perspective? [JC]

### Is your research question good? [GB]
- Is the answer to your question known? How would you know? Meta-analysis, systematic review, disagreement in literature
- Is your question important? (Change decisions; change answer in a literature)
- Important question answered poorly / Irrelevant question answered well. We want important questions answered well.

NOTE: for next four sections, four subsections: 1) walk through of how you specify this step, using a probit exposure variable with binary outcome (primary); 2) common choices you have to make about this step; 3) illustrations how these choices have to make for other designs; 4) anticipating cross-MIDA problems from this step. LOTS of pointers to the design library.

## Specifying the model [JC 10p]

- Introduce DAGs and potential outcomes
- What is your DAG? How do you write a DAG? What must be in it? What will be missing from it (heterogeneity!)? [MH]
- What parts of background variables must be specified, and how can you specify them (existing data, substantive bounds on variables)
- Isn't defining the effect size for potential outcomes begging the question?
- Pilot studies
- Spillovers are potential outcomes
- Compliance is a potential outcome
- Attrition is a potential outcome 
- Measurements are potential outcomes

## Defining the Inquiry [MH]
- Descriptive vs causal
- When would a null finding be interesting? [MH]
- Is the answer defined? (include not only missingness, but ATE for categorical outcomes) [MH]
- What is the set of units which you want to learn the answer about? (since we can't learn about an individual unit) [MH]
- Know what ATE averages over
- Know what is local in a LATE
- Know when SATEs are informative for PATEs
- Choice of inquiry is not valueless
- Make your models vulnerable (something like falsification)
- Relative value of killing bad ideas vs. giving life to good ones
- The many flavors of external validity
- Inquiries for discovery
- Complex counterfactuals
- Expected ATT vs realized ATT
- Vector-valued inquiries (set of predictions for N units), i.e. the CEF 
- Models as estimands

## Crafting a Data Strategy [AC]

### Sampling

Three sampling strategies and how you select (simple, stratified, clustered, nonrandom)

### Assignment

Three assignment strategies and how you select among them (complete, blocked, clustered)

### Measurement

- Studying many questions can be costly

### Ethics



## Choosing an Answer Strategy [GB]

- Analyze as you randomize when you can (not just for experiments!)
- Reducing variance via estimation choices
- Answer strategy procedures (if this then that). Deal with fishing.
- The role of robustness checks
- PAPs as answer strategies (writing, reconciliation)
- Multiple comparisons 
- Missing data (outcomes, covariates)

## Diagnosis

### Diagnosing a single design (single design, single diagnosand)
- Definition and practical details of Monte Carlo and diagnosands (and discussion of formulae) [JC]
- Graphic of simulations (of multiple runs)

### How do you select diagnosands? [JC 8p]

- Diagnose given the purposes of the study
- Single shot vs repeated designs (MSE vs bias)
- Moral questions (Type 1 vs Type 2 errors)
- Power for biased designs
- Standard diagnosands (paragraph on each of the diagnosands in our defaults)
- Ways of getting answers to a question wrong
- Diagnosing inferential statistics (SE bias vs. coverage, error rates for ps, Bayes?)
- How to select diagnosands (some sort of decision tree?)
- Multiple estimates / inquiries [JC]
- How to think about uncertainty about model parameters (multiple designs?)
- Diagnosands that are a function of "multiple designs" like MDE
- Conditional diagnosands different?
- Uncertainty of *diagnosands* (bootstrapping etc.)

### Diagnosis to assess the robustness of designs to models [GB]

- Hold inquiry constant! (read Richard Crump "Moving the Goalposts")
- Hold three constant, vary one of MIDA at a time
- M: ICC, null model, alternative DAGs, heterogeneity
- I: 

## Redesign [MH]

- D: Sample size, fraction randomly assigned, parameters of the randomized response, coding choices (e.g., binary vs. continuous, index vs. multiple, etc.)
- A: Possible estimators, priors(???)
- Conduct subject to constraints: budget, ethical, logistical
- Then recurse through diagnosis as above (including varying M)

- Resist the urge to change M and I to suit a diagnosis -- focus on D and A
- Sometimes, you might need to collect new information in order to make a declaration specific enough.
- Sometimes, designs have to be abandoned

# Design Library

NOTE: include links to declared designs for each study

## Observational designs for descriptive inference
- Random sampling (simple and stratified clustered) [AC]
- Multilevel regression and poststratification [GB]
- Inference about unobserved variables (systematic and nonsystematic measurement error) [AC]
- Structural estimation

- x Topic analysis [GB]
- x Respondent driven sampling 
- x Forecasting (predicting in future) [JC]

## Experimental designs for descriptive inference
- Audit experiments [AC]
- Experiments for sensitive questions [GB]
- Conjoint experiments [AC]
- Experimental games (controlling setting not a treatment) [MH]
	
## Experimental designs for causal inference
- Two-arm and blocks and clusters [MH]
- Multi-arm designs and factorial designs [MH]
- Encouragement designs [JC]
- Stepped-wedge designs [JC]
- Partial population design for spillover analysis [JC]

- x Parallel design for mediation effects [GB]
- x Crossover designs
- x Selective trials [JC]
- x Adaptive trials [AC]
- x Multi-site designs (could be descriptive too; hard part is I) [GB]
- x Experiments where blocks and clusters are also sampled from populations of blocks/clusters [JC]

## Observational designs for causal inference
- Selection on observables (matching and regression etc.) [AC]
- Instrumental variables for continuous instruments
- Difference in differences [GB]
- Regression discontinuity designs [JC]
- Process tracing [JC]
- Synthetic control [GB]

- x Cross national time series [JC]
- x Mediation [GB]
- x Nested designs [MH]
- x Qualitative comparative analysis [MH]
- x Meta-analysis [GB] 

# Conclusion



# Designs brainstorm

- geographic RD
- spatial processes
- IRT
- country-year FE
- EITM
- computational evolutional models
- mapping social networks
- an ANES study
- interrupted time series
- time series (uninterrupted)
- selection on observables
- "is this a good predictor"
- inquiries about "regimes" (i.e., vote share at the precinct level.)  Related to a problem I (ac) have about persistence
- need to but the handbook of lab experiments
- weighting to get PATEs and SATEs under non-compliance and non-response when you have a baseline

