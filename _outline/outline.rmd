---
title: "Research Design: Declaration, Diagnosis, Redesign"
output: 
  html_document:
    number_sections: true
---


# Part A: Introduction to Design Declaration and Diagnosis

This section is only in words, not in code (until the software primer)

- Preamble
- Explain MIDA
- Diagnosands and diagnosis
- Software primer

# Part B: A Guide to Declaration and Diagnosis in Practice

- What is your research question?
- Topics vs. questions; how to come up with a question - lit review, hunch, data
- Do you want to answer a causal or descriptive question?
- Types of questions: COE vs EOC for causal; questions about one of the modes of the distribution of a variable
- What is your inquiry if you donâ€™t know the question in advance? (Discovery)
- Is the answer to your question known? How would you know? 
- Meta-analysis, systematic review, disagreement in literature
- Is your question important? (Change decisions; change answer in a literature)

## How to select M
- Introduce DAGs and potential outcomes
- What is your DAG? How do you write a DAG? What must be in it? What will be missing from it (heterogeneity!)?
- What parts of background variables must be specified, and how can you specify them (existing data, substantive bounds on variables)
- Isn't defining the effect size for potential outcomes begging the question?
- Pilot studies
- Spillovers are potential outcomes
- Compliance is a potential outcome
- Attrition is a potential outcome
- Measurements are potential outcomes

## How to select I
- When would a null finding be interesting?
- Is the answer defined?
- What is the set of units which you want to learn the answer about? (since we can't learn about an individual unit)
- Know what ATE averages over
- Know what is local in a LATE
- Know when SATEs are informative for PATEs
- Choice of inquiry is not valueless
- Make your models vulnerable (something like falsification)
- Relative value of killing bad ideas vs. giving life to good ones

## How to select D
- Need to figure out how to say, how do you go about *selecting* D
- Reducing variance via sampling and assignment procedures
- There is no causation without manipulation
- Studying many questions can be costly

## How to select A
- Analyze as you randomize when you can (not just for experiments!)
- Reducing variance via estimation choices
- Answer strategy procedures (if this then that). Deal with fishing.
- The role of robustness checks
- PAPs as answer strategies (writing, reconciliation)

# Part C: Diagnosis

- Definition and practical details of Monte Carlo and diagnosands (and discussion of formulae)
- How to select diagnosands

## What should you diagnose your design against?
- Hold inquiry constant!
- Compare performance of the design according to choices the researcher makes
    - D: Sample size, fraction randomly assigned, parameters of the randomized response
    - A: Possible estimators, priors(???)
- And against the researcher's beliefs
    - M: ICC, null model, alternative DAGs, heterogeneity

- Conduct subject to constraints: budget, ethical, logistical

## How to diagnose, iterate, and improve (diagnosis as a process)

# Part D: Design Library

## Observational designs for descriptive inference
- Simple random sampling
- Stratified clustered random sampling
- Measuring latent variables
- Topic analysis
- Respondent driven sampling
- Multilevel regression and poststratification
- Meta-analysis
	
## Experimental designs for descriptive inference
- Audit experiments
- Experiments for sensitive questions
- Conjoint experiments
- Experimental games
	
## Experimental designs for causal inference
- Multiarm designs
- Experiments with blocks and clusters
- Experiments where blocks and clusters are also sampled from populations of blocks/clusters
- Factorial designs
- Encouragement designs 
- Stepped-wedge designs
- Crossover designs
- Parallel design for mediation effects
- Partial population design for spillover analysis
- Selective trials
- Adaptive trials 
- Multi-site designs (could be descriptive too; hard part is I)

## Observational designs for causal inference
- Difference in differences
- Matching
- Regression discontinuity designs
- Synthetic control
- Process tracing
- Nested designs
- Qualitative comparative analysis
- Cross national time series

# Part E: Design Criticism

- Peers: Better scholarly critique
- Funders: Evaluating and supporting research


# IDEAs we don't know if to cut or integrate or what

- How to designing multi-study projects and research agendas
- Publication bias (journal of null results)
- Knowledge accumulation
- Post-hoc learning about designs
