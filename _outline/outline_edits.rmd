---
title: "Research Design: Declaration, Diagnosis, Redesign"
output: 
  html_document:
    number_sections: true
---


# Part A: Introduction to Design Declaration and Diagnosis

This section is only in words, not in code (until the software primer)

- Preamble
- Explain MIDA [AC]
- Diagnosands and diagnosis [AC]
- Explain declaration -> diagnosis -> redesign process (graphic?) [GB]
- How to use this book
- Software primer [GB]

# Part B: A Guide to Declaration

## Starting a new research project [**Do we need this section?** -- JC]

- What is your research question?  
- Topics vs. questions; how to come up with a question - lit review, hunch, data
- research questions often begin with a bivariate "idea" (an x and a y for causal qs, or how does y vary according to x for descriptive)

## Kinds of research projects that there are [AC] [JC]
- Do you want to answer a causal or descriptive question? [MH]
- Typology of questions: COE vs EOC for causal; questions about one of the modes of the distribution of a variable; [MH]
- What is your inquiry if you donâ€™t know the question in advance? (Discovery)


## Is your research question good? [GB]
- Is the answer to your question known? How would you know? Meta-analysis, systematic review, disagreement in literature
- Is your question important? (Change decisions; change answer in a literature)
- Important question answered poorly / Irrelevant question answered well. We want important questions answered well.

## Specifying the model [JC]
- Introduce DAGs and potential outcomes
- What is your DAG? How do you write a DAG? What must be in it? What will be missing from it (heterogeneity!)? [MH]
- What parts of background variables must be specified, and how can you specify them (existing data, substantive bounds on variables)
- Isn't defining the effect size for potential outcomes begging the question?
- Pilot studies
- Spillovers are potential outcomes
- Compliance is a potential outcome
- Attrition is a potential outcome
- Measurements are potential outcomes

## Defining the Inquiry
- When would a null finding be interesting? [MH]
- Is the answer defined? (include not only missingness, but ATE for categorical outcomes) [MH]
- What is the set of units which you want to learn the answer about? (since we can't learn about an individual unit) [MH]
- Know what ATE averages over
- Know what is local in a LATE
- Know when SATEs are informative for PATEs
- Choice of inquiry is not valueless
- Make your models vulnerable (something like falsification)
- Relative value of killing bad ideas vs. giving life to good ones

## Crafting a Data Strategy [AC]
- Need to figure out how to say, how do you go about *selecting* D
- Reducing variance via sampling and assignment procedures
- Reducing bias via sampling and assignment procedures
- There is no causation without manipulation
- Studying many questions can be costly
- The ethical questions surrounding a design often come up in the data strategy -- who will be treated and measured, and how. Includes also question of identifying through dataset merges.

## Choosing an Answer Strategy [JC]
- Analyze as you randomize when you can (not just for experiments!)
- Reducing variance via estimation choices
- Answer strategy procedures (if this then that). Deal with fishing.
- The role of robustness checks
- PAPs as answer strategies (writing, reconciliation)

[**Do we need a "putting it all together" section? Preview the fact that these steps go in different orders in different projects / are sometimes missing. One research "project" often has many designs (by our definition). E.g. your JMP might have a series of inquiries and get at them through field exp, observational, qual, and survey exp. How to approach those situations from MIDA_perspective?** -- JC]


# Part C: A Guide to Diagnosis

- Definition and practical details of Monte Carlo and diagnosands (and discussion of formulae) [JC]

## Kinds of diagnosands that there are [JC]
- Beyond power (MDE, bias, coverage)
- Ways of getting answers to a question wrong
- Diagnosing inferential statistics (SE bias vs. coverage, error rates for ps, Bayes?)
- How to select diagnosands (some sort of decision tree?)

## What should you diagnose your design against? [GB]
- Hold inquiry constant!
- Compare performance of the design according to choices the researcher makes
    - D: Sample size, fraction randomly assigned, parameters of the randomized response, coding choices (e.g., binary vs. continuous, index vs. multiple, etc.)
    - A: Possible estimators, priors(???)
- And against the researcher's beliefs
    - M: ICC, null model, alternative DAGs, heterogeneity

- Conduct subject to constraints: budget, ethical, logistical

## Redesign [MH]

- When the diagnosis says the design is weak, improve it!
- Resist the urge to change M and I to suit a diagnosis -- focus on D and A
- Sometimes, you might need to collect new information in order to make a declaration specific enough.
- Sometimes, designs have to be abandoned

# Part D: Putting Design Declaration to Work

## Distributed Research

## Registration

## Library

## Design Criticism

- Peers: Better scholarly critique [MH]
- Funders: Evaluating and supporting research [MH]


# Part E: Design Library

## Observational designs for descriptive inference
- Simple random sampling [AC]
- Stratified clustered random sampling
- Measuring latent variables [AC]
- Topic analysis [GB]
- Respondent driven sampling
- Multilevel regression and poststratification [GB]
- Meta-analysis [GB] [JC]
	
## Experimental designs for descriptive inference
- Audit experiments [AC]
- Experiments for sensitive questions [GB]
- Conjoint experiments [AC]
- Experimental games [MH]
	
## Experimental designs for causal inference
- Multiarm designs [MH]
- Experiments with blocks and clusters [MH]
- Experiments where blocks and clusters are also sampled from populations of blocks/clusters [JC]
- Factorial designs [MH]
- Encouragement designs [JC]
- Stepped-wedge designs [JC]
- Crossover designs 
- Parallel design for mediation effects [GB]
- Partial population design for spillover analysis [JC]
- Selective trials [JC]
- Adaptive trials [AC]
- Multi-site designs (could be descriptive too; hard part is I) [GB]

## Observational designs for causal inference
- Difference in differences [GB]
- Matching [AC]
- Regression discontinuity designs [JC]
- Synthetic control [GB]
- Process tracing [JC]
- Nested designs [MH]
- Qualitative comparative analysis [MH]
- Cross national time series

# IDEAs we don't know if to cut or integrate or what

- How to designing multi-study projects and research agendas
- Publication bias (journal of null results)
- Knowledge accumulation
- Post-hoc learning about designs
- where does "prediction" belong. When an election forcaster comes up with a model, the inquiry is "what's gonna happen," not "what is the truth about the units I have"
- is "discovery" a separate class of designs, or can you have "observational designs for causal inferences that are about discovery"
- PAPs, Registered Reports, SOPs
- Multiple comparisons 
- Missing data 



# Designs brainstorm:

- geographic RD
- spatial processes
- IRT
- country-year FE
- EITM
- computational evolutional models
- mapping social networks
- an ANES study
- interrupted time series
- time series (uninterrupted)
- selection on observables
- mediation
- structural estimation
- "is this a good predictor"
- inquiries about "regimes" (i.e., vote share at the precinct level.)  Related to a problem I (ac) have about persistence
- need to but the handbook of lab experiments
- weighting to get PATEs and SATEs under non-compliance and non-response when you have a baseline


